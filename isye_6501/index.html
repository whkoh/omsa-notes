<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-02-19 Sun 19:23 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ISYE 6501 Intro to Analytics Modeling Notes</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ISYE 6501 Intro to Analytics Modeling Notes</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org901412c">1. Module 01: Intro</a>
<ul>
<li><a href="#org6feb49f">1.1. What's analytics?</a></li>
<li><a href="#org7c69a71">1.2. Modeling</a></li>
<li><a href="#orgcd3844a">1.3. Course structure</a></li>
<li><a href="#org0776684">1.4. Three different things are all models</a></li>
<li><a href="#orgf01b214">1.5. Hence these are all "models":</a></li>
</ul>
</li>
<li><a href="#orgf6a3cf9">2. Module 02: Classification</a>
<ul>
<li><a href="#org5db835b">2.1. M1L1: Intro to classification</a></li>
<li><a href="#org4add716">2.2. M1L2: Choosing a Classifier</a>
<ul>
<li><a href="#org43ab3e4">2.2.1. Example: Loan payment (Income vs credit score)</a></li>
</ul>
</li>
<li><a href="#org13e507d">2.3. M2L3 Data definitions</a>
<ul>
<li><a href="#org000dda1">2.3.1. Data terminology</a></li>
<li><a href="#org0ccd748">2.3.2. Data types</a></li>
</ul>
</li>
<li><a href="#org8b762f9">2.4. M2L4: Support vector machines</a>
<ul>
<li><a href="#org809cf3f">2.4.1. When not possible to get full separation</a></li>
</ul>
</li>
<li><a href="#org1f8a70f">2.5. M2L5: What SVM means</a></li>
<li><a href="#org98dfb5e">2.6. M2L6: Advanced SVM</a></li>
<li><a href="#org503d2f6">2.7. M2L7: Scaling and standardization</a>
<ul>
<li><a href="#org057350d">2.7.1. Scaling data</a></li>
<li><a href="#orgb6dec82">2.7.2. Standardization of data</a></li>
<li><a href="#org469dc4d">2.7.3. Choosing between scaling vs standardization</a></li>
</ul>
</li>
<li><a href="#org9c5a773">2.8. M2L8: K Nearest Neighbour model (KNN)</a></li>
</ul>
</li>
<li><a href="#org82ed53c">3. Module 03: Validation</a>
<ul>
<li><a href="#orgd55d0c4">3.1. M3L1: Training, validation and test data</a></li>
<li><a href="#org9a741df">3.2. M3L2: Splitting data</a></li>
<li><a href="#orgc6a9b57">3.3. M3L3: Cross-validation</a></li>
<li><a href="#orga84e253">3.4. M3L4: Summary</a></li>
</ul>
</li>
<li><a href="#org34b0e34">4. Module 04: Clustering</a>
<ul>
<li><a href="#org3277d39">4.1. M4L1: Introduction to clustering</a></li>
<li><a href="#org1507f23">4.2. M4L2: Distance Norms</a></li>
<li><a href="#org11717b1">4.3. M4L3: K-Means Clustering</a></li>
<li><a href="#org45f376d">4.4. M4L4: Practical details for K-Means</a></li>
<li><a href="#org1622059">4.5. M4L5: Clustering for prediction</a></li>
<li><a href="#org74cfb91">4.6. M4L6: Clustering vs Classification</a></li>
</ul>
</li>
<li><a href="#org84c12d3">5. Module 05: Data preparation</a>
<ul>
<li><a href="#org5975a56">5.1. M5L1: Common techniques and problems</a></li>
<li><a href="#orgaf3d681">5.2. M5L2: Outliers</a></li>
<li><a href="#org3b5a8db">5.3. M5L3: What to do with outliers?</a>
<ul>
<li><a href="#org3ca2f13">5.3.1. Bad data</a></li>
<li><a href="#org1cea494">5.3.2. Real / correct data</a></li>
<li><a href="#org0db3403">5.3.3. Another way to handle outliers</a></li>
<li><a href="#orgc21030c">5.3.4. Summary</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org12723d9">6. Module 06: Change detection</a>
<ul>
<li><a href="#org12705d6">6.1. M6L1: Examples</a></li>
<li><a href="#orge44229d">6.2. M6L2: Cumulative sum for change detection</a>
<ul>
<li><a href="#org54dd000">6.2.1. Interpretation</a></li>
</ul>
</li>
<li><a href="#org447c85d">6.3. M6L3: Ethics: Honestly reporting our results</a></li>
</ul>
</li>
<li><a href="#org24c9993">7. Module 07: Time series</a>
<ul>
<li><a href="#org353f5a3">7.1. M7L1: Introduction to exponential smoothing</a>
<ul>
<li><a href="#org84a7cfa">7.1.1. Random variation</a></li>
<li><a href="#org2eea417">7.1.2. Definitions:</a></li>
<li><a href="#org9880fc7">7.1.3. Exponential smoothing method</a></li>
</ul>
</li>
<li><a href="#org61ad1bf">7.2. M7L2: Trend and cyclic effects</a>
<ul>
<li><a href="#org0563937">7.2.1. Trends</a></li>
<li><a href="#orgecfbc8a">7.2.2. Cyclical patterns</a></li>
<li><a href="#org98d390c">7.2.3. Summary</a></li>
</ul>
</li>
<li><a href="#org745a9a4">7.3. M7L3: Etymology (what the name means)</a>
<ul>
<li><a href="#orgca4f534">7.3.1. Summary</a></li>
</ul>
</li>
<li><a href="#org881611f">7.4. M7L4: Forecasting</a></li>
<li><a href="#org92cc688">7.5. M3L5: ARIMA</a>
<ul>
<li><a href="#orge5dbc15">7.5.1. (I): Differences</a></li>
<li><a href="#orge22cf5f">7.5.2. (II): Autogression</a></li>
<li><a href="#org166f3fd">7.5.3. (III): Moving Average</a></li>
<li><a href="#org42f824b">7.5.4. ARIMA model</a></li>
</ul>
</li>
<li><a href="#orgbd3a47a">7.6. M7L6: GARCH</a>
<ul>
<li><a href="#org52f1f08">7.6.1. Variance</a></li>
<li><a href="#org9598887">7.6.2. GARCH</a></li>
<li><a href="#orgbcb9076">7.6.3. Summary - three models for time series analysis</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgcab1977">8. Module 08: Regression</a>
<ul>
<li><a href="#org4542431">8.1. M8L1: Intro to Regression</a>
<ul>
<li><a href="#org44bf7d1">8.1.1. What questions can regression answer?</a></li>
<li><a href="#org6fefb94">8.1.2. Simple linear regression</a></li>
</ul>
</li>
<li><a href="#orgd98c1e4">8.2. M8L2: Maximum Likelihood and Information Criteria</a>
<ul>
<li><a href="#org04b2d89">8.2.1. Likelihood</a></li>
<li><a href="#org7d0c619">8.2.2. Maximum likelihood fitting</a></li>
<li><a href="#orgdd1167b">8.2.3. Akaike Information Criterion</a></li>
<li><a href="#org34c145e">8.2.4. Corrected AIC (AIC<sub>c</sub>)</a></li>
<li><a href="#orgce92fa1">8.2.5. AIC<sub>c</sub> example</a></li>
<li><a href="#org6868343">8.2.6. Bayesian Information Criterion</a></li>
<li><a href="#org4f8fd5b">8.2.7. Summary</a></li>
</ul>
</li>
<li><a href="#org5d2d55b">8.3. M8L3: Using Regression</a>
<ul>
<li><a href="#orgd09f1dd">8.3.1. Regression coefficients</a></li>
</ul>
</li>
<li><a href="#orga8f64bd">8.4. M8L4: Causation vs Correlation</a>
<ul>
<li><a href="#orge4bb16b">8.4.1. Example: winter recreation</a></li>
<li><a href="#org328bf3a">8.4.2. Example: tiredness vs scruffiness</a></li>
<li><a href="#orgcfa8b53">8.4.3. How to tell causation?</a></li>
<li><a href="#orgbe0b795">8.4.4. Meaningless correlations</a></li>
</ul>
</li>
<li><a href="#org239f3d1">8.5. M8L5: Transformations and Interactions</a>
<ul>
<li><a href="#orgc82127c">8.5.1. Transforming the data</a></li>
<li><a href="#org1050c90">8.5.2. Interaction terms, e.g. product of inputs</a></li>
</ul>
</li>
<li><a href="#org575e042">8.6. M6L6: Output</a>
<ul>
<li><a href="#org84ea6ba">8.6.1. p-Values</a></li>
<li><a href="#org5702c0d">8.6.2. Confidence interval</a></li>
<li><a href="#orgae1633c">8.6.3. T-statistic</a></li>
<li><a href="#orgb3ce565">8.6.4. Coefficient itself</a></li>
<li><a href="#org5a0ee7f">8.6.5. \(R^2\)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgef5cf72">9. Module 09: Advanced Data Preparation</a>
<ul>
<li><a href="#org4348ef4">9.1. M9L1: Box-Cox Transformations</a></li>
<li><a href="#orgbd411da">9.2. M9L2: Detrending</a></li>
<li><a href="#org9b55be8">9.3. M9L3: Intro to PCA</a></li>
<li><a href="#org617ae16">9.4. M9L4: Using PCA</a>
<ul>
<li><a href="#orgd968ba0">9.4.1. Math of PCA</a></li>
<li><a href="#orgb634f3d">9.4.2. PCA as linear combination</a></li>
<li><a href="#orga461da0">9.4.3. PCA for regression</a></li>
<li><a href="#orgd9c6f89">9.4.4. Summary of PCA</a></li>
</ul>
</li>
<li><a href="#org38ead68">9.5. M9L5: Eigenvalues and Eigenvectors</a>
<ul>
<li><a href="#org04e5506">9.5.1. Initial example</a></li>
<li><a href="#orgb679a7b">9.5.2. Important: know how eigenvalues and eigenvectors are important to PCA</a></li>
</ul>
</li>
<li><a href="#org36f4292">9.6. M9L6: PCA: The good and the bad</a>
<ul>
<li><a href="#orgb832156">9.6.1. Example where PCA is good</a></li>
<li><a href="#org8d884ab">9.6.2. Example where PCA is bad</a></li>
<li><a href="#org1850a35">9.6.3. Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org901412c" class="outline-2">
<h2 id="org901412c"><span class="section-number-2">1.</span> Module 01: Intro</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org6feb49f" class="outline-3">
<h3 id="org6feb49f"><span class="section-number-3">1.1.</span> What's analytics?</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Analytics answers these questions
</p>
<ol class="org-ol">
<li>Descriptive - what happened</li>
<li>Predictive - what will happen</li>
<li>Prescriptive - what action is best</li>
<li>General questions</li>
</ol>
</div>
</div>
<div id="outline-container-org7c69a71" class="outline-3">
<h3 id="org7c69a71"><span class="section-number-3">1.2.</span> Modeling</h3>
<div class="outline-text-3" id="text-1-2">
<ol class="org-ol">
<li>Describe real-life situation with math</li>
<li>Analyze math</li>
<li>Turn math answer back to real situation</li>
</ol>
</div>
</div>
<div id="outline-container-orgcd3844a" class="outline-3">
<h3 id="orgcd3844a"><span class="section-number-3">1.3.</span> Course structure</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Enough math intuition and detail
</p>
<ul class="org-ul">
<li>Models
<ul class="org-ul">
<li>Machine learning</li>
<li>Regression</li>
<li>Optimizaton</li>
</ul></li>
<li>Cross-cutting
<ul class="org-ul">
<li>Data prep</li>
<li>Output quality</li>
<li>Missing data</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org0776684" class="outline-3">
<h3 id="org0776684"><span class="section-number-3">1.4.</span> Three different things are all models</h3>
<div class="outline-text-3" id="text-1-4">
<ol class="org-ol">
<li>Real life situation expressed as math</li>
<li>Analyse the math</li>
<li>Turn mathematical analyse to real-life solution</li>
</ol>
</div>
</div>
<div id="outline-container-orgf01b214" class="outline-3">
<h3 id="orgf01b214"><span class="section-number-3">1.5.</span> Hence these are all "models":</h3>
<div class="outline-text-3" id="text-1-5">
<ol class="org-ol">
<li>Regression</li>
<li>Regression on size, weight, distance</li>
<li>Regression estimate = 37+81*Size +76*Wt, etc</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgf6a3cf9" class="outline-2">
<h2 id="orgf6a3cf9"><span class="section-number-2">2.</span> Module 02: Classification</h2>
<div class="outline-text-2" id="text-2">
<blockquote>
<p>
Definition: putting things into groups
</p>
</blockquote>
</div>
<div id="outline-container-org5db835b" class="outline-3">
<h3 id="org5db835b"><span class="section-number-3">2.1.</span> M1L1: Intro to classification</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Types of classification models
</p>
<ol class="org-ol">
<li>Number of groups</li>
<li>Number of dimensions
<ul class="org-ul">
<li>Can 1 dimension be sufficient to classify?</li>
</ul></li>
<li>Soft vs hard classifiers (is it 100% error-free?)</li>
</ol>
</div>
</div>
<div id="outline-container-org4add716" class="outline-3">
<h3 id="org4add716"><span class="section-number-3">2.2.</span> M1L2: Choosing a Classifier</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Definition of bad classification
</p>
<ul class="org-ul">
<li>Cost: is one type of mistake worse than the other?</li>
</ul>
</div>
<div id="outline-container-org43ab3e4" class="outline-4">
<h4 id="org43ab3e4"><span class="section-number-4">2.2.1.</span> Example: Loan payment (Income vs credit score)</h4>
<div class="outline-text-4" id="text-2-2-1">
<ul class="org-ul">
<li>Plot lines and find one that can separate default vs non-default.</li>
<li>How do we know the right lines are drawn?</li>
<li>We want to be as conservative as possible (less error prone)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org13e507d" class="outline-3">
<h3 id="org13e507d"><span class="section-number-3">2.3.</span> M2L3 Data definitions</h3>
<div class="outline-text-3" id="text-2-3">
</div>
<div id="outline-container-org000dda1" class="outline-4">
<h4 id="org000dda1"><span class="section-number-4">2.3.1.</span> Data terminology</h4>
<div class="outline-text-4" id="text-2-3-1">
<ol class="org-ol">
<li>Row = data point</li>
<li>Column = dimension, attribute, feature, predictor, covariate
<ol class="org-ol">
<li>Special column = response, outcome</li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-org0ccd748" class="outline-4">
<h4 id="org0ccd748"><span class="section-number-4">2.3.2.</span> Data types</h4>
<div class="outline-text-4" id="text-2-3-2">
<ol class="org-ol">
<li>Structured data
<ol class="org-ol">
<li>Quantitative
<ul class="org-ul">
<li>Numbers with meaning</li>
</ul></li>
<li>Categorical
<ul class="org-ul">
<li>Numbers without meaning</li>
</ul></li>
<li>Binary data (subset of categorical)</li>
<li>Unrelated data</li>
<li>Time series data</li>
</ol></li>
<li>Unstructured
<ol class="org-ol">
<li>Text data</li>
</ol></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org8b762f9" class="outline-3">
<h3 id="org8b762f9"><span class="section-number-3">2.4.</span> M2L4: Support vector machines</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li><b>Supervised</b> method (algorithm uses known results when training)</li>
<li>Terminology
<ul class="org-ul">
<li>m = number of data points</li>
<li>n = number of attributes</li>
<li>x<sub>ij</sub> = j attribute of i data point
<ul class="org-ul">
<li>e.g. x<sub>51</sub> = credit score of person 5; x<sub>52</sub> = income of person 5</li>
</ul></li>
<li>y<sub>i</sub> = response of data point i
<ul class="org-ul">
<li>e.g. 1 if data point is group 1</li>
<li>-1 if data point is group 2</li>
</ul></li>
<li>Line: \(a_1 x_1\) + \(a_2 x_2\) + &#x2026; + \(a_n x_n\) + \(a_0\) = 0</li>
<li>Note the intercept \(a_0\)</li>
</ul></li>
<li>In general: \(\sum_{j=1}^{n} a_j x_j + a_0 = 0\)</li>
<li>Separation problem: get max distance between lines</li>
<li>\(2\over{\sqrt(\sum_{j} \left(a_j\right)^2)}\)</li>
<li>i.e. Min<sub>a<sub>0</sub> &#x2026; a<sub>n</sub></sub>: \(\sum_{j=1}^{n}\left(a_j\right)^2\)</li>
<li>Subject to constraints</li>
</ul>
</div>
<div id="outline-container-org809cf3f" class="outline-4">
<h4 id="org809cf3f"><span class="section-number-4">2.4.1.</span> When not possible to get full separation</h4>
<div class="outline-text-4" id="text-2-4-1">
<ul class="org-ul">
<li>Then we minimize error</li>
<li>There's a trade-off between margin and error</li>
<li>Error for data point is:
\[
  \text{max} \{ 0, 1-(\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}
  \]</li>
<li>Total error is:
\[
  \sum_{i=1}^{m} \text{max} \{ 0, 1 - (\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}
  \]</li>
<li>Margin denominator: \(\sum_{j=1}^{n}(a_j)^2\)</li>

<li>We multiply margin by \(\lambda\) to <b>assign its importance of margin vs error</b>.</li>
<li>Hence, the full equation is:
\[
  \text{Minimize}_{a_0,...,a_n} \sum_{i=1}^{m} \text{max} \{ 0, 1 - (\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}  + \lambda \sum_{j=1}^{n}(a_j)^2
  \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1f8a70f" class="outline-3">
<h3 id="org1f8a70f"><span class="section-number-3">2.5.</span> M2L5: What SVM means</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>Etymology
<ul class="org-ul">
<li>Vector = point</li>
<li><b>Support vector</b> = points that holds up (or, supports) a shape. Shape is correctly balanced on parallel lines</li>
<li>Model determines the "support vectors"</li>
<li>Automatically from data hence "<b>machine</b>"</li>
</ul></li>
<li>Support can be from top or side</li>
<li>Looking for max separation i.e. the support vector touches the data points</li>
<li>Classifier is in between the two support vectors</li>
</ul>
</div>
</div>
<div id="outline-container-org98dfb5e" class="outline-3">
<h3 id="org98dfb5e"><span class="section-number-3">2.6.</span> M2L6: Advanced SVM</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>The constant term a<sub>0</sub> can be used to adjust the intercept and hence tweak the SVM model.
<ul class="org-ul">
<li>If it's more costly to grant a bad loan, e.g.: \(\frac{2}{3}(a_0-1) + \frac{1}{3}(a_0+1)\)</li>
</ul></li>
<li>For soft classification, you can add a multiplier m<sub>j</sub> for each type of error:
<ul class="org-ul">
<li>m<sub>j</sub> &gt; 1 for more costly</li>
<li>m<sub>j</sub> &lt; 1 for less costly</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org503d2f6" class="outline-3">
<h3 id="org503d2f6"><span class="section-number-3">2.7.</span> M2L7: Scaling and standardization</h3>
<div class="outline-text-3" id="text-2-7">
<ul class="org-ul">
<li>Predictive factors may have different orders of magnitude, i.e.
<ul class="org-ul">
<li>Income in \(10^5\)</li>
<li>Credit score in \(10^2\)</li>
<li>Classifier is \(0 = a_0 + \sum_{j} a_j x_j\)</li>
<li>Maximise gap by minimizing: \(\sum_{j} a_j^2\)</li>
<li>Coefficients might be 10<sup>6</sup> + 5*income + 701*credit score
<ul class="org-ul">
<li>Sum of squared coefficients:
\(\sum_j a_j^2 = 5^2 + 700^2 = 490,025\)</li>
<li>Changing credit score by 1 increases the sum by 1,401:
\(\sum_j a_j^2 = 5^2 + 701^2 = 491,426\)</li>
</ul></li>
<li>Small change in one coefficient affects the sum a lot due to difference in scales.
<ul class="org-ul">
<li>As data has such different scale.</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="outline-container-org057350d" class="outline-4">
<h4 id="org057350d"><span class="section-number-4">2.7.1.</span> Scaling data</h4>
<div class="outline-text-4" id="text-2-7-1">
<ul class="org-ul">
<li>Common scale is between 0 and 1</li>
<li>Scale data by factor
\[
  x_{ij}^{\text{scaled}} = \frac{x_{ij}-x_{\text{min}j}}{x_{\text{max}j} - x_{\text{min}j}}
  \]</li>
<li>General scaling between a, b:
\[
  x_{ij}^{\text{scaled}[b,a]} = x_{ij}^{\text{scaled}[0,1]}(a-b)+b
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgb6dec82" class="outline-4">
<h4 id="orgb6dec82"><span class="section-number-4">2.7.2.</span> Standardization of data</h4>
<div class="outline-text-4" id="text-2-7-2">
<ul class="org-ul">
<li>Scale to normal distribution</li>
<li>Common scale is:
<ul class="org-ul">
<li>Mean = 0</li>
<li>SD = 1</li>
</ul></li>
<li>Factor j has:
<ul class="org-ul">
<li>mean \(\mu_j = \frac{\sum_{i=1}^n x_{ij}}{n}\)</li>
<li>SD \(\sigma_j\)</li>
</ul></li>
<li>For each data point \(i\):
\[
  x_{ij}^{\text{standardized}} = \frac{x_{ij}-\mu_j}{\sigma_j}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org469dc4d" class="outline-4">
<h4 id="org469dc4d"><span class="section-number-4">2.7.3.</span> Choosing between scaling vs standardization</h4>
<div class="outline-text-4" id="text-2-7-3">
<ul class="org-ul">
<li>Scale when:
<ul class="org-ul">
<li>Data is in bounded (defined) range, e.g.
<ul class="org-ul">
<li>Neural networks</li>
<li>Optimization models requiring bounded data</li>
<li>Batting averages (between 0 and 1)</li>
<li>RGB color scale (0-255)</li>
<li>SAT scores (200-800)</li>
</ul></li>
</ul></li>
<li>Standardization, examples:
<ul class="org-ul">
<li>PCA</li>
<li>Clustering</li>
</ul></li>
<li>Try both when not clear</li>
<li>Should be used throughout course even when not stated explicitly</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9c5a773" class="outline-3">
<h3 id="org9c5a773"><span class="section-number-3">2.8.</span> M2L8: K Nearest Neighbour model (KNN)</h3>
<div class="outline-text-3" id="text-2-8">
<ul class="org-ul">
<li><b>Classification</b></li>
<li>e.g. loan dataset with two predictors and a response</li>
<li>Assume each point has similar characteristics with its neighbors</li>
<li>Choice of number of points is denoted by \(k\)</li>
<li>Algorithm to find color (class) of a new point:
<ol class="org-ol">
<li>Pick \(k\) closest points (i.e., nearest neighbours) to the new one</li>
<li>The new point's class is the most common among the \(k\) neighbors</li>
</ol></li>
<li>Complexities
<ul class="org-ul">
<li>More than one distance metric (<i>c.f.</i> distance selection topic_).
<ul class="org-ul">
<li>Straight line is: \(\sqrt{\sum_{i=1}^n |x_i-y_i|^2}\)</li>
</ul></li>
<li>Attributes can be given more weight if more important, \(w_i\)
<ul class="org-ul">
<li>Weights to be found with other techniques e.g. regression</li>
</ul></li>
<li>Unimportant metrics can be removed
<ul class="org-ul">
<li><i>c.f.</i> variable selection topic</li>
<li>Choose good value of \(k\), <i>c.f.</i> validation @ <a href="#org82ed53c">3</a></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org82ed53c" class="outline-2">
<h2 id="org82ed53c"><span class="section-number-2">3.</span> Module 03: Validation</h2>
<div class="outline-text-2" id="text-3">
<blockquote>
<p>
Check how good a model is
</p>
</blockquote>
</div>
<div id="outline-container-orgd55d0c4" class="outline-3">
<h3 id="orgd55d0c4"><span class="section-number-3">3.1.</span> M3L1: Training, validation and test data</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li><b>Cannot</b> calculate accuracy or effectiveness metrics from training dataset
<ul class="org-ul">
<li>Since model was trained on it</li>
<li>This doesn't allow separation of real effects from random effects</li>
</ul></li>
<li>When fitting a model, this captures both real and random effects
<ul class="org-ul">
<li>Real effects: exist in all datasets (or subsets)</li>
<li>Random effects: different in all datasets</li>
</ul></li>
<li>Use a <b>training</b> set of data to fit model</li>
<li>Use another <b>validation</b> set of data to judge model effectiveness</li>
<li>When comparing &gt;1 model, use a <b>test</b> dataset.
<ul class="org-ul">
<li>e.g. SVM and KNN, with 10 total models, we cannot use the effectiveness metric calculated on the validation set.</li>
</ul></li>
<li>Test data is required as high performing models have above average random effects
<ul class="org-ul">
<li>Too optimistic; it might have performed well but also likely received a boost from random effects</li>
</ul></li>
<li>Analogize with models equally good</li>
<li>Flowchart:
<img src="./img/validation01.png" alt="validation01.png" /></li>
</ul>
</div>
</div>
<div id="outline-container-org9a741df" class="outline-3">
<h3 id="org9a741df"><span class="section-number-3">3.2.</span> M3L2: Splitting data</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>How much data goes to each set?
<ol class="org-ol">
<li>70 to 90% train, remaining test</li>
<li>50 to 70% train, remaining evenly split validation &amp; test</li>
</ol></li>
<li>Methods of splitting data
<ol class="org-ol">
<li>Random</li>
<li>Rotation (take turn selecting data points into training, test, valid across the sets of split data)
<ul class="org-ul">
<li>Advantage: in time series data, may avoid all datasets having early/late data</li>
<li>Need to ensure rotation doesn't introduce bias</li>
</ul></li>
<li>Combined:
60% of Monday data for training, 60^% of Tuesday data for training, etc.</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgc6a9b57" class="outline-3">
<h3 id="orgc6a9b57"><span class="section-number-3">3.3.</span> M3L3: Cross-validation</h3>
<div class="outline-text-3" id="text-3-3">
<blockquote>
<p>
What happens with important data appears only in one data set e.g., validation?
</p>
</blockquote>
<ul class="org-ul">
<li>Use cross-validation!</li>
<li>k-fold cross validation
<ol class="org-ol">
<li>Split data for testing (e.g. 20%)</li>
<li>With remaining data, use it for both training and validation by splitting into 4 x 20%, then:
<ol class="org-ol">
<li>Train 1, 2, 3, Validate 4</li>
<li>Train 1, 2, 4, Validate 3</li>
<li>Train 1, 3, 4, Validate 2</li>
<li>Train 2, 3, 4, Validate 1</li>
</ol></li>
</ol></li>
<li>Summary of k-fold cross-validation:
<ul class="org-ul">
<li>Train model on all other parts</li>
<li>Evaluate model on remaining part</li>
<li>Average \(k\) evaluations to estimate the model quality.</li>
<li>\(10\) is commonly selected for \(k\).</li>
<li><b>But</b>, the model selected from cross-validation is not used. Coefficients should also not be averaged.</li>
<li>Once model is selected, <b>retrain</b> with all data</li>
</ul></li>
<li>Advantages of k-fold cross-validation:
<ol class="org-ol">
<li>Better uses data</li>
<li>Better estimates model quality</li>
<li>Choose model more effectively</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orga84e253" class="outline-3">
<h3 id="orga84e253"><span class="section-number-3">3.4.</span> M3L4: Summary</h3>
<div class="outline-text-3" id="text-3-4">
<ol class="org-ol">
<li>Build model with training data</li>
<li>Pick model with validation data</li>
<li>Estimate performance with test data</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org34b0e34" class="outline-2">
<h2 id="org34b0e34"><span class="section-number-2">4.</span> Module 04: Clustering</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org3277d39" class="outline-3">
<h3 id="org3277d39"><span class="section-number-3">4.1.</span> M4L1: Introduction to clustering</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li><b>Unsupervised</b> method (response not available for use in training)</li>
<li>Grouping data points</li>
<li>Might help discover attributes in the dataset</li>
<li>Example of use
<ul class="org-ul">
<li>Segmenting market of car buyers by:
<ol class="org-ol">
<li>Size</li>
<li>Price</li>
<li>Versatility, etc</li>
</ol></li>
<li>Personalized medicine</li>
<li>Locating facilities</li>
<li>Image analysis</li>
<li>Exploratory data analysis (different model for each attribute)</li>
</ul></li>
<li>Example: Miles driven vs. Age</li>
</ul>
</div>
</div>
<div id="outline-container-org1507f23" class="outline-3">
<h3 id="org1507f23"><span class="section-number-3">4.2.</span> M4L2: Distance Norms</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>Straight line distance (Euclidean)
\(\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}\)</li>
<li>Rectilinear distance (Manhattan, 1-norm)
\(|x_1-y_1| + |x_2-y_2|\)</li>
<li>Generalized p-norm (Minkowski)
\(\sqrt[p]{|x_1-y_1|^p+|x_2-y_2|^p}\)</li>
<li>&infin;-norm distance
\(\sqrt[\infty]{\sum_{i=1}^n|x_i-y_i|^{\infty}}\)
<ul class="org-ul">
<li>sum = \(|x_i-y_i|^{\infty}\)</li>
<li>\(\sqrt[\infty]{\text{max}_{i}^n|x_i-y_i|^{\infty}}\)</li>
<li>Largest term dominates the rest, hence simplifies to:</li>
<li>\(\text{max}_i |x_i-y_i|\)</li>
</ul></li>
<li>Analogize with warehouse picking robot. The operation that takes the longest dominates the total operation time.</li>
</ul>
</div>
</div>
<div id="outline-container-org11717b1" class="outline-3">
<h3 id="org11717b1"><span class="section-number-3">4.3.</span> M4L3: K-Means Clustering</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li><b>Unsupervised</b> technique</li>
<li>Steps to implement K-Means:
<ol class="org-ol">
<li>Plot data points on suitable axes (e.g., age vs temperature, sepal width vs sepal height)</li>
<li>Let:
<ul class="org-ul">
<li>\(x_{ij}\) = attribute \(j\) of data point \(i\)</li>
<li>\(y_{ik}\) = \(1\) iif data point \(i\) in cluster \(k\), else \(0\)</li>
<li>\(z_{jk}\) = coordinate \(j\) of cluster center \(k\)</li>
<li>Mathematically, but it takes too long:
\[
       \text{Min}_{y,z}\sum_i\sum_k \sqrt{\sum_{j} (x_{ij} - z_{jk})^2}
       \]
subject to: \(\sum_k y_{ik} = 1\) for each \(i\)</li>
</ul></li>
<li>Practical method:
<ul class="org-ul">
<li>Pick \(k\) cluster centers in data</li>
<li>Assign each point to nearest cluster center</li>
<li>Recalculate cluster center (centroid)
<ul class="org-ul">
<li>Now, data points might not belong to the right cluster</li>
</ul></li>
<li>Go back to assign, then re-calc, then assign, then re-calc iteratively until stable</li>
</ul></li>
<li>K-Means is a <b>heuristic</b>, i.e.:
<ul class="org-ul">
<li>it is fast and good</li>
<li><b>not guaranteed</b> to find global best solution.</li>
</ul></li>
<li>It is expectation-maximization (EM), and alternates between expectation (<i>finding cluster centers</i>) and maximization (<i>assigning points to clusters</i>)</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org45f376d" class="outline-3">
<h3 id="org45f376d"><span class="section-number-3">4.4.</span> M4L4: Practical details for K-Means</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Algorithm just assigns outliers to nearest clusters.
</p>
<ul class="org-ul">
<li>Choosing <b>starting points</b>:
<ol class="org-ol">
<li>Run several times with different initial cluster centers</li>
<li>Algorithm is non-deterministic, <i>i.e.</i> can produce different results when run with different inputs</li>
<li>Choose the best solution from the results produced</li>
</ol></li>
<li>Handling <b>outliers</b>:
<ol class="org-ol">
<li>Discard, but may not be the 'right' answer</li>
<li>Ask why the outlier happens
<ul class="org-ul">
<li>What it means to discard or include the outlier</li>
<li>Ultimately, algorithm is just a guide. Best solution is what fits the situation.</li>
</ul></li>
</ol></li>
<li>Choosing <b>number of clusters</b>. Is adding a cluster always better?
<ul class="org-ul">
<li>It may increase the metric (total distance of each data point to their cluster center), hence clustering appears to work better.</li>
<li>However, it may defeat the purpose of clustering if every cluster just consists of one data point.</li>
</ul></li>
<li>Total distance can be compared to find the 'kink' or 'elbow'.
<ul class="org-ul">
<li>After this point, the marginal benefit of adding another cluster decreases.</li>
<li><p>
Elbow diagram:
</p>


<div id="org7284362" class="figure">
<p><img src="./img/04-elbow.png" alt="04-elbow.png" />
</p>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1622059" class="outline-3">
<h3 id="org1622059"><span class="section-number-3">4.5.</span> M4L5: Clustering for prediction</h3>
<div class="outline-text-3" id="text-4-5">
<blockquote>
<p>
Given a new point, which cluster should it be in?
</p>
</blockquote>
<ul class="org-ul">
<li>Is point inside cluster?</li>
<li>Otherwise, what's the nearest cluster center?</li>
<li>Asked another way: for the range of the dataset, which areas would we assign to each cluster if a new point appears there?
<ul class="org-ul">
<li>This is a <a href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi diagram</a>.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org74cfb91" class="outline-3">
<h3 id="org74cfb91"><span class="section-number-3">4.6.</span> M4L6: Clustering vs Classification</h3>
<div class="outline-text-3" id="text-4-6">
<ul class="org-ul">
<li>Since both group data points&#x2026;</li>
<li>The difference is what we know about the data points.</li>
<li>For classification, the correct response is known, i.e.
<ul class="org-ul">
<li>supervised learning</li>
<li>model uses both attributes <b>and</b> response</li>
</ul></li>
<li>For clustering, the 'correct' classification is unknown
<ul class="org-ul">
<li>unsupervised learning</li>
<li>model decides clusters <b>only</b> based on the attributes</li>
</ul></li>
<li>Supervised learning is more common</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org84c12d3" class="outline-2">
<h2 id="org84c12d3"><span class="section-number-2">5.</span> Module 05: Data preparation</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org5975a56" class="outline-3">
<h3 id="org5975a56"><span class="section-number-3">5.1.</span> M5L1: Common techniques and problems</h3>
<div class="outline-text-3" id="text-5-1">
<ol class="org-ol">
<li>Scale data
<ul class="org-ul">
<li>Outliers?</li>
</ul></li>
<li>Extraneous (unnecessary data)
<ul class="org-ul">
<li>Complicates the model and</li>
<li>Makes it harder to interpret the solution</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-orgaf3d681" class="outline-3">
<h3 id="orgaf3d681"><span class="section-number-3">5.2.</span> M5L2: Outliers</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>Types
<dl class="org-dl">
<dt>Point outliers</dt><dd>one / few points very different from others</dd>
<dt>Contextual outlier</dt><dd>Value far from other points in time (not in absolute value)</dd>
<dt>Collective outlier</dt><dd>Something missing in a range of points, but not sure exactly where. Outlier by omission.</dd>
</dl></li>
<li>How to detect?
<ul class="org-ul">
<li>Box-and-whisker plot if data can be plotted in 1-dimension
<ul class="org-ul">
<li>Box: 25/75th percentile
<ul class="org-ul">
<li>Line: 50th percentile</li>
</ul></li>
<li>Whiskers: 10/90th percentile, 5/95th, etc</li>
</ul></li>
<li>For multi-dimensional, no good way. We can still:
<ol class="org-ol">
<li>Fit a model.</li>
<li>Points with large error might be outlier</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3b5a8db" class="outline-3">
<h3 id="org3b5a8db"><span class="section-number-3">5.3.</span> M5L3: What to do with outliers?</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li>Need to understand why there's outliers
<ol class="org-ol">
<li>Bad data
<ul class="org-ul">
<li>Sensor fail</li>
<li>Contaminated experiment</li>
<li>Wrong data input</li>
</ul></li>
<li>Unexpected, real, data
<ul class="org-ul">
<li>Need to understand more about the data, e.g.</li>
<li>Where it came from</li>
<li>How it was compiled</li>
<li>Unique situations</li>
</ul></li>
</ol></li>
</ul>
</div>
<div id="outline-container-org3ca2f13" class="outline-4">
<h4 id="org3ca2f13"><span class="section-number-4">5.3.1.</span> Bad data</h4>
<div class="outline-text-4" id="text-5-3-1">
<ul class="org-ul">
<li>Omit the points</li>
<li>Use imputation to replace the points</li>
</ul>
</div>
</div>
<div id="outline-container-org1cea494" class="outline-4">
<h4 id="org1cea494"><span class="section-number-4">5.3.2.</span> Real / correct data</h4>
<div class="outline-text-4" id="text-5-3-2">
<ul class="org-ul">
<li>Outliers are somewhat expected in large datasets</li>
<li>E.g., for normally-distributed data:
<ul class="org-ul">
<li>4% will be outside 2 &sigma;</li>
<li>1e6 data points = 2000 outside 3 &sigma;</li>
</ul></li>
<li>Removing <b>real</b> outliers might make model too optimistic. <i>e.g.</i> not account for actual shipments that take a long time from US to Africa</li>
<li>Outliers might be due to weather, political events</li>
</ul>
</div>
</div>
<div id="outline-container-org0db3403" class="outline-4">
<h4 id="org0db3403"><span class="section-number-4">5.3.3.</span> Another way to handle outliers</h4>
<div class="outline-text-4" id="text-5-3-3">
<ol class="org-ol">
<li>First build a logistic regression model
<ul class="org-ul">
<li>This estimated probability of outliers under different conditions</li>
</ul></li>
<li>Next, build the regular model i.e. estimate delivery time under <b>normal conditions</b>
<ul class="org-ul">
<li>Use data without outliers</li>
<li>Report different outcomes&#x2026;</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-orgc21030c" class="outline-4">
<h4 id="orgc21030c"><span class="section-number-4">5.3.4.</span> Summary</h4>
<div class="outline-text-4" id="text-5-3-4">
<ul class="org-ul">
<li>Outliers aren't predictable</li>
<li>Investigate the data in case you're wrong</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org12723d9" class="outline-2">
<h2 id="org12723d9"><span class="section-number-2">6.</span> Module 06: Change detection</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org12705d6" class="outline-3">
<h3 id="org12705d6"><span class="section-number-3">6.1.</span> M6L1: Examples</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>Usually with time series data</li>
<li>Determine if action is needed, e.g.,
<ul class="org-ul">
<li>Time for machine maintenance?</li>
<li>Have sales increased?</li>
</ul></li>
<li>Determine impact of some past action, e.g.,
<ul class="org-ul">
<li>Did new tax / increase rate decrease sales?</li>
<li>Did price discount increase sales?</li>
</ul></li>
<li>Determine changes of current actions, e.g.
<ul class="org-ul">
<li>Did voting patterns change?</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge44229d" class="outline-3">
<h3 id="orge44229d"><span class="section-number-3">6.2.</span> M6L2: Cumulative sum for change detection</h3>
<div class="outline-text-3" id="text-6-2">
<blockquote>
<p>
Answers whether mean of the observed distribution gone above a critical level
</p>
</blockquote>
<ul class="org-ul">
<li>x<sub>t</sub> is observed value at time \(t\)</li>
<li>&mu; is mean of \(x\), if no change in distribution</li>
<li>Hence, \((x_t - \mu)\) is how much the observation is above mean at time \(t\)</li>
<li>Detecting an increase
\[
  S_t = \text{max}\{ 0, s_{t-1}+(x_t-\mu-C) \}
  \]
<ul class="org-ul">
<li>Determine threshold \(T\) and ask whether S<sub>t</sub> ⩾ T?
<ul class="org-ul">
<li>If running total &lt; 0, it's irrelevant</li>
<li>There should still be some randomness</li>
<li>C is a term to control how faster S<sub>t</sub> increases</li>
</ul></li>
</ul></li>
<li>Detecting a decrease
\[
  S_t = \text{max}\{ 0, s_{t-1}+(\mu-x_t-C) \}
  \]
<ul class="org-ul">
<li>Is S<sub>t</sub> ⩾ T?</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org54dd000" class="outline-4">
<h4 id="org54dd000"><span class="section-number-4">6.2.1.</span> Interpretation</h4>
<div class="outline-text-4" id="text-6-2-1">
<ul class="org-ul">
<li>Choices of model parameters
<dl class="org-dl">
<dt>T</dt><dd>the threshold, above which alarm is raised</dd>
<dt>C</dt><dd>the control term (smaller = more sensitive)</dd>
</dl></li>
<li>Consider / trade off:
<ol class="org-ol">
<li>How costly is it to delay detection? (false negative) -&gt; if it's costly, use small C</li>
<li>How costly is false positive? -&gt; if it's costly, use big C</li>
</ol></li>
<li>Use a control chart and plot S<sub>t</sub> vs t with \(T\) as a horizontal line
<img src="./img/06-control-chart.png" alt="06-control-chart.png" /></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org447c85d" class="outline-3">
<h3 id="org447c85d"><span class="section-number-3">6.3.</span> M6L3: Ethics: Honestly reporting our results</h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li>Be faithful to data</li>
<li>Have sound conclusions drawn from the model and not your own conceptions</li>
<li>Always be honest and true to your analysis</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org24c9993" class="outline-2">
<h2 id="org24c9993"><span class="section-number-2">7.</span> Module 07: Time series</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org353f5a3" class="outline-3">
<h3 id="org353f5a3"><span class="section-number-3">7.1.</span> M7L1: Introduction to exponential smoothing</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>Data for the same response is known for many time periods</li>
<li>Examples:
<ul class="org-ul">
<li>Temperature readings</li>
<li>Price of stocks</li>
<li>Daily sales of hamburgers</li>
<li>Blood pressure readings</li>
</ul></li>
<li>Variation in time series data:
<ol class="org-ol">
<li>Trends increase or decrease</li>
<li>Cyclical variables over a year or a week</li>
</ol></li>
</ul>
</div>
<div id="outline-container-org84a7cfa" class="outline-4">
<h4 id="org84a7cfa"><span class="section-number-4">7.1.1.</span> Random variation</h4>
<div class="outline-text-4" id="text-7-1-1">
<ul class="org-ul">
<li>No underlying reason for the variation</li>
</ul>
</div>
</div>
<div id="outline-container-org2eea417" class="outline-4">
<h4 id="org2eea417"><span class="section-number-4">7.1.2.</span> Definitions:</h4>
<div class="outline-text-4" id="text-7-1-2">
<ul class="org-ul">
<li>\(S_t\): expected <b>baseline</b> response at time period \(t\)</li>
<li>\(x_t\): the observed response at \(t\)</li>
<li>Seeing a increase over time, is it
<ol class="org-ol">
<li>A real increase?</li>
<li>Random?</li>
</ol></li>
<li>There are two ways to answer:
<ol class="org-ol">
<li>It's a real increase, hence \(S_t = x_t\)
<ul class="org-ul">
<li>the observed reading is real indicator of revised baseline</li>
</ul></li>
<li>It's random, hence \(S_t = S_{t-1}\)
<ul class="org-ul">
<li>today's baseline = yesterday's baseline</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org9880fc7" class="outline-4">
<h4 id="org9880fc7"><span class="section-number-4">7.1.3.</span> Exponential smoothing method</h4>
<div class="outline-text-4" id="text-7-1-3">
<p>
Combines both, i.e.
\(S_t = \alpha x_t + (1-\alpha)S_{t-1}\)
</p>
<ul class="org-ul">
<li><p>
0 &lt; &alpha; &lt;1
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&alpha;</th>
<th scope="col" class="org-left">example value of &alpha;</th>
<th scope="col" class="org-left">randomness</th>
<th scope="col" class="org-left">trust</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">small</td>
<td class="org-left">&rarr; 0 (e.g. 0.01)</td>
<td class="org-left">high</td>
<td class="org-left">previous baseline i.e. \(S_{t-1}\)</td>
</tr>

<tr>
<td class="org-left">large</td>
<td class="org-left">&rarr; 1 (e.g. 0.99)</td>
<td class="org-left">low</td>
<td class="org-left">today's estimate i.e. \(x_t\)</td>
</tr>
</tbody>
</table></li>

<li>How to start? \(S_1 = x_1\)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org61ad1bf" class="outline-3">
<h3 id="org61ad1bf"><span class="section-number-3">7.2.</span> M7L2: Trend and cyclic effects</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Complexities!
</p>
<ul class="org-ul">
<li>Trends, increasing or decreasing</li>
<li>Cyclical patterns, e.g. annual, weekly, daily</li>
</ul>
</div>
<div id="outline-container-org0563937" class="outline-4">
<h4 id="org0563937"><span class="section-number-4">7.2.1.</span> Trends</h4>
<div class="outline-text-4" id="text-7-2-1">
<ul class="org-ul">
<li>\(T_t\): the trend at time period \(t\)</li>
<li>\(S_t = \alpha x_t + (1-\alpha)S_{t-1}\)</li>
<li>\(T_t = \beta(S_t - S_{t-1}) + (1-\beta)T_{t-1}\)</li>
<li>Initial condition: \(T_1=0\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgecfbc8a" class="outline-4">
<h4 id="orgecfbc8a"><span class="section-number-4">7.2.2.</span> Cyclical patterns</h4>
<div class="outline-text-4" id="text-7-2-2">
<ul class="org-ul">
<li>Make cycles additive: behaves like trend</li>
<li>Make cycles multiplicative: more notation required
<ul class="org-ul">
<li>L = length of cycle</li>
<li>\(C_t\) = the multiplicative seasonality factor
<ul class="org-ul">
<li>This inflates or deflates the observation</li>
</ul></li>
<li>New baseline formula
\[
    S_t = \frac{\alpha x_t}{C_{t-L}} + (1-\alpha)(S_{t-1}+T_{t-1})
    \]</li>
<li>Need to use the factor from \(L\) time periods ago
<ul class="org-ul">
<li>as that's the most recent cyclic factor we have from that part of the cycle</li>
</ul></li>
<li>Update the cyclic factor in a similar way i.e.:
<ul class="org-ul">
<li>\(C_t = \gamma(x_t/S_t) + (1-\gamma)(C_{t-L})\)</li>
<li>C<sub>1</sub>, &#x2026;, C<sub>L</sub> = 1
<ul class="org-ul">
<li>meaning there's no initial cyclic effect</li>
</ul></li>
<li>If C = 1.1 on Sunday:
<ul class="org-ul">
<li>sales are higher by 10% just because it's Sunday</li>
</ul></li>
</ul></li>
<li>Initial values: first \(L\) are set to 1. Multiplying by 1 = no effect</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org98d390c" class="outline-4">
<h4 id="org98d390c"><span class="section-number-4">7.2.3.</span> Summary</h4>
<div class="outline-text-4" id="text-7-2-3">
<ul class="org-ul">
<li>Exponential smoothing
<ul class="org-ul">
<li>Single</li>
<li>Double (with trend)</li>
<li>Triple (with trend and cyclic effects)
<ul class="org-ul">
<li>AKA Winter's method, or Holt-Winters</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org745a9a4" class="outline-3">
<h3 id="org745a9a4"><span class="section-number-3">7.3.</span> M7L3: Etymology (what the name means)</h3>
<div class="outline-text-3" id="text-7-3">
<p>
Example equation when \(\alpha = \frac{1}{2}\):
\(S_t = 0.5 x_t + 0.5 S_{t-1}\)
</p>
</div>
<ol class="org-ol">
<li><a id="org68de7da"></a>Smoothing<br />
<div class="outline-text-5" id="text-7-3-0-1">
<ul class="org-ul">
<li>Note: when x<sub>t</sub> is high, S<sub>t</sub> is <b>not</b> as high, as \((1-\alpha)S_{t-1}\) pulls it down</li>
<li>Conversely: when x<sub>t</sub> is low, S<sub>t</sub> is <b>not</b> as low, as \((1-\alpha)S_{t-1}\) pulls it up</li>
<li>Peaks and valleys are smoothed out
<img src="./img/07-smoothed-graph.png" alt="07-smoothed-graph.png" /></li>
</ul>
</div>
</li>
<li><a id="org7fcbd79"></a>Exponential<br />
<div class="outline-text-5" id="text-7-3-0-2">
<ul class="org-ul">
<li>Each \(S_{t-1}\) actually contains every previous value of x!
<ul class="org-ul">
<li>When written or expanded out, e.g.
\[
    S_t = \alpha x_t + (1-\alpha)S_{t-1}
    \]
\[
    S_{t} = \alpha x_t + (1-\alpha)[\alpha x_{t-1} + (1-\alpha)S_{t-2}]
    \]
\[
    S_{t} = \alpha x_t + (1-\alpha)\alpha x_{t-1} + (1-\alpha)^2S_{t-2}
    \]</li>
</ul></li>
<li>Each S<sub>t</sub> is weighed by (1-&alpha;) to an increasing <b>exponent</b></li>
<li>This means not only the current observation matters; instead, every past observation contributes to the current baseline estimate</li>
<li>However, more recent observations are more important as they have higher weight</li>
</ul>
</div>
</li>
</ol>
<div id="outline-container-orgca4f534" class="outline-4">
<h4 id="orgca4f534"><span class="section-number-4">7.3.1.</span> Summary</h4>
<div class="outline-text-4" id="text-7-3-1">
<ul class="org-ul">
<li>Exponential smoothing smooths out jumps in observed data</li>
<li>It's an exponential weighting of all past observations</li>
<li>More recent observations are more important to the current baseline estimate</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org881611f" class="outline-3">
<h3 id="org881611f"><span class="section-number-3">7.4.</span> M7L4: Forecasting</h3>
<div class="outline-text-3" id="text-7-4">
<ul class="org-ul">
<li>Recap: \(S_t = \alpha x_t + (1-\alpha)S_{t-1}\)</li>
<li>Prediction:
<ul class="org-ul">
<li>\(S_{t+1} = \alpha x_{t+1} + (1-\alpha)S_{t}\)</li>
<li>However x<sub>t+1</sub> is unknown</li>
<li>Best guess for x<sub>t+1</sub> is \(S_t\)</li>
</ul></li>
<li>Our forecast for \(t+1\) is hence (after substituting):
\[
  F_{t+1}=\alpha S_t + (1-\alpha) S_t \\
  F_{t+1} = S_t \\
  F_{t+k} = S_t \text{when } k=1, 2, ...
  \]
<ul class="org-ul">
<li>note that forecast error becomes larger for larger \(k\)</li>
</ul></li>
<li>If including trend:
\[
  S_t = \alpha x_t + (1-\alpha)(S_{t-1}+T_{t-1}) \\
  T_t = \beta (S_t-S_{t-1})+(1-\beta)T_{t-1} \\
  F_{t+1} = S_t + T_t \\
  F_{t+k} = S_t + kT_t, k=1,2,...
  \]
<dl class="org-dl">
<dt>Best estimate of next baseline</dt><dd>the most current baseline estimate</dd>
<dt>Best estimate of the trend</dt><dd>the most current trend estimate</dd>
</dl></li>
<li>If including multiplicative seasonality:
\[
  S_t = \alpha x_t/C_{t-L} + (1-\alpha)(S_{t-1}+T_{t-1})
  F_{t+1} = (S_t+T_t)C_{(t+1)-L}
  \]
<dl class="org-dl">
<dt>Best estimate of next time period seasonal factor</dt><dd>the corresponding (lagged) seasonal factor, i.e. \(C_{t+1}=C_{t+1-L}\)</dd>
</dl></li>
<li>Finding the right &alpha;, &beta;, &gamma;: use optimization, to be covered in future
<ul class="org-ul">
<li>\(\min{(F_t-x_t)^2}\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org92cc688" class="outline-3">
<h3 id="org92cc688"><span class="section-number-3">7.5.</span> M3L5: ARIMA</h3>
<div class="outline-text-3" id="text-7-5">
<p>
AutoRegressive Integrated Moving Average.
</p>
<ul class="org-ul">
<li>ARIMA <b>theory</b> is not covered in IAM.</li>
</ul>
</div>
<div id="outline-container-orge5dbc15" class="outline-4">
<h4 id="orge5dbc15"><span class="section-number-4">7.5.1.</span> (I): Differences</h4>
<div class="outline-text-4" id="text-7-5-1">
<ul class="org-ul">
<li>Exponential smoothing assumes <b>stationary</b> data, i.e.
<ul class="org-ul">
<li>mean, variance, other measures are constant over time</li>
</ul></li>
<li>ARIMA works for data that's not stationary
<ul class="org-ul">
<li>if differences in data are stationary
<dl class="org-dl">
<dt>1st order difference D<sub>(1)</sub></dt><dd>difference of consecutive observations, i.e. \(D_{(1)t}=(x_t-x_{t-1})\)</dd>
<dt>2nd order difference D<sub>(2)</sub></dt><dd>difference of the differences i.e.
\(D_{(2)t}=(x_t-x_{t-1})-(x_{t-1}-x_{t-2})\)</dd>
<dt>d<sup>th</sup> order difference D<sub>(d)</sub></dt><dd>diff&#x2026; d times</dd>
</dl></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge22cf5f" class="outline-4">
<h4 id="orge22cf5f"><span class="section-number-4">7.5.2.</span> (II): Autogression</h4>
<div class="outline-text-4" id="text-7-5-2">
<ul class="org-ul">
<li>Predicting current values based on previous period's values</li>
<li>Regression: predicting value based on other factors</li>
<li>Auto: use earlier values to predict. Only works with time series</li>
<li>When used to forecast, exponential smoothing is an order-&infin; autoregressive model
<ul class="org-ul">
<li>All previous values are used to make current prediction</li>
</ul></li>
<li>Order-p autoregressive model: S<sub>t</sub> is function of \(\{x_t, x_{t-1}, ..., x_{t-(p-1)}\}\)
<ul class="org-ul">
<li>Only go back \(p\) periods</li>
</ul></li>
<li>ARIMA: combines autoregression and differencing
<ul class="org-ul">
<li>Autoregression on the differences</li>
<li>Use \(p\) time periods of previous observations to predict \(d^{th}\) order differences</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org166f3fd" class="outline-4">
<h4 id="org166f3fd"><span class="section-number-4">7.5.3.</span> (III): Moving Average</h4>
<div class="outline-text-4" id="text-7-5-3">
<ul class="org-ul">
<li>Use previous errors &epsilon;<sub>t</sub> as predictors
<ul class="org-ul">
<li>\(\epsilon_t = (\hat{x_t}-x_t)\)</li>
</ul></li>
<li>Order-q moving average
<ul class="org-ul">
<li>go back \(q\) time periods</li>
<li>\(\epsilon_{t-1}, \epsilon_{t-2}, ..., \epsilon_{t-q}\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org42f824b" class="outline-4">
<h4 id="org42f824b"><span class="section-number-4">7.5.4.</span> ARIMA model</h4>
<div class="outline-text-4" id="text-7-5-4">
<p>
ARIMA (p,d,q) model
</p>
<ul class="org-ul">
<li>\(d^{th}\) order differences</li>
<li>\(p^{th}\) order autoregression</li>
<li>\(q^{th}\) order moving average</li>
<li><a id="orgaa6d614"></a>Equation:
\[
  D_{(d)t} = \mu + \sum^p_{i=1}\alpha_i D_{(d)t-i} - \sum^q_{i=1} \theta_i (\hat{x}_{t-i}-x_{t-i})
  \]</li>
<li>Software can find \(d, p, q\)</li>
<li>Extensions
<ol class="org-ol">
<li>Add seasonality (out of scope for IAM)</li>
<li>Specific models:
<dl class="org-dl">
<dt>ARIMA(0,0,0)</dt><dd>white noise</dd>
<dt>ARIMA(0,1,0)</dt><dd>random walk</dd>
<dt>ARIMA(p,0,0)</dt><dd>AR (autoregressive) model</dd>
<dt>ARIMA(0,0,q)</dt><dd>MA (moving avg) model</dd>
<dt>ARIMA(0,1,1)</dt><dd>basic exponential smoothing model</dd>
</dl></li>
</ol></li>
<li>Can be used for short-term forecasting
<ul class="org-ul">
<li>ARIMA is better than ES when data is more stable with fewer peaks, valleys, outliers</li>
<li>ARIMA needs 40+ historical data points to work well</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgbd3a47a" class="outline-3">
<h3 id="orgbd3a47a"><span class="section-number-3">7.6.</span> M7L6: GARCH</h3>
<div class="outline-text-3" id="text-7-6">
<dl class="org-dl">
<dt>GARCH</dt><dd>Generalized Autoregressive Conditional Heteroskedasticity</dd>
</dl>
<p>
To estimate or forecast the variance
</p>
</div>
<div id="outline-container-org52f1f08" class="outline-4">
<h4 id="org52f1f08"><span class="section-number-4">7.6.1.</span> Variance</h4>
<div class="outline-text-4" id="text-7-6-1">
<ul class="org-ul">
<li>Estimates the amount of error</li>
<li>E.g. forecasting demand for trucks
<ul class="org-ul">
<li>tell you how much forecast might be higher/lower than the actual value you see (later) to plan accordingly</li>
</ul></li>
<li>In investment (portfolio optimization):
<ul class="org-ul">
<li>Balance the expected return in investment with amount of volatility.
<dl class="org-dl">
<dt>Riskier</dt><dd>higher expected return</dd>
<dt>Less risky</dt><dd>lower expected return</dd>
</dl></li>
<li>Variance is a proxy for amount of volatility or risk</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org9598887" class="outline-4">
<h4 id="org9598887"><span class="section-number-4">7.6.2.</span> GARCH</h4>
<div class="outline-text-4" id="text-7-6-2">
<p>
\[
\sigma^2_t = \omega + \sum^p_{i=1}\beta_i\sigma^2_{t-1}+\sum^q_{i=1} \gamma_i \epsilon^2_{t-i}
\]
It looks very similar to <a href="#orgaa6d614">ARIMA Equation</a>, but:
<b>Differences</b>:
</p>
<dl class="org-dl">
<dt>GARCH deals with variances and squared errors</dt><dd>ARIMA deals with observations and linear errors</dd>
<dt>GARCH deals with raw variances</dt><dd>ARIMA deals with differences of variances</dd>
</dl>
</div>
</div>
<div id="outline-container-orgbcb9076" class="outline-4">
<h4 id="orgbcb9076"><span class="section-number-4">7.6.3.</span> Summary - three models for time series analysis</h4>
<div class="outline-text-4" id="text-7-6-3">
<ol class="org-ol">
<li>Exponential smoothing</li>
<li>ARIMA, a generalization of exponential smoothing</li>
<li>GARCH, an ARIMA-like model for analyzing variance</li>
</ol>
</div>
</div>
</div>
</div>
<div id="outline-container-orgcab1977" class="outline-2">
<h2 id="orgcab1977"><span class="section-number-2">8.</span> Module 08: Regression</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org4542431" class="outline-3">
<h3 id="org4542431"><span class="section-number-3">8.1.</span> M8L1: Intro to Regression</h3>
<div class="outline-text-3" id="text-8-1">
</div>
<div id="outline-container-org44bf7d1" class="outline-4">
<h4 id="org44bf7d1"><span class="section-number-4">8.1.1.</span> What questions can regression answer?</h4>
<div class="outline-text-4" id="text-8-1-1">
<ol class="org-ol">
<li>How do systems work? (descriptive)</li>
<li>What will happen in the future? (predictive)</li>
</ol>
</div>
</div>
<div id="outline-container-org6fefb94" class="outline-4">
<h4 id="org6fefb94"><span class="section-number-4">8.1.2.</span> Simple linear regression</h4>
<div class="outline-text-4" id="text-8-1-2">
<ul class="org-ul">
<li>Linear regression with one predictor, e.g. \(y = a_0 + a_1x+1\)
<ul class="org-ul">
<li>Date point \(i\)'s prediction error
\(= y_i - \hat{y}_i -(a_0+a_1x_1)\)</li>
<li>Sum of squared errors
\(=\sum^n_{i=1}(y_i - \hat{y}_i)^2\)
\(=\sum^n_{i=1}(y_i-(a_0+a_1x_1))^2\)</li>
</ul></li>
<li>Best fit regression line
<ul class="org-ul">
<li>Minimizes sum of squared errors</li>
<li>Defined by a<sub>0</sub> and a<sub>1</sub></li>
</ul></li>
<li>Underlying math
<ul class="org-ul">
<li>Minimize convex quadratic function</li>
<li>Set partial derivatives to 0</li>
<li>Solve simultaneous equations</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd98c1e4" class="outline-3">
<h3 id="orgd98c1e4"><span class="section-number-3">8.2.</span> M8L2: Maximum Likelihood and Information Criteria</h3>
<div class="outline-text-3" id="text-8-2">
</div>
<div id="outline-container-org04b2d89" class="outline-4">
<h4 id="org04b2d89"><span class="section-number-4">8.2.1.</span> Likelihood</h4>
<div class="outline-text-4" id="text-8-2-1">
<ul class="org-ul">
<li>Measure the probability density of a parameter set</li>
<li id="Maximum likelihood">the parameters that give the highest probability</li>
<li>Assuming:
<ol class="org-ol">
<li>Errors are normally distributed with mean 0, variance &sigma;<sup>2</sup>, independently and identically distributed</li>
<li>Observations are z<sub>1</sub> to z<sub>n</sub></li>
<li>Model estimates are y<sub>1</sub> to y<sub>n</sub></li>
</ol></li>
<li>Probability density of observing z<sub>i</sub> if true value y<sub>i</sub> is
\[
  \frac{1}{\sigma\sqrt(2\pi)}e^-\frac{(z_u-y_i)^2}{2\sigma^2}
  \]</li>
<li>Joint density over \(n\) terms
\[
  \prod^n_{i=1}\frac{1}{\sigma\sqrt(2\pi)}e^-\frac{(z_u-y_i)^2}{2\sigma^2} \\
  = (\frac{1}{\sigma\sqrt{2\pi}})^n e^-\frac{1}{2\sigma^2}\sum^n_{i=1}(z_i-y_i)^2
  \]</li>
<li>Hence <b>to minimize</b> \(\sum^n_{i=1}(z_i-y_i)^2\) over a<sub>0</sub>, &#x2026;, a<sub>m</sub>:
equals to minimizing \(\sum^n_{i=1}(z_i-(a_0 + \sum^m_{j=1}a_jx_{ij}))^2\)</li>
</ul>
</div>
</div>
<div id="outline-container-org7d0c619" class="outline-4">
<h4 id="org7d0c619"><span class="section-number-4">8.2.2.</span> Maximum likelihood fitting</h4>
<div class="outline-text-4" id="text-8-2-2">
<ul class="org-ul">
<li>Simplest example is regression with i.i.d. errors</li>
<li>Complex examples:
<ul class="org-ul">
<li>Different estimation formulas</li>
<li>Different error assumptions</li>
</ul></li>
<li>Good software can handle complex cases</li>
</ul>
</div>
</div>
<div id="outline-container-orgdd1167b" class="outline-4">
<h4 id="orgdd1167b"><span class="section-number-4">8.2.3.</span> Akaike Information Criterion</h4>
<div class="outline-text-4" id="text-8-2-3">
<dl class="org-dl">
<dt>L<sup>*</sup></dt><dd>maximum likelihood value</dd>
<dt>\(k\)</dt><dd>number of parameters being estimated</dd>
<dt>AIC</dt><dd>\(2k-2\log(L^{*})\)</dd>
<dt>Penalty term</dt><dd>balances likelihood with simplicity, helps avoid overfitting</dd>
</dl>
</div>
<ol class="org-ol">
<li><a id="org891e259"></a>For simple regression<br />
<div class="outline-text-5" id="text-8-2-3-1">
<dl class="org-dl">
<dt>AIC</dt><dd>\[
  2(m+1) - 2\log(\frac{1}{\sigma\sqrt(2\pi)}^ne^{-\frac{1}{2\sigma^2}}\sum^n_{i=1}(z_i-(a_0 + \sum^m_{j=1}a_jx_{ij}))^2)
  \]</dd>
<dt>Preference</dt><dd>smaller AIC models</dd>
<dt>Requires</dt><dd>infinitely many data points</dd>
</dl>
</div>
</li>
</ol>
</div>
<div id="outline-container-org34c145e" class="outline-4">
<h4 id="org34c145e"><span class="section-number-4">8.2.4.</span> Corrected AIC (AIC<sub>c</sub>)</h4>
<div class="outline-text-4" id="text-8-2-4">
<p>
Use for smaller datasets.
\[
AIC_c = AIC + \frac{2k(k+1)}{n-k-1} \\
= 2k-2\log(L^{*})+\frac{2k(k+1)}{n-k-1}
\]
</p>
</div>
</div>
<div id="outline-container-orgce92fa1" class="outline-4">
<h4 id="orgce92fa1"><span class="section-number-4">8.2.5.</span> AIC<sub>c</sub> example</h4>
<div class="outline-text-4" id="text-8-2-5">
<ul class="org-ul">
<li>Model 1: AIC 75; Model 2: AIC 80</li>
<li>Relative likelihood equals:
\[
  e^\frac{AIC_1-AIC_2}{2} \\
  = 8.2%
  \]</li>
<li>Hence, Model 2 (larger AIC) is 8.2% as likely as Model 1 to be better
<ul class="org-ul">
<li>Model 1 is probably better</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6868343" class="outline-4">
<h4 id="org6868343"><span class="section-number-4">8.2.6.</span> Bayesian Information Criterion</h4>
<div class="outline-text-4" id="text-8-2-6">
<ul class="org-ul">
<li>BIC:
\[
  k\log(n)-2\log(L^{*})
  \]</li>
<li>Similar to AIC, but
<ul class="org-ul">
<li>bigger penalty term than AIC's penalty term</li>
<li>encourages models with fewer parameters</li>
</ul></li>
<li>Use BIC when there are <b>more data points</b> than parameters</li>
<li><p>
Rule of thumb for |BIC<sub>1</sub> - BIC<sub>2</sub>|
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">value</th>
<th scope="col" class="org-left">interpretation for smaller BIC model</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">&gt;10</td>
<td class="org-left">very likely better</td>
</tr>

<tr>
<td class="org-right">6-10</td>
<td class="org-left">likely better</td>
</tr>

<tr>
<td class="org-right">2-6</td>
<td class="org-left">somewhat likely better</td>
</tr>

<tr>
<td class="org-right">0-2</td>
<td class="org-left">slightly likely better</td>
</tr>
</tbody>
</table></li>
</ul>
</div>
</div>
<div id="outline-container-org4f8fd5b" class="outline-4">
<h4 id="org4f8fd5b"><span class="section-number-4">8.2.7.</span> Summary</h4>
<div class="outline-text-4" id="text-8-2-7">
<ul class="org-ul">
<li>No definite rules for AIC, BIC, maximum likelihood</li>
<li>Can look at all 3 to decide what's best</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5d2d55b" class="outline-3">
<h3 id="org5d2d55b"><span class="section-number-3">8.3.</span> M8L3: Using Regression</h3>
<div class="outline-text-3" id="text-8-3">
</div>
<div id="outline-container-orgd09f1dd" class="outline-4">
<h4 id="orgd09f1dd"><span class="section-number-4">8.3.1.</span> Regression coefficients</h4>
<div class="outline-text-4" id="text-8-3-1">
<ul class="org-ul">
<li>a<sub>0</sub>, a<sub>1</sub>, &#x2026;, a<sub>m</sub> for the equation:</li>
<li>\(y=a_0+a_1x_1+...+a_mx_m\)</li>
<li>Example for baseball, descriptive question:
<ul class="org-ul">
<li>How many runs is associated with every homerun</li>
<li>Response: how many runs are scored by a team</li>
<li>Predictors:
<ol class="org-ol">
<li>Number of HR</li>
<li>Triples</li>
<li>Doubles</li>
<li>Singles</li>
<li>Outs</li>
<li>Double Plays</li>
<li>Stolen bases, etc</li>
</ol></li>
<li>Equation:
\[
    \text{Runs scored} = a_0 + a_1\text{Number of HR} + a_2\text{Number of triples} + ... + a_7\text{Number of stolen bases}
    \]</li>
<li>a<sub>1</sub> = 1.4
<ul class="org-ul">
<li>Means that every HR adds 1.4 runs scored on average, ceteris paribus</li>
</ul></li>
</ul></li>
<li>Example: height, predictive question:
<ul class="org-ul">
<li>How tall will a 2-year old be as an adult?</li>
<li>Response: a person's adult height</li>
<li>Predictors:
<ol class="org-ol">
<li>Father's height</li>
<li>Mother's height</li>
<li>Height at age 2</li>
<li>Male, female</li>
</ol></li>
<li>Equation:
\[
    \text{Adult height} = a_0 + a_1\text{Father's height} + a_2 \text{Mother's height} + ... + a_4\text{Male or female}
    \]</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga8f64bd" class="outline-3">
<h3 id="orga8f64bd"><span class="section-number-3">8.4.</span> M8L4: Causation vs Correlation</h3>
<div class="outline-text-3" id="text-8-4">
<dl class="org-dl">
<dt>Causation</dt><dd>one thing causes another thing</dd>
<dt>Correlation</dt><dd>two things tend to happen / not happen together, but neither one causes the other</dd>
</dl>
</div>
<div id="outline-container-orge4bb16b" class="outline-4">
<h4 id="orge4bb16b"><span class="section-number-4">8.4.1.</span> Example: winter recreation</h4>
<div class="outline-text-4" id="text-8-4-1">
<ul class="org-ul">
<li>y: hours per day spent outdoors in winter</li>
<li>x<sub>1</sub>: city's average daily winter temperature</li>
<li>Equation: \(y=a_0+a_1x_1\)</li>
<li>Correlation between y and x<sub>1</sub>
<ul class="org-ul">
<li>with low p-value of a<sub>1</sub></li>
</ul></li>
<li>Does higher temperature in winter cause people to go outside?
<ul class="org-ul">
<li>Probably</li>
</ul></li>
<li>Reversing the equation:
<ul class="org-ul">
<li>y: hours per day spent outdoors in winter</li>
<li>x<sub>1</sub>: city's average daily winter temperature</li>
<li>Equation: \(x_1 = b_0 + b_1y\)</li>
<li>Same correlation between y, x<sub>1</sub>
<ul class="org-ul">
<li>Same p-value of b<sub>1</sub> and a<sub>1</sub></li>
</ul></li>
<li>Hence, does spending more time outside <b>cause</b> higher winter temperatures?
<ul class="org-ul">
<li>No</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org328bf3a" class="outline-4">
<h4 id="org328bf3a"><span class="section-number-4">8.4.2.</span> Example: tiredness vs scruffiness</h4>
<div class="outline-text-4" id="text-8-4-2">
<ul class="org-ul">
<li>Neither one caused another, they're just related to a common tired factor, kids</li>
</ul>
</div>
</div>
<div id="outline-container-orgcfa8b53" class="outline-4">
<h4 id="orgcfa8b53"><span class="section-number-4">8.4.3.</span> How to tell causation?</h4>
<div class="outline-text-4" id="text-8-4-3">
<ul class="org-ul">
<li>When is there causation?
<ol class="org-ol">
<li>Cause before effect</li>
<li>Idea of causation makes sense</li>
<li>No outside factors can cause the relationship (hard to ensure this - need to consider <b>all</b> other factors)</li>
</ol></li>
<li>Be careful before claiming causation</li>
</ul>
</div>
</div>
<div id="outline-container-orgbe0b795" class="outline-4">
<h4 id="orgbe0b795"><span class="section-number-4">8.4.4.</span> Meaningless correlations</h4>
<div class="outline-text-4" id="text-8-4-4">
<ul class="org-ul">
<li>Per capita consumption of mozzarella with number of civil engineering doctorates awarded</li>
<li><a href="http://tylervigen.com/spurious-correlations">link</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org239f3d1" class="outline-3">
<h3 id="org239f3d1"><span class="section-number-3">8.5.</span> M8L5: Transformations and Interactions</h3>
<div class="outline-text-3" id="text-8-5">
<ul class="org-ul">
<li>Recall: \(y=a_0+a_1x_1+...+a_mx_m\)</li>
<li>What if the fit isn't linear for x?</li>
<li>Answer: transforming the data!</li>
</ul>
</div>
<div id="outline-container-orgc82127c" class="outline-4">
<h4 id="orgc82127c"><span class="section-number-4">8.5.1.</span> Transforming the data</h4>
<div class="outline-text-4" id="text-8-5-1">
<ul class="org-ul">
<li>Quadratic regression, e.g.
\[
  y = a_0 + a_1 x_1 + a_2 x_1^2
  \]</li>
<li>Trigonometric:
\[
  y = a_0 + a_2 \sin(x^2)
  \]</li>
<li>Response transform:
\[
  \log(y) = a_0 + a_1 x_1 + ... + a_m x_m
  \]</li>
<li>etc.</li>
<li>Box-Cox transforms can be automated</li>
</ul>
</div>
</div>
<div id="outline-container-org1050c90" class="outline-4">
<h4 id="org1050c90"><span class="section-number-4">8.5.2.</span> Interaction terms, e.g. product of inputs</h4>
<div class="outline-text-4" id="text-8-5-2">
<ul class="org-ul">
<li>Child's heights might be influenced by <b>product</b> of father and mother's heights (\(x_1x_2\))</li>
<li>\(y = a_0 + a_1 x_1 + a_2 x_2 + a_3 (x_1 x_2)\)</li>
<li><b>Treat x<sub>1</sub> x<sub>2</sub> as new input, x<sub>3</sub></b></li>
<li>Then find best fit coefficients in the last module</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org575e042" class="outline-3">
<h3 id="org575e042"><span class="section-number-3">8.6.</span> M6L6: Output</h3>
<div class="outline-text-3" id="text-8-6">
</div>
<div id="outline-container-org84ea6ba" class="outline-4">
<h4 id="org84ea6ba"><span class="section-number-4">8.6.1.</span> p-Values</h4>
<div class="outline-text-4" id="text-8-6-1">
<ul class="org-ul">
<li>Estimates the <b>probability</b> that coefficient is actually 0
<ul class="org-ul">
<li>A hypothesis test</li>
</ul></li>
<li>If p-value is big (&gt;0.05), the coefficient is likely 0, hence remove its attribute from the model</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org682e747"></a>Other thresholds<br />
<div class="outline-text-5" id="text-8-6-1-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Threshold</th>
<th scope="col" class="org-left">Number of factors included</th>
<th scope="col" class="org-left">Risk</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Higher (&gt;0.05)</td>
<td class="org-left">More factors included</td>
<td class="org-left">Irrelevant factors included</td>
</tr>

<tr>
<td class="org-left">Lower (&lt;0.05)</td>
<td class="org-left">Less factors included</td>
<td class="org-left">Relevant factors left out</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="orge01e3f3"></a>Warnings<br />
<div class="outline-text-5" id="text-8-6-1-2">
<ul class="org-ul">
<li>With lots of data:
<ul class="org-ul">
<li>p-values can get small and seem significant even when attributes aren't related to response</li>
</ul></li>
<li>Even when meaningful, p-values only represent <b>probabilities</b>
<ol class="org-ol">
<li>With 100 attributes having p-value 0.02:</li>
<li>Each of them have 2% chance of <b>not</b> being significant</li>
<li>Hence on average 2/100 are actually irrelevant</li>
</ol></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org5702c0d" class="outline-4">
<h4 id="org5702c0d"><span class="section-number-4">8.6.2.</span> Confidence interval</h4>
<div class="outline-text-4" id="text-8-6-2">
<ul class="org-ul">
<li>Mostly given at 95% level around the coefficient</li>
<li>Range of where the coefficient probably lies</li>
<li>And how close that is to zero</li>
<li>Related to p-value</li>
</ul>
</div>
</div>
<div id="outline-container-orgae1633c" class="outline-4">
<h4 id="orgae1633c"><span class="section-number-4">8.6.3.</span> T-statistic</h4>
<div class="outline-text-4" id="text-8-6-3">
<ul class="org-ul">
<li>\(\frac{\text{Coefficient}}{\text{Std error}}\)</li>
<li>Related to p-value</li>
</ul>
</div>
</div>
<div id="outline-container-orgb3ce565" class="outline-4">
<h4 id="orgb3ce565"><span class="section-number-4">8.6.4.</span> Coefficient itself</h4>
<div class="outline-text-4" id="text-8-6-4">
<ul class="org-ul">
<li>If very small, then when multiplied by attribute, it's likely irrelevant</li>
</ul>
</div>
</div>
<div id="outline-container-org5a0ee7f" class="outline-4">
<h4 id="org5a0ee7f"><span class="section-number-4">8.6.5.</span> \(R^2\)</h4>
<div class="outline-text-4" id="text-8-6-5">
<ul class="org-ul">
<li>Estimate of how much variability can be explained by the model</li>
<li>E.g. R<sup>2</sup> = 0.59:
<ul class="org-ul">
<li>0.59 of data variability can be explained by the model</li>
<li>remaining 0.41 is either:
<ul class="org-ul">
<li>random variation</li>
<li>or other factors</li>
</ul></li>
</ul></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgb2c3a10"></a>Adjusted \(R^2\)<br />
<div class="outline-text-5" id="text-8-6-5-1">
<ul class="org-ul">
<li>Accounts for (penalizes) the number of attributes used</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgef5cf72" class="outline-2">
<h2 id="orgef5cf72"><span class="section-number-2">9.</span> Module 09: Advanced Data Preparation</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-org4348ef4" class="outline-3">
<h3 id="org4348ef4"><span class="section-number-3">9.1.</span> M9L1: Box-Cox Transformations</h3>
<div class="outline-text-3" id="text-9-1">
<ul class="org-ul">
<li>Models may require data to be normally distributed</li>
<li>What happens when this assumption isn't valid in the data?
<ul class="org-ul">
<li>Results will have bias in this case</li>
<li>Data exhibits heteroskedasicity</li>
<li>i.e., variances are not i.i.d.</li>
</ul></li>
<li>Another example is time series data, where later values have higher variance</li>
<li>Box-Cox is a logarithmic transformation that:
<ol class="org-ol">
<li>Stretches smaller range to increase variability</li>
<li>Shrinks larger range to reduce variability</li>
</ol></li>
<li>E.g. \(t(y)=\frac{y^\lambda-a}{\lambda}\)
<ul class="org-ul">
<li>t(y) can become close to normally distributed</li>
</ul></li>
<li>Need to remember to check for normality (e.g., with Normal Q-Q plot)</li>
</ul>
</div>
</div>
<div id="outline-container-orgbd411da" class="outline-3">
<h3 id="orgbd411da"><span class="section-number-3">9.2.</span> M9L2: Detrending</h3>
<div class="outline-text-3" id="text-9-2">
<ul class="org-ul">
<li>For time series data with trends, i.e. an increase or decrease over time
<ul class="org-ul">
<li>For example: increase in price of gold over time but need to account for inflation over time (value of $ decreases over time)</li>
<li>The trend if not correct can mess up a factor-based analysis</li>
<li>Can detrend:
<ul class="org-ul">
<li>Response</li>
<li>Predictors</li>
<li>Factor-based model (consider whenever using these models)
<ul class="org-ul">
<li>Regression, SVM, etc.</li>
</ul></li>
</ul></li>
</ul></li>
<li>How to detrend:
<ul class="org-ul">
<li>Factor by factor for one-dimensional regression, y=a<sub>0</sub>+a<sub>1x</sub>
<ul class="org-ul">
<li>Simple, works well to remove trend for factor-based analysis</li>
<li>This requires going factor by factor and fitting a linear regression model on it</li>
<li>E.g. for simple linear regression for gold prices:
<ul class="org-ul">
<li>Price = - 45600 + 23.2xYear</li>
<li>Detrend end price = Actual price -(-45600+23.2+year)</li>
<li>This produces a similar graph to the inflation-adjusted rate</li>
<li>Useful when we don’t know the response (as in most cases)</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org9b55be8" class="outline-3">
<h3 id="org9b55be8"><span class="section-number-3">9.3.</span> M9L3: Intro to PCA</h3>
<div class="outline-text-3" id="text-9-3">
<ul class="org-ul">
<li>Works on high dimensional and correlated data
<ul class="org-ul">
<li>Which subset of features are important to predict response?</li>
<li>e.g. which stocks can predict how well the market performs the next day?
<ul class="org-ul">
<li>6K securities</li>
<li>Remove days that have major events</li>
</ul></li>
<li>Issues:
<ul class="org-ul">
<li>6K predictors need many many data points to avoid overfitting (need to reduce predictors)
<ul class="org-ul">
<li>However, even with unlimited data, the underlying situation could have changed over time</li>
<li>E.g. TSLA stock is good predictor now but it was only listed 5 years ago</li>
</ul></li>
<li>High correlation between predictors</li>
</ul></li>
</ul></li>
<li>PCA transforms data by:
<ul class="org-ul">
<li>Removing correlations within predictors</li>
<li>Ranking coordinates by importance
<ul class="org-ul">
<li>Most important are first</li>
</ul></li>
<li>By concentrating on first <b>n</b> principal components
<ul class="org-ul">
<li>This reduces random effects and</li>
<li>These PCs have higher signal to noise ratio</li>
</ul></li>
<li>Graphically:
<ul class="org-ul">
<li>rotate plot until it’s orthogonal to the correlation</li>
</ul></li>
<li>If D1 and D2 are the new PCs,
<ul class="org-ul">
<li>D1 (that explains more variance) will be the first factor</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org617ae16" class="outline-3">
<h3 id="org617ae16"><span class="section-number-3">9.4.</span> M9L4: Using PCA</h3>
<div class="outline-text-3" id="text-9-4">
</div>
<div id="outline-container-orgd968ba0" class="outline-4">
<h4 id="orgd968ba0"><span class="section-number-4">9.4.1.</span> Math of PCA</h4>
<div class="outline-text-4" id="text-9-4-1">
<ul class="org-ul">
<li>Definitions
<dl class="org-dl">
<dt>\(X\)</dt><dd>initial matrix of data</dd>
<dt>\(x_{ij}\)</dt><dd>j<sup>th</sup> factor of i<sup>th</sup> data point</dd>
<dt>Scale to:</dt><dd>\(\frac{1}{m}\sum_i x_{ij} = \mu_j = 0\)</dd>
</dl></li>
<li>To find all eigenvectors of X<sup>T</sup> X, where:
<dl class="org-dl">
<dt>V</dt><dd>Matrix of eigenvectors, sorted by eigenvalue</dd>
<dt>V</dt><dd>[V<sub>1</sub> V<sub>2</sub> &#x2026; ]</dd>
<dt>V<sub>j</sub></dt><dd>j<sup>th</sup> eigenvector of X<sup>T</sup> X</dd>
</dl></li>
<li>PCA is a linear combination:
<ul class="org-ul">
<li>1st component is XV<sub>1</sub>, then 2nd is XV<sub>2</sub>, &#x2026;</li>
<li>k<sup>th</sup> new factor for i<sup>th</sup> data point:
\[
    t_{ik} = \sum^m_{ik} x_{ij}v_{jk}
    \]</li>
<li>t<sub>ik</sub> is the factor after PCA</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb634f3d" class="outline-4">
<h4 id="orgb634f3d"><span class="section-number-4">9.4.2.</span> PCA as linear combination</h4>
<div class="outline-text-4" id="text-9-4-2">
<ul class="org-ul">
<li>It <b><b>removes</b></b> correlation between factors</li>
<li>In order to have fewer variables/factors in the model:
<ul class="org-ul">
<li>Choose to include only first <i>n</i> principal components</li>
</ul></li>
<li>PCA can also deal with non-linear functions, using kernels. This is similar to SVM modeling</li>
</ul>
</div>
</div>
<div id="outline-container-orga461da0" class="outline-4">
<h4 id="orga461da0"><span class="section-number-4">9.4.3.</span> PCA for regression</h4>
<div class="outline-text-4" id="text-9-4-3">
<ul class="org-ul">
<li>How to interpret the new model in terms of original factors?</li>
<li>Example: PCA finds \(L\) new factors (each \(t_{ik}\)), and regression coefficients b<sub>0</sub>, b<sub>1</sub>, &#x2026; b<sub>L</sub>:
\[
  y_i = b_0 + \sum^L_{k=1}b_k t_{ik} \\
 = b_0 + \sum^L_{k=1}b_k [\sum^m_{j=1} x_{ij} v{jk}] \\
 = b_0 + \sum^m_{j=1}x_{ij} [\sum^L_{k=1}b_kv_{jk}]
 \\
 = b_0 + \sum^m_{j=1}x_{ij}[a_j]
  \]</li>
<li>Each t vector does not have nice intuitive explanations as they are linear combinations of original factors.</li>
<li>Hence just plug in the transformation for each t factor:
\[
  a_j = \sum^L_{k=1}b_kv_{jk}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgd9c6f89" class="outline-4">
<h4 id="orgd9c6f89"><span class="section-number-4">9.4.4.</span> Summary of PCA</h4>
<div class="outline-text-4" id="text-9-4-4">
<ul class="org-ul">
<li>Use PCA for high-dimensional and correlated data</li>
<li>PCA removes these correlations and ranks coordinates by importance (i.e., variability explained)
<ul class="org-ul">
<li>PC1 &gt; PC2 &gt; PC3, etc</li>
</ul></li>
<li>PCA can be transformed back to the original factor space to get intuitive explanations</li>
<li>PCA allows use of fewer variables
<ul class="org-ul">
<li>Pick the ones that explain the most variability</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org38ead68" class="outline-3">
<h3 id="org38ead68"><span class="section-number-3">9.5.</span> M9L5: Eigenvalues and Eigenvectors</h3>
<div class="outline-text-3" id="text-9-5">
</div>
<div id="outline-container-org04e5506" class="outline-4">
<h4 id="org04e5506"><span class="section-number-4">9.5.1.</span> Initial example</h4>
<div class="outline-text-4" id="text-9-5-1">
<ul class="org-ul">
<li>Definitions:
<dl class="org-dl">
<dt>\(A\)</dt><dd>a square matrix</dd>
<dt>\(v\)</dt><dd>a vector such that \(Av=\lambda v\)</dd>
<dt>\(V\)</dt><dd>eigenvector of \(A\)</dd>
<dt>\(\lambda\)</dt><dd>eigenvalue of \(A\), i.e. det(A-&lambda; I) = 0. Every &lambda; is eigenvalue of A</dd>
</dl></li>
<li>Given &lambda;, solve \(Av=\lambda v\) to find the eigenvector \(v\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgb679a7b" class="outline-4">
<h4 id="orgb679a7b"><span class="section-number-4">9.5.2.</span> Important: know how eigenvalues and eigenvectors are important to PCA</h4>
<div class="outline-text-4" id="text-9-5-2">
<ul class="org-ul">
<li>With a scaled matrix \(X\) of data, and x<sub>ij</sub> is factor value for i<sup>th</sup> data point after scaling,</li>
<li>Find eigenvectors v<sub>1</sub> &#x2026; v<sub>n</sub> of \((X^TX)\)</li>
<li>Then, find the principal components:
<ol class="org-ol">
<li>Multiply \(X\) by the eigenvectors</li>
<li>\(Xv_1, Xv_2, ..., Xv_n\) are the principal components
<ul class="org-ul">
<li>i.e. the transformed set of orthogonal coordinate directions</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org36f4292" class="outline-3">
<h3 id="org36f4292"><span class="section-number-3">9.6.</span> M9L6: PCA: The good and the bad</h3>
<div class="outline-text-3" id="text-9-6">
<ul class="org-ul">
<li>Summary
<img src="./img/m9l6-pca-summary.png" alt="m9l6-pca-summary.png" /></li>
<li>D<sub>1</sub> has more explanatory power (vaariation)</li>
<li>But it may not be the most helpful for explanatory/predictive modeling</li>
<li><b>PCA depends only on the independent variables</b>, not the response variable
<ul class="org-ul">
<li>It's possible response is affected by variables with low variability instead of those with high variability!</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgb832156" class="outline-4">
<h4 id="orgb832156"><span class="section-number-4">9.6.1.</span> Example where PCA is good</h4>
<div class="outline-text-4" id="text-9-6-1">
<ul class="org-ul">
<li>Assume PCA is used for classification</li>
<li>PC1 has most of the variance and PC1 can classify red and blue points
<img src="./img/m9l6-pca-good.png" alt="m9l6-pca-good.png" /></li>
</ul>
</div>
</div>
<div id="outline-container-org8d884ab" class="outline-4">
<h4 id="org8d884ab"><span class="section-number-4">9.6.2.</span> Example where PCA is bad</h4>
<div class="outline-text-4" id="text-9-6-2">
<ul class="org-ul">
<li>PC1, though it has more variance, cannot classify the red and blue points</li>
<li>PC2 has less variance but it can classify the points exactly
<img src="./img/m9l6-pca-bad.png" alt="m9l6-pca-bad.png" /></li>
</ul>
</div>
</div>
<div id="outline-container-org1850a35" class="outline-4">
<h4 id="org1850a35"><span class="section-number-4">9.6.3.</span> Summary</h4>
<div class="outline-text-4" id="text-9-6-3">
<ul class="org-ul">
<li>We still use PCA (or try to!) as dimensions have higher variation specifically because they contain more information</li>
<li>However, this is not always true</li>
<li>PCA is a helpful approach to try</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2023-02-19 Sun 19:23</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
