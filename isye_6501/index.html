<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2023-02-28 Tue 22:48 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ISYE 6501 Intro to Analytics Modeling Notes</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ISYE 6501 Intro to Analytics Modeling Notes</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb26add4">1. Module 01: Intro</a>
<ul>
<li><a href="#org15ee901">1.1. What's analytics?</a></li>
<li><a href="#org393df24">1.2. Modeling</a></li>
<li><a href="#orgda25d88">1.3. Course structure</a></li>
<li><a href="#org458c8bf">1.4. Three different things are all models</a></li>
<li><a href="#org5cd631f">1.5. Hence these are all "models":</a></li>
</ul>
</li>
<li><a href="#orgb466971">2. Module 02: Classification</a>
<ul>
<li><a href="#orgb6ee300">2.1. M1L1: Intro to classification</a></li>
<li><a href="#orgd1a3cc4">2.2. M1L2: Choosing a Classifier</a>
<ul>
<li><a href="#orgd88ac13">2.2.1. Example: Loan payment (Income vs credit score)</a></li>
</ul>
</li>
<li><a href="#org70154de">2.3. M2L3 Data definitions</a>
<ul>
<li><a href="#orgde12473">2.3.1. Data terminology</a></li>
<li><a href="#org5e757a4">2.3.2. Data types</a></li>
</ul>
</li>
<li><a href="#org495f380">2.4. M2L4: Support vector machines</a>
<ul>
<li><a href="#org6f01c1e">2.4.1. When not possible to get full separation</a></li>
</ul>
</li>
<li><a href="#org299fffa">2.5. M2L5: What SVM means</a></li>
<li><a href="#org94b30ec">2.6. M2L6: Advanced SVM</a></li>
<li><a href="#org57b2953">2.7. M2L7: Scaling and standardization</a>
<ul>
<li><a href="#org7564c60">2.7.1. Scaling data</a></li>
<li><a href="#orge7baaa3">2.7.2. Standardization of data</a></li>
<li><a href="#org8fbe540">2.7.3. Choosing between scaling vs standardization</a></li>
</ul>
</li>
<li><a href="#org9424cb4">2.8. M2L8: K Nearest Neighbour model (KNN)</a></li>
</ul>
</li>
<li><a href="#orgc511c89">3. Module 03: Validation</a>
<ul>
<li><a href="#orgb95b204">3.1. M3L1: Training, validation and test data</a></li>
<li><a href="#orga278b89">3.2. M3L2: Splitting data</a></li>
<li><a href="#orgb80cac5">3.3. M3L3: Cross-validation</a></li>
<li><a href="#orgd7765ed">3.4. M3L4: Summary</a></li>
</ul>
</li>
<li><a href="#orgeae5a8c">4. Module 04: Clustering</a>
<ul>
<li><a href="#org6539d7b">4.1. M4L1: Introduction to clustering</a></li>
<li><a href="#org9cee775">4.2. M4L2: Distance Norms</a></li>
<li><a href="#orgff9c74c">4.3. M4L3: K-Means Clustering</a></li>
<li><a href="#org6a9cd26">4.4. M4L4: Practical details for K-Means</a></li>
<li><a href="#org6f7391b">4.5. M4L5: Clustering for prediction</a></li>
<li><a href="#org3ee3896">4.6. M4L6: Clustering vs Classification</a></li>
</ul>
</li>
<li><a href="#org1c7ee77">5. Module 05: Data preparation</a>
<ul>
<li><a href="#org32d908e">5.1. M5L1: Common techniques and problems</a></li>
<li><a href="#org3d2aec2">5.2. M5L2: Outliers</a></li>
<li><a href="#orgb07bff8">5.3. M5L3: What to do with outliers?</a>
<ul>
<li><a href="#org42ad1f6">5.3.1. Bad data</a></li>
<li><a href="#org9207f56">5.3.2. Real / correct data</a></li>
<li><a href="#orgac4af23">5.3.3. Another way to handle outliers</a></li>
<li><a href="#orgc9fd873">5.3.4. Summary</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3387ff6">6. Module 06: Change detection</a>
<ul>
<li><a href="#org880efce">6.1. M6L1: Examples</a></li>
<li><a href="#orge779f64">6.2. M6L2: Cumulative sum for change detection</a>
<ul>
<li><a href="#org9f10ee5">6.2.1. Interpretation</a></li>
</ul>
</li>
<li><a href="#org20fef05">6.3. M6L3: Ethics: Honestly reporting our results</a></li>
</ul>
</li>
<li><a href="#org8d3ba92">7. Module 07: Time series</a>
<ul>
<li><a href="#orge003385">7.1. M7L1: Introduction to exponential smoothing</a>
<ul>
<li><a href="#org316ce13">7.1.1. Random variation</a></li>
<li><a href="#org29b6837">7.1.2. Definitions:</a></li>
<li><a href="#orgd1db548">7.1.3. Exponential smoothing method</a></li>
</ul>
</li>
<li><a href="#orga21adb8">7.2. M7L2: Trend and cyclic effects</a>
<ul>
<li><a href="#org782969e">7.2.1. Trends</a></li>
<li><a href="#org92f5a24">7.2.2. Cyclical patterns</a></li>
<li><a href="#orgcc12e7f">7.2.3. Summary</a></li>
</ul>
</li>
<li><a href="#orgf08aee7">7.3. M7L3: Etymology (what the name means)</a>
<ul>
<li><a href="#org6d32a56">7.3.1. Summary</a></li>
</ul>
</li>
<li><a href="#org85e2565">7.4. M7L4: Forecasting</a></li>
<li><a href="#org16ef322">7.5. M3L5: ARIMA</a>
<ul>
<li><a href="#orgb81e0c7">7.5.1. (I): Differences</a></li>
<li><a href="#orga0cddfc">7.5.2. (II): Autogression</a></li>
<li><a href="#orgf719213">7.5.3. (III): Moving Average</a></li>
<li><a href="#orge8d63a5">7.5.4. ARIMA model</a></li>
</ul>
</li>
<li><a href="#org5731307">7.6. M7L6: GARCH</a>
<ul>
<li><a href="#org1f709d2">7.6.1. Variance</a></li>
<li><a href="#org6ae355f">7.6.2. GARCH</a></li>
<li><a href="#org5065708">7.6.3. Summary - three models for time series analysis</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org2474268">8. Module 08: Regression</a>
<ul>
<li><a href="#org0e8e452">8.1. M8L1: Intro to Regression</a>
<ul>
<li><a href="#orgade8387">8.1.1. What questions can regression answer?</a></li>
<li><a href="#org06f2c37">8.1.2. Simple linear regression</a></li>
</ul>
</li>
<li><a href="#org155ed89">8.2. M8L2: Maximum Likelihood and Information Criteria</a>
<ul>
<li><a href="#org273b542">8.2.1. Likelihood</a></li>
<li><a href="#org02149ce">8.2.2. Maximum likelihood fitting</a></li>
<li><a href="#orgce85cb7">8.2.3. Akaike Information Criterion</a></li>
<li><a href="#orgf57a564">8.2.4. Corrected AIC (AIC<sub>c</sub>)</a></li>
<li><a href="#org3f30f7a">8.2.5. AIC<sub>c</sub> example</a></li>
<li><a href="#orgbf53aba">8.2.6. Bayesian Information Criterion</a></li>
<li><a href="#orgce85ea2">8.2.7. Summary</a></li>
</ul>
</li>
<li><a href="#orgf5efa9b">8.3. M8L3: Using Regression</a>
<ul>
<li><a href="#org4344c57">8.3.1. Regression coefficients</a></li>
</ul>
</li>
<li><a href="#org9a11631">8.4. M8L4: Causation vs Correlation</a>
<ul>
<li><a href="#orgd60501e">8.4.1. Example: winter recreation</a></li>
<li><a href="#org3b1dcb9">8.4.2. Example: tiredness vs scruffiness</a></li>
<li><a href="#org446ad68">8.4.3. How to tell causation?</a></li>
<li><a href="#orgef81325">8.4.4. Meaningless correlations</a></li>
</ul>
</li>
<li><a href="#org3efe009">8.5. M8L5: Transformations and Interactions</a>
<ul>
<li><a href="#orgdbcda60">8.5.1. Transforming the data</a></li>
<li><a href="#org8297be6">8.5.2. Interaction terms, e.g. product of inputs</a></li>
</ul>
</li>
<li><a href="#orgb63db86">8.6. M6L6: Output</a>
<ul>
<li><a href="#orgb82c4e3">8.6.1. p-Values</a></li>
<li><a href="#org3b50b9b">8.6.2. Confidence interval</a></li>
<li><a href="#org4e11089">8.6.3. T-statistic</a></li>
<li><a href="#orgb54543d">8.6.4. Coefficient itself</a></li>
<li><a href="#org0c9f4ee">8.6.5. \(R^2\)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6f4dad8">9. Module 09: Advanced Data Preparation</a>
<ul>
<li><a href="#org085f2c5">9.1. M9L1: Box-Cox Transformations</a></li>
<li><a href="#org0048363">9.2. M9L2: Detrending</a></li>
<li><a href="#org42a4951">9.3. M9L3: Intro to PCA</a></li>
<li><a href="#org262c9a6">9.4. M9L4: Using PCA</a>
<ul>
<li><a href="#org0e41634">9.4.1. Math of PCA</a></li>
<li><a href="#orgb198daa">9.4.2. PCA as linear combination</a></li>
<li><a href="#org831b653">9.4.3. PCA for regression</a></li>
<li><a href="#org83cae80">9.4.4. Summary of PCA</a></li>
</ul>
</li>
<li><a href="#org00b043b">9.5. M9L5: Eigenvalues and Eigenvectors</a>
<ul>
<li><a href="#org3086280">9.5.1. Initial example</a></li>
<li><a href="#orgf0a7da4">9.5.2. Important: know how eigenvalues and eigenvectors are important to PCA</a></li>
</ul>
</li>
<li><a href="#org08eea3b">9.6. M9L6: PCA: The good and the bad</a>
<ul>
<li><a href="#org1fadf63">9.6.1. Example where PCA is good</a></li>
<li><a href="#org1a7d33d">9.6.2. Example where PCA is bad</a></li>
<li><a href="#org53d0520">9.6.3. Summary</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgb010402">10. Module 10: Advanced Regression</a>
<ul>
<li><a href="#org2a9d9de">10.1. M10L01: Introduction to CART</a>
<ul>
<li><a href="#org924b5e0">10.1.1. Trees in regression</a></li>
<li><a href="#org02aed9c">10.1.2. Recall</a></li>
<li><a href="#org382183d">10.1.3. What if responses can be differentiated by a factor?</a></li>
<li><a href="#org6cb9770">10.1.4. Further splits are possible</a></li>
<li><a href="#orgf6faada">10.1.5. Disadvantages</a></li>
<li><a href="#org3802094">10.1.6. Other places to use trees</a></li>
<li><a href="#org122aee4">10.1.7. Common questions</a></li>
<li><a href="#org9c0c4e0">10.1.8. Etymology of "tree"</a></li>
</ul>
</li>
<li><a href="#org6dd7874">10.2. M10L02: Branching</a>
<ul>
<li><a href="#orgf45461a">10.2.1. Main questions</a></li>
<li><a href="#org1ee6af1">10.2.2. Branching methods</a></li>
<li><a href="#orgf1f7527">10.2.3. Generic branching concept</a></li>
</ul>
</li>
<li><a href="#org13f5087">10.3. M10L03: Random Forests</a>
<ul>
<li><a href="#org4065c80">10.3.1. Intro summary</a></li>
<li><a href="#org254b70a">10.3.2. Introducing randomness</a></li>
<li><a href="#orgc582ce8">10.3.3. Note</a></li>
<li><a href="#org55100e3">10.3.4. Summary of RF</a></li>
</ul>
</li>
<li><a href="#org16e0474">10.4. M10L04: Explainability and Interpretability</a>
<ul>
<li><a href="#org2fe9559">10.4.1. Example: linear regression</a></li>
<li><a href="#orgab84f12">10.4.2. Example: regression tree</a></li>
<li><a href="#org36ce5dd">10.4.3. Example: random forests</a></li>
<li><a href="#orgac8ff12">10.4.4. Comparisons</a></li>
<li><a href="#org7dff6e7">10.4.5. Tradeoff</a></li>
</ul>
</li>
<li><a href="#org637affa">10.5. M10L05: Confusion Matrices</a>
<ul>
<li><a href="#org4b49f39">10.5.1. Details</a></li>
<li><a href="#orgd714016">10.5.2. Definitions</a></li>
</ul>
</li>
<li><a href="#org1c4a8ee">10.6. M10L06: Situationally-Driven Comparisons</a>
<ul>
<li><a href="#org5ad0edf">10.6.1. Example: from spam detection</a></li>
<li><a href="#org01c19bf">10.6.2. Evaluating quality / changing metrics</a></li>
</ul>
</li>
<li><a href="#org1a77a08">10.7. L10L07: Advanced Topics in Regression</a>
<ul>
<li><a href="#org4906658">10.7.1. Poisson regression</a></li>
<li><a href="#orgb0537ed">10.7.2. Regression splines</a></li>
<li><a href="#org5240604">10.7.3. Bayesian regression</a></li>
<li><a href="#org7be27e0">10.7.4. k-Nearest Neighbour Regression</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgb26add4" class="outline-2">
<h2 id="orgb26add4"><span class="section-number-2">1.</span> Module 01: Intro</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org15ee901" class="outline-3">
<h3 id="org15ee901"><span class="section-number-3">1.1.</span> What's analytics?</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Analytics answers these questions
</p>
<ol class="org-ol">
<li>Descriptive - what happened</li>
<li>Predictive - what will happen</li>
<li>Prescriptive - what action is best</li>
<li>General questions</li>
</ol>
</div>
</div>
<div id="outline-container-org393df24" class="outline-3">
<h3 id="org393df24"><span class="section-number-3">1.2.</span> Modeling</h3>
<div class="outline-text-3" id="text-1-2">
<ol class="org-ol">
<li>Describe real-life situation with math</li>
<li>Analyze math</li>
<li>Turn math answer back to real situation</li>
</ol>
</div>
</div>
<div id="outline-container-orgda25d88" class="outline-3">
<h3 id="orgda25d88"><span class="section-number-3">1.3.</span> Course structure</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Enough math intuition and detail
</p>
<ul class="org-ul">
<li>Models
<ul class="org-ul">
<li>Machine learning</li>
<li>Regression</li>
<li>Optimizaton</li>
</ul></li>
<li>Cross-cutting
<ul class="org-ul">
<li>Data prep</li>
<li>Output quality</li>
<li>Missing data</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org458c8bf" class="outline-3">
<h3 id="org458c8bf"><span class="section-number-3">1.4.</span> Three different things are all models</h3>
<div class="outline-text-3" id="text-1-4">
<ol class="org-ol">
<li>Real life situation expressed as math</li>
<li>Analyse the math</li>
<li>Turn mathematical analyse to real-life solution</li>
</ol>
</div>
</div>
<div id="outline-container-org5cd631f" class="outline-3">
<h3 id="org5cd631f"><span class="section-number-3">1.5.</span> Hence these are all "models":</h3>
<div class="outline-text-3" id="text-1-5">
<ol class="org-ol">
<li>Regression</li>
<li>Regression on size, weight, distance</li>
<li>Regression estimate = 37+81*Size +76*Wt, etc</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgb466971" class="outline-2">
<h2 id="orgb466971"><span class="section-number-2">2.</span> Module 02: Classification</h2>
<div class="outline-text-2" id="text-2">
<blockquote>
<p>
Definition: putting things into groups
</p>
</blockquote>
</div>
<div id="outline-container-orgb6ee300" class="outline-3">
<h3 id="orgb6ee300"><span class="section-number-3">2.1.</span> M1L1: Intro to classification</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Types of classification models
</p>
<ol class="org-ol">
<li>Number of groups</li>
<li>Number of dimensions
<ul class="org-ul">
<li>Can 1 dimension be sufficient to classify?</li>
</ul></li>
<li>Soft vs hard classifiers (is it 100% error-free?)</li>
</ol>
</div>
</div>
<div id="outline-container-orgd1a3cc4" class="outline-3">
<h3 id="orgd1a3cc4"><span class="section-number-3">2.2.</span> M1L2: Choosing a Classifier</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Definition of bad classification
</p>
<ul class="org-ul">
<li>Cost: is one type of mistake worse than the other?</li>
</ul>
</div>
<div id="outline-container-orgd88ac13" class="outline-4">
<h4 id="orgd88ac13"><span class="section-number-4">2.2.1.</span> Example: Loan payment (Income vs credit score)</h4>
<div class="outline-text-4" id="text-2-2-1">
<ul class="org-ul">
<li>Plot lines and find one that can separate default vs non-default.</li>
<li>How do we know the right lines are drawn?</li>
<li>We want to be as conservative as possible (less error prone)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org70154de" class="outline-3">
<h3 id="org70154de"><span class="section-number-3">2.3.</span> M2L3 Data definitions</h3>
<div class="outline-text-3" id="text-2-3">
</div>
<div id="outline-container-orgde12473" class="outline-4">
<h4 id="orgde12473"><span class="section-number-4">2.3.1.</span> Data terminology</h4>
<div class="outline-text-4" id="text-2-3-1">
<ol class="org-ol">
<li>Row = data point</li>
<li>Column = dimension, attribute, feature, predictor, covariate
<ol class="org-ol">
<li>Special column = response, outcome</li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-org5e757a4" class="outline-4">
<h4 id="org5e757a4"><span class="section-number-4">2.3.2.</span> Data types</h4>
<div class="outline-text-4" id="text-2-3-2">
<ol class="org-ol">
<li>Structured data
<ol class="org-ol">
<li>Quantitative
<ul class="org-ul">
<li>Numbers with meaning</li>
</ul></li>
<li>Categorical
<ul class="org-ul">
<li>Numbers without meaning</li>
</ul></li>
<li>Binary data (subset of categorical)</li>
<li>Unrelated data</li>
<li>Time series data</li>
</ol></li>
<li>Unstructured
<ol class="org-ol">
<li>Text data</li>
</ol></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org495f380" class="outline-3">
<h3 id="org495f380"><span class="section-number-3">2.4.</span> M2L4: Support vector machines</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li><b>Supervised</b> method (algorithm uses known results when training)</li>
<li>Terminology
<ul class="org-ul">
<li>m = number of data points</li>
<li>n = number of attributes</li>
<li>x<sub>ij</sub> = j attribute of i data point
<ul class="org-ul">
<li>e.g. x<sub>51</sub> = credit score of person 5; x<sub>52</sub> = income of person 5</li>
</ul></li>
<li>y<sub>i</sub> = response of data point i
<ul class="org-ul">
<li>e.g. 1 if data point is group 1</li>
<li>-1 if data point is group 2</li>
</ul></li>
<li>Line: \(a_1 x_1\) + \(a_2 x_2\) + &#x2026; + \(a_n x_n\) + \(a_0\) = 0</li>
<li>Note the intercept \(a_0\)</li>
</ul></li>
<li>In general: \(\sum_{j=1}^{n} a_j x_j + a_0 = 0\)</li>
<li>Separation problem: get max distance between lines</li>
<li>\(2\over{\sqrt(\sum_{j} \left(a_j\right)^2)}\)</li>
<li>i.e. Min<sub>a<sub>0</sub> &#x2026; a<sub>n</sub></sub>: \(\sum_{j=1}^{n}\left(a_j\right)^2\)</li>
<li>Subject to constraints</li>
</ul>
</div>
<div id="outline-container-org6f01c1e" class="outline-4">
<h4 id="org6f01c1e"><span class="section-number-4">2.4.1.</span> When not possible to get full separation</h4>
<div class="outline-text-4" id="text-2-4-1">
<ul class="org-ul">
<li>Then we minimize error</li>
<li>There's a trade-off between margin and error</li>
<li>Error for data point is:
\[
  \text{max} \{ 0, 1-(\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}
  \]</li>
<li>Total error is:
\[
  \sum_{i=1}^{m} \text{max} \{ 0, 1 - (\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}
  \]</li>
<li>Margin denominator: \(\sum_{j=1}^{n}(a_j)^2\)</li>

<li>We multiply margin by \(\lambda\) to <b>assign its importance of margin vs error</b>.</li>
<li>Hence, the full equation is:
\[
  \text{Minimize}_{a_0,...,a_n} \sum_{i=1}^{m} \text{max} \{ 0, 1 - (\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}  + \lambda \sum_{j=1}^{n}(a_j)^2
  \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org299fffa" class="outline-3">
<h3 id="org299fffa"><span class="section-number-3">2.5.</span> M2L5: What SVM means</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>Etymology
<ul class="org-ul">
<li>Vector = point</li>
<li><b>Support vector</b> = points that holds up (or, supports) a shape. Shape is correctly balanced on parallel lines</li>
<li>Model determines the "support vectors"</li>
<li>Automatically from data hence "<b>machine</b>"</li>
</ul></li>
<li>Support can be from top or side</li>
<li>Looking for max separation i.e. the support vector touches the data points</li>
<li>Classifier is in between the two support vectors</li>
</ul>
</div>
</div>
<div id="outline-container-org94b30ec" class="outline-3">
<h3 id="org94b30ec"><span class="section-number-3">2.6.</span> M2L6: Advanced SVM</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>The constant term a<sub>0</sub> can be used to adjust the intercept and hence tweak the SVM model.
<ul class="org-ul">
<li>If it's more costly to grant a bad loan, e.g.: \(\frac{2}{3}(a_0-1) + \frac{1}{3}(a_0+1)\)</li>
</ul></li>
<li>For soft classification, you can add a multiplier m<sub>j</sub> for each type of error:
<ul class="org-ul">
<li>m<sub>j</sub> &gt; 1 for more costly</li>
<li>m<sub>j</sub> &lt; 1 for less costly</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org57b2953" class="outline-3">
<h3 id="org57b2953"><span class="section-number-3">2.7.</span> M2L7: Scaling and standardization</h3>
<div class="outline-text-3" id="text-2-7">
<ul class="org-ul">
<li>Predictive factors may have different orders of magnitude, i.e.
<ul class="org-ul">
<li>Income in \(10^5\)</li>
<li>Credit score in \(10^2\)</li>
<li>Classifier is \(0 = a_0 + \sum_{j} a_j x_j\)</li>
<li>Maximise gap by minimizing: \(\sum_{j} a_j^2\)</li>
<li>Coefficients might be 10<sup>6</sup> + 5*income + 701*credit score
<ul class="org-ul">
<li>Sum of squared coefficients:
\(\sum_j a_j^2 = 5^2 + 700^2 = 490,025\)</li>
<li>Changing credit score by 1 increases the sum by 1,401:
\(\sum_j a_j^2 = 5^2 + 701^2 = 491,426\)</li>
</ul></li>
<li>Small change in one coefficient affects the sum a lot due to difference in scales.
<ul class="org-ul">
<li>As data has such different scale.</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="outline-container-org7564c60" class="outline-4">
<h4 id="org7564c60"><span class="section-number-4">2.7.1.</span> Scaling data</h4>
<div class="outline-text-4" id="text-2-7-1">
<ul class="org-ul">
<li>Common scale is between 0 and 1</li>
<li>Scale data by factor
\[
  x_{ij}^{\text{scaled}} = \frac{x_{ij}-x_{\text{min}j}}{x_{\text{max}j} - x_{\text{min}j}}
  \]</li>
<li>General scaling between a, b:
\[
  x_{ij}^{\text{scaled}[b,a]} = x_{ij}^{\text{scaled}[0,1]}(a-b)+b
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orge7baaa3" class="outline-4">
<h4 id="orge7baaa3"><span class="section-number-4">2.7.2.</span> Standardization of data</h4>
<div class="outline-text-4" id="text-2-7-2">
<ul class="org-ul">
<li>Scale to normal distribution</li>
<li>Common scale is:
<ul class="org-ul">
<li>Mean = 0</li>
<li>SD = 1</li>
</ul></li>
<li>Factor j has:
<ul class="org-ul">
<li>mean \(\mu_j = \frac{\sum_{i=1}^n x_{ij}}{n}\)</li>
<li>SD \(\sigma_j\)</li>
</ul></li>
<li>For each data point \(i\):
\[
  x_{ij}^{\text{standardized}} = \frac{x_{ij}-\mu_j}{\sigma_j}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org8fbe540" class="outline-4">
<h4 id="org8fbe540"><span class="section-number-4">2.7.3.</span> Choosing between scaling vs standardization</h4>
<div class="outline-text-4" id="text-2-7-3">
<ul class="org-ul">
<li>Scale when:
<ul class="org-ul">
<li>Data is in bounded (defined) range, e.g.
<ul class="org-ul">
<li>Neural networks</li>
<li>Optimization models requiring bounded data</li>
<li>Batting averages (between 0 and 1)</li>
<li>RGB color scale (0-255)</li>
<li>SAT scores (200-800)</li>
</ul></li>
</ul></li>
<li>Standardization, examples:
<ul class="org-ul">
<li>PCA</li>
<li>Clustering</li>
</ul></li>
<li>Try both when not clear</li>
<li>Should be used throughout course even when not stated explicitly</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9424cb4" class="outline-3">
<h3 id="org9424cb4"><span class="section-number-3">2.8.</span> M2L8: K Nearest Neighbour model (KNN)</h3>
<div class="outline-text-3" id="text-2-8">
<ul class="org-ul">
<li><b>Classification</b></li>
<li>e.g. loan dataset with two predictors and a response</li>
<li>Assume each point has similar characteristics with its neighbors</li>
<li>Choice of number of points is denoted by \(k\)</li>
<li>Algorithm to find color (class) of a new point:
<ol class="org-ol">
<li>Pick \(k\) closest points (i.e., nearest neighbours) to the new one</li>
<li>The new point's class is the most common among the \(k\) neighbors</li>
</ol></li>
<li>Complexities
<ul class="org-ul">
<li>More than one distance metric (<i>c.f.</i> distance selection topic_).
<ul class="org-ul">
<li>Straight line is: \(\sqrt{\sum_{i=1}^n |x_i-y_i|^2}\)</li>
</ul></li>
<li>Attributes can be given more weight if more important, \(w_i\)
<ul class="org-ul">
<li>Weights to be found with other techniques e.g. regression</li>
</ul></li>
<li>Unimportant metrics can be removed
<ul class="org-ul">
<li><i>c.f.</i> variable selection topic</li>
<li>Choose good value of \(k\), <i>c.f.</i> validation @ <a href="#orgc511c89">3</a></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc511c89" class="outline-2">
<h2 id="orgc511c89"><span class="section-number-2">3.</span> Module 03: Validation</h2>
<div class="outline-text-2" id="text-3">
<blockquote>
<p>
Check how good a model is
</p>
</blockquote>
</div>
<div id="outline-container-orgb95b204" class="outline-3">
<h3 id="orgb95b204"><span class="section-number-3">3.1.</span> M3L1: Training, validation and test data</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li><b>Cannot</b> calculate accuracy or effectiveness metrics from training dataset
<ul class="org-ul">
<li>Since model was trained on it</li>
<li>This doesn't allow separation of real effects from random effects</li>
</ul></li>
<li>When fitting a model, this captures both real and random effects
<ul class="org-ul">
<li>Real effects: exist in all datasets (or subsets)</li>
<li>Random effects: different in all datasets</li>
</ul></li>
<li>Use a <b>training</b> set of data to fit model</li>
<li>Use another <b>validation</b> set of data to judge model effectiveness</li>
<li>When comparing &gt;1 model, use a <b>test</b> dataset.
<ul class="org-ul">
<li>e.g. SVM and KNN, with 10 total models, we cannot use the effectiveness metric calculated on the validation set.</li>
</ul></li>
<li>Test data is required as high performing models have above average random effects
<ul class="org-ul">
<li>Too optimistic; it might have performed well but also likely received a boost from random effects</li>
</ul></li>
<li>Analogize with models equally good</li>
<li>Flowchart:
<img src="./img/validation01.png" alt="validation01.png" /></li>
</ul>
</div>
</div>
<div id="outline-container-orga278b89" class="outline-3">
<h3 id="orga278b89"><span class="section-number-3">3.2.</span> M3L2: Splitting data</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>How much data goes to each set?
<ol class="org-ol">
<li>70 to 90% train, remaining test</li>
<li>50 to 70% train, remaining evenly split validation &amp; test</li>
</ol></li>
<li>Methods of splitting data
<ol class="org-ol">
<li>Random</li>
<li>Rotation (take turn selecting data points into training, test, valid across the sets of split data)
<ul class="org-ul">
<li>Advantage: in time series data, may avoid all datasets having early/late data</li>
<li>Need to ensure rotation doesn't introduce bias</li>
</ul></li>
<li>Combined:
60% of Monday data for training, 60^% of Tuesday data for training, etc.</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgb80cac5" class="outline-3">
<h3 id="orgb80cac5"><span class="section-number-3">3.3.</span> M3L3: Cross-validation</h3>
<div class="outline-text-3" id="text-3-3">
<blockquote>
<p>
What happens with important data appears only in one data set e.g., validation?
</p>
</blockquote>
<ul class="org-ul">
<li>Use cross-validation!</li>
<li>k-fold cross validation
<ol class="org-ol">
<li>Split data for testing (e.g. 20%)</li>
<li>With remaining data, use it for both training and validation by splitting into 4 x 20%, then:
<ol class="org-ol">
<li>Train 1, 2, 3, Validate 4</li>
<li>Train 1, 2, 4, Validate 3</li>
<li>Train 1, 3, 4, Validate 2</li>
<li>Train 2, 3, 4, Validate 1</li>
</ol></li>
</ol></li>
<li>Summary of k-fold cross-validation:
<ul class="org-ul">
<li>Train model on all other parts</li>
<li>Evaluate model on remaining part</li>
<li>Average \(k\) evaluations to estimate the model quality.</li>
<li>\(10\) is commonly selected for \(k\).</li>
<li><b>But</b>, the model selected from cross-validation is not used. Coefficients should also not be averaged.</li>
<li>Once model is selected, <b>retrain</b> with all data</li>
</ul></li>
<li>Advantages of k-fold cross-validation:
<ol class="org-ol">
<li>Better uses data</li>
<li>Better estimates model quality</li>
<li>Choose model more effectively</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgd7765ed" class="outline-3">
<h3 id="orgd7765ed"><span class="section-number-3">3.4.</span> M3L4: Summary</h3>
<div class="outline-text-3" id="text-3-4">
<ol class="org-ol">
<li>Build model with training data</li>
<li>Pick model with validation data</li>
<li>Estimate performance with test data</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgeae5a8c" class="outline-2">
<h2 id="orgeae5a8c"><span class="section-number-2">4.</span> Module 04: Clustering</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org6539d7b" class="outline-3">
<h3 id="org6539d7b"><span class="section-number-3">4.1.</span> M4L1: Introduction to clustering</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li><b>Unsupervised</b> method (response not available for use in training)</li>
<li>Grouping data points</li>
<li>Might help discover attributes in the dataset</li>
<li>Example of use
<ul class="org-ul">
<li>Segmenting market of car buyers by:
<ol class="org-ol">
<li>Size</li>
<li>Price</li>
<li>Versatility, etc</li>
</ol></li>
<li>Personalized medicine</li>
<li>Locating facilities</li>
<li>Image analysis</li>
<li>Exploratory data analysis (different model for each attribute)</li>
</ul></li>
<li>Example: Miles driven vs. Age</li>
</ul>
</div>
</div>
<div id="outline-container-org9cee775" class="outline-3">
<h3 id="org9cee775"><span class="section-number-3">4.2.</span> M4L2: Distance Norms</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>Straight line distance (Euclidean)
\(\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}\)</li>
<li>Rectilinear distance (Manhattan, 1-norm)
\(|x_1-y_1| + |x_2-y_2|\)</li>
<li>Generalized p-norm (Minkowski)
\(\sqrt[p]{|x_1-y_1|^p+|x_2-y_2|^p}\)</li>
<li>&infin;-norm distance
\(\sqrt[\infty]{\sum_{i=1}^n|x_i-y_i|^{\infty}}\)
<ul class="org-ul">
<li>sum = \(|x_i-y_i|^{\infty}\)</li>
<li>\(\sqrt[\infty]{\text{max}_{i}^n|x_i-y_i|^{\infty}}\)</li>
<li>Largest term dominates the rest, hence simplifies to:</li>
<li>\(\text{max}_i |x_i-y_i|\)</li>
</ul></li>
<li>Analogize with warehouse picking robot. The operation that takes the longest dominates the total operation time.</li>
</ul>
</div>
</div>
<div id="outline-container-orgff9c74c" class="outline-3">
<h3 id="orgff9c74c"><span class="section-number-3">4.3.</span> M4L3: K-Means Clustering</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li><b>Unsupervised</b> technique</li>
<li>Steps to implement K-Means:
<ol class="org-ol">
<li>Plot data points on suitable axes (e.g., age vs temperature, sepal width vs sepal height)</li>
<li>Let:
<ul class="org-ul">
<li>\(x_{ij}\) = attribute \(j\) of data point \(i\)</li>
<li>\(y_{ik}\) = \(1\) iif data point \(i\) in cluster \(k\), else \(0\)</li>
<li>\(z_{jk}\) = coordinate \(j\) of cluster center \(k\)</li>
<li>Mathematically, but it takes too long:
\[
       \text{Min}_{y,z}\sum_i\sum_k \sqrt{\sum_{j} (x_{ij} - z_{jk})^2}
       \]
subject to: \(\sum_k y_{ik} = 1\) for each \(i\)</li>
</ul></li>
<li>Practical method:
<ul class="org-ul">
<li>Pick \(k\) cluster centers in data</li>
<li>Assign each point to nearest cluster center</li>
<li>Recalculate cluster center (centroid)
<ul class="org-ul">
<li>Now, data points might not belong to the right cluster</li>
</ul></li>
<li>Go back to assign, then re-calc, then assign, then re-calc iteratively until stable</li>
</ul></li>
<li>K-Means is a <b>heuristic</b>, i.e.:
<ul class="org-ul">
<li>it is fast and good</li>
<li><b>not guaranteed</b> to find global best solution.</li>
</ul></li>
<li>It is expectation-maximization (EM), and alternates between expectation (<i>finding cluster centers</i>) and maximization (<i>assigning points to clusters</i>)</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org6a9cd26" class="outline-3">
<h3 id="org6a9cd26"><span class="section-number-3">4.4.</span> M4L4: Practical details for K-Means</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Algorithm just assigns outliers to nearest clusters.
</p>
<ul class="org-ul">
<li>Choosing <b>starting points</b>:
<ol class="org-ol">
<li>Run several times with different initial cluster centers</li>
<li>Algorithm is non-deterministic, <i>i.e.</i> can produce different results when run with different inputs</li>
<li>Choose the best solution from the results produced</li>
</ol></li>
<li>Handling <b>outliers</b>:
<ol class="org-ol">
<li>Discard, but may not be the 'right' answer</li>
<li>Ask why the outlier happens
<ul class="org-ul">
<li>What it means to discard or include the outlier</li>
<li>Ultimately, algorithm is just a guide. Best solution is what fits the situation.</li>
</ul></li>
</ol></li>
<li>Choosing <b>number of clusters</b>. Is adding a cluster always better?
<ul class="org-ul">
<li>It may increase the metric (total distance of each data point to their cluster center), hence clustering appears to work better.</li>
<li>However, it may defeat the purpose of clustering if every cluster just consists of one data point.</li>
</ul></li>
<li>Total distance can be compared to find the 'kink' or 'elbow'.
<ul class="org-ul">
<li>After this point, the marginal benefit of adding another cluster decreases.</li>
<li><p>
Elbow diagram:
</p>


<div id="org6f0febb" class="figure">
<p><img src="./img/04-elbow.png" alt="04-elbow.png" />
</p>
</div></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6f7391b" class="outline-3">
<h3 id="org6f7391b"><span class="section-number-3">4.5.</span> M4L5: Clustering for prediction</h3>
<div class="outline-text-3" id="text-4-5">
<blockquote>
<p>
Given a new point, which cluster should it be in?
</p>
</blockquote>
<ul class="org-ul">
<li>Is point inside cluster?</li>
<li>Otherwise, what's the nearest cluster center?</li>
<li>Asked another way: for the range of the dataset, which areas would we assign to each cluster if a new point appears there?
<ul class="org-ul">
<li>This is a <a href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi diagram</a>.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3ee3896" class="outline-3">
<h3 id="org3ee3896"><span class="section-number-3">4.6.</span> M4L6: Clustering vs Classification</h3>
<div class="outline-text-3" id="text-4-6">
<ul class="org-ul">
<li>Since both group data points&#x2026;</li>
<li>The difference is what we know about the data points.</li>
<li>For classification, the correct response is known, i.e.
<ul class="org-ul">
<li>supervised learning</li>
<li>model uses both attributes <b>and</b> response</li>
</ul></li>
<li>For clustering, the 'correct' classification is unknown
<ul class="org-ul">
<li>unsupervised learning</li>
<li>model decides clusters <b>only</b> based on the attributes</li>
</ul></li>
<li>Supervised learning is more common</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1c7ee77" class="outline-2">
<h2 id="org1c7ee77"><span class="section-number-2">5.</span> Module 05: Data preparation</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org32d908e" class="outline-3">
<h3 id="org32d908e"><span class="section-number-3">5.1.</span> M5L1: Common techniques and problems</h3>
<div class="outline-text-3" id="text-5-1">
<ol class="org-ol">
<li>Scale data
<ul class="org-ul">
<li>Outliers?</li>
</ul></li>
<li>Extraneous (unnecessary data)
<ul class="org-ul">
<li>Complicates the model and</li>
<li>Makes it harder to interpret the solution</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-org3d2aec2" class="outline-3">
<h3 id="org3d2aec2"><span class="section-number-3">5.2.</span> M5L2: Outliers</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>Types
<dl class="org-dl">
<dt>Point outliers</dt><dd>one / few points very different from others</dd>
<dt>Contextual outlier</dt><dd>Value far from other points in time (not in absolute value)</dd>
<dt>Collective outlier</dt><dd>Something missing in a range of points, but not sure exactly where. Outlier by omission.</dd>
</dl></li>
<li>How to detect?
<ul class="org-ul">
<li>Box-and-whisker plot if data can be plotted in 1-dimension
<ul class="org-ul">
<li>Box: 25/75th percentile
<ul class="org-ul">
<li>Line: 50th percentile</li>
</ul></li>
<li>Whiskers: 10/90th percentile, 5/95th, etc</li>
</ul></li>
<li>For multi-dimensional, no good way. We can still:
<ol class="org-ol">
<li>Fit a model.</li>
<li>Points with large error might be outlier</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb07bff8" class="outline-3">
<h3 id="orgb07bff8"><span class="section-number-3">5.3.</span> M5L3: What to do with outliers?</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li>Need to understand why there's outliers
<ol class="org-ol">
<li>Bad data
<ul class="org-ul">
<li>Sensor fail</li>
<li>Contaminated experiment</li>
<li>Wrong data input</li>
</ul></li>
<li>Unexpected, real, data
<ul class="org-ul">
<li>Need to understand more about the data, e.g.</li>
<li>Where it came from</li>
<li>How it was compiled</li>
<li>Unique situations</li>
</ul></li>
</ol></li>
</ul>
</div>
<div id="outline-container-org42ad1f6" class="outline-4">
<h4 id="org42ad1f6"><span class="section-number-4">5.3.1.</span> Bad data</h4>
<div class="outline-text-4" id="text-5-3-1">
<ul class="org-ul">
<li>Omit the points</li>
<li>Use imputation to replace the points</li>
</ul>
</div>
</div>
<div id="outline-container-org9207f56" class="outline-4">
<h4 id="org9207f56"><span class="section-number-4">5.3.2.</span> Real / correct data</h4>
<div class="outline-text-4" id="text-5-3-2">
<ul class="org-ul">
<li>Outliers are somewhat expected in large datasets</li>
<li>E.g., for normally-distributed data:
<ul class="org-ul">
<li>4% will be outside 2 &sigma;</li>
<li>1e6 data points = 2000 outside 3 &sigma;</li>
</ul></li>
<li>Removing <b>real</b> outliers might make model too optimistic. <i>e.g.</i> not account for actual shipments that take a long time from US to Africa</li>
<li>Outliers might be due to weather, political events</li>
</ul>
</div>
</div>
<div id="outline-container-orgac4af23" class="outline-4">
<h4 id="orgac4af23"><span class="section-number-4">5.3.3.</span> Another way to handle outliers</h4>
<div class="outline-text-4" id="text-5-3-3">
<ol class="org-ol">
<li>First build a logistic regression model
<ul class="org-ul">
<li>This estimated probability of outliers under different conditions</li>
</ul></li>
<li>Next, build the regular model i.e. estimate delivery time under <b>normal conditions</b>
<ul class="org-ul">
<li>Use data without outliers</li>
<li>Report different outcomes&#x2026;</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-orgc9fd873" class="outline-4">
<h4 id="orgc9fd873"><span class="section-number-4">5.3.4.</span> Summary</h4>
<div class="outline-text-4" id="text-5-3-4">
<ul class="org-ul">
<li>Outliers aren't predictable</li>
<li>Investigate the data in case you're wrong</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org3387ff6" class="outline-2">
<h2 id="org3387ff6"><span class="section-number-2">6.</span> Module 06: Change detection</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org880efce" class="outline-3">
<h3 id="org880efce"><span class="section-number-3">6.1.</span> M6L1: Examples</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>Usually with time series data</li>
<li>Determine if action is needed, e.g.,
<ul class="org-ul">
<li>Time for machine maintenance?</li>
<li>Have sales increased?</li>
</ul></li>
<li>Determine impact of some past action, e.g.,
<ul class="org-ul">
<li>Did new tax / increase rate decrease sales?</li>
<li>Did price discount increase sales?</li>
</ul></li>
<li>Determine changes of current actions, e.g.
<ul class="org-ul">
<li>Did voting patterns change?</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge779f64" class="outline-3">
<h3 id="orge779f64"><span class="section-number-3">6.2.</span> M6L2: Cumulative sum for change detection</h3>
<div class="outline-text-3" id="text-6-2">
<blockquote>
<p>
Answers whether mean of the observed distribution gone above a critical level
</p>
</blockquote>
<ul class="org-ul">
<li>x<sub>t</sub> is observed value at time \(t\)</li>
<li>&mu; is mean of \(x\), if no change in distribution</li>
<li>Hence, \((x_t - \mu)\) is how much the observation is above mean at time \(t\)</li>
<li>Detecting an increase
\[
  S_t = \text{max}\{ 0, s_{t-1}+(x_t-\mu-C) \}
  \]
<ul class="org-ul">
<li>Determine threshold \(T\) and ask whether S<sub>t</sub> ⩾ T?
<ul class="org-ul">
<li>If running total &lt; 0, it's irrelevant</li>
<li>There should still be some randomness</li>
<li>C is a term to control how faster S<sub>t</sub> increases</li>
</ul></li>
</ul></li>
<li>Detecting a decrease
\[
  S_t = \text{max}\{ 0, s_{t-1}+(\mu-x_t-C) \}
  \]
<ul class="org-ul">
<li>Is S<sub>t</sub> ⩾ T?</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org9f10ee5" class="outline-4">
<h4 id="org9f10ee5"><span class="section-number-4">6.2.1.</span> Interpretation</h4>
<div class="outline-text-4" id="text-6-2-1">
<ul class="org-ul">
<li>Choices of model parameters
<dl class="org-dl">
<dt>T</dt><dd>the threshold, above which alarm is raised</dd>
<dt>C</dt><dd>the control term (smaller = more sensitive)</dd>
</dl></li>
<li>Consider / trade off:
<ol class="org-ol">
<li>How costly is it to delay detection? (false negative) -&gt; if it's costly, use small C</li>
<li>How costly is false positive? -&gt; if it's costly, use big C</li>
</ol></li>
<li>Use a control chart and plot S<sub>t</sub> vs t with \(T\) as a horizontal line
<img src="./img/06-control-chart.png" alt="06-control-chart.png" /></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org20fef05" class="outline-3">
<h3 id="org20fef05"><span class="section-number-3">6.3.</span> M6L3: Ethics: Honestly reporting our results</h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li>Be faithful to data</li>
<li>Have sound conclusions drawn from the model and not your own conceptions</li>
<li>Always be honest and true to your analysis</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8d3ba92" class="outline-2">
<h2 id="org8d3ba92"><span class="section-number-2">7.</span> Module 07: Time series</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-orge003385" class="outline-3">
<h3 id="orge003385"><span class="section-number-3">7.1.</span> M7L1: Introduction to exponential smoothing</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>Data for the same response is known for many time periods</li>
<li>Examples:
<ul class="org-ul">
<li>Temperature readings</li>
<li>Price of stocks</li>
<li>Daily sales of hamburgers</li>
<li>Blood pressure readings</li>
</ul></li>
<li>Variation in time series data:
<ol class="org-ol">
<li>Trends increase or decrease</li>
<li>Cyclical variables over a year or a week</li>
</ol></li>
</ul>
</div>
<div id="outline-container-org316ce13" class="outline-4">
<h4 id="org316ce13"><span class="section-number-4">7.1.1.</span> Random variation</h4>
<div class="outline-text-4" id="text-7-1-1">
<ul class="org-ul">
<li>No underlying reason for the variation</li>
</ul>
</div>
</div>
<div id="outline-container-org29b6837" class="outline-4">
<h4 id="org29b6837"><span class="section-number-4">7.1.2.</span> Definitions:</h4>
<div class="outline-text-4" id="text-7-1-2">
<ul class="org-ul">
<li>\(S_t\): expected <b>baseline</b> response at time period \(t\)</li>
<li>\(x_t\): the observed response at \(t\)</li>
<li>Seeing a increase over time, is it
<ol class="org-ol">
<li>A real increase?</li>
<li>Random?</li>
</ol></li>
<li>There are two ways to answer:
<ol class="org-ol">
<li>It's a real increase, hence \(S_t = x_t\)
<ul class="org-ul">
<li>the observed reading is real indicator of revised baseline</li>
</ul></li>
<li>It's random, hence \(S_t = S_{t-1}\)
<ul class="org-ul">
<li>today's baseline = yesterday's baseline</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgd1db548" class="outline-4">
<h4 id="orgd1db548"><span class="section-number-4">7.1.3.</span> Exponential smoothing method</h4>
<div class="outline-text-4" id="text-7-1-3">
<p>
Combines both, i.e.
\(S_t = \alpha x_t + (1-\alpha)S_{t-1}\)
</p>
<ul class="org-ul">
<li><p>
0 &lt; &alpha; &lt;1
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&alpha;</th>
<th scope="col" class="org-left">example value of &alpha;</th>
<th scope="col" class="org-left">randomness</th>
<th scope="col" class="org-left">trust</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">small</td>
<td class="org-left">&rarr; 0 (e.g. 0.01)</td>
<td class="org-left">high</td>
<td class="org-left">previous baseline i.e. \(S_{t-1}\)</td>
</tr>

<tr>
<td class="org-left">large</td>
<td class="org-left">&rarr; 1 (e.g. 0.99)</td>
<td class="org-left">low</td>
<td class="org-left">today's estimate i.e. \(x_t\)</td>
</tr>
</tbody>
</table></li>

<li>How to start? \(S_1 = x_1\)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga21adb8" class="outline-3">
<h3 id="orga21adb8"><span class="section-number-3">7.2.</span> M7L2: Trend and cyclic effects</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Complexities!
</p>
<ul class="org-ul">
<li>Trends, increasing or decreasing</li>
<li>Cyclical patterns, e.g. annual, weekly, daily</li>
</ul>
</div>
<div id="outline-container-org782969e" class="outline-4">
<h4 id="org782969e"><span class="section-number-4">7.2.1.</span> Trends</h4>
<div class="outline-text-4" id="text-7-2-1">
<ul class="org-ul">
<li>\(T_t\): the trend at time period \(t\)</li>
<li>\(S_t = \alpha x_t + (1-\alpha)(S_{t-1}+T_{t-1})\)</li>
<li>\(T_t = \beta(S_t - S_{t-1}) + (1-\beta)T_{t-1}\)</li>
<li>Initial condition: \(T_1=0\)</li>
</ul>
</div>
</div>
<div id="outline-container-org92f5a24" class="outline-4">
<h4 id="org92f5a24"><span class="section-number-4">7.2.2.</span> Cyclical patterns</h4>
<div class="outline-text-4" id="text-7-2-2">
<ul class="org-ul">
<li>Make cycles additive: behaves like trend</li>
<li>Make cycles multiplicative: more notation required
<ul class="org-ul">
<li>L = length of cycle</li>
<li>\(C_t\) = the multiplicative seasonality factor
<ul class="org-ul">
<li>This inflates or deflates the observation</li>
</ul></li>
<li>New baseline formula
\[
    S_t = \frac{\alpha x_t}{C_{t-L}} + (1-\alpha)(S_{t-1}+T_{t-1})
    \]</li>
<li>Need to use the factor from \(L\) time periods ago
<ul class="org-ul">
<li>as that's the most recent cyclic factor we have from that part of the cycle</li>
</ul></li>
<li>Update the cyclic factor in a similar way i.e.:
<ul class="org-ul">
<li>\(C_t = \gamma(x_t/S_t) + (1-\gamma)(C_{t-L})\)</li>
<li>C<sub>1</sub>, &#x2026;, C<sub>L</sub> = 1
<ul class="org-ul">
<li>meaning there's no initial cyclic effect</li>
</ul></li>
<li>If C = 1.1 on Sunday:
<ul class="org-ul">
<li>sales are higher by 10% just because it's Sunday</li>
</ul></li>
</ul></li>
<li>Initial values: first \(L\) are set to 1. Multiplying by 1 = no effect</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgcc12e7f" class="outline-4">
<h4 id="orgcc12e7f"><span class="section-number-4">7.2.3.</span> Summary</h4>
<div class="outline-text-4" id="text-7-2-3">
<ul class="org-ul">
<li>Exponential smoothing
<ul class="org-ul">
<li>Single</li>
<li>Double (with trend)</li>
<li>Triple (with trend and cyclic effects)
<ul class="org-ul">
<li>AKA Winter's method, or Holt-Winters</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf08aee7" class="outline-3">
<h3 id="orgf08aee7"><span class="section-number-3">7.3.</span> M7L3: Etymology (what the name means)</h3>
<div class="outline-text-3" id="text-7-3">
<p>
Example equation when \(\alpha = \frac{1}{2}\):
\(S_t = 0.5 x_t + 0.5 S_{t-1}\)
</p>
</div>
<ol class="org-ol">
<li><a id="org66db7a3"></a>Smoothing<br />
<div class="outline-text-5" id="text-7-3-0-1">
<ul class="org-ul">
<li>Note: when x<sub>t</sub> is high, S<sub>t</sub> is <b>not</b> as high, as \((1-\alpha)S_{t-1}\) pulls it down</li>
<li>Conversely: when x<sub>t</sub> is low, S<sub>t</sub> is <b>not</b> as low, as \((1-\alpha)S_{t-1}\) pulls it up</li>
<li>Peaks and valleys are smoothed out
<img src="./img/07-smoothed-graph.png" alt="07-smoothed-graph.png" /></li>
</ul>
</div>
</li>
<li><a id="org09dd355"></a>Exponential<br />
<div class="outline-text-5" id="text-7-3-0-2">
<ul class="org-ul">
<li>Each \(S_{t-1}\) actually contains every previous value of x!
<ul class="org-ul">
<li>When written or expanded out, e.g.
\[
    S_t = \alpha x_t + (1-\alpha)S_{t-1}
    \]
\[
    S_{t} = \alpha x_t + (1-\alpha)[\alpha x_{t-1} + (1-\alpha)S_{t-2}]
    \]
\[
    S_{t} = \alpha x_t + (1-\alpha)\alpha x_{t-1} + (1-\alpha)^2S_{t-2}
    \]</li>
</ul></li>
<li>Each S<sub>t</sub> is weighed by (1-&alpha;) to an increasing <b>exponent</b></li>
<li>This means not only the current observation matters; instead, every past observation contributes to the current baseline estimate</li>
<li>However, more recent observations are more important as they have higher weight</li>
</ul>
</div>
</li>
</ol>
<div id="outline-container-org6d32a56" class="outline-4">
<h4 id="org6d32a56"><span class="section-number-4">7.3.1.</span> Summary</h4>
<div class="outline-text-4" id="text-7-3-1">
<ul class="org-ul">
<li>Exponential smoothing smooths out jumps in observed data</li>
<li>It's an exponential weighting of all past observations</li>
<li>More recent observations are more important to the current baseline estimate</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org85e2565" class="outline-3">
<h3 id="org85e2565"><span class="section-number-3">7.4.</span> M7L4: Forecasting</h3>
<div class="outline-text-3" id="text-7-4">
<ul class="org-ul">
<li>Recap: \(S_t = \alpha x_t + (1-\alpha)S_{t-1}\)</li>
<li>Prediction:
<ul class="org-ul">
<li>\(S_{t+1} = \alpha x_{t+1} + (1-\alpha)S_{t}\)</li>
<li>However x<sub>t+1</sub> is unknown</li>
<li>Best guess for x<sub>t+1</sub> is \(S_t\)</li>
</ul></li>
<li>Our forecast for \(t+1\) is hence (after substituting):
\[
  F_{t+1}=\alpha S_t + (1-\alpha) S_t \\
  F_{t+1} = S_t \\
  F_{t+k} = S_t \text{when } k=1, 2, ...
  \]
<ul class="org-ul">
<li>note that forecast error becomes larger for larger \(k\)</li>
</ul></li>
<li>If including trend:
\[
  S_t = \alpha x_t + (1-\alpha)(S_{t-1}+T_{t-1}) \\
  T_t = \beta (S_t-S_{t-1})+(1-\beta)T_{t-1} \\
  F_{t+1} = S_t + T_t \\
  F_{t+k} = S_t + kT_t, k=1,2,...
  \]
<dl class="org-dl">
<dt>Best estimate of next baseline</dt><dd>the most current baseline estimate</dd>
<dt>Best estimate of the trend</dt><dd>the most current trend estimate</dd>
</dl></li>
<li>If including multiplicative seasonality:
\[
  S_t = \alpha x_t/C_{t-L} + (1-\alpha)(S_{t-1}+T_{t-1})
  F_{t+1} = (S_t+T_t)C_{(t+1)-L}
  \]
<dl class="org-dl">
<dt>Best estimate of next time period seasonal factor</dt><dd>the corresponding (lagged) seasonal factor, i.e. \(C_{t+1}=C_{t+1-L}\)</dd>
</dl></li>
<li>Finding the right &alpha;, &beta;, &gamma;: use optimization, to be covered in future
<ul class="org-ul">
<li>\(\min{(F_t-x_t)^2}\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org16ef322" class="outline-3">
<h3 id="org16ef322"><span class="section-number-3">7.5.</span> M3L5: ARIMA</h3>
<div class="outline-text-3" id="text-7-5">
<p>
AutoRegressive Integrated Moving Average.
</p>
<ul class="org-ul">
<li>ARIMA <b>theory</b> is not covered in IAM.</li>
</ul>
</div>
<div id="outline-container-orgb81e0c7" class="outline-4">
<h4 id="orgb81e0c7"><span class="section-number-4">7.5.1.</span> (I): Differences</h4>
<div class="outline-text-4" id="text-7-5-1">
<ul class="org-ul">
<li>Exponential smoothing assumes <b>stationary</b> data, i.e.
<ul class="org-ul">
<li>mean, variance, other measures are constant over time</li>
</ul></li>
<li>ARIMA works for data that's not stationary
<ul class="org-ul">
<li>if differences in data are stationary
<dl class="org-dl">
<dt>1st order difference D<sub>(1)</sub></dt><dd>difference of consecutive observations, i.e. \(D_{(1)t}=(x_t-x_{t-1})\)</dd>
<dt>2nd order difference D<sub>(2)</sub></dt><dd>difference of the differences i.e.
\(D_{(2)t}=(x_t-x_{t-1})-(x_{t-1}-x_{t-2})\)</dd>
<dt>d<sup>th</sup> order difference D<sub>(d)</sub></dt><dd>diff&#x2026; d times</dd>
</dl></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga0cddfc" class="outline-4">
<h4 id="orga0cddfc"><span class="section-number-4">7.5.2.</span> (II): Autogression</h4>
<div class="outline-text-4" id="text-7-5-2">
<ul class="org-ul">
<li>Predicting current values based on previous period's values</li>
<li>Regression: predicting value based on other factors</li>
<li>Auto: use earlier values to predict. Only works with time series</li>
<li>When used to forecast, exponential smoothing is an order-&infin; autoregressive model
<ul class="org-ul">
<li>All previous values are used to make current prediction</li>
</ul></li>
<li>Order-p autoregressive model: S<sub>t</sub> is function of \(\{x_t, x_{t-1}, ..., x_{t-(p-1)}\}\)
<ul class="org-ul">
<li>Only go back \(p\) periods</li>
</ul></li>
<li>ARIMA: combines autoregression and differencing
<ul class="org-ul">
<li>Autoregression on the differences</li>
<li>Use \(p\) time periods of previous observations to predict \(d^{th}\) order differences</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf719213" class="outline-4">
<h4 id="orgf719213"><span class="section-number-4">7.5.3.</span> (III): Moving Average</h4>
<div class="outline-text-4" id="text-7-5-3">
<ul class="org-ul">
<li>Use previous errors &epsilon;<sub>t</sub> as predictors
<ul class="org-ul">
<li>\(\epsilon_t = (\hat{x_t}-x_t)\)</li>
</ul></li>
<li>Order-q moving average
<ul class="org-ul">
<li>go back \(q\) time periods</li>
<li>\(\epsilon_{t-1}, \epsilon_{t-2}, ..., \epsilon_{t-q}\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge8d63a5" class="outline-4">
<h4 id="orge8d63a5"><span class="section-number-4">7.5.4.</span> ARIMA model</h4>
<div class="outline-text-4" id="text-7-5-4">
<p>
ARIMA (p,d,q) model
</p>
<ul class="org-ul">
<li>\(d^{th}\) order differences</li>
<li>\(p^{th}\) order autoregression</li>
<li>\(q^{th}\) order moving average</li>
<li><a id="org95ec7a4"></a>Equation:
\[
  D_{(d)t} = \mu + \sum^p_{i=1}\alpha_i D_{(d)t-i} - \sum^q_{i=1} \theta_i (\hat{x}_{t-i}-x_{t-i})
  \]</li>
<li>Software can find \(d, p, q\)</li>
<li>Extensions
<ol class="org-ol">
<li>Add seasonality (out of scope for IAM)</li>
<li>Specific models:
<dl class="org-dl">
<dt>ARIMA(0,0,0)</dt><dd>white noise</dd>
<dt>ARIMA(0,1,0)</dt><dd>random walk</dd>
<dt>ARIMA(p,0,0)</dt><dd>AR (autoregressive) model</dd>
<dt>ARIMA(0,0,q)</dt><dd>MA (moving avg) model</dd>
<dt>ARIMA(0,1,1)</dt><dd>basic exponential smoothing model</dd>
</dl></li>
</ol></li>
<li>Can be used for short-term forecasting
<ul class="org-ul">
<li>ARIMA is better than ES when data is more stable with fewer peaks, valleys, outliers</li>
<li>ARIMA needs 40+ historical data points to work well</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5731307" class="outline-3">
<h3 id="org5731307"><span class="section-number-3">7.6.</span> M7L6: GARCH</h3>
<div class="outline-text-3" id="text-7-6">
<dl class="org-dl">
<dt>GARCH</dt><dd>Generalized Autoregressive Conditional Heteroskedasticity</dd>
</dl>
<p>
To estimate or forecast the variance
</p>
</div>
<div id="outline-container-org1f709d2" class="outline-4">
<h4 id="org1f709d2"><span class="section-number-4">7.6.1.</span> Variance</h4>
<div class="outline-text-4" id="text-7-6-1">
<ul class="org-ul">
<li>Estimates the amount of error</li>
<li>E.g. forecasting demand for trucks
<ul class="org-ul">
<li>tell you how much forecast might be higher/lower than the actual value you see (later) to plan accordingly</li>
</ul></li>
<li>In investment (portfolio optimization):
<ul class="org-ul">
<li>Balance the expected return in investment with amount of volatility.
<dl class="org-dl">
<dt>Riskier</dt><dd>higher expected return</dd>
<dt>Less risky</dt><dd>lower expected return</dd>
</dl></li>
<li>Variance is a proxy for amount of volatility or risk</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6ae355f" class="outline-4">
<h4 id="org6ae355f"><span class="section-number-4">7.6.2.</span> GARCH</h4>
<div class="outline-text-4" id="text-7-6-2">
<p>
\[
\sigma^2_t = \omega + \sum^p_{i=1}\beta_i\sigma^2_{t-1}+\sum^q_{i=1} \gamma_i \epsilon^2_{t-i}
\]
It looks very similar to <a href="#org95ec7a4">ARIMA Equation</a>, but:
<b>Differences</b>:
</p>
<dl class="org-dl">
<dt>GARCH deals with variances and squared errors</dt><dd>ARIMA deals with observations and linear errors</dd>
<dt>GARCH deals with raw variances</dt><dd>ARIMA deals with differences of variances</dd>
</dl>
</div>
</div>
<div id="outline-container-org5065708" class="outline-4">
<h4 id="org5065708"><span class="section-number-4">7.6.3.</span> Summary - three models for time series analysis</h4>
<div class="outline-text-4" id="text-7-6-3">
<ol class="org-ol">
<li>Exponential smoothing</li>
<li>ARIMA, a generalization of exponential smoothing</li>
<li>GARCH, an ARIMA-like model for analyzing variance</li>
</ol>
</div>
</div>
</div>
</div>
<div id="outline-container-org2474268" class="outline-2">
<h2 id="org2474268"><span class="section-number-2">8.</span> Module 08: Regression</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org0e8e452" class="outline-3">
<h3 id="org0e8e452"><span class="section-number-3">8.1.</span> M8L1: Intro to Regression</h3>
<div class="outline-text-3" id="text-8-1">
</div>
<div id="outline-container-orgade8387" class="outline-4">
<h4 id="orgade8387"><span class="section-number-4">8.1.1.</span> What questions can regression answer?</h4>
<div class="outline-text-4" id="text-8-1-1">
<ol class="org-ol">
<li>How do systems work? (descriptive)</li>
<li>What will happen in the future? (predictive)</li>
</ol>
</div>
</div>
<div id="outline-container-org06f2c37" class="outline-4">
<h4 id="org06f2c37"><span class="section-number-4">8.1.2.</span> Simple linear regression</h4>
<div class="outline-text-4" id="text-8-1-2">
<ul class="org-ul">
<li>Linear regression with one predictor, e.g. \(y = a_0 + a_1x+1\)
<ul class="org-ul">
<li>Date point \(i\)'s prediction error
\(= y_i - \hat{y}_i -(a_0+a_1x_1)\)</li>
<li>Sum of squared errors
\(=\sum^n_{i=1}(y_i - \hat{y}_i)^2\)
\(=\sum^n_{i=1}(y_i-(a_0+a_1x_1))^2\)</li>
</ul></li>
<li>Best fit regression line
<ul class="org-ul">
<li>Minimizes sum of squared errors</li>
<li>Defined by a<sub>0</sub> and a<sub>1</sub></li>
</ul></li>
<li>Underlying math
<ul class="org-ul">
<li>Minimize convex quadratic function</li>
<li>Set partial derivatives to 0</li>
<li>Solve simultaneous equations</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org155ed89" class="outline-3">
<h3 id="org155ed89"><span class="section-number-3">8.2.</span> M8L2: Maximum Likelihood and Information Criteria</h3>
<div class="outline-text-3" id="text-8-2">
</div>
<div id="outline-container-org273b542" class="outline-4">
<h4 id="org273b542"><span class="section-number-4">8.2.1.</span> Likelihood</h4>
<div class="outline-text-4" id="text-8-2-1">
<ul class="org-ul">
<li>Measure the probability density of a parameter set</li>
<li id="Maximum likelihood">the parameters that give the highest probability</li>
<li>Assuming:
<ol class="org-ol">
<li>Errors are normally distributed with mean 0, variance &sigma;<sup>2</sup>, independently and identically distributed</li>
<li>Observations are z<sub>1</sub> to z<sub>n</sub></li>
<li>Model estimates are y<sub>1</sub> to y<sub>n</sub></li>
</ol></li>
<li>Probability density of observing z<sub>i</sub> if true value y<sub>i</sub> is
\[
  \frac{1}{\sigma\sqrt(2\pi)}e^-\frac{(z_u-y_i)^2}{2\sigma^2}
  \]</li>
<li>Joint density over \(n\) terms
\[
  \prod^n_{i=1}\frac{1}{\sigma\sqrt(2\pi)}e^-\frac{(z_u-y_i)^2}{2\sigma^2} \\
  = (\frac{1}{\sigma\sqrt{2\pi}})^n e^-\frac{1}{2\sigma^2}\sum^n_{i=1}(z_i-y_i)^2
  \]</li>
<li>Hence <b>to minimize</b> \(\sum^n_{i=1}(z_i-y_i)^2\) over a<sub>0</sub>, &#x2026;, a<sub>m</sub>:
equals to minimizing \(\sum^n_{i=1}(z_i-(a_0 + \sum^m_{j=1}a_jx_{ij}))^2\)</li>
</ul>
</div>
</div>
<div id="outline-container-org02149ce" class="outline-4">
<h4 id="org02149ce"><span class="section-number-4">8.2.2.</span> Maximum likelihood fitting</h4>
<div class="outline-text-4" id="text-8-2-2">
<ul class="org-ul">
<li>Simplest example is regression with i.i.d. errors</li>
<li>Complex examples:
<ul class="org-ul">
<li>Different estimation formulas</li>
<li>Different error assumptions</li>
</ul></li>
<li>Good software can handle complex cases</li>
</ul>
</div>
</div>
<div id="outline-container-orgce85cb7" class="outline-4">
<h4 id="orgce85cb7"><span class="section-number-4">8.2.3.</span> Akaike Information Criterion</h4>
<div class="outline-text-4" id="text-8-2-3">
<dl class="org-dl">
<dt>L<sup>*</sup></dt><dd>maximum likelihood value</dd>
<dt>\(k\)</dt><dd>number of parameters being estimated</dd>
<dt>AIC</dt><dd>\(2k-2\log(L^{*})\)</dd>
<dt>Penalty term</dt><dd>balances likelihood with simplicity, helps avoid overfitting</dd>
</dl>
</div>
<ol class="org-ol">
<li><a id="org7db7f1c"></a>For simple regression<br />
<div class="outline-text-5" id="text-8-2-3-1">
<dl class="org-dl">
<dt>AIC</dt><dd>\[
  2(m+1) - 2\log(\frac{1}{\sigma\sqrt(2\pi)}^ne^{-\frac{1}{2\sigma^2}}\sum^n_{i=1}(z_i-(a_0 + \sum^m_{j=1}a_jx_{ij}))^2)
  \]</dd>
<dt>Preference</dt><dd>smaller AIC models</dd>
<dt>Requires</dt><dd>infinitely many data points</dd>
</dl>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgf57a564" class="outline-4">
<h4 id="orgf57a564"><span class="section-number-4">8.2.4.</span> Corrected AIC (AIC<sub>c</sub>)</h4>
<div class="outline-text-4" id="text-8-2-4">
<p>
Use for smaller datasets.
\[
AIC_c = AIC + \frac{2k(k+1)}{n-k-1} \\
= 2k-2\log(L^{*})+\frac{2k(k+1)}{n-k-1}
\]
</p>
</div>
</div>
<div id="outline-container-org3f30f7a" class="outline-4">
<h4 id="org3f30f7a"><span class="section-number-4">8.2.5.</span> AIC<sub>c</sub> example</h4>
<div class="outline-text-4" id="text-8-2-5">
<ul class="org-ul">
<li>Model 1: AIC 75; Model 2: AIC 80</li>
<li>Relative likelihood equals:
\[
  e^\frac{AIC_1-AIC_2}{2} \\
  = 8.2%
  \]</li>
<li>Hence, Model 2 (larger AIC) is 8.2% as likely as Model 1 to be better
<ul class="org-ul">
<li>Model 1 is probably better</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgbf53aba" class="outline-4">
<h4 id="orgbf53aba"><span class="section-number-4">8.2.6.</span> Bayesian Information Criterion</h4>
<div class="outline-text-4" id="text-8-2-6">
<ul class="org-ul">
<li>BIC:
\[
  k\log(n)-2\log(L^{*})
  \]</li>
<li>Similar to AIC, but
<ul class="org-ul">
<li>bigger penalty term than AIC's penalty term</li>
<li>encourages models with fewer parameters</li>
</ul></li>
<li>Use BIC when there are <b>more data points</b> than parameters</li>
<li><p>
Rule of thumb for |BIC<sub>1</sub> - BIC<sub>2</sub>|
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">value</th>
<th scope="col" class="org-left">interpretation for smaller BIC model</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">&gt;10</td>
<td class="org-left">very likely better</td>
</tr>

<tr>
<td class="org-right">6-10</td>
<td class="org-left">likely better</td>
</tr>

<tr>
<td class="org-right">2-6</td>
<td class="org-left">somewhat likely better</td>
</tr>

<tr>
<td class="org-right">0-2</td>
<td class="org-left">slightly likely better</td>
</tr>
</tbody>
</table></li>
</ul>
</div>
</div>
<div id="outline-container-orgce85ea2" class="outline-4">
<h4 id="orgce85ea2"><span class="section-number-4">8.2.7.</span> Summary</h4>
<div class="outline-text-4" id="text-8-2-7">
<ul class="org-ul">
<li>No definite rules for AIC, BIC, maximum likelihood</li>
<li>Can look at all 3 to decide what's best</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf5efa9b" class="outline-3">
<h3 id="orgf5efa9b"><span class="section-number-3">8.3.</span> M8L3: Using Regression</h3>
<div class="outline-text-3" id="text-8-3">
</div>
<div id="outline-container-org4344c57" class="outline-4">
<h4 id="org4344c57"><span class="section-number-4">8.3.1.</span> Regression coefficients</h4>
<div class="outline-text-4" id="text-8-3-1">
<ul class="org-ul">
<li>a<sub>0</sub>, a<sub>1</sub>, &#x2026;, a<sub>m</sub> for the equation:</li>
<li>\(y=a_0+a_1x_1+...+a_mx_m\)</li>
<li>Example for baseball, descriptive question:
<ul class="org-ul">
<li>How many runs is associated with every homerun</li>
<li>Response: how many runs are scored by a team</li>
<li>Predictors:
<ol class="org-ol">
<li>Number of HR</li>
<li>Triples</li>
<li>Doubles</li>
<li>Singles</li>
<li>Outs</li>
<li>Double Plays</li>
<li>Stolen bases, etc</li>
</ol></li>
<li>Equation:
\[
    \text{Runs scored} = a_0 + a_1\text{Number of HR} + a_2\text{Number of triples} + ... + a_7\text{Number of stolen bases}
    \]</li>
<li>a<sub>1</sub> = 1.4
<ul class="org-ul">
<li>Means that every HR adds 1.4 runs scored on average, ceteris paribus</li>
</ul></li>
</ul></li>
<li>Example: height, predictive question:
<ul class="org-ul">
<li>How tall will a 2-year old be as an adult?</li>
<li>Response: a person's adult height</li>
<li>Predictors:
<ol class="org-ol">
<li>Father's height</li>
<li>Mother's height</li>
<li>Height at age 2</li>
<li>Male, female</li>
</ol></li>
<li>Equation:
\[
    \text{Adult height} = a_0 + a_1\text{Father's height} + a_2 \text{Mother's height} + ... + a_4\text{Male or female}
    \]</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9a11631" class="outline-3">
<h3 id="org9a11631"><span class="section-number-3">8.4.</span> M8L4: Causation vs Correlation</h3>
<div class="outline-text-3" id="text-8-4">
<dl class="org-dl">
<dt>Causation</dt><dd>one thing causes another thing</dd>
<dt>Correlation</dt><dd>two things tend to happen / not happen together, but neither one causes the other</dd>
</dl>
</div>
<div id="outline-container-orgd60501e" class="outline-4">
<h4 id="orgd60501e"><span class="section-number-4">8.4.1.</span> Example: winter recreation</h4>
<div class="outline-text-4" id="text-8-4-1">
<ul class="org-ul">
<li>y: hours per day spent outdoors in winter</li>
<li>x<sub>1</sub>: city's average daily winter temperature</li>
<li>Equation: \(y=a_0+a_1x_1\)</li>
<li>Correlation between y and x<sub>1</sub>
<ul class="org-ul">
<li>with low p-value of a<sub>1</sub></li>
</ul></li>
<li>Does higher temperature in winter cause people to go outside?
<ul class="org-ul">
<li>Probably</li>
</ul></li>
<li>Reversing the equation:
<ul class="org-ul">
<li>y: hours per day spent outdoors in winter</li>
<li>x<sub>1</sub>: city's average daily winter temperature</li>
<li>Equation: \(x_1 = b_0 + b_1y\)</li>
<li>Same correlation between y, x<sub>1</sub>
<ul class="org-ul">
<li>Same p-value of b<sub>1</sub> and a<sub>1</sub></li>
</ul></li>
<li>Hence, does spending more time outside <b>cause</b> higher winter temperatures?
<ul class="org-ul">
<li>No</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3b1dcb9" class="outline-4">
<h4 id="org3b1dcb9"><span class="section-number-4">8.4.2.</span> Example: tiredness vs scruffiness</h4>
<div class="outline-text-4" id="text-8-4-2">
<ul class="org-ul">
<li>Neither one caused another, they're just related to a common tired factor, kids</li>
</ul>
</div>
</div>
<div id="outline-container-org446ad68" class="outline-4">
<h4 id="org446ad68"><span class="section-number-4">8.4.3.</span> How to tell causation?</h4>
<div class="outline-text-4" id="text-8-4-3">
<ul class="org-ul">
<li>When is there causation?
<ol class="org-ol">
<li>Cause before effect</li>
<li>Idea of causation makes sense</li>
<li>No outside factors can cause the relationship (hard to ensure this - need to consider <b>all</b> other factors)</li>
</ol></li>
<li>Be careful before claiming causation</li>
</ul>
</div>
</div>
<div id="outline-container-orgef81325" class="outline-4">
<h4 id="orgef81325"><span class="section-number-4">8.4.4.</span> Meaningless correlations</h4>
<div class="outline-text-4" id="text-8-4-4">
<ul class="org-ul">
<li>Per capita consumption of mozzarella with number of civil engineering doctorates awarded</li>
<li><a href="http://tylervigen.com/spurious-correlations">link</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3efe009" class="outline-3">
<h3 id="org3efe009"><span class="section-number-3">8.5.</span> M8L5: Transformations and Interactions</h3>
<div class="outline-text-3" id="text-8-5">
<ul class="org-ul">
<li>Recall: \(y=a_0+a_1x_1+...+a_mx_m\)</li>
<li>What if the fit isn't linear for x?</li>
<li>Answer: transforming the data!</li>
</ul>
</div>
<div id="outline-container-orgdbcda60" class="outline-4">
<h4 id="orgdbcda60"><span class="section-number-4">8.5.1.</span> Transforming the data</h4>
<div class="outline-text-4" id="text-8-5-1">
<ul class="org-ul">
<li>Quadratic regression, e.g.
\[
  y = a_0 + a_1 x_1 + a_2 x_1^2
  \]</li>
<li>Trigonometric:
\[
  y = a_0 + a_2 \sin(x^2)
  \]</li>
<li>Response transform:
\[
  \log(y) = a_0 + a_1 x_1 + ... + a_m x_m
  \]</li>
<li>etc.</li>
<li>Box-Cox transforms can be automated</li>
</ul>
</div>
</div>
<div id="outline-container-org8297be6" class="outline-4">
<h4 id="org8297be6"><span class="section-number-4">8.5.2.</span> Interaction terms, e.g. product of inputs</h4>
<div class="outline-text-4" id="text-8-5-2">
<ul class="org-ul">
<li>Child's heights might be influenced by <b>product</b> of father and mother's heights (\(x_1x_2\))</li>
<li>\(y = a_0 + a_1 x_1 + a_2 x_2 + a_3 (x_1 x_2)\)</li>
<li><b>Treat x<sub>1</sub> x<sub>2</sub> as new input, x<sub>3</sub></b></li>
<li>Then find best fit coefficients in the last module</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgb63db86" class="outline-3">
<h3 id="orgb63db86"><span class="section-number-3">8.6.</span> M6L6: Output</h3>
<div class="outline-text-3" id="text-8-6">
</div>
<div id="outline-container-orgb82c4e3" class="outline-4">
<h4 id="orgb82c4e3"><span class="section-number-4">8.6.1.</span> p-Values</h4>
<div class="outline-text-4" id="text-8-6-1">
<ul class="org-ul">
<li>Estimates the <b>probability</b> that coefficient is actually 0
<ul class="org-ul">
<li>A hypothesis test</li>
</ul></li>
<li>If p-value is big (&gt;0.05), the coefficient is likely 0, hence remove its attribute from the model</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgd6fe616"></a>Other thresholds<br />
<div class="outline-text-5" id="text-8-6-1-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Threshold</th>
<th scope="col" class="org-left">Number of factors included</th>
<th scope="col" class="org-left">Risk</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Higher (&gt;0.05)</td>
<td class="org-left">More factors included</td>
<td class="org-left">Irrelevant factors included</td>
</tr>

<tr>
<td class="org-left">Lower (&lt;0.05)</td>
<td class="org-left">Less factors included</td>
<td class="org-left">Relevant factors left out</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="orgf0eed5c"></a>Warnings<br />
<div class="outline-text-5" id="text-8-6-1-2">
<ul class="org-ul">
<li>With lots of data:
<ul class="org-ul">
<li>p-values can get small and seem significant even when attributes aren't related to response</li>
</ul></li>
<li>Even when meaningful, p-values only represent <b>probabilities</b>
<ol class="org-ol">
<li>With 100 attributes having p-value 0.02:</li>
<li>Each of them have 2% chance of <b>not</b> being significant</li>
<li>Hence on average 2/100 are actually irrelevant</li>
</ol></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org3b50b9b" class="outline-4">
<h4 id="org3b50b9b"><span class="section-number-4">8.6.2.</span> Confidence interval</h4>
<div class="outline-text-4" id="text-8-6-2">
<ul class="org-ul">
<li>Mostly given at 95% level around the coefficient</li>
<li>Range of where the coefficient probably lies</li>
<li>And how close that is to zero</li>
<li>Related to p-value</li>
</ul>
</div>
</div>
<div id="outline-container-org4e11089" class="outline-4">
<h4 id="org4e11089"><span class="section-number-4">8.6.3.</span> T-statistic</h4>
<div class="outline-text-4" id="text-8-6-3">
<ul class="org-ul">
<li>\(\frac{\text{Coefficient}}{\text{Std error}}\)</li>
<li>Related to p-value</li>
</ul>
</div>
</div>
<div id="outline-container-orgb54543d" class="outline-4">
<h4 id="orgb54543d"><span class="section-number-4">8.6.4.</span> Coefficient itself</h4>
<div class="outline-text-4" id="text-8-6-4">
<ul class="org-ul">
<li>If very small, then when multiplied by attribute, it's likely irrelevant</li>
</ul>
</div>
</div>
<div id="outline-container-org0c9f4ee" class="outline-4">
<h4 id="org0c9f4ee"><span class="section-number-4">8.6.5.</span> \(R^2\)</h4>
<div class="outline-text-4" id="text-8-6-5">
<ul class="org-ul">
<li>Estimate of how much variability can be explained by the model</li>
<li>E.g. R<sup>2</sup> = 0.59:
<ul class="org-ul">
<li>0.59 of data variability can be explained by the model</li>
<li>remaining 0.41 is either:
<ul class="org-ul">
<li>random variation</li>
<li>or other factors</li>
</ul></li>
</ul></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org3c02781"></a>Adjusted \(R^2\)<br />
<div class="outline-text-5" id="text-8-6-5-1">
<ul class="org-ul">
<li>Accounts for (penalizes) the number of attributes used</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org6f4dad8" class="outline-2">
<h2 id="org6f4dad8"><span class="section-number-2">9.</span> Module 09: Advanced Data Preparation</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-org085f2c5" class="outline-3">
<h3 id="org085f2c5"><span class="section-number-3">9.1.</span> M9L1: Box-Cox Transformations</h3>
<div class="outline-text-3" id="text-9-1">
<ul class="org-ul">
<li>Models may require data to be normally distributed</li>
<li>What happens when this assumption isn't valid in the data?
<ul class="org-ul">
<li>Results will have bias in this case</li>
<li>Data exhibits heteroskedasicity</li>
<li>i.e., variances are not i.i.d.</li>
</ul></li>
<li>Another example is time series data, where later values have higher variance</li>
<li>Box-Cox is a logarithmic transformation that:
<ol class="org-ol">
<li>Stretches smaller range to increase variability</li>
<li>Shrinks larger range to reduce variability</li>
</ol></li>
<li>E.g. \(t(y)=\frac{y^\lambda-a}{\lambda}\)
<ul class="org-ul">
<li>t(y) can become close to normally distributed</li>
</ul></li>
<li>Need to remember to check for normality (e.g., with Normal Q-Q plot)</li>
</ul>
</div>
</div>
<div id="outline-container-org0048363" class="outline-3">
<h3 id="org0048363"><span class="section-number-3">9.2.</span> M9L2: Detrending</h3>
<div class="outline-text-3" id="text-9-2">
<ul class="org-ul">
<li>For time series data with trends, i.e. an increase or decrease over time
<ul class="org-ul">
<li>For example: increase in price of gold over time but need to account for inflation over time (value of $ decreases over time)</li>
<li>The trend if not correct can mess up a factor-based analysis</li>
<li>Can detrend:
<ul class="org-ul">
<li>Response</li>
<li>Predictors</li>
<li>Factor-based model (consider whenever using these models)
<ul class="org-ul">
<li>Regression, SVM, etc.</li>
</ul></li>
</ul></li>
</ul></li>
<li>How to detrend:
<ul class="org-ul">
<li>Factor by factor for one-dimensional regression, y=a<sub>0</sub>+a<sub>1x</sub>
<ul class="org-ul">
<li>Simple, works well to remove trend for factor-based analysis</li>
<li>This requires going factor by factor and fitting a linear regression model on it</li>
<li>E.g. for simple linear regression for gold prices:
<ul class="org-ul">
<li>Price = - 45600 + 23.2xYear</li>
<li>Detrend end price = Actual price -(-45600+23.2+year)</li>
<li>This produces a similar graph to the inflation-adjusted rate</li>
<li>Useful when we don’t know the response (as in most cases)</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org42a4951" class="outline-3">
<h3 id="org42a4951"><span class="section-number-3">9.3.</span> M9L3: Intro to PCA</h3>
<div class="outline-text-3" id="text-9-3">
<ul class="org-ul">
<li>Works on high dimensional and correlated data
<ul class="org-ul">
<li>Which subset of features are important to predict response?</li>
<li>e.g. which stocks can predict how well the market performs the next day?
<ul class="org-ul">
<li>6K securities</li>
<li>Remove days that have major events</li>
</ul></li>
<li>Issues:
<ul class="org-ul">
<li>6K predictors need many many data points to avoid overfitting (need to reduce predictors)
<ul class="org-ul">
<li>However, even with unlimited data, the underlying situation could have changed over time</li>
<li>E.g. TSLA stock is good predictor now but it was only listed 5 years ago</li>
</ul></li>
<li>High correlation between predictors</li>
</ul></li>
</ul></li>
<li>PCA transforms data by:
<ul class="org-ul">
<li>Removing correlations within predictors</li>
<li>Ranking coordinates by importance
<ul class="org-ul">
<li>Most important are first</li>
</ul></li>
<li>By concentrating on first <b>n</b> principal components
<ul class="org-ul">
<li>This reduces random effects and</li>
<li>These PCs have higher signal to noise ratio</li>
</ul></li>
<li>Graphically:
<ul class="org-ul">
<li>rotate plot until it’s orthogonal to the correlation</li>
</ul></li>
<li>If D1 and D2 are the new PCs,
<ul class="org-ul">
<li>D1 (that explains more variance) will be the first factor</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org262c9a6" class="outline-3">
<h3 id="org262c9a6"><span class="section-number-3">9.4.</span> M9L4: Using PCA</h3>
<div class="outline-text-3" id="text-9-4">
</div>
<div id="outline-container-org0e41634" class="outline-4">
<h4 id="org0e41634"><span class="section-number-4">9.4.1.</span> Math of PCA</h4>
<div class="outline-text-4" id="text-9-4-1">
<ul class="org-ul">
<li>Definitions
<dl class="org-dl">
<dt>\(X\)</dt><dd>initial matrix of data</dd>
<dt>\(x_{ij}\)</dt><dd>j<sup>th</sup> factor of i<sup>th</sup> data point</dd>
<dt>Scale to:</dt><dd>\(\frac{1}{m}\sum_i x_{ij} = \mu_j = 0\)</dd>
</dl></li>
<li>To find all eigenvectors of X<sup>T</sup> X, where:
<dl class="org-dl">
<dt>V</dt><dd>Matrix of eigenvectors, sorted by eigenvalue</dd>
<dt>V</dt><dd>[V<sub>1</sub> V<sub>2</sub> &#x2026; ]</dd>
<dt>V<sub>j</sub></dt><dd>j<sup>th</sup> eigenvector of X<sup>T</sup> X</dd>
</dl></li>
<li>PCA is a linear combination:
<ul class="org-ul">
<li>1st component is XV<sub>1</sub>, then 2nd is XV<sub>2</sub>, &#x2026;</li>
<li>k<sup>th</sup> new factor for i<sup>th</sup> data point:
\[
    t_{ik} = \sum^m_{ik} x_{ij}v_{jk}
    \]</li>
<li>t<sub>ik</sub> is the factor after PCA</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb198daa" class="outline-4">
<h4 id="orgb198daa"><span class="section-number-4">9.4.2.</span> PCA as linear combination</h4>
<div class="outline-text-4" id="text-9-4-2">
<ul class="org-ul">
<li>It <b><b>removes</b></b> correlation between factors</li>
<li>In order to have fewer variables/factors in the model:
<ul class="org-ul">
<li>Choose to include only first <i>n</i> principal components</li>
</ul></li>
<li>PCA can also deal with non-linear functions, using kernels. This is similar to SVM modeling</li>
</ul>
</div>
</div>
<div id="outline-container-org831b653" class="outline-4">
<h4 id="org831b653"><span class="section-number-4">9.4.3.</span> PCA for regression</h4>
<div class="outline-text-4" id="text-9-4-3">
<ul class="org-ul">
<li>How to interpret the new model in terms of original factors?</li>
<li>Example: PCA finds \(L\) new factors (each \(t_{ik}\)), and regression coefficients b<sub>0</sub>, b<sub>1</sub>, &#x2026; b<sub>L</sub>:
\[
  y_i = b_0 + \sum^L_{k=1}b_k t_{ik} \\
 = b_0 + \sum^L_{k=1}b_k [\sum^m_{j=1} x_{ij} v{jk}] \\
 = b_0 + \sum^m_{j=1}x_{ij} [\sum^L_{k=1}b_kv_{jk}]
 \\
 = b_0 + \sum^m_{j=1}x_{ij}[a_j]
  \]</li>
<li>Each t vector does not have nice intuitive explanations as they are linear combinations of original factors.</li>
<li>Hence just plug in the transformation for each t factor:
\[
  a_j = \sum^L_{k=1}b_kv_{jk}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org83cae80" class="outline-4">
<h4 id="org83cae80"><span class="section-number-4">9.4.4.</span> Summary of PCA</h4>
<div class="outline-text-4" id="text-9-4-4">
<ul class="org-ul">
<li>Use PCA for high-dimensional and correlated data</li>
<li>PCA removes these correlations and ranks coordinates by importance (i.e., variability explained)
<ul class="org-ul">
<li>PC1 &gt; PC2 &gt; PC3, etc</li>
</ul></li>
<li>PCA can be transformed back to the original factor space to get intuitive explanations</li>
<li>PCA allows use of fewer variables
<ul class="org-ul">
<li>Pick the ones that explain the most variability</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org00b043b" class="outline-3">
<h3 id="org00b043b"><span class="section-number-3">9.5.</span> M9L5: Eigenvalues and Eigenvectors</h3>
<div class="outline-text-3" id="text-9-5">
</div>
<div id="outline-container-org3086280" class="outline-4">
<h4 id="org3086280"><span class="section-number-4">9.5.1.</span> Initial example</h4>
<div class="outline-text-4" id="text-9-5-1">
<ul class="org-ul">
<li>Definitions:
<dl class="org-dl">
<dt>\(A\)</dt><dd>a square matrix</dd>
<dt>\(v\)</dt><dd>a vector such that \(Av=\lambda v\)</dd>
<dt>\(V\)</dt><dd>eigenvector of \(A\)</dd>
<dt>\(\lambda\)</dt><dd>eigenvalue of \(A\), i.e. det(A-&lambda; I) = 0. Every &lambda; is eigenvalue of A</dd>
</dl></li>
<li>Given &lambda;, solve \(Av=\lambda v\) to find the eigenvector \(v\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgf0a7da4" class="outline-4">
<h4 id="orgf0a7da4"><span class="section-number-4">9.5.2.</span> Important: know how eigenvalues and eigenvectors are important to PCA</h4>
<div class="outline-text-4" id="text-9-5-2">
<ul class="org-ul">
<li>With a scaled matrix \(X\) of data, and x<sub>ij</sub> is factor value for i<sup>th</sup> data point after scaling,</li>
<li>Find eigenvectors v<sub>1</sub> &#x2026; v<sub>n</sub> of \((X^TX)\)</li>
<li>Then, find the principal components:
<ol class="org-ol">
<li>Multiply \(X\) by the eigenvectors</li>
<li>\(Xv_1, Xv_2, ..., Xv_n\) are the principal components
<ul class="org-ul">
<li>i.e. the transformed set of orthogonal coordinate directions</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org08eea3b" class="outline-3">
<h3 id="org08eea3b"><span class="section-number-3">9.6.</span> M9L6: PCA: The good and the bad</h3>
<div class="outline-text-3" id="text-9-6">
<ul class="org-ul">
<li>Summary
<img src="./img/m9l6-pca-summary.png" alt="m9l6-pca-summary.png" /></li>
<li>D<sub>1</sub> has more explanatory power (vaariation)</li>
<li>But it may not be the most helpful for explanatory/predictive modeling</li>
<li><b>PCA depends only on the independent variables</b>, not the response variable
<ul class="org-ul">
<li>It's possible response is affected by variables with low variability instead of those with high variability!</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org1fadf63" class="outline-4">
<h4 id="org1fadf63"><span class="section-number-4">9.6.1.</span> Example where PCA is good</h4>
<div class="outline-text-4" id="text-9-6-1">
<ul class="org-ul">
<li>Assume PCA is used for classification</li>
<li>PC1 has most of the variance and PC1 can classify red and blue points
<img src="./img/m9l6-pca-good.png" alt="m9l6-pca-good.png" /></li>
</ul>
</div>
</div>
<div id="outline-container-org1a7d33d" class="outline-4">
<h4 id="org1a7d33d"><span class="section-number-4">9.6.2.</span> Example where PCA is bad</h4>
<div class="outline-text-4" id="text-9-6-2">
<ul class="org-ul">
<li>PC1, though it has more variance, cannot classify the red and blue points</li>
<li>PC2 has less variance but it can classify the points exactly
<img src="./img/m9l6-pca-bad.png" alt="m9l6-pca-bad.png" /></li>
</ul>
</div>
</div>
<div id="outline-container-org53d0520" class="outline-4">
<h4 id="org53d0520"><span class="section-number-4">9.6.3.</span> Summary</h4>
<div class="outline-text-4" id="text-9-6-3">
<ul class="org-ul">
<li>We still use PCA (or try to!) as dimensions have higher variation specifically because they contain more information</li>
<li>However, this is not always true</li>
<li>PCA is a helpful approach to try</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb010402" class="outline-2">
<h2 id="orgb010402"><span class="section-number-2">10.</span> Module 10: Advanced Regression</h2>
<div class="outline-text-2" id="text-10">
<blockquote>
<p>
Midterm 1 covers up to Module 10
</p>
</blockquote>
</div>
<div id="outline-container-org2a9d9de" class="outline-3">
<h3 id="org2a9d9de"><span class="section-number-3">10.1.</span> M10L01: Introduction to CART</h3>
<div class="outline-text-3" id="text-10-1">
<blockquote>
<p>
Classification and Regression Trees
</p>
</blockquote>
</div>
<div id="outline-container-org924b5e0" class="outline-4">
<h4 id="org924b5e0"><span class="section-number-4">10.1.1.</span> Trees in regression</h4>
<div class="outline-text-4" id="text-10-1-1">
</div>
<ol class="org-ol">
<li><a id="org3a3b6b5"></a>Uses<br />
<div class="outline-text-5" id="text-10-1-1-1">
<ol class="org-ol">
<li>Classification</li>
<li>Decision tree</li>
</ol>
</div>
</li>
</ol>
</div>
<div id="outline-container-org02aed9c" class="outline-4">
<h4 id="org02aed9c"><span class="section-number-4">10.1.2.</span> Recall</h4>
<div class="outline-text-4" id="text-10-1-2">
<p>
In simple linear regression, e.g.:
impact of marketing email on recipient spending.
</p>
<ul class="org-ul">
<li>Predictors:
<ol class="org-ol">
<li>Demographics e.g. age, sex, number of children, income</li>
<li>Purchasing factors e.g. amount spent per month</li>
<li>Binary factor e.g. was email received and opened</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org382183d" class="outline-4">
<h4 id="org382183d"><span class="section-number-4">10.1.3.</span> What if responses can be differentiated by a factor?</h4>
<div class="outline-text-4" id="text-10-1-3">
<p>
(The above example of simple linear regression assumes every data point behaves the same way)
If each group instead have their own characteristics and responses, two regressions can thus be fitted, e.g.:
</p>
<ol class="org-ol">
<li>25 years or younger
\(\text{Money spent}=50+13.75\times\text{Number of Children}+0\times\text{Income over 30,000}+\text{...}\)</li>
<li>older than 25
\(\text{Money spent}=32+28.13\times\text{Number of children}+7.13\times\text{Income over 30,000}+\text{...}\)</li>
</ol>
</div>
</div>
<div id="outline-container-org6cb9770" class="outline-4">
<h4 id="org6cb9770"><span class="section-number-4">10.1.4.</span> Further splits are possible</h4>
<div class="outline-text-4" id="text-10-1-4">
<ul class="org-ul">
<li>Each branch is further split</li>
<li>Each ending is a <i>"leaf"</i>
<ul class="org-ul">
<li><i>Descriptively</i>: Each leaf's coefficients explains behaviour in that leaf</li>
<li><i>Predictively</i>: Each leaf's regression model can be used to predict a new point in that branch.</li>
</ul></li>
<li>Each branch can run \(R^2\) and those with low \(R^2\) can be investigated/improved.</li>
</ul>
</div>
</div>
<div id="outline-container-orgf6faada" class="outline-4">
<h4 id="orgf6faada"><span class="section-number-4">10.1.5.</span> Disadvantages</h4>
<div class="outline-text-4" id="text-10-1-5">
<ul class="org-ul">
<li>Lots of computations and regressions</li>
<li>Fewer and fewer data points in each node</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org37c05cf"></a>Hence:<br />
<div class="outline-text-5" id="text-10-1-5-1">
<p>
Simplify the regression by just using the constant term input
</p>
<ul class="org-ul">
<li>e.g. \(y=a_0\) instead of \(y=a_0+a_1x+\text{...}\)</li>
<li>This is the mean response over all data points in the node, i.e.
\[
  a_0 = \frac{\sum_i\text{in node }y_i}{\text{count of data points in node}} \\
  = \text{avg response in node}
  \]</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org3802094" class="outline-4">
<h4 id="org3802094"><span class="section-number-4">10.1.6.</span> Other places to use trees</h4>
<div class="outline-text-4" id="text-10-1-6">
<ul class="org-ul">
<li>The branching concept can be applied to:
<ul class="org-ul">
<li>Logistic regression model
<ul class="org-ul">
<li>fraction of node's data points with True response</li>
</ul></li>
<li>Classification model
<ul class="org-ul">
<li>Most common classification among node's data points</li>
</ul></li>
<li>Decision model
<ul class="org-ul">
<li>Each leaf is the decision "do I send a marketing email?"</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org122aee4" class="outline-4">
<h4 id="org122aee4"><span class="section-number-4">10.1.7.</span> Common questions</h4>
<div class="outline-text-4" id="text-10-1-7">
<ol class="org-ol">
<li>How to choose the branches?</li>
<li>When to stop branching?</li>
<li>Why is this called a regression tree?</li>
</ol>
</div>
</div>
<div id="outline-container-org9c0c4e0" class="outline-4">
<h4 id="org9c0c4e0"><span class="section-number-4">10.1.8.</span> Etymology of "tree"</h4>
<div class="outline-text-4" id="text-10-1-8">

<div id="org888fff3" class="figure">
<p><img src="./img/m1001-tree.png" alt="m1001-tree.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org6dd7874" class="outline-3">
<h3 id="org6dd7874"><span class="section-number-3">10.2.</span> M10L02: Branching</h3>
<div class="outline-text-3" id="text-10-2">
</div>
<div id="outline-container-orgf45461a" class="outline-4">
<h4 id="orgf45461a"><span class="section-number-4">10.2.1.</span> Main questions</h4>
<div class="outline-text-4" id="text-10-2-1">
<ol class="org-ol">
<li>Which factors are used to decide on branching?</li>
<li>How to split data?</li>
</ol>
<p>
In practice, no good algorithm to help decide. Instead, branch on 1 factor at a time.
</p>
</div>
</div>
<div id="outline-container-org1ee6af1" class="outline-4">
<h4 id="org1ee6af1"><span class="section-number-4">10.2.2.</span> Branching methods</h4>
<div class="outline-text-4" id="text-10-2-2">
<ol class="org-ol">
<li>Start with half of data and run a regression</li>
<li>Split the data into 2 halves based on some factor we can branch from (e.g. age &gt;25)</li>
<li>For each leaf:
<ol class="org-ol">
<li>Calculate variance of response amongst all data points in each leaf</li>
<li>Test splitting on each factor to see how much lower total variance of two branches would be vs. the least variance.</li>
<li>Choose the factor with lowest total variance.</li>
<li>Make split iif:
<ol class="org-ol">
<li>Enough data points in each branch</li>
<li>Decrease is lower than threshold &delta;</li>
</ol></li>
<li>Otherwise, don't split the leaf</li>
</ol></li>
<li>Go backwards to <b>prune</b> using the 2nd half of data not used in initial branching. For each pair of leaves created in each branch:
<ol class="org-ol">
<li>Use the other half of data in each branch to see if estimation error was improved by branching
<ol class="org-ol">
<li>If error increases/no change, then remove branch</li>
<li>Else, keep the branch</li>
</ol></li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-orgf1f7527" class="outline-4">
<h4 id="orgf1f7527"><span class="section-number-4">10.2.3.</span> Generic branching concept</h4>
<div class="outline-text-4" id="text-10-2-3">
<blockquote>
<p>
Overfitting can be costly; make sure the benefit of each branch is greater than its cost
</p>
</blockquote>
<p>
Key ideas
</p>
<ol class="org-ol">
<li>Use a metric related to model quality</li>
<li>Find 'best factor' to branch with</li>
<li><b>Check</b>: did this improve the model?
<ol class="org-ol">
<li>If not, prune the branch back.</li>
</ol></li>
</ol>
<p>
Rejecting potential branches
</p>
<ol class="org-ol">
<li>Low improvement benefit</li>
<li>Some side of branch has too few data points after branching
<ol class="org-ol">
<li>Each leaf should contain &ge;5% of original data (we don't want to over-fit the model)</li>
</ol></li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org13f5087" class="outline-3">
<h3 id="org13f5087"><span class="section-number-3">10.3.</span> M10L03: Random Forests</h3>
<div class="outline-text-3" id="text-10-3">
</div>
<div id="outline-container-org4065c80" class="outline-4">
<h4 id="org4065c80"><span class="section-number-4">10.3.1.</span> Intro summary</h4>
<div class="outline-text-4" id="text-10-3-1">
<ul class="org-ul">
<li>Introduce randomness</li>
<li>Generate lots of random trees
<ul class="org-ul">
<li>Each has its strengths and weaknesses</li>
</ul></li>
<li>Key concept: average better than single tree</li>
</ul>
</div>
</div>
<div id="outline-container-org254b70a" class="outline-4">
<h4 id="org254b70a"><span class="section-number-4">10.3.2.</span> Introducing randomness</h4>
<div class="outline-text-4" id="text-10-3-2">
<blockquote>
<p>
via bootstrapping
</p>
</blockquote>
<ul class="org-ul">
<li>With \(n\) number of original data points, we make trees each with \(n\) points.
<ul class="org-ul">
<li>However, some points can be picked multiple times while others get picked 0 times.</li>
</ul></li>
<li>When branching:
<ul class="org-ul">
<li>Not as before (before: we choose 1 factor at a time)</li>
<li>In RF: pick a small number of factors \(X\)</li>
<li>Choose the best factor in that set to branch on</li>
<li>Common number of factors used: \(1+\log(n)\)</li>
</ul></li>
<li>No need to prune tree</li>
</ul>
</div>
</div>
<div id="outline-container-orgc582ce8" class="outline-4">
<h4 id="orgc582ce8"><span class="section-number-4">10.3.3.</span> Note</h4>
<div class="outline-text-4" id="text-10-3-3">
<ul class="org-ul">
<li>Each tree has slightly different data</li>
<li>Will end up with lots of different trees (500-1000 are common)</li>
<li>Each tree gives a slightly different regression model</li>
<li>Which one to use?
<ul class="org-ul">
<li>Regression trees: use the <b>average</b> predicted response</li>
<li>Classification trees: use the <b>most common</b> predicted response</li>
</ul></li>
<li>Benefits of RF:
<ul class="org-ul">
<li>Better overall estimate</li>
<li>Average between tree can somewhat address over-fitting</li>
</ul></li>
<li>Disadvantages of RF:
<ul class="org-ul">
<li>Harder to explain and interpret results</li>
<li>Cannot explain how variables interact, or how some sequence of branches is helpful/meaningful (which can be found in single tree)</li>
<li>Can't give specific model from the data</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org55100e3" class="outline-4">
<h4 id="org55100e3"><span class="section-number-4">10.3.4.</span> Summary of RF</h4>
<div class="outline-text-4" id="text-10-3-4">
<ul class="org-ul">
<li>Method: introduce randomness into the trees</li>
<li>Good as 'black box' predictor</li>
<li>Cannot give much detailed insight</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org16e0474" class="outline-3">
<h3 id="org16e0474"><span class="section-number-3">10.4.</span> M10L04: Explainability and Interpretability</h3>
<div class="outline-text-3" id="text-10-4">
<blockquote>
<p>
How easy or not it is to understand how models create their output?
</p>
</blockquote>
</div>
<div id="outline-container-org2fe9559" class="outline-4">
<h4 id="org2fe9559"><span class="section-number-4">10.4.1.</span> Example: linear regression</h4>
<div class="outline-text-4" id="text-10-4-1">
<p>
\[y=a_0 + \sum^n_{j=1}a_j x_{ij}\]
To answer "how is the value of \(y\) affected by different values of the predictor?" with:
</p>
<dl class="org-dl">
<dt>y</dt><dd>Number of tickets sold this year</dd>
<dt>x<sub>1</sub></dt><dd>Salary of top 4 stars</dd>
<dt>x<sub>2</sub></dt><dd>Number of movies with similar plots this year</dd>
<dt>x<sub>3</sub></dt><dd>Rated <b>R</b> or more restrictive (1=yes)</dd>
<dt>x<sub>4</sub></dt><dd>Number of days left in the year</dd>
<dt>a<sub>0</sub></dt><dd>1,000,000</dd>
<dt>a<sub>1</sub></dt><dd>0.25</dd>
<dt>a<sub>2</sub></dt><dd>-1,000,000</dd>
<dt>a<sub>3</sub></dt><dd>-1,000,000</dd>
<dt>a<sub>4</sub></dt><dd>20,000</dd>
</dl>
<p>
The interpretation is thus:
</p>
<ul class="org-ul">
<li>Baseline is 1,000,000 tickets (a<sub>0</sub>)</li>
<li>Salary of star, each dollar increases number of tickets sold by $0.25 (a<sub>1</sub>)</li>
<li>Similar movie: each similar movie decreases number of tickets sold by 1,000,000 (a<sub>2</sub>)</li>
<li>Restrictive rating: decreases number of tickets sold by 1,000,000</li>
<li>Days left in year: each day increases number of tickets sold by 20,000</li>
</ul>
</div>
</div>
<div id="outline-container-orgab84f12" class="outline-4">
<h4 id="orgab84f12"><span class="section-number-4">10.4.2.</span> Example: regression tree</h4>
<div class="outline-text-4" id="text-10-4-2">
<ul class="org-ul">
<li>With the same values above, it becomes a long if<sub>else</sub> statement</li>
<li>Can describe detail of tree but it's not helpful for understanding</li>
</ul>
</div>
</div>
<div id="outline-container-org36ce5dd" class="outline-4">
<h4 id="org36ce5dd"><span class="section-number-4">10.4.3.</span> Example: random forests</h4>
<div class="outline-text-4" id="text-10-4-3">
<ul class="org-ul">
<li>If one tree is hard to explain, 500 are even worse</li>
<li>Although random forests give relative branching importance of each variable,
<ul class="org-ul">
<li>They do not say <b>how</b></li>
<li>Hence RF are not precise for interpretability or explainability</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgac8ff12" class="outline-4">
<h4 id="orgac8ff12"><span class="section-number-4">10.4.4.</span> Comparisons</h4>
<div class="outline-text-4" id="text-10-4-4">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-left">Explainability</th>
<th scope="col" class="org-left">Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Linear regression</td>
<td class="org-left">Higher</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Regression tree</td>
<td class="org-left">Medium</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Random forest</td>
<td class="org-left">Lower</td>
<td class="org-left">Sometimes higher</td>
</tr>
</tbody>
</table>

<p>
More explainable models:
</p>
<ul class="org-ul">
<li>help us understand "why"</li>
<li>help decision makers to choose between models</li>
<li>can be a legal requirement, e.g. in finance</li>
</ul>
<p>
However, less explainable models can give better results at times as they can identify and model more complex patterns
</p>
</div>
</div>
<div id="outline-container-org7dff6e7" class="outline-4">
<h4 id="org7dff6e7"><span class="section-number-4">10.4.5.</span> Tradeoff</h4>
<div class="outline-text-4" id="text-10-4-5">
<p>
Pay attention to tradeoffs when proposing a model:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Explainability</th>
<th scope="col" class="org-left">Value</th>
<th scope="col" class="org-left">Adoption</th>
<th scope="col" class="org-left">Legal requirement</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Less</td>
<td class="org-left">Potentially more</td>
<td class="org-left">?</td>
<td class="org-left">?</td>
</tr>

<tr>
<td class="org-left">More</td>
<td class="org-left">?</td>
<td class="org-left">More likely</td>
<td class="org-left">Might be required</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-org637affa" class="outline-3">
<h3 id="org637affa"><span class="section-number-3">10.5.</span> M10L05: Confusion Matrices</h3>
<div class="outline-text-3" id="text-10-5">
<blockquote>
<p>
Answers: How to measure how well a classification-type model works?
</p>
</blockquote>
<p>
This is a confusion matrix:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-left">Classification</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Yes</td>
<td class="org-left">No</td>
</tr>

<tr>
<td class="org-left">True</td>
<td class="org-left">Yes</td>
<td class="org-left">Correct</td>
<td class="org-left">Wrong</td>
</tr>

<tr>
<td class="org-left">Classification</td>
<td class="org-left">No</td>
<td class="org-left">Wrong</td>
<td class="org-left">Correct</td>
</tr>
</tbody>
</table>

<p>
It shows how much the model is confusing the two categories.
</p>
</div>
<div id="outline-container-org4b49f39" class="outline-4">
<h4 id="org4b49f39"><span class="section-number-4">10.5.1.</span> Details</h4>
<div class="outline-text-4" id="text-10-5-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-left">Classification</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">Yes</td>
<td class="org-left">No</td>
</tr>

<tr>
<td class="org-left">True</td>
<td class="org-left">Yes</td>
<td class="org-left">True Pos</td>
<td class="org-left">False Neg</td>
</tr>

<tr>
<td class="org-left">Classification</td>
<td class="org-left">No</td>
<td class="org-left">False Pos</td>
<td class="org-left">True Neg</td>
</tr>
</tbody>
</table>
<ul class="org-ul">
<li>Positive: model <b>says</b> it's in the category</li>
<li>Negative: model <b>says</b> it's NOT in the category</li>
<li>True: model got it right</li>
<li>False: model got it wrong</li>
</ul>
</div>
</div>
<div id="outline-container-orgd714016" class="outline-4">
<h4 id="orgd714016"><span class="section-number-4">10.5.2.</span> Definitions</h4>
<div class="outline-text-4" id="text-10-5-2">
<dl class="org-dl">
<dt>Sensitivity</dt><dd>TP/(TP+FN), i.e. TP / All actual positives</dd>
<dt>Specificity</dt><dd>TN/(TN+FP), i.e. TN/ All actual negatives</dd>
</dl>
<p>
Others: don't memorize, just refer.
</p>
</div>
</div>
</div>
<div id="outline-container-org1c4a8ee" class="outline-3">
<h3 id="org1c4a8ee"><span class="section-number-3">10.6.</span> M10L06: Situationally-Driven Comparisons</h3>
<div class="outline-text-3" id="text-10-6">
</div>
<div id="outline-container-org5ad0edf" class="outline-4">
<h4 id="org5ad0edf"><span class="section-number-4">10.6.1.</span> Example: from spam detection</h4>
<div class="outline-text-4" id="text-10-6-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Model</th>
<th scope="col" class="org-right">Classification</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">Yes</td>
<td class="org-right">No</td>
</tr>

<tr>
<td class="org-left">True</td>
<td class="org-left">Yes</td>
<td class="org-right">490</td>
<td class="org-right">10</td>
</tr>

<tr>
<td class="org-left">Classification</td>
<td class="org-left">No</td>
<td class="org-right">100</td>
<td class="org-right">400</td>
</tr>
</tbody>
</table>
<ul class="org-ul">
<li>Cost: $0 for correct classification
<ul class="org-ul">
<li>$0.04 to read spam</li>
<li>$1 to miss a real message</li>
</ul></li>
<li>If 50% of email is spam, the total cost is &sum;:
<ol class="org-ol">
<li>490 &times; $0 + 440 &times; $0 = 0</li>
<li>10 &times; $1 = $10</li>
<li>100 &times; $0.04 = $4, i.e.</li>
</ol></li>
</ul>
<p>
$0.014 per email
</p>
<ul class="org-ul">
<li>If 40% of email is spam: total cost is &sum;:
<ol class="org-ol">
<li>490 &times; 0.6/0.5 &times; 0 + 400 &times; 0.4/0.5 \ times 0 = $0</li>
<li>10 &times; 0.6/0.5 &times; $1 = $12</li>
<li>100 &times; 0.4/0.5 &times; $0.04 = $3.2</li>
</ol></li>
</ul>
<p>
$0.0152 per email
</p>
</div>
</div>
<div id="outline-container-org01c19bf" class="outline-4">
<h4 id="org01c19bf"><span class="section-number-4">10.6.2.</span> Evaluating quality / changing metrics</h4>
<div class="outline-text-4" id="text-10-6-2">
<ul class="org-ul">
<li>E.g. make model stricter that can reject more spam</li>
<li>This can make model reject more spam, but also cause more false negatives (which are much more costly)
Hence overall cost is higher at $52 ($0.104 per email)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1a77a08" class="outline-3">
<h3 id="org1a77a08"><span class="section-number-3">10.7.</span> L10L07: Advanced Topics in Regression</h3>
<div class="outline-text-3" id="text-10-7">
</div>
<div id="outline-container-org4906658" class="outline-4">
<h4 id="org4906658"><span class="section-number-4">10.7.1.</span> Poisson regression</h4>
<div class="outline-text-4" id="text-10-7-1">
<p>
Use when response follows a Poisson distribution i.e.
\[
f(z) = \frac{\lambda^Z e^{-z}}{z!}
\]
Examples:
</p>
<ul class="org-ul">
<li>Count of arrivals at airport security</li>
<li>Arrival rate might be function of <b>time</b></li>
<li>Hence, estimate &lambda;(x)</li>
</ul>
</div>
</div>
<div id="outline-container-orgb0537ed" class="outline-4">
<h4 id="orgb0537ed"><span class="section-number-4">10.7.2.</span> Regression splines</h4>
<div class="outline-text-4" id="text-10-7-2">
<p>
Spline: function of polynomials that connect to each other
<img src="./img/10-07-spline.png" alt="10-07-spline.png" />
</p>
<ul class="org-ul">
<li>Different functions are fitted to different parts of the data set</li>
<li>Hence, smoothens connections between parts</li>
<li>"Order-k" regression spline means that polynomials are all order k.</li>
<li>Example: multi-adaptive regression splines (MARS)
<ul class="org-ul">
<li>Called "Earth" in many stats libraries</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5240604" class="outline-4">
<h4 id="org5240604"><span class="section-number-4">10.7.3.</span> Bayesian regression</h4>
<div class="outline-text-4" id="text-10-7-3">
<ul class="org-ul">
<li>Start with:
<ul class="org-ul">
<li>Data AND</li>
<li>Estimate of how regression coefficients and random error is distributed</li>
</ul></li>
<li>Example: to predict how tall a child will be as an adult, based on:
<ul class="org-ul">
<li>Data: heights of child's parents</li>
<li>Expert opinion: starting distribution, coefficients of father's and mother's heights are normally distributed between 0.8 and 1.2</li>
</ul></li>
<li>Then use Baye's theorem to update estimate</li>
<li><b>Helpful when data is lacking</b>:
<ul class="org-ul">
<li>Combines expert opinion with the data we do have.</li>
<li>Can replace expert opinion with a broad prior distribution (e.g., uniform over a large interval) as seed data</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7be27e0" class="outline-4">
<h4 id="org7be27e0"><span class="section-number-4">10.7.4.</span> k-Nearest Neighbour Regression</h4>
<div class="outline-text-4" id="text-10-7-4">
<ul class="org-ul">
<li>Similar to KNN for classification</li>
<li>Hence, <b>KNN can be used for both regression and classification</b></li>
<li>Implementation:
<ul class="org-ul">
<li>No estimate of prediction function (function-less)</li>
<li>Plot all data</li>
<li>To predict response for a new point:
<ul class="org-ul">
<li>Average response of \(k\) closest data points</li>
</ul></li>
<li>Can be made fancier, e.g.:
<ul class="org-ul">
<li>Weight each dimension of distance</li>
<li>Removing dimensions that are not predictive</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2023-02-28 Tue 22:48</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
