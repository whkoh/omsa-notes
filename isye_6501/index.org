#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local.setup
#+TITLE: ISYE 6501 Intro to Analytics Modeling Notes
* Module 01: Intro
** What's analytics?
Analytics answers these questions
1. Descriptive - what happened
2. Predictive - what will happen
3. Prescriptive - what action is best
4. General questions
** Modeling
1. Describe real-life situation with math
2. Analyze math
3. Turn math answer back to real situation
** Course structure
Enough math intuition and detail
- Models
  - Machine learning
  - Regression
  - Optimizaton
- Cross-cutting
  - Data prep
  - Output quality
  - Missing data
** Three different things are all models
1. Real life situation expressed as math
2. Analyse the math
3. Turn mathematical analyse to real-life solution
** Hence these are all "models":
1. Regression
2. Regression on size, weight, distance
3. Regression estimate = 37+81*Size +76*Wt, etc
* Module 02: Classification
#+BEGIN_QUOTE
Definition: putting things into groups
#+END_QUOTE
** M1L1: Intro to classification
Types of classification models
1. Number of groups
2. Number of dimensions
   - Can 1 dimension be sufficient to classify?
3. Soft vs hard classifiers (is it 100% error-free?)
** M1L2: Choosing a Classifier
Definition of bad classification
- Cost: is one type of mistake worse than the other?
*** Example: Loan payment (Income vs credit score)
- Plot lines and find one that can separate default vs non-default.
- How do we know the right lines are drawn?
- We want to be as conservative as possible (less error prone)
** M2L3 Data definitions
*** Data terminology
1. Row = data point
2. Column = dimension, attribute, feature, predictor, covariate
   1. Special column = response, outcome
*** Data types
1. Structured data
   1. Quantitative
      - Numbers with meaning
   2. Categorical
      - Numbers without meaning
   3. Binary data (subset of categorical)
   4. Unrelated data
   5. Time series data
2. Unstructured
   1. Text data
** M2L4: Support vector machines
- *Supervised* method (algorithm uses known results when training)
- Terminology
  - m = number of data points
  - n = number of attributes
  - x_ij = j attribute of i data point
    - e.g. x_51 = credit score of person 5; x_52 = income of person 5
  - y_i = response of data point i
    - e.g. 1 if data point is group 1
    - -1 if data point is group 2
  - Line: $a_1 x_1$ + $a_2 x_2$ + ... + $a_n x_n$ + $a_0$ = 0
  - Note the intercept $a_0$
- In general: $\sum_{j=1}^{n} a_j x_j + a_0 = 0$
- Separation problem: get max distance between lines
- $2\over{\sqrt(\sum_{j} \left(a_j\right)^2)}$
- i.e. Min_{a_0 ... a_n}: $\sum_{j=1}^{n}\left(a_j\right)^2$
- Subject to constraints
*** When not possible to get full separation
- Then we minimize error
- There's a trade-off between margin and error
- Error for data point is:
  $$
  \text{max} \{ 0, 1-(\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}
  $$
- Total error is:
  $$
  \sum_{i=1}^{m} \text{max} \{ 0, 1 - (\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}
  $$
- Margin denominator: $\sum_{j=1}^{n}(a_j)^2$

- We multiply margin by $\lambda$ to *assign its importance of margin vs error*.
- Hence, the full equation is:
   $$
  \text{Minimize}_{a_0,...,a_n} \sum_{i=1}^{m} \text{max} \{ 0, 1 - (\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}  + \lambda \sum_{j=1}^{n}(a_j)^2
  $$
** M2L5: What SVM means
- Etymology
  - Vector = point
  - *Support vector* = points that holds up (or, supports) a shape. Shape is correctly balanced on parallel lines
  - Model determines the "support vectors"
  - Automatically from data hence "*machine*"
- Support can be from top or side
- Looking for max separation i.e. the support vector touches the data points
- Classifier is in between the two support vectors
** M2L6: Advanced SVM
- The constant term a_0 can be used to adjust the intercept and hence tweak the SVM model.
  - If it's more costly to grant a bad loan, e.g.: $\frac{2}{3}(a_0-1) + \frac{1}{3}(a_0+1)$
- For soft classification, you can add a multiplier m_j for each type of error:
  - m_j > 1 for more costly
  - m_j < 1 for less costly
** M2L7: Scaling and standardization
- Predictive factors may have different orders of magnitude, i.e.
  - Income in $10^5$
  - Credit score in $10^2$
  - Classifier is $0 = a_0 + \sum_{j} a_j x_j$
  - Maximise gap by minimizing: $\sum_{j} a_j^2$
  - Coefficients might be 10^6 + 5*income + 701*credit score
    - Sum of squared coefficients:
      $\sum_j a_j^2 = 5^2 + 700^2 = 490,025$
    - Changing credit score by 1 increases the sum by 1,401:
      $\sum_j a_j^2 = 5^2 + 701^2 = 491,426$
  - Small change in one coefficient affects the sum a lot due to difference in scales.
    - As data has such different scale.
*** Scaling data
- Common scale is between 0 and 1
- Scale data by factor
  $$
  x_{ij}^{\text{scaled}} = \frac{x_{ij}-x_{\text{min}j}}{x_{\text{max}j} - x_{\text{min}j}}
  $$
- General scaling between a, b:
  $$
  x_{ij}^{\text{scaled}[b,a]} = x_{ij}^{\text{scaled}[0,1]}(a-b)+b
  $$
*** Standardization of data
- Scale to normal distribution
- Common scale is:
  - Mean = 0
  - SD = 1
- Factor j has:
  - mean $\mu_j = \frac{\sum_{i=1}^n x_{ij}}{n}$
  - SD $\sigma_j$
- For each data point $i$:
  $$
  x_{ij}^{\text{standardized}} = \frac{x_{ij}-\mu_j}{\sigma_j}
  $$
*** Choosing between scaling vs standardization
- Scale when:
  - Data is in bounded (defined) range, e.g.
    - Neural networks
    - Optimization models requiring bounded data
    - Batting averages (between 0 and 1)
    - RGB color scale (0-255)
    - SAT scores (200-800)
- Standardization, examples:
  - PCA
  - Clustering
- Try both when not clear
- Should be used throughout course even when not stated explicitly
** M2L8: K Nearest Neighbour model (KNN)
- *Classification*
- e.g. loan dataset with two predictors and a response
- Assume each point has similar characteristics with its neighbors
- Choice of number of points is denoted by $k$
- Algorithm to find color (class) of a new point:
  1. Pick $k$ closest points (i.e., nearest neighbours) to the new one
  2. The new point's class is the most common among the $k$ neighbors
- Complexities
  - More than one distance metric (/c.f./ distance selection topic_).
    - Straight line is: $\sqrt{\sum_{i=1}^n |x_i-y_i|^2}$
  - Attributes can be given more weight if more important, $w_i$
    - Weights to be found with other techniques e.g. regression
  - Unimportant metrics can be removed
    - /c.f./ variable selection topic
    - Choose good value of $k$, /c.f./ validation @ [[Module 03: Validation]]
* Module 03: Validation
#+BEGIN_QUOTE
Check how good a model is
#+END_QUOTE
** M3L1: Training, validation and test data
- *Cannot* calculate accuracy or effectiveness metrics from training dataset
  - Since model was trained on it
  - This doesn't allow separation of real effects from random effects
- When fitting a model, this captures both real and random effects
  - Real effects: exist in all datasets (or subsets)
  - Random effects: different in all datasets
- Use a *training* set of data to fit model
- Use another *validation* set of data to judge model effectiveness
- When comparing >1 model, use a *test* dataset.
  - e.g. SVM and KNN, with 10 total models, we cannot use the effectiveness metric calculated on the validation set.
- Test data is required as high performing models have above average random effects
  - Too optimistic; it might have performed well but also likely received a boost from random effects
- Analogize with models equally good
- Flowchart:
  [[./img/validation01.png]]
** M3L2: Splitting data
- How much data goes to each set?
  1. 70 to 90% train, remaining test
  2. 50 to 70% train, remaining evenly split validation & test
- Methods of splitting data
  1. Random
  2. Rotation (take turn selecting data points into training, test, valid across the sets of split data)
     - Advantage: in time series data, may avoid all datasets having early/late data
     - Need to ensure rotation doesn't introduce bias
  3. Combined:
     60% of Monday data for training, 60^% of Tuesday data for training, etc.
** M3L3: Cross-validation
#+BEGIN_QUOTE
What happens with important data appears only in one data set e.g., validation?
#+END_QUOTE
- Use cross-validation!
- k-fold cross validation
  1. Split data for testing (e.g. 20%)
  2. With remaining data, use it for both training and validation by splitting into 4 x 20%, then:
     1. Train 1, 2, 3, Validate 4
     2. Train 1, 2, 4, Validate 3
     3. Train 1, 3, 4, Validate 2
     4. Train 2, 3, 4, Validate 1
- Summary of k-fold cross-validation:
  - Train model on all other parts
  - Evaluate model on remaining part
  - Average $k$ evaluations to estimate the model quality.
  - $10$ is commonly selected for $k$.
  - *But*, the model selected from cross-validation is not used. Coefficients should also not be averaged.
  - Once model is selected, *retrain* with all data
- Advantages of k-fold cross-validation:
  1. Better uses data
  2. Better estimates model quality
  3. Choose model more effectively
** M3L4: Summary
1. Build model with training data
2. Pick model with validation data
3. Estimate performance with test data
* Module 04: Clustering
** M4L1: Introduction to clustering
- *Unsupervised* method (response not available for use in training)
- Grouping data points
- Might help discover attributes in the dataset
- Example of use
  - Segmenting market of car buyers by:
    1. Size
    2. Price
    3. Versatility, etc
  - Personalized medicine
  - Locating facilities
  - Image analysis
  - Exploratory data analysis (different model for each attribute)
- Example: Miles driven vs. Age
** M4L2: Distance Norms
- Straight line distance (Euclidean)
  $\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$
- Rectilinear distance (Manhattan, 1-norm)
  $|x_1-y_1| + |x_2-y_2|$
- Generalized p-norm (Minkowski)
  $\sqrt[p]{|x_1-y_1|^p+|x_2-y_2|^p}$
- \infty-norm distance
  $\sqrt[\infty]{\sum_{i=1}^n|x_i-y_i|^{\infty}}$
  - sum = $|x_i-y_i|^{\infty}$
  - $\sqrt[\infty]{\text{max}_{i}^n|x_i-y_i|^{\infty}}$
  - Largest term dominates the rest, hence simplifies to:
  - $\text{max}_i |x_i-y_i|$
- Analogize with warehouse picking robot. The operation that takes the longest dominates the total operation time.
** M4L3: K-Means Clustering
- *Unsupervised* technique
- Steps to implement K-Means:
  1. Plot data points on suitable axes (e.g., age vs temperature, sepal width vs sepal height)
  2. Let:
     - $x_{ij}$ = attribute $j$ of data point $i$
     - $y_{ik}$ = $1$ iif data point $i$ in cluster $k$, else $0$
     - $z_{jk}$ = coordinate $j$ of cluster center $k$
     - Mathematically, but it takes too long:
       $$
       \text{Min}_{y,z}\sum_i\sum_k \sqrt{\sum_{j} (x_{ij} - z_{jk})^2}
       $$
       subject to: $\sum_k y_{ik} = 1$ for each $i$
  3. Practical method:
     - Pick $k$ cluster centers in data
     - Assign each point to nearest cluster center
     - Recalculate cluster center (centroid)
       - Now, data points might not belong to the right cluster
     - Go back to assign, then re-calc, then assign, then re-calc iteratively until stable
  4. K-Means is a *heuristic*, i.e.:
     - it is fast and good
     - *not guaranteed* to find global best solution.
  5. It is expectation-maximization (EM), and alternates between expectation (/finding cluster centers/) and maximization (/assigning points to clusters/)
** M4L4: Practical details for K-Means
Algorithm just assigns outliers to nearest clusters.
- Choosing *starting points*:
  1. Run several times with different initial cluster centers
  2. Algorithm is non-deterministic, /i.e./ can produce different results when run with different inputs
  3. Choose the best solution from the results produced
- Handling *outliers*:
  1. Discard, but may not be the 'right' answer
  2. Ask why the outlier happens
     - What it means to discard or include the outlier
     - Ultimately, algorithm is just a guide. Best solution is what fits the situation.
- Choosing *number of clusters*. Is adding a cluster always better?
  - It may increase the metric (total distance of each data point to their cluster center), hence clustering appears to work better.
  - However, it may defeat the purpose of clustering if every cluster just consists of one data point.
- Total distance can be compared to find the 'kink' or 'elbow'.
  - After this point, the marginal benefit of adding another cluster decreases.
  - Elbow diagram:

    [[./img/04-elbow.png]]
** M4L5: Clustering for prediction
#+BEGIN_QUOTE
Given a new point, which cluster should it be in?
#+END_QUOTE
- Is point inside cluster?
- Otherwise, what's the nearest cluster center?
- Asked another way: for the range of the dataset, which areas would we assign to each cluster if a new point appears there?
  - This is a [[https://en.wikipedia.org/wiki/Voronoi_diagram][Voronoi diagram]].
** M4L6: Clustering vs Classification
- Since both group data points...
- The difference is what we know about the data points.
- For classification, the correct response is known, i.e.
  - supervised learning
  - model uses both attributes *and* response
- For clustering, the 'correct' classification is unknown
  - unsupervised learning
  - model decides clusters *only* based on the attributes
- Supervised learning is more common
* Module 05: Data preparation
** M5L1: Common techniques and problems
1. Scale data
   - Outliers?
2. Extraneous (unnecessary data)
   - Complicates the model and
   - Makes it harder to interpret the solution
** M5L2: Outliers
- Types
  - Point outliers :: one / few points very different from others
  - Contextual outlier :: Value far from other points in time (not in absolute value)
  - Collective outlier :: Something missing in a range of points, but not sure exactly where. Outlier by omission.
- How to detect?
  - Box-and-whisker plot if data can be plotted in 1-dimension
    - Box: 25/75th percentile
      - Line: 50th percentile
    - Whiskers: 10/90th percentile, 5/95th, etc
  - For multi-dimensional, no good way. We can still:
    1. Fit a model.
    2. Points with large error might be outlier
** M5L3: What to do with outliers?
- Need to understand why there's outliers
  1. Bad data
     - Sensor fail
     - Contaminated experiment
     - Wrong data input
  2. Unexpected, real, data
     - Need to understand more about the data, e.g.
     - Where it came from
     - How it was compiled
     - Unique situations
*** Bad data
- Omit the points
- Use imputation to replace the points
*** Real / correct data
- Outliers are somewhat expected in large datasets
- E.g., for normally-distributed data:
  - 4% will be outside 2 \sigma
  - 1e6 data points = 2000 outside 3 \sigma
- Removing *real* outliers might make model too optimistic. /e.g./ not account for actual shipments that take a long time from US to Africa
- Outliers might be due to weather, political events
*** Another way to handle outliers
1. First build a logistic regression model
   - This estimated probability of outliers under different conditions
2. Next, build the regular model i.e. estimate delivery time under *normal conditions*
   - Use data without outliers
   - Report different outcomes...
*** Summary
- Outliers aren't predictable
- Investigate the data in case you're wrong
* Module 06: Change detection
** M6L1: Examples
- Usually with time series data
- Determine if action is needed, e.g.,
  - Time for machine maintenance?
  - Have sales increased?
- Determine impact of some past action, e.g.,
  - Did new tax / increase rate decrease sales?
  - Did price discount increase sales?
- Determine changes of current actions, e.g.
  - Did voting patterns change?
** M6L2: Cumulative sum for change detection
#+BEGIN_QUOTE
Answers whether mean of the observed distribution gone above a critical level
#+END_QUOTE
- x_t is observed value at time $t$
- \mu is mean of $x$, if no change in distribution
- Hence, $(x_t - \mu)$ is how much the observation is above mean at time $t$
- Detecting an increase
  $$
  S_t = \text{max}\{ 0, s_{t-1}+(x_t-\mu-C) \}
  $$
  - Determine threshold $T$ and ask whether S_t ⩾ T?
    - If running total < 0, it's irrelevant
    - There should still be some randomness
    - C is a term to control how faster S_t increases
- Detecting a decrease
  $$
  S_t = \text{max}\{ 0, s_{t-1}+(\mu-x_t-C) \}
  $$
  - Is S_t ⩾ T?
*** Interpretation
- Choices of model parameters
  - T :: the threshold, above which alarm is raised
  - C :: the control term (smaller = more sensitive)
- Consider / trade off:
  1. How costly is it to delay detection? (false negative) -> if it's costly, use small C
  2. How costly is false positive? -> if it's costly, use big C
- Use a control chart and plot S_t vs t with $T$ as a horizontal line
  [[./img/06-control-chart.png]]
** M6L3: Ethics: Honestly reporting our results
- Be faithful to data
- Have sound conclusions drawn from the model and not your own conceptions
- Always be honest and true to your analysis
* Module 07: Time series
** M7L1: Introduction to exponential smoothing
- Data for the same response is known for many time periods
- Examples:
  - Temperature readings
  - Price of stocks
  - Daily sales of hamburgers
  - Blood pressure readings
- Variation in time series data:
  1. Trends increase or decrease
  2. Cyclical variables over a year or a week
*** Random variation
- No underlying reason for the variation
*** Definitions:
- $S_t$: expected *baseline* response at time period $t$
- $x_t$: the observed response at $t$
- Seeing a increase over time, is it
  1. A real increase?
  2. Random?
- There are two ways to answer:
  1. It's a real increase, hence $S_t = x_t$
     - the observed reading is real indicator of revised baseline
  2. It's random, hence $S_t = S_{t-1}$
     - today's baseline = yesterday's baseline
*** Exponential smoothing method
Combines both, i.e.
$S_t = \alpha x_t + (1-\alpha)S_{t-1}$
- 0 < \alpha <1
  | \alpha | example value of \alpha   | randomness | trust                            |
  |--------+---------------------------+------------+----------------------------------|
  | small  | \rightarrow 0 (e.g. 0.01) | high       | previous baseline i.e. $S_{t-1}$ |
  | large  | \rightarrow 1 (e.g. 0.99) | low        | today's estimate i.e. $x_t$      |

- How to start? $S_1 = x_1$
** M7L2: Trend and cyclic effects
Complexities!
- Trends, increasing or decreasing
- Cyclical patterns, e.g. annual, weekly, daily
*** Trends
- $T_t$: the trend at time period $t$
- $S_t = \alpha x_t + (1-\alpha)S_{t-1}$
- $T_t = \beta(S_t - S_{t-1}) + (1-\beta)T_{t-1}$
- Initial condition: $T_1=0$
*** Cyclical patterns
- Make cycles additive: behaves like trend
- Make cycles multiplicative: more notation required
  - L = length of cycle
  - $C_t$ = the multiplicative seasonality factor
    - This inflates or deflates the observation
  - New baseline formula
    $$
    S_t = \frac{\alpha x_t}{C_{t-L}} + (1-\alpha)(S_{t-1}+T_{t-1})
    $$
  - Need to use the factor from $L$ time periods ago
    - as that's the most recent cyclic factor we have from that part of the cycle
  - Update the cyclic factor in a similar way i.e.:
    - $C_t = \gamma(x_t/S_t) + (1-\gamma)(C_{t-L})$
    - C_1, ..., C_L = 1
      - meaning there's no initial cyclic effect
    - If C = 1.1 on Sunday:
      - sales are higher by 10% just because it's Sunday
  - Initial values: first $L$ are set to 1. Multiplying by 1 = no effect
*** Summary
- Exponential smoothing
  - Single
  - Double (with trend)
  - Triple (with trend and cyclic effects)
    - AKA Winter's method, or Holt-Winters
** M7L3: Etymology (what the name means)
Example equation when $\alpha = \frac{1}{2}$:
$S_t = 0.5 x_t + 0.5 S_{t-1}$
**** Smoothing
- Note: when x_t is high, S_t is *not* as high, as $(1-\alpha)S_{t-1}$ pulls it down
- Conversely: when x_t is low, S_t is *not* as low, as $(1-\alpha)S_{t-1}$ pulls it up
- Peaks and valleys are smoothed out
  [[./img/07-smoothed-graph.png]]
**** Exponential
- Each $S_{t-1}$ actually contains every previous value of x!
  - When written or expanded out, e.g.
    $$
    S_t = \alpha x_t + (1-\alpha)S_{t-1}
    $$
    $$
    S_{t} = \alpha x_t + (1-\alpha)[\alpha x_{t-1} + (1-\alpha)S_{t-2}]
    $$
    $$
    S_{t} = \alpha x_t + (1-\alpha)\alpha x_{t-1} + (1-\alpha)^2S_{t-2}
    $$
- Each S_t is weighed by (1-\alpha) to an increasing *exponent*
- This means not only the current observation matters; instead, every past observation contributes to the current baseline estimate
- However, more recent observations are more important as they have higher weight
*** Summary
- Exponential smoothing smooths out jumps in observed data
- It's an exponential weighting of all past observations
- More recent observations are more important to the current baseline estimate
** M7L4: Forecasting
- Recap: $S_t = \alpha x_t + (1-\alpha)S_{t-1}$
- Prediction:
  - $S_{t+1} = \alpha x_{t+1} + (1-\alpha)S_{t}$
  - However x_{t+1} is unknown
  - Best guess for x_{t+1} is $S_t$
- Our forecast for $t+1$ is hence (after substituting):
  $$
  F_{t+1}=\alpha S_t + (1-\alpha) S_t \\
  F_{t+1} = S_t \\
  F_{t+k} = S_t \text{when } k=1, 2, ...
  $$
  - note that forecast error becomes larger for larger $k$
- If including trend:
  $$
  S_t = \alpha x_t + (1-\alpha)(S_{t-1}+T_{t-1}) \\
  T_t = \beta (S_t-S_{t-1})+(1-\beta)T_{t-1} \\
  F_{t+1} = S_t + T_t \\
  F_{t+k} = S_t + kT_t, k=1,2,...
  $$
  - Best estimate of next baseline :: the most current baseline estimate
  - Best estimate of the trend :: the most current trend estimate
- If including multiplicative seasonality:
  $$
  S_t = \alpha x_t/C_{t-L} + (1-\alpha)(S_{t-1}+T_{t-1})
  F_{t+1} = (S_t+T_t)C_{(t+1)-L}
  $$
  - Best estimate of next time period seasonal factor :: the corresponding (lagged) seasonal factor, i.e. $C_{t+1}=C_{t+1-L}$
- Finding the right \alpha, \beta, \gamma: use optimization, to be covered in future
  - $\min{(F_t-x_t)^2}$
** M3L5: ARIMA
AutoRegressive Integrated Moving Average.
- ARIMA *theory* is not covered in IAM.
*** (I): Differences
- Exponential smoothing assumes *stationary* data, i.e.
  - mean, variance, other measures are constant over time
- ARIMA works for data that's not stationary
  - if differences in data are stationary
    - 1st order difference D_(1) :: difference of consecutive observations, i.e. $D_{(1)t}=(x_t-x_{t-1})$
    - 2nd order difference D_(2) :: difference of the differences i.e.
      $D_{(2)t}=(x_t-x_{t-1})-(x_{t-1}-x_{t-2})$
    - d^{th} order difference D_(d) :: diff... d times
*** (II): Autogression
- Predicting current values based on previous period's values
- Regression: predicting value based on other factors
- Auto: use earlier values to predict. Only works with time series
- When used to forecast, exponential smoothing is an order-\infty autoregressive model
  - All previous values are used to make current prediction
- Order-p autoregressive model: S_t is function of $\{x_t, x_{t-1}, ..., x_{t-(p-1)}\}$
  - Only go back $p$ periods
- ARIMA: combines autoregression and differencing
  - Autoregression on the differences
  - Use $p$ time periods of previous observations to predict $d^{th}$ order differences
*** (III): Moving Average
- Use previous errors \epsilon_t as predictors
  - $\epsilon_t = (\hat{x_t}-x_t)$
- Order-q moving average
  - go back $q$ time periods
  - $\epsilon_{t-1}, \epsilon_{t-2}, ..., \epsilon_{t-q}$
*** ARIMA model
ARIMA (p,d,q) model
- $d^{th}$ order differences
- $p^{th}$ order autoregression
- $q^{th}$ order moving average
- <<ARIMA Equation>>Equation:
  $$
  D_{(d)t} = \mu + \sum^p_{i=1}\alpha_i D_{(d)t-i} - \sum^q_{i=1} \theta_i (\hat{x}_{t-i}-x_{t-i})
  $$
- Software can find $d, p, q$
- Extensions
  1. Add seasonality (out of scope for IAM)
  2. Specific models:
     - ARIMA(0,0,0) :: white noise
     - ARIMA(0,1,0) :: random walk
     - ARIMA(p,0,0) :: AR (autoregressive) model
     - ARIMA(0,0,q) :: MA (moving avg) model
     - ARIMA(0,1,1) :: basic exponential smoothing model
- Can be used for short-term forecasting
  - ARIMA is better than ES when data is more stable with fewer peaks, valleys, outliers
  - ARIMA needs 40+ historical data points to work well
** M7L6: GARCH
- GARCH :: Generalized Autoregressive Conditional Heteroskedasticity
To estimate or forecast the variance
*** Variance
- Estimates the amount of error
- E.g. forecasting demand for trucks
  - tell you how much forecast might be higher/lower than the actual value you see (later) to plan accordingly
- In investment (portfolio optimization):
  - Balance the expected return in investment with amount of volatility.
    - Riskier :: higher expected return
    - Less risky :: lower expected return
  - Variance is a proxy for amount of volatility or risk
*** GARCH
$$
\sigma^2_t = \omega + \sum^p_{i=1}\beta_i\sigma^2_{t-1}+\sum^q_{i=1} \gamma_i \epsilon^2_{t-i}
$$
It looks very similar to [[ARIMA Equation][ARIMA Equation]], but:
*Differences*:
- GARCH deals with variances and squared errors :: ARIMA deals with observations and linear errors
- GARCH deals with raw variances :: ARIMA deals with differences of variances
*** Summary - three models for time series analysis
1. Exponential smoothing
2. ARIMA, a generalization of exponential smoothing
3. GARCH, an ARIMA-like model for analyzing variance
* Module 08: Regression
** M8L1: Intro to Regression
*** What questions can regression answer?
1. How do systems work? (descriptive)
2. What will happen in the future? (predictive)
*** Simple linear regression
- Linear regression with one predictor, e.g. $y = a_0 + a_1x+1$
  - Date point $i$'s prediction error
    $= y_i - \hat{y}_i -(a_0+a_1x_1)$
  - Sum of squared errors
    $=\sum^n_{i=1}(y_i - \hat{y}_i)^2$
    $=\sum^n_{i=1}(y_i-(a_0+a_1x_1))^2$
- Best fit regression line
  - Minimizes sum of squared errors
  - Defined by a_0 and a_1
- Underlying math
  - Minimize convex quadratic function
  - Set partial derivatives to 0
  - Solve simultaneous equations
** M8L2: Maximum Likelihood and Information Criteria
*** Likelihood
- Measure the probability density of a parameter set
- Maximum likelihood :: the parameters that give the highest probability
- Assuming:
  1. Errors are normally distributed with mean 0, variance \sigma^2, independently and identically distributed
  2. Observations are z_1 to z_n
  3. Model estimates are y_1 to y_n
- Probability density of observing z_i if true value y_i is
  $$
  \frac{1}{\sigma\sqrt(2\pi)}e^-\frac{(z_u-y_i)^2}{2\sigma^2}
  $$
- Joint density over $n$ terms
  $$
  \prod^n_{i=1}\frac{1}{\sigma\sqrt(2\pi)}e^-\frac{(z_u-y_i)^2}{2\sigma^2} \\
  = (\frac{1}{\sigma\sqrt{2\pi}})^n e^-\frac{1}{2\sigma^2}\sum^n_{i=1}(z_i-y_i)^2
  $$
- Hence *to minimize* $\sum^n_{i=1}(z_i-y_i)^2$ over a_0, ..., a_m:
  equals to minimizing $\sum^n_{i=1}(z_i-(a_0 + \sum^m_{j=1}a_jx_{ij}))^2$
*** Maximum likelihood fitting
- Simplest example is regression with i.i.d. errors
- Complex examples:
  - Different estimation formulas
  - Different error assumptions
- Good software can handle complex cases
*** Akaike Information Criterion
- L^{*} :: maximum likelihood value
- $k$ :: number of parameters being estimated
- AIC :: $2k-2\log(L^{*})$
- Penalty term :: balances likelihood with simplicity, helps avoid overfitting
**** For simple regression
- AIC ::
  $$
  2(m+1) - 2\log(\frac{1}{\sigma\sqrt(2\pi)}^ne^{-\frac{1}{2\sigma^2}}\sum^n_{i=1}(z_i-(a_0 + \sum^m_{j=1}a_jx_{ij}))^2)
  $$
- Preference :: smaller AIC models
- Requires :: infinitely many data points
*** Corrected AIC (AIC_c)
Use for smaller datasets.
$$
AIC_c = AIC + \frac{2k(k+1)}{n-k-1} \\
= 2k-2\log(L^{*})+\frac{2k(k+1)}{n-k-1}
$$
*** AIC_c example
- Model 1: AIC 75; Model 2: AIC 80
- Relative likelihood equals:
  $$
  e^\frac{AIC_1-AIC_2}{2} \\
  = 8.2%
  $$
- Hence, Model 2 (larger AIC) is 8.2% as likely as Model 1 to be better
  - Model 1 is probably better
*** Bayesian Information Criterion
- BIC:
  $$
  k\log(n)-2\log(L^{*})
  $$
- Similar to AIC, but
  - bigger penalty term than AIC's penalty term
  - encourages models with fewer parameters
- Use BIC when there are *more data points* than parameters
- Rule of thumb for |BIC_1 - BIC_2|
  | value | interpretation for smaller BIC model |
  |-------+--------------------------------------|
  |   >10 | very likely better                   |
  |  6-10 | likely better                        |
  |   2-6 | somewhat likely better               |
  |   0-2 | slightly likely better               |
*** Summary
- No definite rules for AIC, BIC, maximum likelihood
- Can look at all 3 to decide what's best
** M8L3: Using Regression
*** Regression coefficients
- a_0, a_1, ..., a_m for the equation:
- $y=a_0+a_1x_1+...+a_mx_m$
- Example for baseball, descriptive question:
  - How many runs is associated with every homerun
  - Response: how many runs are scored by a team
  - Predictors:
    1. Number of HR
    2. Triples
    3. Doubles
    4. Singles
    5. Outs
    6. Double Plays
    7. Stolen bases, etc
  - Equation:
    $$
    \text{Runs scored} = a_0 + a_1\text{Number of HR} + a_2\text{Number of triples} + ... + a_7\text{Number of stolen bases}
    $$
  - a_1 = 1.4
    - Means that every HR adds 1.4 runs scored on average, ceteris paribus
- Example: height, predictive question:
  - How tall will a 2-year old be as an adult?
  - Response: a person's adult height
  - Predictors:
    1. Father's height
    2. Mother's height
    3. Height at age 2
    4. Male, female
  - Equation:
    $$
    \text{Adult height} = a_0 + a_1\text{Father's height} + a_2 \text{Mother's height} + ... + a_4\text{Male or female}
    $$
** M8L4: Causation vs Correlation
- Causation :: one thing causes another thing
- Correlation :: two things tend to happen / not happen together, but neither one causes the other
*** Example: winter recreation
- y: hours per day spent outdoors in winter
- x_1: city's average daily winter temperature
- Equation: $y=a_0+a_1x_1$
- Correlation between y and x_1
  - with low p-value of a_1
- Does higher temperature in winter cause people to go outside?
  - Probably
- Reversing the equation:
  - y: hours per day spent outdoors in winter
  - x_1: city's average daily winter temperature
  - Equation: $x_1 = b_0 + b_1y$
  - Same correlation between y, x_1
    - Same p-value of b_1 and a_1
  - Hence, does spending more time outside *cause* higher winter temperatures?
    - No
*** Example: tiredness vs scruffiness
- Neither one caused another, they're just related to a common tired factor, kids
*** How to tell causation?
- When is there causation?
  1. Cause before effect
  2. Idea of causation makes sense
  3. No outside factors can cause the relationship (hard to ensure this - need to consider *all* other factors)
- Be careful before claiming causation
*** Meaningless correlations
- Per capita consumption of mozzarella with number of civil engineering doctorates awarded
- [[http://tylervigen.com/spurious-correlations][link]]
** M8L5: Transformations and Interactions
- Recall: $y=a_0+a_1x_1+...+a_mx_m$
- What if the fit isn't linear for x?
- Answer: transforming the data!
*** Transforming the data
- Quadratic regression, e.g.
  $$
  y = a_0 + a_1 x_1 + a_2 x_1^2
  $$
- Trigonometric:
  $$
  y = a_0 + a_2 \sin(x^2)
  $$
- Response transform:
  $$
  \log(y) = a_0 + a_1 x_1 + ... + a_m x_m
  $$
- etc.
- Box-Cox transforms can be automated
*** Interaction terms, e.g. product of inputs
- Child's heights might be influenced by *product* of father and mother's heights ($x_1x_2$)
- $y = a_0 + a_1 x_1 + a_2 x_2 + a_3 (x_1 x_2)$
- *Treat x_1 x_2 as new input, x_3*
-  Then find best fit coefficients in the last module
** M6L6: Output
*** p-Values
- Estimates the *probability* that coefficient is actually 0
  - A hypothesis test
- If p-value is big (>0.05), the coefficient is likely 0, hence remove its attribute from the model
**** Other thresholds
| Threshold      | Number of factors included | Risk                        |
|----------------+----------------------------+-----------------------------|
| Higher (>0.05) | More factors included      | Irrelevant factors included |
| Lower (<0.05)  | Less factors included      | Relevant factors left out |
**** Warnings
- With lots of data:
  - p-values can get small and seem significant even when attributes aren't related to response
- Even when meaningful, p-values only represent *probabilities*
  1. With 100 attributes having p-value 0.02:
  2. Each of them have 2% chance of *not* being significant
  3. Hence on average 2/100 are actually irrelevant
*** Confidence interval
- Mostly given at 95% level around the coefficient
- Range of where the coefficient probably lies
- And how close that is to zero
- Related to p-value
*** T-statistic
- $\frac{\text{Coefficient}}{\text{Std error}}$
- Related to p-value
*** Coefficient itself
- If very small, then when multiplied by attribute, it's likely irrelevant
*** $R^2$
- Estimate of how much variability can be explained by the model
- E.g. R^2 = 0.59:
  - 0.59 of data variability can be explained by the model
  - remaining 0.41 is either:
    - random variation
    - or other factors
**** Adjusted $R^2$
- Accounts for (penalizes) the number of attributes used
* Module 09: Advanced Data Preparation
** M9L1: Box-Cox Transformations
- Models may require data to be normally distributed
- What happens when this assumption isn't valid in the data?
  - Results will have bias in this case
  - Data exhibits heteroskedasicity
  - i.e., variances are not i.i.d.
- Another example is time series data, where later values have higher variance
- Box-Cox is a logarithmic transformation that:
  1. Stretches smaller range to increase variability
  2. Shrinks larger range to reduce variability
- E.g. $t(y)=\frac{y^\lambda-a}{\lambda}$
  - t(y) can become close to normally distributed
- Need to remember to check for normality (e.g., with Normal Q-Q plot)
** M9L2: Detrending
- For time series data with trends, i.e. an increase or decrease over time
  - For example: increase in price of gold over time but need to account for inflation over time (value of $ decreases over time)
  - The trend if not correct can mess up a factor-based analysis
  - Can detrend:
    - Response
    - Predictors
    - Factor-based model (consider whenever using these models)
      - Regression, SVM, etc.
- How to detrend:
  - Factor by factor for one-dimensional regression, y=a_0+a_1x
    - Simple, works well to remove trend for factor-based analysis
    - This requires going factor by factor and fitting a linear regression model on it
    - E.g. for simple linear regression for gold prices:
      - Price = - 45600 + 23.2xYear
      - Detrend end price = Actual price -(-45600+23.2+year)
      - This produces a similar graph to the inflation-adjusted rate
      - Useful when we don’t know the response (as in most cases)
** M9L3: Intro to PCA
  - Works on high dimensional and correlated data
    - Which subset of features are important to predict response?
    - e.g. which stocks can predict how well the market performs the next day?
      - 6K securities
      - Remove days that have major events
    - Issues:
      - 6K predictors need many many data points to avoid overfitting (need to reduce predictors)
        - However, even with unlimited data, the underlying situation could have changed over time
        - E.g. TSLA stock is good predictor now but it was only listed 5 years ago
      - High correlation between predictors
  - PCA transforms data by:
    - Removing correlations within predictors
    - Ranking coordinates by importance
      - Most important are first
    - By concentrating on first *n* principal components
      - This reduces random effects and
      - These PCs have higher signal to noise ratio
    - Graphically:
      - rotate plot until it’s orthogonal to the correlation
    - If D1 and D2 are the new PCs,
      - D1 (that explains more variance) will be the first factor

** M9L4: Using PCA
*** Math of PCA
- Definitions
  - $X$ :: initial matrix of data
  - $x_{ij}$ :: j^{th} factor of i^{th} data point
  - Scale to: :: $\frac{1}{m}\sum_i x_{ij} = \mu_j = 0$
- To find all eigenvectors of X^T X, where:
  - V :: Matrix of eigenvectors, sorted by eigenvalue
  - V :: [V_1 V_2 ... ]
  - V_j :: j^{th} eigenvector of X^T X
- PCA is a linear combination:
  - 1st component is XV_1, then 2nd is XV_2, ...
  - k^{th} new factor for i^{th} data point:
    $$
    t_{ik} = \sum^m_{ik} x_{ij}v_{jk}
    $$
  - t_{ik} is the factor after PCA
*** PCA as linear combination
- It **removes** correlation between factors
- In order to have fewer variables/factors in the model:
  - Choose to include only first /n/ principal components
- PCA can also deal with non-linear functions, using kernels. This is similar to SVM modeling
*** PCA for regression
- How to interpret the new model in terms of original factors?
- Example: PCA finds $L$ new factors (each $t_{ik}$), and regression coefficients b_0, b_1, ... b_L:
  $$
  y_i = b_0 + \sum^L_{k=1}b_k t_{ik} \\
 = b_0 + \sum^L_{k=1}b_k [\sum^m_{j=1} x_{ij} v{jk}] \\
 = b_0 + \sum^m_{j=1}x_{ij} [\sum^L_{k=1}b_kv_{jk}]
 \\
 = b_0 + \sum^m_{j=1}x_{ij}[a_j]
  $$
- Each t vector does not have nice intuitive explanations as they are linear combinations of original factors.
- Hence just plug in the transformation for each t factor:
  $$
  a_j = \sum^L_{k=1}b_kv_{jk}
  $$
*** Summary of PCA
- Use PCA for high-dimensional and correlated data
- PCA removes these correlations and ranks coordinates by importance (i.e., variability explained)
  - PC1 > PC2 > PC3, etc
- PCA can be transformed back to the original factor space to get intuitive explanations
- PCA allows use of fewer variables
  - Pick the ones that explain the most variability
** M9L5: Eigenvalues and Eigenvectors
*** Initial example
- Definitions:
  - $A$ :: a square matrix
  - $v$ :: a vector such that $Av=\lambda v$
  - $V$ :: eigenvector of $A$
  - $\lambda$ :: eigenvalue of $A$, i.e. det(A-\lambda I) = 0. Every \lambda is eigenvalue of A
- Given \lambda, solve $Av=\lambda v$ to find the eigenvector $v$
*** Important: know how eigenvalues and eigenvectors are important to PCA
- With a scaled matrix $X$ of data, and x_{ij} is factor value for i^{th} data point after scaling,
- Find eigenvectors v_1 ... v_n of $(X^TX)$
- Then, find the principal components:
  1. Multiply $X$ by the eigenvectors
  2. $Xv_1, Xv_2, ..., Xv_n$ are the principal components
     - i.e. the transformed set of orthogonal coordinate directions

** M9L6: PCA: The good and the bad
- Summary
  [[./img/m9l6-pca-summary.png]]
- D_1 has more explanatory power (vaariation)
- But it may not be the most helpful for explanatory/predictive modeling
- *PCA depends only on the independent variables*, not the response variable
  - It's possible response is affected by variables with low variability instead of those with high variability!
*** Example where PCA is good
- Assume PCA is used for classification
- PC1 has most of the variance and PC1 can classify red and blue points
  [[./img/m9l6-pca-good.png]]
*** Example where PCA is bad
- PC1, though it has more variance, cannot classify the red and blue points
- PC2 has less variance but it can classify the points exactly
  [[./img/m9l6-pca-bad.png]]
*** Summary
- We still use PCA (or try to!) as dimensions have higher variation specifically because they contain more information
- However, this is not always true
- PCA is a helpful approach to try
* Module 10: Advanced Regression
#+BEGIN_QUOTE
Midterm 1 covers up to Module 10
#+END_QUOTE
** M10L01: Introduction to CART
#+BEGIN_QUOTE
Classification and Regression Trees
#+END_QUOTE
*** Trees in regression
**** Uses
1. Classification
2. Decision tree
*** Recall
In simple linear regression, e.g.:
impact of marketing email on recipient spending.
- Predictors:
  1. Demographics e.g. age, sex, number of children, income
  2. Purchasing factors e.g. amount spent per month
  3. Binary factor e.g. was email received and opened
*** What if responses can be differentiated by a factor?
(The above example of simple linear regression assumes every data point behaves the same way)
If each group instead have their own characteristics and responses, two regressions can thus be fitted, e.g.:
1. 25 years or younger
   $\text{Money spent}=50+13.75\times\text{Number of Children}+0\times\text{Income over 30,000}+\text{...}$
2. older than 25
   $\text{Money spent}=32+28.13\times\text{Number of children}+7.13\times\text{Income over 30,000}+\text{...}$
*** Further splits are possible
- Each branch is further split
- Each ending is a /"leaf"/
  - /Descriptively/: Each leaf's coefficients explains behaviour in that leaf
  - /Predictively/: Each leaf's regression model can be used to predict a new point in that branch.
- Each branch can run $R^2$ and those with low $R^2$ can be investigated/improved.
*** Disadvantages
- Lots of computations and regressions
- Fewer and fewer data points in each node
**** Hence:
Simplify the regression by just using the constant term input
- e.g. $y=a_0$ instead of $y=a_0+a_1x+\text{...}$
- This is the mean response over all data points in the node, i.e.
  $$
  a_0 = \frac{\sum_i\text{in node }y_i}{\text{count of data points in node}} \\
  = \text{avg response in node}
  $$
*** Other places to use trees
- The branching concept can be applied to:
  - Logistic regression model
    - fraction of node's data points with True response
  - Classification model
    - Most common classification among node's data points
  - Decision model
    - Each leaf is the decision "do I send a marketing email?"
*** Common questions
1. How to choose the branches?
2. When to stop branching?
3. Why is this called a regression tree?
*** Etymology of "tree"
[[./img/m1001-tree.png]]
** M10L02: Branching
*** Main questions
1. Which factors are used to decide on branching?
2. How to split data?
In practice, no good algorithm to help decide. Instead, branch on 1 factor at a time.
*** Branching methods
1. Start with half of data and run a regression
2. Split the data into 2 halves based on some factor we can branch from (e.g. age >25)
3. For each leaf:
   1. Calculate variance of response amongst all data points in each leaf
   2. Test splitting on each factor to see how much lower total variance of two branches would be vs. the least variance.
   3. Choose the factor with lowest total variance.
   4. Make split iif:
      1. Enough data points in each branch
      2. Decrease is lower than threshold \delta
   5. Otherwise, don't split the leaf
4. Go backwards to *prune* using the 2nd half of data not used in initial branching. For each pair of leaves created in each branch:
   1. Use the other half of data in each branch to see if estimation error was improved by branching
      1. If error increases/no change, then remove branch
      2. Else, keep the branch
*** Generic branching concept
#+BEGIN_QUOTE
Overfitting can be costly; make sure the benefit of each branch is greater than its cost
#+END_QUOTE
Key ideas
1. Use a metric related to model quality
2. Find 'best factor' to branch with
3. *Check*: did this improve the model?
   1. If not, prune the branch back.
Rejecting potential branches
1. Low improvement benefit
2. Some side of branch has too few data points after branching
   1. Each leaf should contain \geq5% of original data (we don't want to over-fit the model)
** M10L03: Random Forests
*** Intro summary
- Introduce randomness
- Generate lots of random trees
  - Each has its strengths and weaknesses
- Key concept: average better than single tree
*** Introducing randomness
#+BEGIN_QUOTE
via bootstrapping
#+END_QUOTE
- With $n$ number of original data points, we make trees each with $n$ points.
  - However, some points can be picked multiple times while others get picked 0 times.
- When branching:
  - Not as before (before: we choose 1 factor at a time)
  - In RF: pick a small number of factors $X$
  - Choose the best factor in that set to branch on
  - Common number of factors used: $1+\log(n)$
- No need to prune tree
*** Note
- Each tree has slightly different data
- Will end up with lots of different trees (500-1000 are common)
- Each tree gives a slightly different regression model
- Which one to use?
  - Regression trees: use the *average* predicted response
  - Classification trees: use the *most common* predicted response
- Benefits of RF:
  - Better overall estimate
  - Average between tree can somewhat address over-fitting
- Disadvantages of RF:
  - Harder to explain and interpret results
  - Cannot explain how variables interact, or how some sequence of branches is helpful/meaningful (which can be found in single tree)
  - Can't give specific model from the data
*** Summary of RF
- Method: introduce randomness into the trees
- Good as 'black box' predictor
- Cannot give much detailed insight
** M10L04: Explainability and Interpretability
** M10L05: Confusion Matrices
** M10L06: Situationally-Driven Comparisons
** L10L07: Advanced Topics in Regression
