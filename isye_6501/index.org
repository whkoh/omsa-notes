#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local.setup
#+TITLE: ISYE 6501 Intro to Analytics Modeling Notes
* Module 01: Intro
** What's analytics?
Analytics answers these questions
1. Descriptive - what happened
2. Predictive - what will happen
3. Prescriptive - what action is best
4. General questions
** Modeling
1. Describe real-life situation with math
2. Analyze math
3. Turn math answer back to real situation
** Course structure
Enough math intuition and detail
- Models
  - Machine learning
  - Regression
  - Optimizaton
- Cross-cutting
  - Data prep
  - Output quality
  - Missing data
** Three different things are all models
1. Real life situation expressed as math
2. Analyse the math
3. Turn mathematical analyse to real-life solution
** Hence these are all "models":
1. Regression
2. Regression on size, weight, distance
3. Regression estimate = 37+81*Size +76*Wt, etc
* Module 02: Classification
- Definition: putting things into groups
** Types of classification models
1. Number of groups
2. Number of dimensions
   - Can 1 dimension be sufficient to classify?
3. Soft vs hard classifiers (is it 100% error free?)
** Definition of bad classification
- Cost: is one type of mistake worse than the other?

** Examples
*** Loan payment (Income vs credit score)
- Plot lines and find one that can separate default vs non-default.
- How do we know the right lines are drawn?
- We want to be as conservative as possible (less error prone)
** Data terminology
1. Row = data point
2. Column = dimension, attribute, feature, predictor, covariate
   1. Special column = response, outcome
** Data types
1. Structured data
   1. Quantitative
      - Numbers with meaning
   2. Categorical
      - Numbers without meaning
   3. Binary data (subset of categorical)
   4. Unrelated data
   5. Time series data
2. Unstructured
   1. Text data
** Support vector machines
- *Supervised* method (algorithm uses known results when training)
- Terminology
  - m = number of data points
  - n = number of attributes
  - x_ij = j attribute of i data point
    - e.g. x_51 = credit score of person 5; x_52 = income of person 5
  - y_i = response of data point i
    - e.g. 1 if data point is group 1
    - -1 if data point is group 2
  - Line: $a_1 x_1$ + $a_2 x_2$ + ... + $a_n x_n$ + $a_0$ = 0
  - Note the intercept $a_0$
- In general: $\sum_{j=1}^{n} a_j x_j + a_0 = 0$
- Separation problem: get max distance between lines
- $2\over{\sqrt(\sum_{j} \left(a_j\right)^2)}$
- i.e. Min_{a_0 ... a_n}: $\sum_{j=1}^{n}\left(a_j\right)^2$
- Subject to constraints
*** When not possible to get full separation
- Then we minimize error
- There's a trade-off between margin and error
- Error for data point is:
  $$
  \text{max} \{ 0, 1-(\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}
  $$
- Total error is:
  $$
  \sum_{i=1}^{m} \text{max} \{ 0, 1 - (\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}
  $$
- Margin denominator: $\sum_{j=1}^{n}(a_j)^2$

- We multiply margin by $\lambda$ to *assign its importance of margin vs error*.
- Hence, the full equation is:
   $$
  \text{Minimize}_{a_0,...,a_n} \sum_{i=1}^{m} \text{max} \{ 0, 1 - (\sum_{j=1}^{n} a_j x_{ij} + a_0) y_i \}  + \lambda \sum_{j=1}^{n}(a_j)^2
  $$
*** What SVM means
- Etymology
  - Vector = point
  - *Support vector* = points that holds up (or, supports) a shape. Shape is correctly balanced on parallel lines
  - Model determines the "support vectors"
  - Automatically from data hence "*machine*"
- Support can be from top or side
- Looking for max separation i.e. the support vector touches the data points
- Classifier is in between the two support vectors
** Scaling and standardization
- Predictive factors may have different orders of magnitude, i.e.
  - Income in $10^5$
  - Credit score in $10^2$
  - Classifier is $0 = a_0 + \sum_{j} a_j x_j$
  - Maximise gap by minimizing: $\sum_{j} a_j^2$
  - Coefficients might be 10^6 + 5*income + 701*credit score
    - Sum of squared coefficients:
      $\sum_j a_j^2 = 5^2 + 700^2 = 490,025$
    - Changing credit score by 1 increases the sum by 1,401:
      $\sum_j a_j^2 = 5^2 + 701^2 = 491,426$
  - Small change in one coefficient affects the sum a lot due to difference in scales.
    - As data has such different scale.
*** Scaling data
- Common scale is between 0 and 1
- Scale data by factor
  $$
  x_{ij}^{\text{scaled}} = \frac{x_{ij}-x_{\text{min}j}}{x_{\text{max}j} - x_{\text{min}j}}
  $$
- General scaling between a, b:
  $$
  x_{ij}^{\text{scaled}[b,a]} = x_{ij}^{\text{scaled}[0,1]}(a-b)+b
  $$
*** Standardization of data
- Scale to normal distribution
- Common scale is:
  - Mean = 0
  - SD = 1
- Factor j has:
  - mean $\mu_j = \frac{\sum_{i=1}^n x_{ij}}{n}$
  - SD $\sigma_j$
- For each data point $i$:
  $$
  x_{ij}^{\text{standardized}} = \frac{x_{ij}-\mu_j}{\sigma_j}
  $$
** Choosing between scaling vs standardization
- Scale when:
  - Data is in bounded (defined) range, e.g.
    - Neural networks
    - Optimization models requiring bounded data
    - Batting averages (between 0 and 1)
    - RGB color scale (0-255)
    - SAT scores (200-800)
- Standardization, examples:
  - PCA
  - Clustering
- Try both when not clear
- Should be used throughout course even when not stated explicitly
** K Nearest Neighbour model (KNN)
- *Classification*
- e.g. loan dataset with two predictors and a response
- Assume each point has similar characteristics with its neighbors
- Choice of number of points is denoted by $k$
- Algorithm to find color (class) of a new point:
  1. Pick $k$ closest points (i.e., nearest neighbours) to the new one
  2. The new point's class is the most common among the $k$ neighbors
- Complexities
  - More than one distance metric (/c.f./ distance selection topic_).
    - Straight line is: $\sqrt{\sum_{i=1}^n |x_i-y_i|^2}$
  - Attributes can be given more weight if more important, $w_i$
    - Weights to be found with other techniques e.g. regression
  - Unimportant metrics can be removed
    - /c.f./ variable selection topic
    - Choose good value of $k$, /c.f./ validation @ [[Module 03: Validation]]
* Module 03: Validation
#+BEGIN_QUOTE
Check how good a model is
#+END_QUOTE
- *Cannot* calculate accuracy or effectiveness metrics from training dataset
  - Since model was trained on it
  - This doesn't allow separation of real effects from random effects
- When fitting a model, this captures both real and random effects
  - Real effects: exist in all datasets (or subsets)
  - Random effects: different in all datasets
** Training, validation and test data
- Use a *training* set of data to fit model
- Use another *validation* set of data to judge model effectiveness
- When comparing >1 model, use a *test* dataset.
  - e.g. SVM and KNN, with 10 total models, we cannot use the effectiveness metric calculated on the validation set.
- Test data is required as high performing models have above average random effects
  - Too optimistic; it might have performed well but also likely received a boost from random effects
- Analogize with models equally good
- Flowchart:
  [[./img/validation01.png]]
** Splitting data
- How much data goes to each set?
  1. 70 to 90% train, remaining test
  2. 50 to 70% train, remaining evenly split validation & test
- Methods of splitting data
  1. Random
  2. Rotation (take turn selecting data points into training, test, valid across the sets of split data)
     - Advantage: in time series data, may avoid all datasets having early/late data
     - Need to ensure rotation doesn't introduce bias
  3. Combined:
     60% of Monday data for training, 60^% of Tuesday data for training, etc.
** Cross-validation
#+BEGIN_QUOTE
What happens with important data appears only in one data set e.g., validation?
#+END_QUOTE
- Use cross-validation!
- k-fold cross validation
  1. Split data for testing (e.g. 20%)
  2. With remaining data, use it for both training and validation by splitting into 4 x 20%, then:
     1. Train 1, 2, 3, Validate 4
     2. Train 1, 2, 4, Validate 3
     3. Train 1, 3, 4, Validate 2
     4. Train 2, 3, 4, Validate 1
- Summary of k-fold cross-validation:
  - Train model on all other parts
  - Evaluate model on remaining part
  - Average $k$ evaluations to estimate the model quality.
  - $10$ is commonly selected for $k$.
  - *But*, the model selected from cross-validation is not used. Coefficients should also not be averaged.
  - Once model is selected, *retrain* with all data
- Advantages of k-fold cross-validation:
  1. Better uses data
  2. Better estimates model quality
  3. Choose model more effectively
** Summary
1. Build model with training data
2. Pick model with validation data
3. Estimate performance with test data
* Module 04: Clustering
** Introduction to clustering
- *Unsupervised* method (response not available for use in training)
- Grouping data points
- Might help discover attributes in the dataset
- Example of use
  - Segmenting market of car buyers by:
    1. Size
    2. Price
    3. Versatility, etc
  - Personalized medicine
  - Locating facilities
  - Image analysis
  - Exploratory data analysis (different model for each attribute)
- Example: Miles driven vs. Age
** Distance Norms
- Straight line distance (Euclidean)
  $\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$
- Rectilinear distance (Manhattan, 1-norm)
  $|x_1-y_1| + |x_2-y_2|$
- Generalized p-norm (Minkowski)
  $\sqrt[p]{|x_1-y_1|^p+|x_2-y_2|^p}$
- \infty-norm distance
  $\sqrt[\infty]{\sum_{i=1}^n|x_i-y_i|^{\infty}}$
  - sum = $|x_i-y_i|^{\infty}$
  - $\sqrt[\infty]{\text{max}_{i}^n|x_i-y_i|^{\infty}}$
  - Largest term dominates the rest, hence simplifies to:
  - $\text{max}_i |x_i-y_i|$
- Analogize with warehouse picking robot. The operation that takes the longest dominates the total operation time.
** K-Means Clustering
- *Unsupervised* technique
- Steps to implement K-Means:
  1. Plot data points on suitable axes (e.g., age vs temperature, sepal width vs sepal height)
  2. Let:
     - $x_{ij}$ = attribute $j$ of data point $i$
     - $y_{ik}$ = $1$ iif data point $i$ in cluster $k$, else $0$
     - $z_{jk}$ = coordinate $j$ of cluster center $k$
     - Mathematically, but it takes too long:
       $$
       \text{Min}_{y,z}\sum_i\sum_k \sqrt{\sum_{j} (x_{ij} - z_{jk})^2}
       $$
       subject to: $\sum_k y_{ik} = 1$ for each $i$
  3. Practical method:
     - Pick $k$ cluster centers in data
     - Assign each point to nearest cluster center
     - Recalculate cluster center (centroid)
       - Now, data points might not belong to the right cluster
     - Go back to assign, then re-calc, then assign, then re-calc iteratively until stable
  4. K-Means is a *heuristic*, i.e.:
     - it is fast and good
     - *not guaranteed* to find global best solution.
  5. It is expectation-maximization (EM), and alternates between expectation (/finding cluster centers/) and maximization (/assigning points to clusters/)
** Practical details for K-Means
Algorithm just assigns outliers to nearest clusters.
- Choosing *starting points*:
  1. Run several times with different initial cluster centers
  2. Algorithm is non-deterministic, /i.e./ can produce different results when run with different inputs
  3. Choose the best solution from the results produced
- Handling *outliers*:
  1. Discard, but may not be the 'right' answer
  2. Ask why the outlier happens
     - What it means to discard or include the outlier
     - Ultimately, algorithm is just a guide. Best solution is what fits the situation.
- Choosing *number of clusters*. Is adding a cluster always better?
  - It may increase the metric (total distance of each data point to their cluster center), hence clustering appears to work better.
  - However, it may defeat the purpose of clustering if every cluster just consists of one data point.
- Total distance can be compared to find the 'kink' or 'elbow'.
  - After this point, the marginal benefit of adding another cluster decreases.
  - Elbow diagram:

    [[./img/04-elbow.png]]
** Clustering for prediction
#+BEGIN_QUOTE
Given a new point, which cluster should it be in?
#+END_QUOTE
- Is point inside cluster?
- Otherwise, what's the nearest cluster center?
- Asked another way: for the range of the dataset, which areas would we assign to each cluster if a new point appears there?
  - This is a [[https://en.wikipedia.org/wiki/Voronoi_diagram][Voronoi diagram]].
** Clustering vs Classification
- Since both group data points...
- The difference is what we know about the data points.
- For classification, the correct response is known, i.e.
  - supervised learning
  - model uses both attributes *and* response
- For clustering, the 'correct' classification is unknown
  - unsupervised learning
  - model decides clusters *only* based on the attributes
- Supervised learning is more common
* Module 05: Data preparation
** Common techniques and problems
1. Scale data
   - Outliers?
2. Extraneous (unnecessary data)
   - Complicates the model and
   - Makes it harder to interpret the solution
** Outliers
- Types
  - Point outliers :: one / few points very different from others
  - Contextual outlier :: Value far from other points in time (not in absolute value)
  - Collective outlier :: Something missing in a range of points, but not sure exactly where. Outlier by omission.
- How to detect?
  - Box-and-whisker plot if data can be plotted in 1-dimension
    - Box: 25/75th percentile
      - Line: 50th percentile
    - Whiskers: 10/90th percentile, 5/95th, etc
  - For multi-dimensional, no good way. We can still:
    1. Fit a model.
    2. Points with large error might be outlier
** What to do with outliers?
- Need to understand why there's outliers
  1. Bad data
     - Sensor fail
     - Contaminated experiment
     - Wrong data input
  2. Unexpected, real, data
     - Need to understand more about the data, e.g.
     - Where it came from
     - How it was compiled
     - Unique situations
*** Bad data
- Omit the points
- Use imputation to replace the points
*** Real / correct data
- Outliers are somewhat expected in large datasets
- E.g., for normally-distributed data:
  - 4% will be outside 2 \sigma
  - 1e6 data points = 2000 outside 3 \sigma
- Removing *real* outliers might make model too optimistic. /e.g./ not account for actual shipments that take a long time from US to Africa
- Outliers might be due to weather, political events
*** Another way to handle outliers
1. First build a logistic regression model
   - This estimated probability of outliers under different conditions
2. Next, build the regular model i.e. estimate delivery time under *normal conditions*
   - Use data without outliers
   - Report different outcomes...
*** Summary
- Outliers aren't predictable
- Investigate the data in case you're wrong
* Module 06: Change detection
** Examples
- Usually with time series data
- Determine if action is needed, e.g.,
  - Time for machine maintenance?
  - Have sales increased?
- Determine impact of some past action, e.g.,
  - Did new tax / increase rate decrease sales?
  - Did price discount increase sales?
- Determine changes of current actions, e.g.
  - Did voting patterns change?
** Cumulative sum for change detection
#+BEGIN_QUOTE
Answers whether mean of the observed distribution gone above a critical level
#+END_QUOTE
- x_t is observed value at time $t$
- \mu is mean of $x$, if no change in distribution
- Hence, $(x_t - \mu)$ is how much the observation is above mean at time $t$
- Detecting an increase
  $$
  S_t = \text{max}\{ 0, s_{t-1}+(x_t-\mu-C) \}
  $$
  - Determine threshold $T$ and ask whether S_t ⩾ T?
    - If running total < 0, it's irrelevant
    - There should still be some randomness
    - C is a term to control how faster S_t increases
- Detecting a decrease
  $$
  S_t = \text{max}\{ 0, s_{t-1}+(\mu-x_t-C) \}
  $$
  - Is S_t ⩾ T?
*** Interpretation
- Choices of model parameters
  - T :: the threshold, above which alarm is raised
  - C :: the control term (smaller = more sensitive)
- Consider / trade off:
  1. How costly is it to delay detection? (false negative) -> if it's costly, use small C
  2. How costly is false positive? -> if it's costly, use big C
- Use a control chart and plot S_t vs t with $T$ as a horizontal line
  [[./img/06-control-chart.png]]
