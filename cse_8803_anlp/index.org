#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: CSE 8803: Applied Natural Language Processing
* Week 1: Text data preprocessing + Course Intro
** Why ANLP?
- Text and docs are everywhere
- Hundreds of languages in the world
- Primary information artifacts
- Large volumes of textual data
- Big and small companies looking for this skill
** Lots of text and written information
- Internet
- Webpages, Facebook, Wikipedia, etc.
- Digital libraries: Google Books, ACM, IEEE
- Lyrics, subtitles, etc.
- Police case reports
- Legislation
- Reviews
- Medical reports
- Job descriptions
** Example applications of NLP
- Establish authenticity, detect plagiarism
- Classification of genres
- Classification of tone; sentiment analysis
- Syntax analysis in code
- Machine translation
** Challenges of NLP
- Interdisciplinary field
- Ambiguity at many levels of language:
  1. Lexical (Word level)
  2. Syntactic: different ways of parsing
  3. Partial information: e.g., how to interpret pronouns
  4. Contextual information: context of sentence may affect meaning of sentence
** Class overview
- Preprocessing:
  - Clean text and documents
  - Tokenization
  - Reducing inflectional forms of a word:
    - Stemming
    - Lemmatization
  - Normalization
- Text representation
  - One hot encoding
  - Bag of words (Frequency counting)
  - Term frequency-Inverse document frequency (TF-IDF)
  - Embeddings
- Overview of classification methods
  - Naive Bayes
  - Logistic regression
  - SVM
  - Perceptron
  - Nerual Network
- Overview of Deep Learning
  - Convolutional neural network
  - Recurrent neural network
  - Long short-term memory
- Overview of topic modelling
  - Principal component analysis
  - Singular value decomposition
  - Latent Dirichlet Allocation
- Overview of Transformer methods
  - Bidirectional Encoder Representations from Transformers
  - Generative Pre-trained Transformers (GPT)
** Deliverables
*** Homework
- HW1: Text preprocessing and classification intro
- HW2: Classification methods, dimensionality reduction, SVD
- HW3: Deep learning
- HW4: Transformers and unsupervised methods
*** Quizzes (10)
- Measure understanding of topic
- Mostly conceptual questions
- MCQ
- Limited time to do the test
- Mandatory
** Course goals
- Demonstrate how to pre-process textual data
- Differentiate text representation methods and techniques
- Explain different NLP tasks
- Develop and assess performance of different NLP models using a variety of techniques
** Text Preprocessing Techniques
*** Terminology
- Corpus :: collection of text, e.g. Yelp reviews, Wikipedia articles
- Syntax :: Grammatical structure of text
- Syntactic parsing :: process of analyzing natural language with grammatical rules
- Semantics :: meaning of text
- Tokenization :: splitting long pieces of text into smaller pieces (tokens). e.g.: ~This is a simple sentence~ -> ~["This", "is", "a", "simple", "sentence"]~
- Stop words :: commonly used words, e.g. "the", "a", "an", "is", "are". Do not contribute to overall meaning
- N-grams :: consecutive sequence of words (commonly: 2-5) in a text. 1-gram (unigram), 2-gram (bigram), 3-gram (trigram). Example of bigrams: ~"This is", "is a", "a simple", "simple sentence"~
*** Preprocessing text data
- Text is unstructured, so preprocessing is the first step to prepare and clean text data to perform a NLP task
- Useful libraries:
  - re: regular expressions
  - nltk: natural language toolkit
- Common steps:
  - Noise removal
  - Tokenization
  - Text normalization
*** Noise removal
Removal of unwanted text formatting information, e.g.:
- Punctuation
- Accent marks
- Special characters
- Numeric digits (could be replaced with words)
- Leading, ending and vertical whitespace
- HTML formatting

Example: ~This is a 'simple'' sentence !!! 1+ \n~ -> ~This is a simple sentence~
*** Tokenization
Example:
~This is a simple sentence~ ->
~['This', 'is', 'a', 'simple', 'sentence', '.']~
*** Text normalization
Removing variations in the text to bring it to a standard form.
- Case: Convert all letters to upper or lower case
- Removing stop words, sparse terms, other special / particular words.

Example of text normalization:
~This is a Simple SenTence~ ->
~simple sentence~
- Stemming: reduce words to word stem, base, or root form.
  Example: ~There are several tytpes of stemming algorithms~ -> ~there are sever type fo stem algorithms.~
- Lemmatization: similar to stemming. Reduces inflectional forms to a common base form, **the lemma**. Does **not** simply chop off inflections. Uses **lexical knowledge** to get the correct base form of words.
  Example: ~There are several tytpes of stemming algorithms~ -> ~There are several type of stemming algorithms.~

* Week 2: (Discrete) Text Representations
** Why?
- NLP :: design algorithms to allow computers to understand natural language, so as to perform some task
- Required :: convert text data to numerical data that can be used in model
** Representing Words
- Can be represented by vectors of 0 & 1 where 1 indicates the position of the word, e.g. lorem = ~[1, 0]~, ipsum = ~[0, 1]~, etc.
** Representing sentences/documents
- Vectors of vectors eg ~[[1,0], [0,1]]~
** One Hot Encoding
*** Definitions
- corpus :: all texts
- vocabulary, _V_ :: all unique words
- vocabulary size, _d_ :: number of unique words, "dimensions"
- word, _w_ :: represented by vector $X$
$X^w_i$ = 1 if idw(w) = 1, 0 otherwise
- document :: represented by matrix sized $n \times d$
- _n_ ::  number of words in document
- _d_ :: a single vector with multiple values of 1 where vocab. words are present
- Document, _D_ :: e.g. _this is a sentence_
- Vocabulary, _V_ :: e.g. ~[aardvark, ..., sentence, ..., zither]~
- OHE, $X^D$ :: ~[0, ..., 1, ...1]~
*** Advantages and disadvantages
- Advantages: easy to implement
- Disadvantages:
  - not scalable for large vocabulary
  - high dimensional sparse matrix results in expensive memory + computation
  - each word represented individually, hence *no notion of similarity or meaning*. All vectors are orthogonal

    $(w^{good})^T \cdot w^{great} = (w^{good})^T \cdot w^{bad} = 0$
** Bag of Words (Frequency Counting)
- Summary :: Represents each document as a bag of words. **Ignores** order of words.
- Document :: a column vector of $X$ word counts
- Representation :: Fixed-length representation
- Document, _D_ :: e.g. ~It was the best of times, it was the worst of times~
- Vocabulary, _V_ :: e.g. ~[aardvark, ..., zither]~
- Bag of words: _X_ :: [2, ..., 1]
- Size of _X_ :: $1 \times d$ ($d$ = vocabulary size)
Hence $n$ documents can be represented by matrix of size $n \times d$.
*** Advantages and disadvantages
- Advantages: easy to implement
- Disadvantages:
  - Not scalable for large vocabulary
  - high dimensional sparse matrix results in expensive memory + computation
  - Order of words is disregarded; **no meaning** from context
** TF-IDF (Term Frequency-Inverse Document Frequency)
*** Why needed?
- BoW does not provide logical importance
  - i.e., each word is equally important
- TF-IDF assigns more logical importance to words in each document
*** What is TF-IDF and when to use TF-IDF
- Definition of TF-IDF :: a word's **importance score** in a document among $N$ documents
- _N_ :: total number of documents
- Word count :: likely TF-IDF
- Term frequency, _TF_ :: the number of times a word appears in **a document**.
  TF is high if word appears many times in document, e.g. _the_, _a_, etc.
- Inverse document frequency, _IDF_ :: $\log(\frac{N}{\text{number of docs containing the term}})$.
  If all (or most) documents contain that term, then IDF will be **very small**
- Word's importance score :: $TF \times IDF$.
  Higher score = more "characteristic"
*** Advantages and disadvantages
- Advantages:
  - Easy to implement
  - Higher score = "more characteristic". Common words will have very small scores.
  - Good technique to search for documents, find similar documents, cluster documents
- Disadvantages
  - Does not consider position of words when creating matrix. Similar problem as with BoW.
* Week 3: Linear Text Classification
** Classification introduction
Note: **classification**.
*** Supervised learning: definitions

- Word count matrix / document term matrix :: dataset generated from documents
- Rows of matrix :: each row is 1 document
- Columns of matrix :: each column is 1 unique word
- Unique words: synonyms :: features, dimensions, attributes, variables, columns
- Documents: synonyms :: rows, data points, instances
- Model weights :: = model parameters, i.e. what the model learns
- Function $F$ :: maps $X$ to $Y$
- Training data $(x_i, y_i)$ :: within set of ${X \times Y}$
- Learning - find $\hat{f}$ :: $\hat{f} \in F$ s.t. $y_i \approx \hat{f} (x_i)$
- New data :: $x$
- Prediction $y$ :: $= \hat{f} (x)$

Supervised learning thus takes **labelled** training data and **learns** or **derives** a function $f(x): y = f(x)$.

*** Categories of supervised learning
- continuous $y$ :: regression i.e. curve fitting
- discrete $y$ :: classification i.e. class estimation

*** Regression
- Errors represent how much predictions deviate from actual values.
- Minimum error = 0, however beware of overfitting, where test errors will be high (trained model cannot generalize).
- Example: apartment rent prediction, stock price prediction (difficult due to many predictors, known and unknown).

*** Classification
- Linear classification can be used for spam detection, sentiment analysis, handwriting digit recognition (0.4% error here), etc.
- Prepare, clean data, fit a classifier
- Retraining is required due to new evolving context, new lingo, etc. Can be implemented into a learning system.

** Naive Bayes
*** Method / concepts
Bayes Decision Rule.
- $x$ :: encoded document, e.g. by BoW
- $y$ :: label of document, i.e. whether document contains positive or negative message
- Posterior :: $P(y|x)$
- Likelihood :: $P(x|y)$
- Prior :: $P(y)$
- Normalization constant :: $P(x)$

$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)} = \frac{P(x,y)}{\sum_y P(x,y)}
$$
*** Bayes decision rule
- {{{hl(important)}}}: normalization constant is the same for +ve and -ve labels, hence no need to calculate when predicting sentiment
*** Generative vs discriminative models
Naive Bayes is a generative model
- Generative model: able to generate synthetic data points
  - **Need** to model prior and likelihood distributions.
  - In Naive Bayes, we normally replace likelihood with the conditional distribution.
  - Conditional distribution is the pdf/pmf to generate data points.
    - Determining this distribution might be difficult.
  - Generative models e.g.: Naive Bayes, Hidden Markov Models
- Discriminative models:
  - Directly estimate posteriors
  - No need to model prior and likelihood distributions
  - e.g.: logistic regression, SVM, neural networks
*** Details of Naive Bayes
Bayes decision rule:
$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$
- {{{hl(assumption)}}}: all dimensions (unique words) are independent of each other, i.e. $p(x|y = 1)$ fully factorized, hence: $P(x|y=1) = \prod^d_{i=1} P(x_i|y = 1)$
  - Thus, likelihood can be written in fully factorized way.
  - It becomes a big joint probability of all unique words (dimensions).
  - Conditional independence, hence likelihood can be written as multiplication of every dimension given the label.
  - i.e., the variables corresponding to each dimension are independent given the label.
*** Naive Conditional Independence Assumption
$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$

For vocabulary $V$, ~[nice, give, us, this, iu, ssn, information, job, a]~

$P(\text{document} | y = \text{positive})P(y=\text{positive})$

= $P(x=\text{nice}) ... P(x=a|y=\text{positive})$ $\cdot P(y= \text{positive})$

similarly for negatives:

$P(\text{document} | y = \text{negative})P(y=\text{negative})$

= $P(x=\text{nice}) ... P(x=a|y=\text{negative})$ $\cdot P(y= \text{negative })$
**** Representing the likelihood
Common distribution: **multinomial distribution**.

$$
P(x=\text{nice} | y = \text{positive})
$$

$$
= \frac{\text{count of word }\textbf{nice} \text{ in all positive label docs }}{\text{count all words with } \textbf{positive}  \text{  labels}}
$$

Then to calc priors:

$$
P(y = \text{positive}) = \frac{\text{count # +ve docs}}{\text{count # all docs}}
$$

Repeat above for negatives.
*** Advantages and disadvantages
- Advantages
  - Simple, easy to implement
  - No training required
  - Good results in general
- Disadvantages
  - Position of words do not matter (no semantic meaning) due to BoW approach
  - Requires / assumes conditional independence

** Classification Model Evaluation
*** Common metrics
- Classification: accuracy, precision, recall, cross-entropy, perplexity, and F1 score
- Regression: MSE, MAE
*** Confusion matrix
- e.g. for multi-label confusion matrix
- rows are the actual classes (sport, news politics)
- columns are the predicted classes
- diagonal elements are number of accurate predictions
- off-diagonals: inaccurate predictions
- But difficult to parse, can consider using a heat map on the confusion matrix instead of raw #
- {{{hl(meaning of positive and negative in a confusion matrix)}}}: not related to sentiment. Only indicator of the label, e.g. sport = positive, news = negative.
*** Accuracy
- Accuracy = (True Positive + True Negative) / Total observations, i.e. sum of diagonals / count observations.
- May not be represent "goodness" since false positives and false negatives have identical treatment.
- FP and FN may be important specifically for some fields e.g. medicine.
- Another metric, false alarm (false positive, type I error) is easy to remember in security contexts.
*** RoC-AUC curve
- ROC: Receiver Operating Characteristic
- Changing thresholds: how to change, what should the new threshold be?
- TP (y-axis) vs FP (x-axis)
- AUC (area under the curve) represents the how performant the predictive model is. Max is 1.0.
- But 0.9 may not be good either.
  - Are there some thresholds where TP = 0? Are these important in the context?
* Week 5: Log Regression, SVM and Perceptron (Module 4)
** Logistic Regression
- Backbone of neural network model
- Created on linear combination of features
- Outputs a *probability*
  - Logistic regression is thus a *soft classification*
*** Generative vs Discriminative Models (again)
- Generative model: able to generate synthetic data points
  - **Need** to model prior and likelihood distributions.
  - Conditional distribution is the pdf/pmf to generate data points.
    - Determining this distribution might be difficult.
  - Generative models e.g.: Naive Bayes, Hidden Markov Models (HMM)
- Discriminative models:
  - Directly estimate posteriors
  - No need to model prior and likelihood distributions
  - e.g.: logistic regression, SVM, neural networks
*** Bayes equation again
$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)} = \frac{P(x,y)}{\sum_y P(x,y)}
$$
- Generative models :: need to calculate likelihood and prior explicitly
- Discriminative models :: can we calculate posterior directly without using Bayes equation?
*** Logistic Function for posterior probability
i.e. the following function

$$
P(y|x) = g(s) = \frac{e^s}{1+e^s} = \frac{1}{1+e^{-s}}
$$

- This function is known as the **sigmoid function**.
- Easy to use this for optimization
- Threshold: always 0.5?
  - Threshold can be investigated with ROC-AUC to determine best threshold
- Neural network with just 1 block is similar to logistic regression
- Logistic regression: sigmoid is the activation function

[[./img/sigmoid.png]]

- Three linear models with different activation functions
  - Using a **sine** activation function: it will be transformed to perceptron, a hard classification
*** Sigmoid is interpreted as probability
- e.g., does a customer like a product based on feedback?
  - Input: $x$ a BoW or TF-IDF of a document that contains customer's feedback
  - $g(s)$ is the probability of whether a customer likes a product
  - Cannot have hard prediction or classification here
  \begin{equation}
  s = x\theta \text{ the risk score} \\
  h_\theta (x) = p(y|x) =
  \begin{cases}
  g(s) & y=1 \\
  1-(gs) & y=0 \text{ using posterior probability directly}
  \end{cases}
  \end{equation}
- Sigmoid is the inverse of *logit* function (or the log-odds ratio)
*** Logistic regression model
- Expanding equation and replacing $g(s)$ with linear combination of features
- Probabilistic model
- Uses MLE to optimize linear combination of features
- Use log-likelihood for better numerical stability
- To find \theta parameters, for $n$ data points:
  \begin{equation}
  P(y|x) =
  \begin{cases}
  \frac{1}{1+ \exp(-x\theta)} & y=1 \\
  1-\frac{1}{1+\exp(-x\theta)} = \frac{\exp(-x\theta)}{1+\exp(-x\theta)} & y=0
  \end{cases}
  \end{equation}
*** The gradient of $l(\theta)$
$$
l(\theta) := \log \prod^n_{i=1} p(y_i, |x_i, \theta) \\
= \sum_i \theta^T x_i^T (y_i -1) - \log(1+\exp(-x_i \theta))
$$
**Gradient**:
$$
\frac{\partial l(\theta)}{\partial \theta} =
\sum_i x_i^T (y_i-1) + x_i^T \frac{\exp(-x_i \theta)}{1+\exp(-x_i \theta)}
$$
- Even when set to 0, there is **no** closed-form solution.
  - Even though there is a global solution
  - Unlike linear regression where there is a closed-form solution
  - Hence, logistic regression is unconstrained, but
  - Can optimize using iterative approach such as gradient descent
*** Gradient descent
- One way to solve unconstrained optimization problem is gradient descent
- Given initial guess, we iteratively refine the guess by taking the direction of the {{{hl(negative gradient)}}}
- Analogous to going down the hill by taking steepest direction at each step
- Update rule
  $$
  x_{k+1} = x_k - \eta_k \nabla f(x_k)
  $$
  $\eta_k$ is the {{{hl(step size or learning rate)}}}
- Step taken should be small enough
*** Gradient ascent (concave) / descent (convex) algorithm
- Initialize parameter $\theta_0$
- Do:
  $$
  \theta_{t+1} \leftarrow \theta^t + \eta \sum_i x_i^T (y_i-1) + x_i^T \frac{\exp(-x_i \theta)}{1+\exp(-x_i \theta)}
  $$
- while:
  $$
  \parallel \theta^{t+1} - \theta^t \parallel > \epsilon
  $$
- ascent: maximize function
- descent: minimize
- Thus:
  - Logical threshold = 0.5, i.e. predict 1 if $g(s) \ge 0.5$
*** Advantages and disadvantages of logistic regression
- Advantages:
  - Simple
  - No need to model prior or likelihood
  - Provides probability output
  - Works with datasets with few features
- Disadvantages:
  - Needs to have discriminative model assumption
  - Model needs to be optimized using numerical approach
  - Might not work with complicated dataset
**
** Support vector machine
- SVM is a large margin classifier
*** Linear separation
- Can have different separating lines, so which line is the best?
  - Why is having bigger margin better?
  - What \theta maximizes margin?
[[./img/lin-sep.png]]
- All cases, error is zero and they are linear, so they are all good for generalization.
- SVM focuses on just one solution (compared to perceptron) and that's the maximum margin solution
- SVM maximizes margin and provides decision line with maximized margin, which is the {{{hl(most stable)}}} under perturbations of inputs
*** Finding \theta that maximizes margin
- Objective function created by constructing linear combination of features.
  - Solution (decision boundary) of the line
    $$
    x \theta = 0
    $$
  - Let $x_i$ be the nearest data point to the line/plane
  - Decision boundary is thus $x\theta + b = 0$
    - Below decision line: \le 0
    - Above decision line: \ge 0
  - Scaling up / down \theta thus allows you to set the nearest point to $1$.
*** Length of margin
$$
\text{distance} = \frac{1}{\parallel \theta \parallel} |(x_i \theta - x \theta)|
= \frac{1}{\parallel \theta \parallel}|(x_i \theta + b - x\theta -b)|
$$
where:
- $x_i \theta + b$ :: my constraint $\equiv |x_i \theta + b| = 1$
- $-x\theta - b$ :: a point on the decision line $\equiv x\theta + b = 0$

Therefore total margin is: $\frac{2}{\parallel \theta \parallel}$ (since there are 2 points on each side of the decision line)

[[./img/large-margin.png]]

- \theta is {{{hl(orthogonal)}}} to the decision line
*** Maximizing margin
- Maximize $\frac{2}{\parallel \theta \parallel}$ in the objective function
- Subject to $\min_{i=1,2,...,N} |x_i \theta + b| = 1$ which is the nearest neighbour, sign-agnostic for labels here, hence absolute.
- Hard to optimize this due to the "min" in the constraint (non-convex form)
- To get rid of the absolute value in the constraint, (and to get the correct prediction, predicted value must have same sign as actual)
  $$
  \left|x_i \theta + b\right| = y_i(x_i \theta + b) \rightarrow \text{for correct classification} \\
  \text{ if} \min |x_i \theta + b | = 1 \rightarrow \text{ it can be at least 1}
  $$
- Hence,
  $$
  \max \frac{2}{\parallel \theta \parallel}
  \\
  \text{subject to } y_i (x_i \theta + b) \ge 1 \text{ for } i=1,2,...,N
  $$
*** Geometric representation
[[./img/geom-rep.png]]
- Decision line :: $x\theta + b = 0$
- Margin line :: $x \theta + b = 1$
- Blue colors :: constraint (data points beyond margin line); beyond margin line the margin \ge 1, correctly classified
**** Converting problem
- Many ML libraries can solve minimization problems instead of maximization
- Hence, convert from:
  $$
  \max(\frac{2}{\parallel \theta \parallel}) \\
  \text{subject to } y_i (x_i \theta + b) \ge 1 \text{ for }i=1,2,...,N
  $$
- to:
  $$
  \min(\frac{1}{2} \theta\theta^T) \\
  \text{subject to } y_i (x_i \theta + b) \ge 1 \text{ for }i=1,2,...,N
  $$
*** Lagrange formulation (not in detail)
$$
\min(\frac{1}{2} \theta\theta^T) \\
\text{subject to } y_i (x_i \theta + b) -1 \ge 0  \\
\textit{L}(\theta, b, \alpha) = \frac{1}{2}\theta\theta^T - \sum^N_{i=1} \alpha_i (y_i(x_i \theta + b)-1)
$$
becomes:
$$
\min \text{w.r.t. } \theta, b \text{ and } \max \text{w.r.t. each } \alpha_i \ge 0 \\
\nabla_\theta L(\theta, b, \alpha) = \theta - \sum^N_{i=1} \alpha_i y_i x_i = 0 \\
\nabla_b L(\theta, b, \alpha) = -sum^N{i=1} \alpha_i y_i = 0
$$
under KKT conditions,
where:
- $\theta$ :: model parameter
- $b$ :: bias term
- $\alpha$ :: Lagrange multiplier
Need to convert primal form to dual form.
Take gradient w.r.t. \theta, b, set to 0.
Calculate parametric value of \theta and new constraints.
Convert objective function to dual form.
$$
\theta = \sum^N_{i=1} \alpha_i y_i x_i \text{ and } \sum^N_{i=1} \alpha_i y_i = 0 \\
L(\theta, b, \alpha) = \sum^N{i=1} \alpha_i - \frac{1}{2} \theta \theta^T \\
L(\theta, b, \alpha) = \sum^N_{i=1} \alpha_i - \frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} y_i y_j \alpha_i \alpha_j x_i x_j^T
\\
\max \text{ w.r.t. each } \alpha_i \ge 0 \text{ for }i=1,...,N \text{ and }
\sum^N_{i=1} \alpha_i y_i = 0
$$
*** Usage
- Dual form good for binary classification, e.g. spam or not spam.
- Training
  $$
  \theta = \sum^N_{i=1} \alpha_i y_i x_i
  $$
  - No need to go over all data points
  - $$
    \rightarrow \theta = \sum_{x \in \text{ SV}} \alpha_i y_i x_i
    $$
  - and for $b$ pick any support vector, and calculate $y_i (x_i \theta + b) = 1$
- Testing
  - For new point $s$, compute:
    $$
    s \theta + b = \sum_{x_i \in \text{ SV}} \alpha_i y_i x_i s^T + b
    $$
  - Classify $s$ as class 1 if positive, else classify as class 2.
*** From $x$ to $z$ space
- SVM can only be used when a linear decision line can be used
- Sometimes it may be possible to work around by moving from Cartesian to Polar space
- Not necessarily applicable to NLP since there are many many dimensions.
- Instead, kernel trick can be utilised in the dual form model, do feature engineering and handle millions of features.
**** Kernel trick
Main premise is to take data from original space to newer space with higher dimensions, which make it more likely to have linear separation in the newer space.

In $x$ space, they are called pre-images of support vectors.
*** Support vector machine
- Can do {{{hl(either)}}}
  - Hard classification
  - Soft classification
** Perceptron
[[./img/spam.png]]
- Needs to be linearly separable to work
- Can be used for text classification, sentiment analysis
- Given training data $(x_i, y_i)$ for $i = 1,...,N, x_i \in \mathbb{R}^d \text{ and }y_i \in {-1,1}$ learn a classifier $f(x)$ such that
  \begin{equation}
  f(x_i)
  \begin{cases}
  \ge 0 & +1 & \text{Non-spam document} \\
  \lt 0 & -1 & \text{Spam document}
  \end{cases}
  \end{equation}
- i.e. $y_i f(x_i) \gt 0$ for a correct classification
*** Linearly separable
[[./img/linear-sep.png]]
- The two labels must be separable by a **straight** line
- Perceptron uses linear classifier, as it uses linear combination of features
*** Linear classifier
[[./img/lin-class.png]]
Linear classifier has the form
$$
f(x) = x\theta + \theta_0
$$
- In 2D, the discriminant is a line
- $\theta$ is the **normal** to the decision line
- $\theta_0$, is the bias term
- $\theta$ is known as the model {{{hl(parameter)}}} or the {{{hl(weight vector)}}}
- Decision boundary has $d-1$ dimensions where $d$ is the number of features
**** Linear classifier for higher dimensions
- In 3D, the discriminant is a plane
- in nD, the discriminant is a hyperplane
*** The Perceptron Classifier
- {{{hl(hard classifier)}}}
- Considering $x$ is linearly separable
- $y$ has 2 labels $\{-1,1 \}$
- $f(x_i) = x_i \theta$, where bias is inside $\theta$
- How to separate data points with label 1 from those with -1 using a **line**?
- Perceptron classifier is a simple for-loop
  - Goes inside every single data point to check whether it's classified correctly
**** Algorithm
[[./img/perceptron.png]]
1. Initialise $\theta = 0$
2. Go through each data point $\{x_i, y_i \}$
   1. If $x_i$ is misclassified, then $\theta^{t+1} \leftarrow \theta^t + \alpha y_i x_i$ (i.e. moving the decision line towards the correct label)
3. Until all data points are correctly classified
*** Perceptron activation
[[./img/perceptron-activation.png]]
- LHS = number of lines = number of features
- output of linear combination of features, $f(x)$ is real number,
- fed into activation function in red, which is +1 or -1
*** Advantages and disadvantages of Perceptron
- Advantages
  - Very simple
  - Fast, does not require any parameters
  - Quick training to optimize parameters
- Disadvantages
  - Works only for linearly separable data
  - Does not provide unique decision boundary
* Week 6: Embeddings/Dimensionality reduction
Singular Value Decomposition (SVD)
** SVD and Co-occurrence Embeddings
*** Motivating example
Dimensionality reduction for text is to understand how they behave in 2D or 3D space.
- This helps to get better perspective of the data.
- High dimensionality data points happens on text and data problems due to many unique words.
*** Bag of words representation
- Has many unique words (dimensions) that leads to:
  - **overfitting**
  - more resources & time needed
- BoW generates a term-document matrix with many many features that's sparse
- Possible solution: **dimension reduction**
*** What is dimensionality reduction?
[[./img/dim-reduct.png]]
- Dimensionality reduction is the process of reducing **random variables** under consideration
- Possible approaches:
  - Combine, transform or select variables
  - With linear or non-linear operations
- New space has lower dimensions than previous space
*** Intuition (of PCA)
- Approximate a $D$ -dimensional dataset using fewer dimensions
- By rotating the axes into a new space
- Highest order dimension captures the most variance in the original dataset
- Next dimension captures the next most variance, etc.
- PCA uses eigendecomposition of covariance of dataset to maximize variance
  - Eigenvector corresponding to the highest eigenvalue is the new dimension that maximises the variance the most
  - Hope of PCA is that a dimension that explains variance the most would explain data better and it's easier to separate and distinguish labels when data points are spread out because of high variance
*** Singular value decomposition
For a matrix $X_{n \times d}$ where:
- n :: number of instances
- d :: dimension

$$
X = U \Sigma V^T
$$

- U, \Sigma, V :: all unitary matrices
- m columns :: represent a dimension in a new latent space s.t. $m$ column vectors are orthogonal to each other, and ordered by the amount of variance in the dataset in each dimension. $m$ has **maximum** of $d$ dimensions
- $U_{n \times m}$ :: unitary matrix \rightarrow $UU^T = I$
- $\Sigma_{m \times m}$ :: diagonal matrix of singular values of $X$
- $V_{m \times d}$ :: unitary matrix $\rightarrow VV^T = I$
*** Co-occurrence matrices
#+BEGIN_BLOCKQUOTE
Instead of matrix.
#+END_BLOCKQUOTE
Each matrix for one value of context length.

- Meaning of a word is defined by the words in its surroundings
- Define a context window as the number of words appearing around a centre word
- Create a co-occurrence matrix:
  1. Go through each central word-context pair in corpus (context window length is commonly in $[1,5]$)
  2. In each iteration, update the row of the count matrix (of central word) by adding +1 in the columns for the context words
  3. Repeat last step many times
[[./img/co-matrix.png
]]
*** SVD on co-occurrence matrices
- For corpus with vocabulary $V$ of size $d$, co-occurrence matrix has size $d \times d$
- Size of co-occurrence matrix increases with vocabulary
- Instead of keeping all dimensions, can instead use truncated SVD to keep only to $k$ singular values
  - e.g. $k=300$
- Result is a least-square approximation to the original co-occurrence matrix $X$
  [[./img/svd-co-occur.png]]
- Single value is directly related to the new dimension that maximizes co-variance
*** Dense word embeddings
[[./img/dense.png]]
- Each row of $U$ is a $k$ -dimensional representation of each word $w$ in the corpus that best preserves variance
- Generally, keep top $k \in [50, 500]$ dimensions.
- Produces dense vectors for word representations, while also considering the word contexts that carry meaning
*** Advantages of dense word embeddings
- Denoising: low-order dimensions may represent unimportant information; higher-order dimensions keep only important information
- Truncation may help models generalize better to unseen data
- Having smaller number of dimensions may make it easier for classifiers to properly weigh the dimensions
- Dense models may do better at capturing higher-order co-occurrence
- Dense vectors work better in word similarity
- Example of word-similarity method is cosine similarity between two word-embeddings $w, v$:
  $$
  \text{cosine} (\vec{v}, \vec{w}) =
  \frac{\vec{v}\cdot \vec{w}}{|\vec{v}| |\vec{w}|}
  = \frac{\sum^N_{i=1}v_i w_i}{\sqrt{\sum^N_{i=1} v_i^2} \sqrt{\sum^N_{i=1} w_i^2}}
  $$
** GloVe
Global Vectors.
*** Definitions
- Global :: global statistics of corpus
- Vectors :: representation of words
*** GloVe model
- Glove uses statistics of word occurrences in a corpus as the primary source of information.
- Combines 2 widely adopted approaches for training word vectors:
  1. Global matrix factorization
  2. Window-based methods
- Uses Co-occurrence matrix as a starting point
*** Extending the co-occurrence matrix
- Definition :: For corpus of vocabulary $V$ of size $d$, the co-occurrence matrix is a symmetrical matrix of size $d\times d$
- $X_{ij}$ :: number of times word $j$ occurs in the context of word $i$ after defining window size
- $X_i = \sum_k X_{ik}$ :: summation over all the words which occur in the context of word $i$
- $P_{ij} = \frac{X_{ij}}{X_i}$ :: the co-occurrence probability where $_{ij}$ is the probability of word $j$ occurring in the context of word $i$
*** Example
#+BEGIN_BLOCKQUOTE
It was the best of times, it was the worst of times.
(Context window=2)
#+END_BLOCKQUOTE
i = "it", j = "was"
- $X_{i=0, j=1} = 2$
- $X_{i=0} = 6$
- $P{i=0, j=1} = 2/6 = 0.33$
*** GloVe cost function
- GloVe suggests finding the relationship between 2 words in terms of probability, rather than occurrence counts
- GloVe looks to find vectors $w_i$ and $w_j$ such that
  $$
  w_i^T w_j = \log(P_{ij}) = \log(\frac{X_{ij}}{X_i})
  $$
- $\log(X_i)$ is independent of word $j$ and can be represented as a bias $b_i$
- Adding a bias term to restore the symmetry for vector $w_j$ we get:
  $$
  w_i^T w_j + b_i + b_j = \log(X_{ij})
  $$
- A weighted least squares is used as a cost function for the GloVe model:
  $$
  J = \sum_{ij} f(X_{ij})(w_i^T w_j + b_i + b_j - log(X_{ij}))^2
  $$
  with:

  \begin{equation}
  f(x) =
  \begin{cases}
  (\frac{x}{x_{\text{max}}})^4 & \text{ if }x < x_{\text{max}} \\
  1 & \text{otherwise}
  \end{cases}
  \end{equation}
 - In original paper, $\alpha = \frac{3}{4}$ gave the best performance
*** GloVe word vectors
- Trained in batches of the training sample with optimizer to **minimize** the cost function and hence generate word and context vectors for each word
- Each word in the corpus is represented bya dense vector of fixed size length
- Word vectors obtained by GloVe showcase the meaning that was captured in these vector representations through similarity and linear structure
- Using Euclidean distance or cosine similarity between word vectors represents **linguistic** or **semantic** similarity of the corresponding words.
- E.g. "summer" is most similar to "winter", "spring", "autumn"
*** GloVe conserves linear relationships
- Word vectors by GloVe conserve linear substructures
- Vector differences capture as much as possible the meaning specified by two words
- E.g.: the underlying concept that differentiates man and woman, i.e. gender, may be equivalently specified by other word pairs such as king and queen:
  $$
  w_{\text{man}} - w_{\text{woman}} = w_{\text{king}} - w_{\text{queen}}
  $$
* Week 8: Neural Networks and Word2Vec
** Neural Networks
M6T1
*** Inspiration from biological neurons
Neurons: core components of the brain and nervous system.
Consists of:
- Dendrites :: collect information from other neurons
- Axon :: generates outgoing spikes
*** Logistic regression block review
[[./img/neuron-block.png]]
- **Summation** part is a linear combination of features or dimensions - i.e. unique words in the document term matrix
  - Receives a data point as an input, which can be multi-dimensional
  - Linearly combines them using model parameters, shown as \theta .
  - Linear combination of features, the output of the output of summation function, captures the linear relationship between input and output
  - Linear function may not be sufficient to capture the non-linear and complex relationship between input data points and their output.
- Hence, **activation** function is needed, which is fed by output of summation term.
  - Typically chosen to be a non-linear function, which helps the network understand and learn the complex relationship between input and output.
  - Well-known activation functions:
    - Linear unit :: does not change the output of the summation function. $z$
    - Threshold/Sign :: used for hard classification algorithm. Positive or negative output for binary classification. Decision line is zero. $\text{sgn}(z)$
    - Sigmoid :: used to scale output between $[0,1]$. Commonly used for classification problems. If used with 1 learning block, it's a logistic regression algorithm with soft classification. $\frac{1}{1+\exp(-z)}$
    - ReLu :: Rectified linear unit. Commonly used in deep learning methods because of friendly optimization, back propagation and fast training of predictive model. $\max(0,z)$
    - Tangent hyperbolic (Tanh) unit :: Scales a real value from -1 to +1. Captures negative values (vs. sigmoid unit), which just scales between $[0,1]$. $\tanh(z)$
*** Connecting blocks to create neural networks
Recap:
- a block received a data point or document having features, and linearly combines them in a summation function, that is then fed to activation function $h(x)$.
- New neuron or feature would help network and original features learn more complex relationship between inputs and outputs.
- Artificial neural network can solve both regression and classification problems.
- Fully connected network: each neuron needs to be connected to all the learning blocks.
  [[./img/fc-neurons.png]]
  - 1 :: Bias value
  - $x_i$ :: Feature
  - $\theta$ :: Weights or parameters
  - $\mu_{21}$ :: summation output for layer 2, depth 1

- {{{hl(last activation function)}}} defines what model / problem we're trying to solve (regression or classification)
- Layers not connected to the last learning block are {{{hl(hidden layers)}}}
- ANN can have many hidden layers, e.g. $\theta_0$ till $\theta_6$
- Changing the "first neuron" (last neuron) to sigmoid changes this to solve a **classification** problem
*** Increasing the depth of each layer
i.e. same number of layers, but more neurons
- Can be doen by increasing the num,ber of learning blocks
- This generates more O's, increasing the number of parameters
*** Increasing layers
- Add more hidden layers
- There can be as many hidden layers as needed
- These are hyperparameters
  - Number of neurons
  - Number of hidden layers
- These hyperparameters need to be tuned for the complexity between inputs and outputs
- Need to {{{hl(prevent overfitting)}}} especially if there's insufficient training or testing data
*** Forward pass

$$
u_{11} = \sum^d_{i=0} x_i \theta_i = \theta_0 + \theta_1 x_1 + ... + \theta_d x_d \\
O_{11} = \frac{1}{1+e^{-u_{11}}}
$$
- In the forward pass, calculate all $u_{ij}$ and $o_{ij}$ values from {{{hl(left to the right)}}} of the network
*** Backpropagation
- Update all $\theta_i$ parameters from the right to the left of the network
- Optimization can be done using iterative techniques such as gradient descent
- Updating parameters depends on the types of loss functions:
  - Regression: RMSE
  - Classification: Cross-entropy
- Minimize the loss function by taking partial derivatives w.r.t. the model parameters
- Last parameters are updated first, and use chaining rule to update the other parameters
- Types of approaches
  - Stochastic gradient descent or iteration :: 1 document at a time. Process one document at a time, need 1000 iterations to go through all data points. Each time going through all documents = 1 epoch. {{{hl(Memory friendly)}}} but computationally slow as we are updating parameters for each document one at a time.
  - Batch gradient descent :: Pass a sub-portion of documents each time. E.g. if 50 documents / iteration, will reduce the number of iterations from 1000 to 20. Also possible to pass **all** data points at once, but usually can't fit in memory.
** Word2Vec, CBOW and Skipgram
M6L2
*** Review of One-Hot Encoding
- Simplest word embedding.
- Example 1 document: "Apple and orange are fruit". Vectors:
  - Apple :: ~[1, 0, 0, 0]~
  - Orange :: ~[0, 1, 0, 0]~
  - Are :: ~[0, 0, 1, 0]~
  - Fruit :: ~[0, 0, 0, 1]~
- Common words are removed first. They don't
  provide distinguishing features for the words in the corpus.
*** Issues with One-Hot Embedding
- The size of each word vector = vocabulary size in the corpus. Creates a huge vector if we have millions of words in the vocabulary.
- Very long OHE vector wastes storage and computation
- Curse of dimensionality can emerge for very large vectors
- With a new corpus, the size of each word vector will be different and the model previously trained will be useless (can't transfer learning).
*** Contextual meaning of the words
- Word2Vec and GLoVE are context-independent.
  - They output just one vector embedding for each word, combining all different senses of the word into 1 vector.
- E.g. for the example above, do "apple" and "fruit" share some common features since they're all fruit?
  - No, because OHE is just 0 and 1 embedding and does not consider the contextual meaning of words.
    - {{{hl(There is no correlation)}}} between words that have similar meanings or usage.
*** What do we want to achieve from word embeddings?
- Can we come up with a word embedding that can capture a numerical similarity value? e.g.
  - Similarity value of (apple, orange) == similarity value of (orange, apple)
  - Similarity value of (apple, orange) > similarity value of (apple, are)
*** Algorithm 1:: Continuous Bag of Words (CBOW)
- Use neural networks to learn the underlying representation of words
- {{{hl(Caveat:)}}} neural network model is a supervised algorithm. Needs labels. Must find a way to synthesize the labels from the corpus.
**** Neighboring words for CBOW (label creation)
- i.e., Given the neighbors of a word, can we predict the center word?
- Given the context or neighbours of a word, can I predict the blank by a window size?
- Simplicity: keep window size of 1, and remove the common words.
- Here, we want to find $P(\text{orange}|\text{context})$, and need to maximize this probability.
**** Embedding every single word in the corpus using its context
- Find the embedding representation of the word "orange".
- First, all words in the vocabulary need to be encoded using OHE. Each word will have $d$ dimensions (i.e., the size of the vocabulary).
**** Using a window size of 1
[Apple, ___, Are]
[[./img/window-size-1.png]]
- Uses a single hidden layer, which allows the embedding of a word.
- Input vectors have the size of context,
- {{{hl(In CHOW)}}} the dimensions of the hidden layer and the output layer are always the same.
- First weight $d \times E$:
  - d :: dimension of one-hot encoded word
  - E :: embedding size
- What's the desirable size to vectorize each word of the corpus?
- Size of $E$ is also a hyperparameter. It's much smaller than $d$.
- Use weight matrix to embed word after it's trained by neural network.
  - Multiply word by word matrix W, results gives us the embedded word.
- Hidden layer will be calculated from average element wise-multiplication of each input vector for weights parameter $w$.
- Need to maximize a lot of likelihood by optimizing the parameters via backpropagation
- Typically minimize negative log-likelihood instead of maximizing the original likelihood
*** Algorithm 2: Skip-Gram model
- Given a center word, what could be the context (neighbours) of the center word?
- Opposite of CBOW.
- Example: ~orange ___ fruit~
- Can I predict the blanks by a window size?
  - Window size is the hyperparameter
- To find and {{{hl(maximize)}}}: $P(\text{context | orange})$.
- Skipgram
  - Input layer: center word, i.e. "orange"
  - Output layer: the probability vector that is the prediction of all context words for each output context word.
- Calculate softmax probability
- Next, in loss function, {{{hl(minimize)}}} the negative log likelihood over all softmax context words.
- Use a {{{hl(cross-entropy loss)}}} as we treat this problem as a **classification** problem.
*** Main differences between CBOW and Skip-gram
- CBOW learns better syntactic relationships between words; Skipgram captures better semantic relationships.
  - CBOW would provide cats and cants as similar, while skipgram will provide cats and dogs as similar.
  - CNOW is trained to predict (i.e., maximize the probaility) of a single word from a fixed window size of context words, while Skip-gram does the opposite and strives to predict several context words from a single input word.
  - {{{hl(CBOW is faster to train)}}} vs. Skip-gram.
