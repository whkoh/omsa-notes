#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: CSE 8803: Applied Natural Language Processing
* Week 1: Text data preprocessing + Course Intro
** Why ANLP?
- Text and docs are everywhere
- Hundreds of languages in the world
- Primary information artifacts
- Large volumes of textual data
- Big and small companies looking for this skill
** Lots of text and written information
- Internet
- Webpages, Facebook, Wikipedia, etc.
- Digital libraries: Google Books, ACM, IEEE
- Lyrics, subtitles, etc.
- Police case reports
- Legislation
- Reviews
- Medical reports
- Job descriptions
** Example applications of NLP
- Establish authenticity, detect plagiarism
- Classification of genres
- Classification of tone; sentiment analysis
- Syntax analysis in code
- Machine translation
** Challenges of NLP
- Interdisciplinary field
- Ambiguity at many levels of language:
  1. Lexical (Word level)
  2. Syntactic: different ways of parsing
  3. Partial information: e.g., how to interpret pronouns
  4. Contextual information: context of sentence may affect meaning of sentence
** Class overview
- Preprocessing:
  - Clean text and documents
  - Tokenization
  - Reducing inflectional forms of a word:
    - Stemming
    - Lemmatization
  - Normalization
- Text representation
  - One hot encoding
  - Bag of words (Frequency counting)
  - Term frequency-Inverse document frequency (TF-IDF)
  - Embeddings
- Overview of classification methods
  - Naive Bayes
  - Logistic regression
  - SVM
  - Perceptron
  - Nerual Network
- Overview of Deep Learning
  - Convolutional neural network
  - Recurrent neural network
  - Long short-term memory
- Overview of topic modelling
  - Principal component analysis
  - Singular value decomposition
  - Latent Dirichlet Allocation
- Overview of Transformer methods
  - Bidirectional Encoder Representations from Transformers
  - Generative Pre-trained Transformers (GPT)
** Deliverables
*** Homework
- HW1: Text preprocessing and classification intro
- HW2: Classification methods, dimensionality reduction, SVD
- HW3: Deep learning
- HW4: Transformers and unsupervised methods
*** Quizzes (10)
- Measure understanding of topic
- Mostly conceptual questions
- MCQ
- Limited time to do the test
- Mandatory
** Course goals
- Demonstrate how to pre-process textual data
- Differentiate text representation methods and techniques
- Explain different NLP tasks
- Develop and assess performance of different NLP models using a variety of techniques
** Text Preprocessing Techniques
*** Terminology
- Corpus :: collection of text, e.g. Yelp reviews, Wikipedia articles
- Syntax :: Grammatical structure of text
- Syntactic parsing :: process of analyzing natural language with grammatical rules
- Semantics :: meaning of text
- Tokenization :: splitting long pieces of text into smaller pieces (tokens). e.g.: ~This is a simple sentence~ -> ~["This", "is", "a", "simple", "sentence"]~
- Stop words :: commonly used words, e.g. "the", "a", "an", "is", "are". Do not contribute to overall meaning
- N-grams :: consecutive sequence of words (commonly: 2-5) in a text. 1-gram (unigram), 2-gram (bigram), 3-gram (trigram). Example of bigrams: ~"This is", "is a", "a simple", "simple sentence"~
*** Preprocessing text data
- Text is unstructured, so preprocessing is the first step to prepare and clean text data to perform a NLP task
- Useful libraries:
  - re: regular expressions
  - nltk: natural language toolkit
- Common steps:
  - Noise removal
  - Tokenization
  - Text normalization
*** Noise removal
Removal of unwanted text formatting information, e.g.:
- Punctuation
- Accent marks
- Special characters
- Numeric digits (could be replaced with words)
- Leading, ending and vertical whitespace
- HTML formatting

Example: ~This is a 'simple'' sentence !!! 1+ \n~ -> ~This is a simple sentence~
*** Tokenization
Example:
~This is a simple sentence~ ->
~['This', 'is', 'a', 'simple', 'sentence', '.']~
*** Text normalization
Removing variations in the text to bring it to a standard form.
- Case: Convert all letters to upper or lower case
- Removing stop words, sparse terms, other special / particular words.

Example of text normalization:
~This is a Simple SenTence~ ->
~simple sentence~
- Stemming: reduce words to word stem, base, or root form.
  Example: ~There are several tytpes of stemming algorithms~ -> ~there are sever type fo stem algorithms.~
- Lemmatization: similar to stemming. Reduces inflectional forms to a common base form, **the lemma**. Does **not** simply chop off inflections. Uses **lexical knowledge** to get the correct base form of words.
  Example: ~There are several tytpes of stemming algorithms~ -> ~There are several type of stemming algorithms.~

* Week 2: (Discrete) Text Representations
** Why?
- NLP :: design algorithms to allow computers to understand natural language, so as to perform some task
- Required :: convert text data to numerical data that can be used in model
** Representing Words
- Can be represented by vectors of 0 & 1 where 1 indicates the position of the word, e.g. lorem = ~[1, 0]~, ipsum = ~[0, 1]~, etc.
** Representing sentences/documents
- Vectors of vectors eg ~[[1,0], [0,1]]~
** One Hot Encoding
*** Definitions
- corpus :: all texts
- vocabulary, _V_ :: all unique words
- vocabulary size, _d_ :: number of unique words, "dimensions"
- word, _w_ :: represented by vector $X$
$X^w_i$ = 1 if idw(w) = 1, 0 otherwise
- document :: represented by matrix sized $n \times d$
- _n_ ::  number of words in document
- _d_ :: a single vector with multiple values of 1 where vocab. words are present
- Document, _D_ :: e.g. _this is a sentence_
- Vocabulary, _V_ :: e.g. ~[aardvark, ..., sentence, ..., zither]~
- OHE, $X^D$ :: ~[0, ..., 1, ...1]~
*** Advantages and disadvantages
- Advantages: easy to implement
- Disadvantages:
  - not scalable for large vocabulary
  - high dimensional sparse matrix results in expensive memory + computation
  - each word represented individually, hence *no notion of similarity or meaning*. All vectors are orthogonal

    $(w^{good})^T \cdot w^{great} = (w^{good})^T \cdot w^{bad} = 0$
** Bag of Words (Frequency Counting)
- Summary :: Represents each document as a bag of words. **Ignores** order of words.
- Document :: a column vector of $X$ word counts
- Representation :: Fixed-length representation
- Document, _D_ :: e.g. ~It was the best of times, it was the worst of times~
- Vocabulary, _V_ :: e.g. ~[aardvark, ..., zither]~
- Bag of words: _X_ :: [2, ..., 1]
- Size of _X_ :: $1 \times d$ ($d$ = vocabulary size)
Hence $n$ documents can be represented by matrix of size $n \times d$.
*** Advantages and disadvantages
- Advantages: easy to implement
- Disadvantages:
  - Not scalable for large vocabulary
  - high dimensional sparse matrix results in expensive memory + computation
  - Order of words is disregarded; **no meaning** from context
** TF-IDF (Term Frequency-Inverse Document Frequency)
*** Why needed?
- BoW does not provide logical importance
  - i.e., each word is equally important
- TF-IDF assigns more logical importance to words in each document
*** What is TF-IDF and when to use TF-IDF
- Definition of TF-IDF :: a word's **importance score** in a document among $N$ documents
- _N_ :: total number of documents
- Word count :: likely TF-IDF
- Term frequency, _TF_ :: the number of times a word appears in **a document**.
  TF is high if word appears many times in document, e.g. _the_, _a_, etc.
- Inverse document frequency, _IDF_ :: $\log(\frac{N}{\text{number of docs containing the term}})$.
  If all (or most) documents contain that term, then IDF will be **very small**
- Word's importance score :: $TF \times IDF$.
  Higher score = more "characteristic"
*** Advantages and disadvantages
- Advantages:
  - Easy to implement
  - Higher score = "more characteristic". Common words will have very small scores.
  - Good technique to search for documents, find similar documents, cluster documents
- Disadvantages
  - Does not consider position of words when creating matrix. Similar problem as with BoW.
* Week 3: Linear Text Classification
** Classification introduction
Note: **classification**.
*** Supervised learning: definitions

- Word count matrix / document term matrix :: dataset generated from documents
- Rows of matrix :: each row is 1 document
- Columns of matrix :: each column is 1 unique word
- Unique words: synonyms :: features, dimensions, attributes, variables, columns
- Documents: synonyms :: rows, data points, instances
- Model weights :: = model parameters, i.e. what the model learns
- Function $F$ :: maps $X$ to $Y$
- Training data $(x_i, y_i)$ :: within set of ${X \times Y}$
- Learning - find $\hat{f}$ :: $\hat{f} \in F$ s.t. $y_i \approx \hat{f} (x_i)$
- New data :: $x$
- Prediction $y$ :: $= \hat{f} (x)$

Supervised learning thus takes **labelled** training data and **learns** or **derives** a function $f(x): y = f(x)$.

*** Categories of supervised learning
- continuous $y$ :: regression i.e. curve fitting
- discrete $y$ :: classification i.e. class estimation

*** Regression
- Errors represent how much predictions deviate from actual values.
- Minimum error = 0, however beware of overfitting, where test errors will be high (trained model cannot generalize).
- Example: apartment rent prediction, stock price prediction (difficult due to many predictors, known and unknown).

*** Classification
- Linear classification can be used for spam detection, sentiment analysis, handwriting digit recognition (0.4% error here), etc.
- Prepare, clean data, fit a classifier
- Retraining is required due to new evolving context, new lingo, etc. Can be implemented into a learning system.

** Naive Bayes
*** Method / concepts
Bayes Decision Rule.
- $x$ :: encoded document, e.g. by BoW
- $y$ :: label of document, i.e. whether document contains positive or negative message
- Posterior :: $P(y|x)$
- Likelihood :: $P(x|y)$
- Prior :: $P(y)$
- Normalization constant :: $P(x)$

$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)} = \frac{P(x,y)}{\sum_y P(x,y)}
$$
*** Bayes decision rule
- {{{hl(important)}}}: normalization constant is the same for +ve and -ve labels, hence no need to calculate when predicting sentiment
*** Generative vs discriminative models
Naive Bayes is a generative model
- Generative model: able to generate synthetic data points
  - **Need** to model prior and likelihood distributions.
  - In Naive Bayes, we normally replace likelihood with the conditional distribution.
  - Conditional distribution is the pdf/pmf to generate data points.
    - Determining this distribution might be difficult.
  - Generative models e.g.: Naive Bayes, Hidden Markov Models
- Discriminative models:
  - Directly estimate posteriors
  - No need to model prior and likelihood distributions
  - e.g.: logistic regression, SVM, neural networks
*** Details of Naive Bayes
Bayes decision rule:
$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$
- {{{hl(assumption)}}}: all dimensions (unique words) are independent of each other, i.e. $p(x|y = 1)$ fully factorized, hence: $P(x|y=1) = \prod^d_{i=1} P(x_i|y = 1)$
  - Thus, likelihood can be written in fully factorized way.
  - It becomes a big joint probability of all unique words (dimensions).
  - Conditional independence, hence likelihood can be written as multiplication of every dimension given the label.
  - i.e., the variables corresponding to each dimension are independent given the label.
*** Naive Conditional Independence Assumption
$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$

For vocabulary $V$, ~[nice, give, us, this, iu, ssn, information, job, a]~

$P(\text{document} | y = \text{positive})P(y=\text{positive})$

= $P(x=\text{nice}) ... P(x=a|y=\text{positive})$ $\cdot P(y= \text{positive})$

similarly for negatives:

$P(\text{document} | y = \text{negative})P(y=\text{negative})$

= $P(x=\text{nice}) ... P(x=a|y=\text{negative})$ $\cdot P(y= \text{negative })$
**** Representing the likelihood
Common distribution: **multinomial distribution**.

$$
P(x=\text{nice} | y = \text{positive})
$$

$$
= \frac{\text{count of word }\textbf{nice} \text{ in all positive label docs }}{\text{count all words with } \textbf{positive}  \text{  labels}}
$$

Then to calc priors:

$$
P(y = \text{positive}) = \frac{\text{count # +ve docs}}{\text{count # all docs}}
$$

Repeat above for negatives.
*** Advantages and disadvantages
- Advantages
  - Simple, easy to implement
  - No training required
  - Good results in general
- Disadvantages
  - Position of words do not matter (no semantic meaning) due to BoW approach
  - Requires / assumes conditional independence

** Classification Model Evaluation
*** Common metrics
- Classification: accuracy, precision, recall, cross-entropy, perplexity, and F1 score
- Regression: MSE, MAE
*** Confusion matrix
- e.g. for multi-label confusion matrix
- rows are the actual classes (sport, news politics)
- columns are the predicted classes
- diagonal elements are number of accurate predictions
- off-diagonals: inaccurate predictions
- But difficult to parse, can consider using a heat map on the confusion matrix instead of raw #
- {{{hl(meaning of positive and negative in a confusion matrix)}}}: not related to sentiment. Only indicator of the label, e.g. sport = positive, news = negative.
*** Accuracy
- Accuracy = (True Positive + True Negative) / Total observations, i.e. sum of diagonals / count observations.
- May not be represent "goodness" since false positives and false negatives have identical treatment.
- FP and FN may be important specifically for some fields e.g. medicine.
- Another metric, false alarm (false positive, type I error) is easy to remember in security contexts.
*** RoC-AUC curve
- ROC: Receiver Operating Characteristic
- Changing thresholds: how to change, what should the new threshold be?
- TP (y-axis) vs FP (x-axis)
- AUC (area under the curve) represents the how performant the predictive model is. Max is 1.0.
- But 0.9 may not be good either.
  - Are there some thresholds where TP = 0? Are these important in the context?
