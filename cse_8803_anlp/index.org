#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: CSE 8803: Applied Natural Language Processing
* Week 1: Text data preprocessing + Course Intro
** Why ANLP?
- Text and docs are everywhere
- Hundreds of languages in the world
- Primary information artifacts
- Large volumes of textual data
- Big and small companies looking for this skill
** Lots of text and written information
- Internet
- Webpages, Facebook, Wikipedia, etc.
- Digital libraries: Google Books, ACM, IEEE
- Lyrics, subtitles, etc.
- Police case reports
- Legislation
- Reviews
- Medical reports
- Job descriptions
** Example applications of NLP
- Establish authenticity, detect plagiarism
- Classification of genres
- Classification of tone; sentiment analysis
- Syntax analysis in code
- Machine translation
** Challenges of NLP
- Interdisciplinary field
- Ambiguity at many levels of language:
  1. Lexical (Word level)
  2. Syntactic: different ways of parsing
  3. Partial information: e.g., how to interpret pronouns
  4. Contextual information: context of sentence may affect meaning of sentence
** Class overview
- Preprocessing:
  - Clean text and documents
  - Tokenization
  - Reducing inflectional forms of a word:
    - Stemming
    - Lemmatization
  - Normalization
- Text representation
  - One hot encoding
  - Bag of words (Frequency counting)
  - Term frequency-Inverse document frequency (TF-IDF)
  - Embeddings
- Overview of classification methods
  - Naive Bayes
  - Logistic regression
  - SVM
  - Perceptron
  - Nerual Network
- Overview of Deep Learning
  - Convolutional neural network
  - Recurrent neural network
  - Long short-term memory
- Overview of topic modelling
  - Principal component analysis
  - Singular value decomposition
  - Latent Dirichlet Allocation
- Overview of Transformer methods
  - Bidirectional Encoder Representations from Transformers
  - Generative Pre-trained Transformers (GPT)
** Deliverables
*** Homework
- HW1: Text preprocessing and classification intro
- HW2: Classification methods, dimensionality reduction, SVD
- HW3: Deep learning
- HW4: Transformers and unsupervised methods
*** Quizzes (10)
- Measure understanding of topic
- Mostly conceptual questions
- MCQ
- Limited time to do the test
- Mandatory
** Course goals
- Demonstrate how to pre-process textual data
- Differentiate text representation methods and techniques
- Explain different NLP tasks
- Develop and assess performance of different NLP models using a variety of techniques
** Text Preprocessing Techniques
*** Terminology
- Corpus :: collection of text, e.g. Yelp reviews, Wikipedia articles
- Syntax :: Grammatical structure of text
- Syntactic parsing :: process of analyzing natural language with grammatical rules
- Semantics :: meaning of text
- Tokenization :: splitting long pieces of text into smaller pieces (tokens). e.g.: ~This is a simple sentence~ -> ~["This", "is", "a", "simple", "sentence"]~
- Stop words :: commonly used words, e.g. "the", "a", "an", "is", "are". Do not contribute to overall meaning
- N-grams :: consecutive sequence of words (commonly: 2-5) in a text. 1-gram (unigram), 2-gram (bigram), 3-gram (trigram). Example of bigrams: ~"This is", "is a", "a simple", "simple sentence"~
*** Preprocessing text data
- Text is unstructured, so preprocessing is the first step to prepare and clean text data to perform a NLP task
- Useful libraries:
  - re: regular expressions
  - nltk: natural language toolkit
- Common steps:
  - Noise removal
  - Tokenization
  - Text normalization
*** Noise removal
Removal of unwanted text formatting information, e.g.:
- Punctuation
- Accent marks
- Special characters
- Numeric digits (could be replaced with words)
- Leading, ending and vertical whitespace
- HTML formatting

Example: ~This is a 'simple'' sentence !!! 1+ \n~ -> ~This is a simple sentence~
*** Tokenization
Example:
~This is a simple sentence~ ->
~['This', 'is', 'a', 'simple', 'sentence', '.']~
*** Text normalization
Removing variations in the text to bring it to a standard form.
- Case: Convert all letters to upper or lower case
- Removing stop words, sparse terms, other special / particular words.

Example of text normalization:
~This is a Simple SenTence~ ->
~simple sentence~
- Stemming: reduce words to word stem, base, or root form.
  Example: ~There are several tytpes of stemming algorithms~ -> ~there are sever type fo stem algorithms.~
- Lemmatization: similar to stemming. Reduces inflectional forms to a common base form, **the lemma**. Does **not** simply chop off inflections. Uses **lexical knowledge** to get the correct base form of words.
  Example: ~There are several tytpes of stemming algorithms~ -> ~There are several type of stemming algorithms.~

* Week 2: (Discrete) Text Representations
** Why?
- NLP :: design algorithms to allow computers to understand natural language, so as to perform some task
- Required :: convert text data to numerical data that can be used in model
** Representing Words
- Can be represented by vectors of 0 & 1 where 1 indicates the position of the word, e.g. lorem = ~[1, 0]~, ipsum = ~[0, 1]~, etc.
** Representing sentences/documents
- Vectors of vectors eg ~[[1,0], [0,1]]~
** One Hot Encoding
*** Definitions
- corpus :: all texts
- vocabulary, _V_ :: all unique words
- vocabulary size, _d_ :: number of unique words, "dimensions"
- word, _w_ :: represented by vector $X$
$X^w_i$ = 1 if idw(w) = 1, 0 otherwise
- document :: represented by matrix sized $n \times d$
- _n_ ::  number of words in document
- _d_ :: a single vector with multiple values of 1 where vocab. words are present
- Document, _D_ :: e.g. _this is a sentence_
- Vocabulary, _V_ :: e.g. ~[aardvark, ..., sentence, ..., zither]~
- OHE, $X^D$ :: ~[0, ..., 1, ...1]~
*** Advantages and disadvantages
- Advantages: easy to implement
- Disadvantages:
  - not scalable for large vocabulary
  - high dimensional sparse matrix results in expensive memory + computation
  - each word represented individually, hence *no notion of similarity or meaning*. All vectors are orthogonal

    $(w^{good})^T \cdot w^{great} = (w^{good})^T \cdot w^{bad} = 0$
** Bag of Words (Frequency Counting)
- Summary :: Represents each document as a bag of words. **Ignores** order of words.
- Document :: a column vector of $X$ word counts
- Representation :: Fixed-length representation
- Document, _D_ :: e.g. ~It was the best of times, it was the worst of times~
- Vocabulary, _V_ :: e.g. ~[aardvark, ..., zither]~
- Bag of words: _X_ :: [2, ..., 1]
- Size of _X_ :: $1 \times d$ ($d$ = vocabulary size)
Hence $n$ documents can be represented by matrix of size $n \times d$.
*** Advantages and disadvantages
- Advantages: easy to implement
- Disadvantages:
  - Not scalable for large vocabulary
  - high dimensional sparse matrix results in expensive memory + computation
  - Order of words is disregarded; **no meaning** from context
** TF-IDF (Term Frequency-Inverse Document Frequency)
*** Why needed?
- BoW does not provide logical importance
  - i.e., each word is equally important
- TF-IDF assigns more logical importance to words in each document
*** What is TF-IDF and when to use TF-IDF
- Definition of TF-IDF :: a word's **importance score** in a document among $N$ documents
- _N_ :: total number of documents
- Word count :: likely TF-IDF
- Term frequency, _TF_ :: the number of times a word appears in **a document**.
  TF is high if word appears many times in document, e.g. _the_, _a_, etc.
- Inverse document frequency, _IDF_ :: $\log(\frac{N}{\text{number of docs containing the term}})$.
  If all (or most) documents contain that term, then IDF will be **very small**
- Word's importance score :: $TF \times IDF$.
  Higher score = more "characteristic"
*** Advantages and disadvantages
- Advantages:
  - Easy to implement
  - Higher score = "more characteristic". Common words will have very small scores.
  - Good technique to search for documents, find similar documents, cluster documents
- Disadvantages
  - Does not consider position of words when creating matrix. Similar problem as with BoW.
* Week 3: Linear Text Classification
** Classification / ML introduction
** Naive Bayes
** Classification Model Evaluation
