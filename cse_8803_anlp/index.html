<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-03-29 Fri 12:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>CSE 8803: Applied Natural Language Processing</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">CSE 8803: Applied Natural Language Processing</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgd6b78bc">1. Week 1: Text data preprocessing + Course Intro</a>
<ul>
<li><a href="#org1a9b265">1.1. Why ANLP?</a></li>
<li><a href="#org5130325">1.2. Lots of text and written information</a></li>
<li><a href="#org15abe50">1.3. Example applications of NLP</a></li>
<li><a href="#orgc302dd7">1.4. Challenges of NLP</a></li>
<li><a href="#org7b4975e">1.5. Class overview</a></li>
<li><a href="#org14e6d49">1.6. Deliverables</a>
<ul>
<li><a href="#orga5ca363">1.6.1. Homework</a></li>
<li><a href="#org525429b">1.6.2. Quizzes (10)</a></li>
</ul>
</li>
<li><a href="#org1619c78">1.7. Course goals</a></li>
<li><a href="#org4f264af">1.8. Text Preprocessing Techniques</a>
<ul>
<li><a href="#org05ef105">1.8.1. Terminology</a></li>
<li><a href="#org8797321">1.8.2. Preprocessing text data</a></li>
<li><a href="#org0738e12">1.8.3. Noise removal</a></li>
<li><a href="#orgcb2cc45">1.8.4. Tokenization</a></li>
<li><a href="#orga97cccd">1.8.5. Text normalization</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgf905f71">2. Week 2: (Discrete) Text Representations</a>
<ul>
<li><a href="#org930ceb9">2.1. Why?</a></li>
<li><a href="#orgb35f874">2.2. Representing Words</a></li>
<li><a href="#org45220db">2.3. Representing sentences/documents</a></li>
<li><a href="#org3cb80c7">2.4. One Hot Encoding</a>
<ul>
<li><a href="#orgad0dafc">2.4.1. Definitions</a></li>
<li><a href="#org0ed9764">2.4.2. Advantages and disadvantages</a></li>
</ul>
</li>
<li><a href="#org06c5ae4">2.5. Bag of Words (Frequency Counting)</a>
<ul>
<li><a href="#orgdaee8cb">2.5.1. Advantages and disadvantages</a></li>
</ul>
</li>
<li><a href="#org5df4e13">2.6. TF-IDF (Term Frequency-Inverse Document Frequency)</a>
<ul>
<li><a href="#orgd238479">2.6.1. Why needed?</a></li>
<li><a href="#org4c5a376">2.6.2. What is TF-IDF and when to use TF-IDF</a></li>
<li><a href="#org5ee7ec9">2.6.3. Advantages and disadvantages</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org6e35b9e">3. Week 3: Linear Text Classification</a>
<ul>
<li><a href="#org8e7a21d">3.1. Classification introduction</a>
<ul>
<li><a href="#orgba1ae14">3.1.1. Supervised learning: definitions</a></li>
<li><a href="#orgfba8787">3.1.2. Categories of supervised learning</a></li>
<li><a href="#org7f6d801">3.1.3. Regression</a></li>
<li><a href="#org3e45cb2">3.1.4. Classification</a></li>
</ul>
</li>
<li><a href="#org93f5311">3.2. Naive Bayes</a>
<ul>
<li><a href="#orgb24fc6b">3.2.1. Method / concepts</a></li>
<li><a href="#org78c105a">3.2.2. Bayes decision rule</a></li>
<li><a href="#orgd8979c1">3.2.3. Generative vs discriminative models</a></li>
<li><a href="#orga919507">3.2.4. Details of Naive Bayes</a></li>
<li><a href="#org29c7729">3.2.5. Naive Conditional Independence Assumption</a></li>
<li><a href="#org60d8d51">3.2.6. Advantages and disadvantages</a></li>
</ul>
</li>
<li><a href="#orgc1a5942">3.3. Classification Model Evaluation</a>
<ul>
<li><a href="#org857ac4d">3.3.1. Common metrics</a></li>
<li><a href="#org0d07d0d">3.3.2. Confusion matrix</a></li>
<li><a href="#org7c59c48">3.3.3. Accuracy</a></li>
<li><a href="#orgd46e740">3.3.4. RoC-AUC curve</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgf067e1a">4. Week 5: Log Regression, SVM and Perceptron (Module 4)</a>
<ul>
<li><a href="#org0a93338">4.1. Logistic Regression</a>
<ul>
<li><a href="#org8dc3577">4.1.1. Generative vs Discriminative Models (again)</a></li>
<li><a href="#orgd8c9212">4.1.2. Bayes equation again</a></li>
<li><a href="#orgc98d322">4.1.3. Logistic Function for posterior probability</a></li>
<li><a href="#org6eaf224">4.1.4. Sigmoid is interpreted as probability</a></li>
<li><a href="#org286902b">4.1.5. Logistic regression model</a></li>
<li><a href="#org916e295">4.1.6. The gradient of \(l(\theta)\)</a></li>
<li><a href="#org7ec8b5a">4.1.7. Gradient descent</a></li>
<li><a href="#orgd7e64a5">4.1.8. Gradient ascent (concave) / descent (convex) algorithm</a></li>
<li><a href="#org87aaea4">4.1.9. Advantages and disadvantages of logistic regression</a></li>
</ul>
</li>
<li><a href="#org9f2c9b6">4.2. Support vector machine</a>
<ul>
<li><a href="#org70f4686">4.2.1. Linear separation</a></li>
<li><a href="#org0a48f45">4.2.2. Finding &theta; that maximizes margin</a></li>
<li><a href="#orgb5368b0">4.2.3. Length of margin</a></li>
<li><a href="#org44140f9">4.2.4. Maximizing margin</a></li>
<li><a href="#org9319b7c">4.2.5. Geometric representation</a></li>
<li><a href="#org150c8e7">4.2.6. Lagrange formulation (not in detail)</a></li>
<li><a href="#orge6f6782">4.2.7. Usage</a></li>
<li><a href="#orgb3009da">4.2.8. From \(x\) to \(z\) space</a></li>
<li><a href="#orgfeaabe7">4.2.9. Support vector machine</a></li>
</ul>
</li>
<li><a href="#orgf89305a">4.3. Perceptron</a>
<ul>
<li><a href="#orgdcf4911">4.3.1. Linearly separable</a></li>
<li><a href="#orgba3fc19">4.3.2. Linear classifier</a></li>
<li><a href="#orgaf3d7ba">4.3.3. The Perceptron Classifier</a></li>
<li><a href="#org480e244">4.3.4. Perceptron activation</a></li>
<li><a href="#orgbf74f04">4.3.5. Advantages and disadvantages of Perceptron</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org5841658">5. Week 6: Embeddings/Dimensionality reduction</a>
<ul>
<li><a href="#orge7fb227">5.1. SVD and Co-occurrence Embeddings</a>
<ul>
<li><a href="#org78c45ac">5.1.1. Motivating example</a></li>
<li><a href="#orgf7e8d46">5.1.2. Bag of words representation</a></li>
<li><a href="#org95ce0af">5.1.3. What is dimensionality reduction?</a></li>
<li><a href="#org426959f">5.1.4. Intuition (of PCA)</a></li>
<li><a href="#org3b7400c">5.1.5. Singular value decomposition</a></li>
<li><a href="#org719c80a">5.1.6. Co-occurrence matrices</a></li>
<li><a href="#org7b5d8cb">5.1.7. SVD on co-occurrence matrices</a></li>
<li><a href="#org6ca5b96">5.1.8. Dense word embeddings</a></li>
<li><a href="#org463a4a2">5.1.9. Advantages of dense word embeddings</a></li>
</ul>
</li>
<li><a href="#org16697f2">5.2. GloVe</a>
<ul>
<li><a href="#org657753a">5.2.1. Definitions</a></li>
<li><a href="#org623048c">5.2.2. GloVe model</a></li>
<li><a href="#org09b6776">5.2.3. Extending the co-occurrence matrix</a></li>
<li><a href="#orge9f6f29">5.2.4. Example</a></li>
<li><a href="#orga21b173">5.2.5. GloVe cost function</a></li>
<li><a href="#orgaa06d23">5.2.6. GloVe word vectors</a></li>
<li><a href="#orga2a2d43">5.2.7. GloVe conserves linear relationships</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga8c0a99">6. Week 8: Neural Networks and Word2Vec</a>
<ul>
<li><a href="#orge4f5863">6.1. Neural Networks</a>
<ul>
<li><a href="#orgf1bc967">6.1.1. Inspiration from biological neurons</a></li>
<li><a href="#org9012322">6.1.2. Logistic regression block review</a></li>
<li><a href="#orgcb60de5">6.1.3. Connecting blocks to create neural networks</a></li>
<li><a href="#org5a944f0">6.1.4. Increasing the depth of each layer</a></li>
<li><a href="#orgd78ea28">6.1.5. Increasing layers</a></li>
<li><a href="#org211f8e6">6.1.6. Forward pass</a></li>
<li><a href="#orge8beb29">6.1.7. Backpropagation</a></li>
</ul>
</li>
<li><a href="#org5e371b6">6.2. Word2Vec, CBOW and Skipgram</a>
<ul>
<li><a href="#org39981f1">6.2.1. Review of One-Hot Encoding</a></li>
<li><a href="#org159708d">6.2.2. Issues with One-Hot Embedding</a></li>
<li><a href="#org7f6dbc6">6.2.3. Contextual meaning of the words</a></li>
<li><a href="#org399659d">6.2.4. What do we want to achieve from word embeddings?</a></li>
<li><a href="#orgd437951">6.2.5. Algorithm 1:: Continuous Bag of Words (CBOW)</a></li>
<li><a href="#org787923f">6.2.6. Algorithm 2: Skip-Gram model</a></li>
<li><a href="#org9371a2f">6.2.7. Main differences between CBOW and Skip-gram</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga145a85">7. Week 9: Convolutional Neural Networks and Recurrent Neural Networks</a>
<ul>
<li><a href="#org2536fa5">7.1. CNN and Deep Learning I</a>
<ul>
<li><a href="#org85c66f5">7.1.1. Example: learning an image</a></li>
<li><a href="#orga9d0162">7.1.2. Why does CNN work well for images</a></li>
</ul>
</li>
<li><a href="#org6d92484">7.2. CNN and Deep Learning II</a>
<ul>
<li><a href="#orge9bbf44">7.2.1. Convolution vs Fully connected</a></li>
<li><a href="#orgf2b1ab7">7.2.2. The whole CNN</a></li>
<li><a href="#org9ba1c99">7.2.3. CNN in Keras</a></li>
</ul>
</li>
<li><a href="#org1943918">7.3. Recurrent neural networks Part I</a>
<ul>
<li><a href="#org9b6e5ee">7.3.1. Name Entity Recognition</a></li>
<li><a href="#org533decc">7.3.2. Recap of feed-forward networks</a></li>
<li><a href="#org95aa412">7.3.3. Simplifying this for RNN</a></li>
</ul>
</li>
<li><a href="#org7c184cb">7.4. Recurrent neural networks Part II</a>
<ul>
<li><a href="#orga61f30b">7.4.1. RNN</a></li>
<li><a href="#org53c8c04">7.4.2. Forward pass: how to calculate past memory (h) in RNN</a></li>
<li><a href="#orge063e1d">7.4.3. Forward pass: how to calculate output of each step in RNN</a></li>
<li><a href="#org3dfd7b9">7.4.4. Backpropagation through time (BPTT)</a></li>
<li><a href="#orgd1b2062">7.4.5. Different RNN models</a></li>
<li><a href="#org98ff65b">7.4.6. Problems with RNN</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgca93d2c">8. Week 10: Long-Short Term Memory and Gated Recurrent Units</a>
<ul>
<li><a href="#org98736ee">8.1. LSTM Part I</a>
<ul>
<li><a href="#org988975b">8.1.1. Why?</a></li>
<li><a href="#org5213781">8.1.2. Gated cell: LSTM</a></li>
<li><a href="#org4617ffa">8.1.3. Different representation of RNN</a></li>
<li><a href="#org32c3326">8.1.4. Simple representation of LSTM</a></li>
<li><a href="#org52cb0f2">8.1.5. The cell state in LSTM</a></li>
<li><a href="#orga56c0db">8.1.6. How LSTM controls information removal and additional for a cell state</a></li>
</ul>
</li>
<li><a href="#org94be7f4">8.2. LSTM Part II</a>
<ul>
<li><a href="#org51b1ea0">8.2.1. Forget gate</a></li>
<li><a href="#orgdc3db54">8.2.2. Gated Recurrent Units</a></li>
</ul>
</li>
<li><a href="#org948264f">8.3. Attention-based LSTM</a>
<ul>
<li><a href="#org6aa772d">8.3.1. Common uses of encoder-decoder architecture</a></li>
<li><a href="#org2f566f6">8.3.2. RNN and LSTM units</a></li>
<li><a href="#orgd972010">8.3.3. In language translation</a></li>
<li><a href="#org56dd024">8.3.4. Attention mechanism</a></li>
<li><a href="#orgbec419a">8.3.5. Need to know weights associated with each attention</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgd6b78bc" class="outline-2">
<h2 id="orgd6b78bc"><span class="section-number-2">1.</span> Week 1: Text data preprocessing + Course Intro</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org1a9b265" class="outline-3">
<h3 id="org1a9b265"><span class="section-number-3">1.1.</span> Why ANLP?</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Text and docs are everywhere</li>
<li>Hundreds of languages in the world</li>
<li>Primary information artifacts</li>
<li>Large volumes of textual data</li>
<li>Big and small companies looking for this skill</li>
</ul>
</div>
</div>
<div id="outline-container-org5130325" class="outline-3">
<h3 id="org5130325"><span class="section-number-3">1.2.</span> Lots of text and written information</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>Internet</li>
<li>Webpages, Facebook, Wikipedia, etc.</li>
<li>Digital libraries: Google Books, ACM, IEEE</li>
<li>Lyrics, subtitles, etc.</li>
<li>Police case reports</li>
<li>Legislation</li>
<li>Reviews</li>
<li>Medical reports</li>
<li>Job descriptions</li>
</ul>
</div>
</div>
<div id="outline-container-org15abe50" class="outline-3">
<h3 id="org15abe50"><span class="section-number-3">1.3.</span> Example applications of NLP</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>Establish authenticity, detect plagiarism</li>
<li>Classification of genres</li>
<li>Classification of tone; sentiment analysis</li>
<li>Syntax analysis in code</li>
<li>Machine translation</li>
</ul>
</div>
</div>
<div id="outline-container-orgc302dd7" class="outline-3">
<h3 id="orgc302dd7"><span class="section-number-3">1.4.</span> Challenges of NLP</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>Interdisciplinary field</li>
<li>Ambiguity at many levels of language:
<ol class="org-ol">
<li>Lexical (Word level)</li>
<li>Syntactic: different ways of parsing</li>
<li>Partial information: e.g., how to interpret pronouns</li>
<li>Contextual information: context of sentence may affect meaning of sentence</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org7b4975e" class="outline-3">
<h3 id="org7b4975e"><span class="section-number-3">1.5.</span> Class overview</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>Preprocessing:
<ul class="org-ul">
<li>Clean text and documents</li>
<li>Tokenization</li>
<li>Reducing inflectional forms of a word:
<ul class="org-ul">
<li>Stemming</li>
<li>Lemmatization</li>
</ul></li>
<li>Normalization</li>
</ul></li>
<li>Text representation
<ul class="org-ul">
<li>One hot encoding</li>
<li>Bag of words (Frequency counting)</li>
<li>Term frequency-Inverse document frequency (TF-IDF)</li>
<li>Embeddings</li>
</ul></li>
<li>Overview of classification methods
<ul class="org-ul">
<li>Naive Bayes</li>
<li>Logistic regression</li>
<li>SVM</li>
<li>Perceptron</li>
<li>Nerual Network</li>
</ul></li>
<li>Overview of Deep Learning
<ul class="org-ul">
<li>Convolutional neural network</li>
<li>Recurrent neural network</li>
<li>Long short-term memory</li>
</ul></li>
<li>Overview of topic modelling
<ul class="org-ul">
<li>Principal component analysis</li>
<li>Singular value decomposition</li>
<li>Latent Dirichlet Allocation</li>
</ul></li>
<li>Overview of Transformer methods
<ul class="org-ul">
<li>Bidirectional Encoder Representations from Transformers</li>
<li>Generative Pre-trained Transformers (GPT)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org14e6d49" class="outline-3">
<h3 id="org14e6d49"><span class="section-number-3">1.6.</span> Deliverables</h3>
<div class="outline-text-3" id="text-1-6">
</div>
<div id="outline-container-orga5ca363" class="outline-4">
<h4 id="orga5ca363"><span class="section-number-4">1.6.1.</span> Homework</h4>
<div class="outline-text-4" id="text-1-6-1">
<ul class="org-ul">
<li>HW1: Text preprocessing and classification intro</li>
<li>HW2: Classification methods, dimensionality reduction, SVD</li>
<li>HW3: Deep learning</li>
<li>HW4: Transformers and unsupervised methods</li>
</ul>
</div>
</div>
<div id="outline-container-org525429b" class="outline-4">
<h4 id="org525429b"><span class="section-number-4">1.6.2.</span> Quizzes (10)</h4>
<div class="outline-text-4" id="text-1-6-2">
<ul class="org-ul">
<li>Measure understanding of topic</li>
<li>Mostly conceptual questions</li>
<li>MCQ</li>
<li>Limited time to do the test</li>
<li>Mandatory</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1619c78" class="outline-3">
<h3 id="org1619c78"><span class="section-number-3">1.7.</span> Course goals</h3>
<div class="outline-text-3" id="text-1-7">
<ul class="org-ul">
<li>Demonstrate how to pre-process textual data</li>
<li>Differentiate text representation methods and techniques</li>
<li>Explain different NLP tasks</li>
<li>Develop and assess performance of different NLP models using a variety of techniques</li>
</ul>
</div>
</div>
<div id="outline-container-org4f264af" class="outline-3">
<h3 id="org4f264af"><span class="section-number-3">1.8.</span> Text Preprocessing Techniques</h3>
<div class="outline-text-3" id="text-1-8">
</div>
<div id="outline-container-org05ef105" class="outline-4">
<h4 id="org05ef105"><span class="section-number-4">1.8.1.</span> Terminology</h4>
<div class="outline-text-4" id="text-1-8-1">
<dl class="org-dl">
<dt>Corpus</dt><dd>collection of text, e.g. Yelp reviews, Wikipedia articles</dd>
<dt>Syntax</dt><dd>Grammatical structure of text</dd>
<dt>Syntactic parsing</dt><dd>process of analyzing natural language with grammatical rules</dd>
<dt>Semantics</dt><dd>meaning of text</dd>
<dt>Tokenization</dt><dd>splitting long pieces of text into smaller pieces (tokens). e.g.: <code>This is a simple sentence</code> -&gt; <code>["This", "is", "a", "simple", "sentence"]</code></dd>
<dt>Stop words</dt><dd>commonly used words, e.g. "the", "a", "an", "is", "are". Do not contribute to overall meaning</dd>
<dt>N-grams</dt><dd>consecutive sequence of words (commonly: 2-5) in a text. 1-gram (unigram), 2-gram (bigram), 3-gram (trigram). Example of bigrams: <code>"This is", "is a", "a simple", "simple sentence"</code></dd>
</dl>
</div>
</div>
<div id="outline-container-org8797321" class="outline-4">
<h4 id="org8797321"><span class="section-number-4">1.8.2.</span> Preprocessing text data</h4>
<div class="outline-text-4" id="text-1-8-2">
<ul class="org-ul">
<li>Text is unstructured, so preprocessing is the first step to prepare and clean text data to perform a NLP task</li>
<li>Useful libraries:
<ul class="org-ul">
<li>re: regular expressions</li>
<li>nltk: natural language toolkit</li>
</ul></li>
<li>Common steps:
<ul class="org-ul">
<li>Noise removal</li>
<li>Tokenization</li>
<li>Text normalization</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org0738e12" class="outline-4">
<h4 id="org0738e12"><span class="section-number-4">1.8.3.</span> Noise removal</h4>
<div class="outline-text-4" id="text-1-8-3">
<p>
Removal of unwanted text formatting information, e.g.:
</p>
<ul class="org-ul">
<li>Punctuation</li>
<li>Accent marks</li>
<li>Special characters</li>
<li>Numeric digits (could be replaced with words)</li>
<li>Leading, ending and vertical whitespace</li>
<li>HTML formatting</li>
</ul>

<p>
Example: <code>This is a 'simple'' sentence !!! 1+ \n</code> -&gt; <code>This is a simple sentence</code>
</p>
</div>
</div>
<div id="outline-container-orgcb2cc45" class="outline-4">
<h4 id="orgcb2cc45"><span class="section-number-4">1.8.4.</span> Tokenization</h4>
<div class="outline-text-4" id="text-1-8-4">
<p>
Example:
<code>This is a simple sentence</code> -&gt;
<code>['This', 'is', 'a', 'simple', 'sentence', '.']</code>
</p>
</div>
</div>
<div id="outline-container-orga97cccd" class="outline-4">
<h4 id="orga97cccd"><span class="section-number-4">1.8.5.</span> Text normalization</h4>
<div class="outline-text-4" id="text-1-8-5">
<p>
Removing variations in the text to bring it to a standard form.
</p>
<ul class="org-ul">
<li>Case: Convert all letters to upper or lower case</li>
<li>Removing stop words, sparse terms, other special / particular words.</li>
</ul>

<p>
Example of text normalization:
<code>This is a Simple SenTence</code> -&gt;
<code>simple sentence</code>
</p>
<ul class="org-ul">
<li>Stemming: reduce words to word stem, base, or root form.
Example: <code>There are several tytpes of stemming algorithms</code> -&gt; <code>there are sever type fo stem algorithms.</code></li>
<li>Lemmatization: similar to stemming. Reduces inflectional forms to a common base form, <b><b>the lemma</b></b>. Does <b><b>not</b></b> simply chop off inflections. Uses <b><b>lexical knowledge</b></b> to get the correct base form of words.
Example: <code>There are several tytpes of stemming algorithms</code> -&gt; <code>There are several type of stemming algorithms.</code></li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgf905f71" class="outline-2">
<h2 id="orgf905f71"><span class="section-number-2">2.</span> Week 2: (Discrete) Text Representations</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org930ceb9" class="outline-3">
<h3 id="org930ceb9"><span class="section-number-3">2.1.</span> Why?</h3>
<div class="outline-text-3" id="text-2-1">
<dl class="org-dl">
<dt>NLP</dt><dd>design algorithms to allow computers to understand natural language, so as to perform some task</dd>
<dt>Required</dt><dd>convert text data to numerical data that can be used in model</dd>
</dl>
</div>
</div>
<div id="outline-container-orgb35f874" class="outline-3">
<h3 id="orgb35f874"><span class="section-number-3">2.2.</span> Representing Words</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Can be represented by vectors of 0 &amp; 1 where 1 indicates the position of the word, e.g. lorem = <code>[1, 0]</code>, ipsum = <code>[0, 1]</code>, etc.</li>
</ul>
</div>
</div>
<div id="outline-container-org45220db" class="outline-3">
<h3 id="org45220db"><span class="section-number-3">2.3.</span> Representing sentences/documents</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>Vectors of vectors eg <code>[[1,0], [0,1]]</code></li>
</ul>
</div>
</div>
<div id="outline-container-org3cb80c7" class="outline-3">
<h3 id="org3cb80c7"><span class="section-number-3">2.4.</span> One Hot Encoding</h3>
<div class="outline-text-3" id="text-2-4">
</div>
<div id="outline-container-orgad0dafc" class="outline-4">
<h4 id="orgad0dafc"><span class="section-number-4">2.4.1.</span> Definitions</h4>
<div class="outline-text-4" id="text-2-4-1">
<dl class="org-dl">
<dt>corpus</dt><dd>all texts</dd>
<dt>vocabulary, <span class="underline">V</span></dt><dd>all unique words</dd>
<dt>vocabulary size, <span class="underline">d</span></dt><dd>number of unique words, "dimensions"</dd>
<dt>word, <span class="underline">w</span></dt><dd>represented by vector \(X\)</dd>
</dl>
<p>
\(X^w_i\) = 1 if idw(w) = 1, 0 otherwise
</p>
<dl class="org-dl">
<dt>document</dt><dd>represented by matrix sized \(n \times d\)</dd>
<dt><span class="underline">n</span></dt><dd>number of words in document</dd>
<dt><span class="underline">d</span></dt><dd>a single vector with multiple values of 1 where vocab. words are present</dd>
<dt>Document, <span class="underline">D</span></dt><dd>e.g. <span class="underline">this is a sentence</span></dd>
<dt>Vocabulary, <span class="underline">V</span></dt><dd>e.g. <code>[aardvark, ..., sentence, ..., zither]</code></dd>
<dt>OHE, \(X^D\)</dt><dd><code>[0, ..., 1, ...1]</code></dd>
</dl>
</div>
</div>
<div id="outline-container-org0ed9764" class="outline-4">
<h4 id="org0ed9764"><span class="section-number-4">2.4.2.</span> Advantages and disadvantages</h4>
<div class="outline-text-4" id="text-2-4-2">
<ul class="org-ul">
<li>Advantages: easy to implement</li>
<li>Disadvantages:
<ul class="org-ul">
<li>not scalable for large vocabulary</li>
<li>high dimensional sparse matrix results in expensive memory + computation</li>
<li><p>
each word represented individually, hence <b>no notion of similarity or meaning</b>. All vectors are orthogonal
</p>

<p>
\((w^{good})^T \cdot w^{great} = (w^{good})^T \cdot w^{bad} = 0\)
</p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org06c5ae4" class="outline-3">
<h3 id="org06c5ae4"><span class="section-number-3">2.5.</span> Bag of Words (Frequency Counting)</h3>
<div class="outline-text-3" id="text-2-5">
<dl class="org-dl">
<dt>Summary</dt><dd>Represents each document as a bag of words. <b><b>Ignores</b></b> order of words.</dd>
<dt>Document</dt><dd>a column vector of \(X\) word counts</dd>
<dt>Representation</dt><dd>Fixed-length representation</dd>
<dt>Document, <span class="underline">D</span></dt><dd>e.g. <code>It was the best of times, it was the worst of times</code></dd>
<dt>Vocabulary, <span class="underline">V</span></dt><dd>e.g. <code>[aardvark, ..., zither]</code></dd>
<dt>Bag of words: <span class="underline">X</span></dt><dd>[2, &#x2026;, 1]</dd>
<dt>Size of <span class="underline">X</span></dt><dd>\(1 \times d\) (\(d\) = vocabulary size)</dd>
</dl>
<p>
Hence \(n\) documents can be represented by matrix of size \(n \times d\).
</p>
</div>
<div id="outline-container-orgdaee8cb" class="outline-4">
<h4 id="orgdaee8cb"><span class="section-number-4">2.5.1.</span> Advantages and disadvantages</h4>
<div class="outline-text-4" id="text-2-5-1">
<ul class="org-ul">
<li>Advantages: easy to implement</li>
<li>Disadvantages:
<ul class="org-ul">
<li>Not scalable for large vocabulary</li>
<li>high dimensional sparse matrix results in expensive memory + computation</li>
<li>Order of words is disregarded; <b><b>no meaning</b></b> from context</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5df4e13" class="outline-3">
<h3 id="org5df4e13"><span class="section-number-3">2.6.</span> TF-IDF (Term Frequency-Inverse Document Frequency)</h3>
<div class="outline-text-3" id="text-2-6">
</div>
<div id="outline-container-orgd238479" class="outline-4">
<h4 id="orgd238479"><span class="section-number-4">2.6.1.</span> Why needed?</h4>
<div class="outline-text-4" id="text-2-6-1">
<ul class="org-ul">
<li>BoW does not provide logical importance
<ul class="org-ul">
<li>i.e., each word is equally important</li>
</ul></li>
<li>TF-IDF assigns more logical importance to words in each document</li>
</ul>
</div>
</div>
<div id="outline-container-org4c5a376" class="outline-4">
<h4 id="org4c5a376"><span class="section-number-4">2.6.2.</span> What is TF-IDF and when to use TF-IDF</h4>
<div class="outline-text-4" id="text-2-6-2">
<dl class="org-dl">
<dt>Definition of TF-IDF</dt><dd>a word's <b><b>importance score</b></b> in a document among \(N\) documents</dd>
<dt><span class="underline">N</span></dt><dd>total number of documents</dd>
<dt>Word count</dt><dd>likely TF-IDF</dd>
<dt>Term frequency, <span class="underline">TF</span></dt><dd>the number of times a word appears in <b><b>a document</b></b>.
TF is high if word appears many times in document, e.g. <span class="underline">the</span>, <span class="underline">a</span>, etc.</dd>
<dt>Inverse document frequency, <span class="underline">IDF</span></dt><dd>\(\log(\frac{N}{\text{number of docs containing the term}})\).
If all (or most) documents contain that term, then IDF will be <b><b>very small</b></b></dd>
<dt>Word's importance score</dt><dd>\(TF \times IDF\).
Higher score = more "characteristic"</dd>
</dl>
</div>
</div>
<div id="outline-container-org5ee7ec9" class="outline-4">
<h4 id="org5ee7ec9"><span class="section-number-4">2.6.3.</span> Advantages and disadvantages</h4>
<div class="outline-text-4" id="text-2-6-3">
<ul class="org-ul">
<li>Advantages:
<ul class="org-ul">
<li>Easy to implement</li>
<li>Higher score = "more characteristic". Common words will have very small scores.</li>
<li>Good technique to search for documents, find similar documents, cluster documents</li>
</ul></li>
<li>Disadvantages
<ul class="org-ul">
<li>Does not consider position of words when creating matrix. Similar problem as with BoW.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org6e35b9e" class="outline-2">
<h2 id="org6e35b9e"><span class="section-number-2">3.</span> Week 3: Linear Text Classification</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org8e7a21d" class="outline-3">
<h3 id="org8e7a21d"><span class="section-number-3">3.1.</span> Classification introduction</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Note: <b><b>classification</b></b>.
</p>
</div>
<div id="outline-container-orgba1ae14" class="outline-4">
<h4 id="orgba1ae14"><span class="section-number-4">3.1.1.</span> Supervised learning: definitions</h4>
<div class="outline-text-4" id="text-3-1-1">
<dl class="org-dl">
<dt>Word count matrix / document term matrix</dt><dd>dataset generated from documents</dd>
<dt>Rows of matrix</dt><dd>each row is 1 document</dd>
<dt>Columns of matrix</dt><dd>each column is 1 unique word</dd>
<dt>Unique words: synonyms</dt><dd>features, dimensions, attributes, variables, columns</dd>
<dt>Documents: synonyms</dt><dd>rows, data points, instances</dd>
<dt>Model weights</dt><dd>= model parameters, i.e. what the model learns</dd>
<dt>Function \(F\)</dt><dd>maps \(X\) to \(Y\)</dd>
<dt>Training data \((x_i, y_i)\)</dt><dd>within set of \({X \times Y}\)</dd>
<dt>Learning - find \(\hat{f}\)</dt><dd>\(\hat{f} \in F\) s.t. \(y_i \approx \hat{f} (x_i)\)</dd>
<dt>New data</dt><dd>\(x\)</dd>
<dt>Prediction \(y\)</dt><dd>\(= \hat{f} (x)\)</dd>
</dl>

<p>
Supervised learning thus takes <b><b>labelled</b></b> training data and <b><b>learns</b></b> or <b><b>derives</b></b> a function \(f(x): y = f(x)\).
</p>
</div>
</div>

<div id="outline-container-orgfba8787" class="outline-4">
<h4 id="orgfba8787"><span class="section-number-4">3.1.2.</span> Categories of supervised learning</h4>
<div class="outline-text-4" id="text-3-1-2">
<dl class="org-dl">
<dt>continuous \(y\)</dt><dd>regression i.e. curve fitting</dd>
<dt>discrete \(y\)</dt><dd>classification i.e. class estimation</dd>
</dl>
</div>
</div>

<div id="outline-container-org7f6d801" class="outline-4">
<h4 id="org7f6d801"><span class="section-number-4">3.1.3.</span> Regression</h4>
<div class="outline-text-4" id="text-3-1-3">
<ul class="org-ul">
<li>Errors represent how much predictions deviate from actual values.</li>
<li>Minimum error = 0, however beware of overfitting, where test errors will be high (trained model cannot generalize).</li>
<li>Example: apartment rent prediction, stock price prediction (difficult due to many predictors, known and unknown).</li>
</ul>
</div>
</div>

<div id="outline-container-org3e45cb2" class="outline-4">
<h4 id="org3e45cb2"><span class="section-number-4">3.1.4.</span> Classification</h4>
<div class="outline-text-4" id="text-3-1-4">
<ul class="org-ul">
<li>Linear classification can be used for spam detection, sentiment analysis, handwriting digit recognition (0.4% error here), etc.</li>
<li>Prepare, clean data, fit a classifier</li>
<li>Retraining is required due to new evolving context, new lingo, etc. Can be implemented into a learning system.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org93f5311" class="outline-3">
<h3 id="org93f5311"><span class="section-number-3">3.2.</span> Naive Bayes</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<div id="outline-container-orgb24fc6b" class="outline-4">
<h4 id="orgb24fc6b"><span class="section-number-4">3.2.1.</span> Method / concepts</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
Bayes Decision Rule.
</p>
<dl class="org-dl">
<dt>\(x\)</dt><dd>encoded document, e.g. by BoW</dd>
<dt>\(y\)</dt><dd>label of document, i.e. whether document contains positive or negative message</dd>
<dt>Posterior</dt><dd>\(P(y|x)\)</dd>
<dt>Likelihood</dt><dd>\(P(x|y)\)</dd>
<dt>Prior</dt><dd>\(P(y)\)</dd>
<dt>Normalization constant</dt><dd>\(P(x)\)</dd>
</dl>

<p>
\[
P(y|x) = \frac{P(x|y)P(y)}{P(x)} = \frac{P(x,y)}{\sum_y P(x,y)}
\]
</p>
</div>
</div>
<div id="outline-container-org78c105a" class="outline-4">
<h4 id="org78c105a"><span class="section-number-4">3.2.2.</span> Bayes decision rule</h4>
<div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li><span style='background-color: #FFFF00;'>important</span>: normalization constant is the same for +ve and -ve labels, hence no need to calculate when predicting sentiment</li>
</ul>
</div>
</div>
<div id="outline-container-orgd8979c1" class="outline-4">
<h4 id="orgd8979c1"><span class="section-number-4">3.2.3.</span> Generative vs discriminative models</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
Naive Bayes is a generative model
</p>
<ul class="org-ul">
<li>Generative model: able to generate synthetic data points
<ul class="org-ul">
<li><b><b>Need</b></b> to model prior and likelihood distributions.</li>
<li>In Naive Bayes, we normally replace likelihood with the conditional distribution.</li>
<li>Conditional distribution is the pdf/pmf to generate data points.
<ul class="org-ul">
<li>Determining this distribution might be difficult.</li>
</ul></li>
<li>Generative models e.g.: Naive Bayes, Hidden Markov Models</li>
</ul></li>
<li>Discriminative models:
<ul class="org-ul">
<li>Directly estimate posteriors</li>
<li>No need to model prior and likelihood distributions</li>
<li>e.g.: logistic regression, SVM, neural networks</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga919507" class="outline-4">
<h4 id="orga919507"><span class="section-number-4">3.2.4.</span> Details of Naive Bayes</h4>
<div class="outline-text-4" id="text-3-2-4">
<p>
Bayes decision rule:
\[
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
\]
</p>
<ul class="org-ul">
<li><span style='background-color: #FFFF00;'>assumption</span>: all dimensions (unique words) are independent of each other, i.e. \(p(x|y = 1)\) fully factorized, hence: \(P(x|y=1) = \prod^d_{i=1} P(x_i|y = 1)\)
<ul class="org-ul">
<li>Thus, likelihood can be written in fully factorized way.</li>
<li>It becomes a big joint probability of all unique words (dimensions).</li>
<li>Conditional independence, hence likelihood can be written as multiplication of every dimension given the label.</li>
<li>i.e., the variables corresponding to each dimension are independent given the label.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org29c7729" class="outline-4">
<h4 id="org29c7729"><span class="section-number-4">3.2.5.</span> Naive Conditional Independence Assumption</h4>
<div class="outline-text-4" id="text-3-2-5">
<p>
\[
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
\]
</p>

<p>
For vocabulary \(V\), <code>[nice, give, us, this, iu, ssn, information, job, a]</code>
</p>

<p>
\(P(\text{document} | y = \text{positive})P(y=\text{positive})\)
</p>

<p>
= \(P(x=\text{nice}) ... P(x=a|y=\text{positive})\) \(\cdot P(y= \text{positive})\)
</p>

<p>
similarly for negatives:
</p>

<p>
\(P(\text{document} | y = \text{negative})P(y=\text{negative})\)
</p>

<p>
= \(P(x=\text{nice}) ... P(x=a|y=\text{negative})\) \(\cdot P(y= \text{negative })\)
</p>
</div>
<ol class="org-ol">
<li><a id="orgdd31de0"></a>Representing the likelihood<br />
<div class="outline-text-5" id="text-3-2-5-1">
<p>
Common distribution: <b><b>multinomial distribution</b></b>.
</p>

<p>
\[
P(x=\text{nice} | y = \text{positive})
\]
</p>

<p>
\[
= \frac{\text{count of word }\textbf{nice} \text{ in all positive label docs }}{\text{count all words with } \textbf{positive}  \text{  labels}}
\]
</p>

<p>
Then to calc priors:
</p>

<p>
\[
P(y = \text{positive}) = \frac{\text{count # +ve docs}}{\text{count # all docs}}
\]
</p>

<p>
Repeat above for negatives.
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org60d8d51" class="outline-4">
<h4 id="org60d8d51"><span class="section-number-4">3.2.6.</span> Advantages and disadvantages</h4>
<div class="outline-text-4" id="text-3-2-6">
<ul class="org-ul">
<li>Advantages
<ul class="org-ul">
<li>Simple, easy to implement</li>
<li>No training required</li>
<li>Good results in general</li>
</ul></li>
<li>Disadvantages
<ul class="org-ul">
<li>Position of words do not matter (no semantic meaning) due to BoW approach</li>
<li>Requires / assumes conditional independence</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgc1a5942" class="outline-3">
<h3 id="orgc1a5942"><span class="section-number-3">3.3.</span> Classification Model Evaluation</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<div id="outline-container-org857ac4d" class="outline-4">
<h4 id="org857ac4d"><span class="section-number-4">3.3.1.</span> Common metrics</h4>
<div class="outline-text-4" id="text-3-3-1">
<ul class="org-ul">
<li>Classification: accuracy, precision, recall, cross-entropy, perplexity, and F1 score</li>
<li>Regression: MSE, MAE</li>
</ul>
</div>
</div>
<div id="outline-container-org0d07d0d" class="outline-4">
<h4 id="org0d07d0d"><span class="section-number-4">3.3.2.</span> Confusion matrix</h4>
<div class="outline-text-4" id="text-3-3-2">
<ul class="org-ul">
<li>e.g. for multi-label confusion matrix</li>
<li>rows are the actual classes (sport, news politics)</li>
<li>columns are the predicted classes</li>
<li>diagonal elements are number of accurate predictions</li>
<li>off-diagonals: inaccurate predictions</li>
<li>But difficult to parse, can consider using a heat map on the confusion matrix instead of raw #</li>
<li><span style='background-color: #FFFF00;'>meaning of positive and negative in a confusion matrix</span>: not related to sentiment. Only indicator of the label, e.g. sport = positive, news = negative.</li>
</ul>
</div>
</div>
<div id="outline-container-org7c59c48" class="outline-4">
<h4 id="org7c59c48"><span class="section-number-4">3.3.3.</span> Accuracy</h4>
<div class="outline-text-4" id="text-3-3-3">
<ul class="org-ul">
<li>Accuracy = (True Positive + True Negative) / Total observations, i.e. sum of diagonals / count observations.</li>
<li>May not be represent "goodness" since false positives and false negatives have identical treatment.</li>
<li>FP and FN may be important specifically for some fields e.g. medicine.</li>
<li>Another metric, false alarm (false positive, type I error) is easy to remember in security contexts.</li>
</ul>
</div>
</div>
<div id="outline-container-orgd46e740" class="outline-4">
<h4 id="orgd46e740"><span class="section-number-4">3.3.4.</span> RoC-AUC curve</h4>
<div class="outline-text-4" id="text-3-3-4">
<ul class="org-ul">
<li>ROC: Receiver Operating Characteristic</li>
<li>Changing thresholds: how to change, what should the new threshold be?</li>
<li>TP (y-axis) vs FP (x-axis)</li>
<li>AUC (area under the curve) represents the how performant the predictive model is. Max is 1.0.</li>
<li>But 0.9 may not be good either.
<ul class="org-ul">
<li>Are there some thresholds where TP = 0? Are these important in the context?</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgf067e1a" class="outline-2">
<h2 id="orgf067e1a"><span class="section-number-2">4.</span> Week 5: Log Regression, SVM and Perceptron (Module 4)</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org0a93338" class="outline-3">
<h3 id="org0a93338"><span class="section-number-3">4.1.</span> Logistic Regression</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Backbone of neural network model</li>
<li>Created on linear combination of features</li>
<li>Outputs a <b>probability</b>
<ul class="org-ul">
<li>Logistic regression is thus a <b>soft classification</b></li>
</ul></li>
</ul>
</div>
<div id="outline-container-org8dc3577" class="outline-4">
<h4 id="org8dc3577"><span class="section-number-4">4.1.1.</span> Generative vs Discriminative Models (again)</h4>
<div class="outline-text-4" id="text-4-1-1">
<ul class="org-ul">
<li>Generative model: able to generate synthetic data points
<ul class="org-ul">
<li><b><b>Need</b></b> to model prior and likelihood distributions.</li>
<li>Conditional distribution is the pdf/pmf to generate data points.
<ul class="org-ul">
<li>Determining this distribution might be difficult.</li>
</ul></li>
<li>Generative models e.g.: Naive Bayes, Hidden Markov Models (HMM)</li>
</ul></li>
<li>Discriminative models:
<ul class="org-ul">
<li>Directly estimate posteriors</li>
<li>No need to model prior and likelihood distributions</li>
<li>e.g.: logistic regression, SVM, neural networks</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd8c9212" class="outline-4">
<h4 id="orgd8c9212"><span class="section-number-4">4.1.2.</span> Bayes equation again</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
\[
P(y|x) = \frac{P(x|y)P(y)}{P(x)} = \frac{P(x,y)}{\sum_y P(x,y)}
\]
</p>
<dl class="org-dl">
<dt>Generative models</dt><dd>need to calculate likelihood and prior explicitly</dd>
<dt>Discriminative models</dt><dd>can we calculate posterior directly without using Bayes equation?</dd>
</dl>
</div>
</div>
<div id="outline-container-orgc98d322" class="outline-4">
<h4 id="orgc98d322"><span class="section-number-4">4.1.3.</span> Logistic Function for posterior probability</h4>
<div class="outline-text-4" id="text-4-1-3">
<p>
i.e. the following function
</p>

<p>
\[
P(y|x) = g(s) = \frac{e^s}{1+e^s} = \frac{1}{1+e^{-s}}
\]
</p>

<ul class="org-ul">
<li>This function is known as the <b><b>sigmoid function</b></b>.</li>
<li>Easy to use this for optimization</li>
<li>Threshold: always 0.5?
<ul class="org-ul">
<li>Threshold can be investigated with ROC-AUC to determine best threshold</li>
</ul></li>
<li>Neural network with just 1 block is similar to logistic regression</li>
<li>Logistic regression: sigmoid is the activation function</li>
</ul>


<div id="orgf2cf9d2" class="figure">
<p><img src="./img/sigmoid.png" alt="sigmoid.png" />
</p>
</div>

<ul class="org-ul">
<li>Three linear models with different activation functions
<ul class="org-ul">
<li>Using a <b><b>sine</b></b> activation function: it will be transformed to perceptron, a hard classification</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6eaf224" class="outline-4">
<h4 id="org6eaf224"><span class="section-number-4">4.1.4.</span> Sigmoid is interpreted as probability</h4>
<div class="outline-text-4" id="text-4-1-4">
<ul class="org-ul">
<li><p>
e.g., does a customer like a product based on feedback?
</p>
<ul class="org-ul">
<li>Input: \(x\) a BoW or TF-IDF of a document that contains customer's feedback</li>
<li>\(g(s)\) is the probability of whether a customer likes a product</li>
<li>Cannot have hard prediction or classification here</li>
</ul>
\begin{equation}
s = x\theta \text{ the risk score} \\
h_\theta (x) = p(y|x) =
\begin{cases}
g(s) & y=1 \\
1-(gs) & y=0 \text{ using posterior probability directly}
\end{cases}
\end{equation}</li>
<li>Sigmoid is the inverse of <b>logit</b> function (or the log-odds ratio)</li>
</ul>
</div>
</div>
<div id="outline-container-org286902b" class="outline-4">
<h4 id="org286902b"><span class="section-number-4">4.1.5.</span> Logistic regression model</h4>
<div class="outline-text-4" id="text-4-1-5">
<ul class="org-ul">
<li>Expanding equation and replacing \(g(s)\) with linear combination of features</li>
<li>Probabilistic model</li>
<li>Uses MLE to optimize linear combination of features</li>
<li>Use log-likelihood for better numerical stability</li>
<li><p>
To find &theta; parameters, for \(n\) data points:
</p>
\begin{equation}
P(y|x) =
\begin{cases}
\frac{1}{1+ \exp(-x\theta)} & y=1 \\
1-\frac{1}{1+\exp(-x\theta)} = \frac{\exp(-x\theta)}{1+\exp(-x\theta)} & y=0
\end{cases}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org916e295" class="outline-4">
<h4 id="org916e295"><span class="section-number-4">4.1.6.</span> The gradient of \(l(\theta)\)</h4>
<div class="outline-text-4" id="text-4-1-6">
<p>
\[
l(\theta) := \log \prod^n_{i=1} p(y_i, |x_i, \theta) \\
= \sum_i \theta^T x_i^T (y_i -1) - \log(1+\exp(-x_i \theta))
\]
<b><b>Gradient</b></b>:
\[
\frac{\partial l(\theta)}{\partial \theta} =
\sum_i x_i^T (y_i-1) + x_i^T \frac{\exp(-x_i \theta)}{1+\exp(-x_i \theta)}
\]
</p>
<ul class="org-ul">
<li>Even when set to 0, there is <b><b>no</b></b> closed-form solution.
<ul class="org-ul">
<li>Even though there is a global solution</li>
<li>Unlike linear regression where there is a closed-form solution</li>
<li>Hence, logistic regression is unconstrained, but</li>
<li>Can optimize using iterative approach such as gradient descent</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7ec8b5a" class="outline-4">
<h4 id="org7ec8b5a"><span class="section-number-4">4.1.7.</span> Gradient descent</h4>
<div class="outline-text-4" id="text-4-1-7">
<ul class="org-ul">
<li>One way to solve unconstrained optimization problem is gradient descent</li>
<li>Given initial guess, we iteratively refine the guess by taking the direction of the <span style='background-color: #FFFF00;'>negative gradient</span></li>
<li>Analogous to going down the hill by taking steepest direction at each step</li>
<li>Update rule
\[
  x_{k+1} = x_k - \eta_k \nabla f(x_k)
  \]
\(\eta_k\) is the <span style='background-color: #FFFF00;'>step size or learning rate</span></li>
<li>Step taken should be small enough</li>
</ul>
</div>
</div>
<div id="outline-container-orgd7e64a5" class="outline-4">
<h4 id="orgd7e64a5"><span class="section-number-4">4.1.8.</span> Gradient ascent (concave) / descent (convex) algorithm</h4>
<div class="outline-text-4" id="text-4-1-8">
<ul class="org-ul">
<li>Initialize parameter \(\theta_0\)</li>
<li>Do:
\[
  \theta_{t+1} \leftarrow \theta^t + \eta \sum_i x_i^T (y_i-1) + x_i^T \frac{\exp(-x_i \theta)}{1+\exp(-x_i \theta)}
  \]</li>
<li>while:
\[
  \parallel \theta^{t+1} - \theta^t \parallel > \epsilon
  \]</li>
<li>ascent: maximize function</li>
<li>descent: minimize</li>
<li>Thus:
<ul class="org-ul">
<li>Logical threshold = 0.5, i.e. predict 1 if \(g(s) \ge 0.5\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org87aaea4" class="outline-4">
<h4 id="org87aaea4"><span class="section-number-4">4.1.9.</span> Advantages and disadvantages of logistic regression</h4>
<div class="outline-text-4" id="text-4-1-9">
<ul class="org-ul">
<li>Advantages:
<ul class="org-ul">
<li>Simple</li>
<li>No need to model prior or likelihood</li>
<li>Provides probability output</li>
<li>Works with datasets with few features</li>
</ul></li>
<li>Disadvantages:
<ul class="org-ul">
<li>Needs to have discriminative model assumption</li>
<li>Model needs to be optimized using numerical approach</li>
<li>Might not work with complicated dataset</li>
</ul></li>
</ul>
<p>
**
</p>
</div>
</div>
</div>
<div id="outline-container-org9f2c9b6" class="outline-3">
<h3 id="org9f2c9b6"><span class="section-number-3">4.2.</span> Support vector machine</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>SVM is a large margin classifier</li>
</ul>
</div>
<div id="outline-container-org70f4686" class="outline-4">
<h4 id="org70f4686"><span class="section-number-4">4.2.1.</span> Linear separation</h4>
<div class="outline-text-4" id="text-4-2-1">
<ul class="org-ul">
<li>Can have different separating lines, so which line is the best?
<ul class="org-ul">
<li>Why is having bigger margin better?</li>
<li>What &theta; maximizes margin?</li>
</ul></li>
</ul>

<div id="org8353662" class="figure">
<p><img src="./img/lin-sep.png" alt="lin-sep.png" />
</p>
</div>
<ul class="org-ul">
<li>All cases, error is zero and they are linear, so they are all good for generalization.</li>
<li>SVM focuses on just one solution (compared to perceptron) and that's the maximum margin solution</li>
<li>SVM maximizes margin and provides decision line with maximized margin, which is the <span style='background-color: #FFFF00;'>most stable</span> under perturbations of inputs</li>
</ul>
</div>
</div>
<div id="outline-container-org0a48f45" class="outline-4">
<h4 id="org0a48f45"><span class="section-number-4">4.2.2.</span> Finding &theta; that maximizes margin</h4>
<div class="outline-text-4" id="text-4-2-2">
<ul class="org-ul">
<li>Objective function created by constructing linear combination of features.
<ul class="org-ul">
<li>Solution (decision boundary) of the line
\[
    x \theta = 0
    \]</li>
<li>Let \(x_i\) be the nearest data point to the line/plane</li>
<li>Decision boundary is thus \(x\theta + b = 0\)
<ul class="org-ul">
<li>Below decision line: &le; 0</li>
<li>Above decision line: &ge; 0</li>
</ul></li>
<li>Scaling up / down &theta; thus allows you to set the nearest point to \(1\).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb5368b0" class="outline-4">
<h4 id="orgb5368b0"><span class="section-number-4">4.2.3.</span> Length of margin</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
\[
\text{distance} = \frac{1}{\parallel \theta \parallel} |(x_i \theta - x \theta)|
= \frac{1}{\parallel \theta \parallel}|(x_i \theta + b - x\theta -b)|
\]
where:
</p>
<dl class="org-dl">
<dt>\(x_i \theta + b\)</dt><dd>my constraint \(\equiv |x_i \theta + b| = 1\)</dd>
<dt>\(-x\theta - b\)</dt><dd>a point on the decision line \(\equiv x\theta + b = 0\)</dd>
</dl>

<p>
Therefore total margin is: \(\frac{2}{\parallel \theta \parallel}\) (since there are 2 points on each side of the decision line)
</p>


<div id="orgffa83a1" class="figure">
<p><img src="./img/large-margin.png" alt="large-margin.png" />
</p>
</div>

<ul class="org-ul">
<li>&theta; is <span style='background-color: #FFFF00;'>orthogonal</span> to the decision line</li>
</ul>
</div>
</div>
<div id="outline-container-org44140f9" class="outline-4">
<h4 id="org44140f9"><span class="section-number-4">4.2.4.</span> Maximizing margin</h4>
<div class="outline-text-4" id="text-4-2-4">
<ul class="org-ul">
<li>Maximize \(\frac{2}{\parallel \theta \parallel}\) in the objective function</li>
<li>Subject to \(\min_{i=1,2,...,N} |x_i \theta + b| = 1\) which is the nearest neighbour, sign-agnostic for labels here, hence absolute.</li>
<li>Hard to optimize this due to the "min" in the constraint (non-convex form)</li>
<li>To get rid of the absolute value in the constraint, (and to get the correct prediction, predicted value must have same sign as actual)
\[
  \left|x_i \theta + b\right| = y_i(x_i \theta + b) \rightarrow \text{for correct classification} \\
  \text{ if} \min |x_i \theta + b | = 1 \rightarrow \text{ it can be at least 1}
  \]</li>
<li>Hence,
\[
  \max \frac{2}{\parallel \theta \parallel}
  \\
  \text{subject to } y_i (x_i \theta + b) \ge 1 \text{ for } i=1,2,...,N
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org9319b7c" class="outline-4">
<h4 id="org9319b7c"><span class="section-number-4">4.2.5.</span> Geometric representation</h4>
<div class="outline-text-4" id="text-4-2-5">

<div id="org0721a23" class="figure">
<p><img src="./img/geom-rep.png" alt="geom-rep.png" />
</p>
</div>
<dl class="org-dl">
<dt>Decision line</dt><dd>\(x\theta + b = 0\)</dd>
<dt>Margin line</dt><dd>\(x \theta + b = 1\)</dd>
<dt>Blue colors</dt><dd>constraint (data points beyond margin line); beyond margin line the margin &ge; 1, correctly classified</dd>
</dl>
</div>
<ol class="org-ol">
<li><a id="orge6e3ce4"></a>Converting problem<br />
<div class="outline-text-5" id="text-4-2-5-1">
<ul class="org-ul">
<li>Many ML libraries can solve minimization problems instead of maximization</li>
<li>Hence, convert from:
\[
  \max(\frac{2}{\parallel \theta \parallel}) \\
  \text{subject to } y_i (x_i \theta + b) \ge 1 \text{ for }i=1,2,...,N
  \]</li>
<li>to:
\[
  \min(\frac{1}{2} \theta\theta^T) \\
  \text{subject to } y_i (x_i \theta + b) \ge 1 \text{ for }i=1,2,...,N
  \]</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org150c8e7" class="outline-4">
<h4 id="org150c8e7"><span class="section-number-4">4.2.6.</span> Lagrange formulation (not in detail)</h4>
<div class="outline-text-4" id="text-4-2-6">
<p>
\[
\min(\frac{1}{2} \theta\theta^T) \\
\text{subject to } y_i (x_i \theta + b) -1 \ge 0  \\
\textit{L}(\theta, b, \alpha) = \frac{1}{2}\theta\theta^T - \sum^N_{i=1} \alpha_i (y_i(x_i \theta + b)-1)
\]
becomes:
\[
\min \text{w.r.t. } \theta, b \text{ and } \max \text{w.r.t. each } \alpha_i \ge 0 \\
\nabla_\theta L(\theta, b, \alpha) = \theta - \sum^N_{i=1} \alpha_i y_i x_i = 0 \\
\nabla_b L(\theta, b, \alpha) = -sum^N{i=1} \alpha_i y_i = 0
\]
under KKT conditions,
where:
</p>
<dl class="org-dl">
<dt>\(\theta\)</dt><dd>model parameter</dd>
<dt>\(b\)</dt><dd>bias term</dd>
<dt>\(\alpha\)</dt><dd>Lagrange multiplier</dd>
</dl>
<p>
Need to convert primal form to dual form.
Take gradient w.r.t. &theta;, b, set to 0.
Calculate parametric value of &theta; and new constraints.
Convert objective function to dual form.
\[
\theta = \sum^N_{i=1} \alpha_i y_i x_i \text{ and } \sum^N_{i=1} \alpha_i y_i = 0 \\
L(\theta, b, \alpha) = \sum^N{i=1} \alpha_i - \frac{1}{2} \theta \theta^T \\
L(\theta, b, \alpha) = \sum^N_{i=1} \alpha_i - \frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} y_i y_j \alpha_i \alpha_j x_i x_j^T
\\
\max \text{ w.r.t. each } \alpha_i \ge 0 \text{ for }i=1,...,N \text{ and }
\sum^N_{i=1} \alpha_i y_i = 0
\]
</p>
</div>
</div>
<div id="outline-container-orge6f6782" class="outline-4">
<h4 id="orge6f6782"><span class="section-number-4">4.2.7.</span> Usage</h4>
<div class="outline-text-4" id="text-4-2-7">
<ul class="org-ul">
<li>Dual form good for binary classification, e.g. spam or not spam.</li>
<li>Training
\[
  \theta = \sum^N_{i=1} \alpha_i y_i x_i
  \]
<ul class="org-ul">
<li>No need to go over all data points</li>
<li>\[
    \rightarrow \theta = \sum_{x \in \text{ SV}} \alpha_i y_i x_i
    \]</li>
<li>and for \(b\) pick any support vector, and calculate \(y_i (x_i \theta + b) = 1\)</li>
</ul></li>
<li>Testing
<ul class="org-ul">
<li>For new point \(s\), compute:
\[
    s \theta + b = \sum_{x_i \in \text{ SV}} \alpha_i y_i x_i s^T + b
    \]</li>
<li>Classify \(s\) as class 1 if positive, else classify as class 2.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb3009da" class="outline-4">
<h4 id="orgb3009da"><span class="section-number-4">4.2.8.</span> From \(x\) to \(z\) space</h4>
<div class="outline-text-4" id="text-4-2-8">
<ul class="org-ul">
<li>SVM can only be used when a linear decision line can be used</li>
<li>Sometimes it may be possible to work around by moving from Cartesian to Polar space</li>
<li>Not necessarily applicable to NLP since there are many many dimensions.</li>
<li>Instead, kernel trick can be utilised in the dual form model, do feature engineering and handle millions of features.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org8aa6caf"></a>Kernel trick<br />
<div class="outline-text-5" id="text-4-2-8-1">
<p>
Main premise is to take data from original space to newer space with higher dimensions, which make it more likely to have linear separation in the newer space.
</p>

<p>
In \(x\) space, they are called pre-images of support vectors.
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgfeaabe7" class="outline-4">
<h4 id="orgfeaabe7"><span class="section-number-4">4.2.9.</span> Support vector machine</h4>
<div class="outline-text-4" id="text-4-2-9">
<ul class="org-ul">
<li>Can do <span style='background-color: #FFFF00;'>either</span>
<ul class="org-ul">
<li>Hard classification</li>
<li>Soft classification</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf89305a" class="outline-3">
<h3 id="orgf89305a"><span class="section-number-3">4.3.</span> Perceptron</h3>
<div class="outline-text-3" id="text-4-3">

<div id="org6556518" class="figure">
<p><img src="./img/spam.png" alt="spam.png" />
</p>
</div>
<ul class="org-ul">
<li>Needs to be linearly separable to work</li>
<li>Can be used for text classification, sentiment analysis</li>
<li><p>
Given training data \((x_i, y_i)\) for \(i = 1,...,N, x_i \in \mathbb{R}^d \text{ and }y_i \in {-1,1}\) learn a classifier \(f(x)\) such that
</p>
\begin{equation}
f(x_i)
\begin{cases}
\ge 0 & +1 & \text{Non-spam document} \\
\lt 0 & -1 & \text{Spam document}
\end{cases}
\end{equation}</li>
<li>i.e. \(y_i f(x_i) \gt 0\) for a correct classification</li>
</ul>
</div>
<div id="outline-container-orgdcf4911" class="outline-4">
<h4 id="orgdcf4911"><span class="section-number-4">4.3.1.</span> Linearly separable</h4>
<div class="outline-text-4" id="text-4-3-1">

<div id="org15da70c" class="figure">
<p><img src="./img/linear-sep.png" alt="linear-sep.png" />
</p>
</div>
<ul class="org-ul">
<li>The two labels must be separable by a <b><b>straight</b></b> line</li>
<li>Perceptron uses linear classifier, as it uses linear combination of features</li>
</ul>
</div>
</div>
<div id="outline-container-orgba3fc19" class="outline-4">
<h4 id="orgba3fc19"><span class="section-number-4">4.3.2.</span> Linear classifier</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
<img src="./img/lin-class.png" alt="lin-class.png" />
Linear classifier has the form
\[
f(x) = x\theta + \theta_0
\]
</p>
<ul class="org-ul">
<li>In 2D, the discriminant is a line</li>
<li>\(\theta\) is the <b><b>normal</b></b> to the decision line</li>
<li>\(\theta_0\), is the bias term</li>
<li>\(\theta\) is known as the model <span style='background-color: #FFFF00;'>parameter</span> or the <span style='background-color: #FFFF00;'>weight vector</span></li>
<li>Decision boundary has \(d-1\) dimensions where \(d\) is the number of features</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org064b151"></a>Linear classifier for higher dimensions<br />
<div class="outline-text-5" id="text-4-3-2-1">
<ul class="org-ul">
<li>In 3D, the discriminant is a plane</li>
<li>in nD, the discriminant is a hyperplane</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgaf3d7ba" class="outline-4">
<h4 id="orgaf3d7ba"><span class="section-number-4">4.3.3.</span> The Perceptron Classifier</h4>
<div class="outline-text-4" id="text-4-3-3">
<ul class="org-ul">
<li><span style='background-color: #FFFF00;'>hard classifier</span></li>
<li>Considering \(x\) is linearly separable</li>
<li>\(y\) has 2 labels \(\{-1,1 \}\)</li>
<li>\(f(x_i) = x_i \theta\), where bias is inside \(\theta\)</li>
<li>How to separate data points with label 1 from those with -1 using a <b><b>line</b></b>?</li>
<li>Perceptron classifier is a simple for-loop
<ul class="org-ul">
<li>Goes inside every single data point to check whether it's classified correctly</li>
</ul></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orga9e9ca3"></a>Algorithm<br />
<div class="outline-text-5" id="text-4-3-3-1">

<div id="org0e3e966" class="figure">
<p><img src="./img/perceptron.png" alt="perceptron.png" />
</p>
</div>
<ol class="org-ol">
<li>Initialise \(\theta = 0\)</li>
<li>Go through each data point \(\{x_i, y_i \}\)
<ol class="org-ol">
<li>If \(x_i\) is misclassified, then \(\theta^{t+1} \leftarrow \theta^t + \alpha y_i x_i\) (i.e. moving the decision line towards the correct label)</li>
</ol></li>
<li>Until all data points are correctly classified</li>
</ol>
</div>
</li>
</ol>
</div>
<div id="outline-container-org480e244" class="outline-4">
<h4 id="org480e244"><span class="section-number-4">4.3.4.</span> Perceptron activation</h4>
<div class="outline-text-4" id="text-4-3-4">

<div id="org46e4e8b" class="figure">
<p><img src="./img/perceptron-activation.png" alt="perceptron-activation.png" />
</p>
</div>
<ul class="org-ul">
<li>LHS = number of lines = number of features</li>
<li>output of linear combination of features, \(f(x)\) is real number,</li>
<li>fed into activation function in red, which is +1 or -1</li>
</ul>
</div>
</div>
<div id="outline-container-orgbf74f04" class="outline-4">
<h4 id="orgbf74f04"><span class="section-number-4">4.3.5.</span> Advantages and disadvantages of Perceptron</h4>
<div class="outline-text-4" id="text-4-3-5">
<ul class="org-ul">
<li>Advantages
<ul class="org-ul">
<li>Very simple</li>
<li>Fast, does not require any parameters</li>
<li>Quick training to optimize parameters</li>
</ul></li>
<li>Disadvantages
<ul class="org-ul">
<li>Works only for linearly separable data</li>
<li>Does not provide unique decision boundary</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org5841658" class="outline-2">
<h2 id="org5841658"><span class="section-number-2">5.</span> Week 6: Embeddings/Dimensionality reduction</h2>
<div class="outline-text-2" id="text-5">
<p>
Singular Value Decomposition (SVD)
</p>
</div>
<div id="outline-container-orge7fb227" class="outline-3">
<h3 id="orge7fb227"><span class="section-number-3">5.1.</span> SVD and Co-occurrence Embeddings</h3>
<div class="outline-text-3" id="text-5-1">
</div>
<div id="outline-container-org78c45ac" class="outline-4">
<h4 id="org78c45ac"><span class="section-number-4">5.1.1.</span> Motivating example</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
Dimensionality reduction for text is to understand how they behave in 2D or 3D space.
</p>
<ul class="org-ul">
<li>This helps to get better perspective of the data.</li>
<li>High dimensionality data points happens on text and data problems due to many unique words.</li>
</ul>
</div>
</div>
<div id="outline-container-orgf7e8d46" class="outline-4">
<h4 id="orgf7e8d46"><span class="section-number-4">5.1.2.</span> Bag of words representation</h4>
<div class="outline-text-4" id="text-5-1-2">
<ul class="org-ul">
<li>Has many unique words (dimensions) that leads to:
<ul class="org-ul">
<li><b><b>overfitting</b></b></li>
<li>more resources &amp; time needed</li>
</ul></li>
<li>BoW generates a term-document matrix with many many features that's sparse</li>
<li>Possible solution: <b><b>dimension reduction</b></b></li>
</ul>
</div>
</div>
<div id="outline-container-org95ce0af" class="outline-4">
<h4 id="org95ce0af"><span class="section-number-4">5.1.3.</span> What is dimensionality reduction?</h4>
<div class="outline-text-4" id="text-5-1-3">

<div id="orgcd88bee" class="figure">
<p><img src="./img/dim-reduct.png" alt="dim-reduct.png" />
</p>
</div>
<ul class="org-ul">
<li>Dimensionality reduction is the process of reducing <b><b>random variables</b></b> under consideration</li>
<li>Possible approaches:
<ul class="org-ul">
<li>Combine, transform or select variables</li>
<li>With linear or non-linear operations</li>
</ul></li>
<li>New space has lower dimensions than previous space</li>
</ul>
</div>
</div>
<div id="outline-container-org426959f" class="outline-4">
<h4 id="org426959f"><span class="section-number-4">5.1.4.</span> Intuition (of PCA)</h4>
<div class="outline-text-4" id="text-5-1-4">
<ul class="org-ul">
<li>Approximate a \(D\) -dimensional dataset using fewer dimensions</li>
<li>By rotating the axes into a new space</li>
<li>Highest order dimension captures the most variance in the original dataset</li>
<li>Next dimension captures the next most variance, etc.</li>
<li>PCA uses eigendecomposition of covariance of dataset to maximize variance
<ul class="org-ul">
<li>Eigenvector corresponding to the highest eigenvalue is the new dimension that maximises the variance the most</li>
<li>Hope of PCA is that a dimension that explains variance the most would explain data better and it's easier to separate and distinguish labels when data points are spread out because of high variance</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3b7400c" class="outline-4">
<h4 id="org3b7400c"><span class="section-number-4">5.1.5.</span> Singular value decomposition</h4>
<div class="outline-text-4" id="text-5-1-5">
<p>
For a matrix \(X_{n \times d}\) where:
</p>
<dl class="org-dl">
<dt>n</dt><dd>number of instances</dd>
<dt>d</dt><dd>dimension</dd>
</dl>

<p>
\[
X = U \Sigma V^T
\]
</p>

<dl class="org-dl">
<dt>U, &Sigma;, V</dt><dd>all unitary matrices</dd>
<dt>m columns</dt><dd>represent a dimension in a new latent space s.t. \(m\) column vectors are orthogonal to each other, and ordered by the amount of variance in the dataset in each dimension. \(m\) has <b><b>maximum</b></b> of \(d\) dimensions</dd>
<dt>\(U_{n \times m}\)</dt><dd>unitary matrix &rarr; \(UU^T = I\)</dd>
<dt>\(\Sigma_{m \times m}\)</dt><dd>diagonal matrix of singular values of \(X\)</dd>
<dt>\(V_{m \times d}\)</dt><dd>unitary matrix \(\rightarrow VV^T = I\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org719c80a" class="outline-4">
<h4 id="org719c80a"><span class="section-number-4">5.1.6.</span> Co-occurrence matrices</h4>
<div class="outline-text-4" id="text-5-1-6">
<div class="BLOCKQUOTE" id="orgb051f75">
<p>
Instead of matrix.
</p>

</div>
<p>
Each matrix for one value of context length.
</p>

<ul class="org-ul">
<li>Meaning of a word is defined by the words in its surroundings</li>
<li>Define a context window as the number of words appearing around a centre word</li>
<li>Create a co-occurrence matrix:
<ol class="org-ol">
<li>Go through each central word-context pair in corpus (context window length is commonly in \([1,5]\))</li>
<li>In each iteration, update the row of the count matrix (of central word) by adding +1 in the columns for the context words</li>
<li>Repeat last step many times</li>
</ol></li>
</ul>

<div id="orgcb7198c" class="figure">
<p><img src="./img/co-matrix.png " alt="co-matrix.png " />
</p>
</div>
</div>
</div>
<div id="outline-container-org7b5d8cb" class="outline-4">
<h4 id="org7b5d8cb"><span class="section-number-4">5.1.7.</span> SVD on co-occurrence matrices</h4>
<div class="outline-text-4" id="text-5-1-7">
<ul class="org-ul">
<li>For corpus with vocabulary \(V\) of size \(d\), co-occurrence matrix has size \(d \times d\)</li>
<li>Size of co-occurrence matrix increases with vocabulary</li>
<li>Instead of keeping all dimensions, can instead use truncated SVD to keep only to \(k\) singular values
<ul class="org-ul">
<li>e.g. \(k=300\)</li>
</ul></li>
<li>Result is a least-square approximation to the original co-occurrence matrix \(X\)
<img src="./img/svd-co-occur.png" alt="svd-co-occur.png" /></li>
<li>Single value is directly related to the new dimension that maximizes co-variance</li>
</ul>
</div>
</div>
<div id="outline-container-org6ca5b96" class="outline-4">
<h4 id="org6ca5b96"><span class="section-number-4">5.1.8.</span> Dense word embeddings</h4>
<div class="outline-text-4" id="text-5-1-8">

<div id="org9165de1" class="figure">
<p><img src="./img/dense.png" alt="dense.png" />
</p>
</div>
<ul class="org-ul">
<li>Each row of \(U\) is a \(k\) -dimensional representation of each word \(w\) in the corpus that best preserves variance</li>
<li>Generally, keep top \(k \in [50, 500]\) dimensions.</li>
<li>Produces dense vectors for word representations, while also considering the word contexts that carry meaning</li>
</ul>
</div>
</div>
<div id="outline-container-org463a4a2" class="outline-4">
<h4 id="org463a4a2"><span class="section-number-4">5.1.9.</span> Advantages of dense word embeddings</h4>
<div class="outline-text-4" id="text-5-1-9">
<ul class="org-ul">
<li>Denoising: low-order dimensions may represent unimportant information; higher-order dimensions keep only important information</li>
<li>Truncation may help models generalize better to unseen data</li>
<li>Having smaller number of dimensions may make it easier for classifiers to properly weigh the dimensions</li>
<li>Dense models may do better at capturing higher-order co-occurrence</li>
<li>Dense vectors work better in word similarity</li>
<li>Example of word-similarity method is cosine similarity between two word-embeddings \(w, v\):
\[
  \text{cosine} (\vec{v}, \vec{w}) =
  \frac{\vec{v}\cdot \vec{w}}{|\vec{v}| |\vec{w}|}
  = \frac{\sum^N_{i=1}v_i w_i}{\sqrt{\sum^N_{i=1} v_i^2} \sqrt{\sum^N_{i=1} w_i^2}}
  \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org16697f2" class="outline-3">
<h3 id="org16697f2"><span class="section-number-3">5.2.</span> GloVe</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Global Vectors.
</p>
</div>
<div id="outline-container-org657753a" class="outline-4">
<h4 id="org657753a"><span class="section-number-4">5.2.1.</span> Definitions</h4>
<div class="outline-text-4" id="text-5-2-1">
<dl class="org-dl">
<dt>Global</dt><dd>global statistics of corpus</dd>
<dt>Vectors</dt><dd>representation of words</dd>
</dl>
</div>
</div>
<div id="outline-container-org623048c" class="outline-4">
<h4 id="org623048c"><span class="section-number-4">5.2.2.</span> GloVe model</h4>
<div class="outline-text-4" id="text-5-2-2">
<ul class="org-ul">
<li>Glove uses statistics of word occurrences in a corpus as the primary source of information.</li>
<li>Combines 2 widely adopted approaches for training word vectors:
<ol class="org-ol">
<li>Global matrix factorization</li>
<li>Window-based methods</li>
</ol></li>
<li>Uses Co-occurrence matrix as a starting point</li>
</ul>
</div>
</div>
<div id="outline-container-org09b6776" class="outline-4">
<h4 id="org09b6776"><span class="section-number-4">5.2.3.</span> Extending the co-occurrence matrix</h4>
<div class="outline-text-4" id="text-5-2-3">
<dl class="org-dl">
<dt>Definition</dt><dd>For corpus of vocabulary \(V\) of size \(d\), the co-occurrence matrix is a symmetrical matrix of size \(d\times d\)</dd>
<dt>\(X_{ij}\)</dt><dd>number of times word \(j\) occurs in the context of word \(i\) after defining window size</dd>
<dt>\(X_i = \sum_k X_{ik}\)</dt><dd>summation over all the words which occur in the context of word \(i\)</dd>
<dt>\(P_{ij} = \frac{X_{ij}}{X_i}\)</dt><dd>the co-occurrence probability where \(_{ij}\) is the probability of word \(j\) occurring in the context of word \(i\)</dd>
</dl>
</div>
</div>
<div id="outline-container-orge9f6f29" class="outline-4">
<h4 id="orge9f6f29"><span class="section-number-4">5.2.4.</span> Example</h4>
<div class="outline-text-4" id="text-5-2-4">
<div class="BLOCKQUOTE" id="org0b73724">
<p>
It was the best of times, it was the worst of times.
(Context window=2)
</p>

</div>
<p>
i = "it", j = "was"
</p>
<ul class="org-ul">
<li>\(X_{i=0, j=1} = 2\)</li>
<li>\(X_{i=0} = 6\)</li>
<li>\(P{i=0, j=1} = 2/6 = 0.33\)</li>
</ul>
</div>
</div>
<div id="outline-container-orga21b173" class="outline-4">
<h4 id="orga21b173"><span class="section-number-4">5.2.5.</span> GloVe cost function</h4>
<div class="outline-text-4" id="text-5-2-5">
<ul class="org-ul">
<li>GloVe suggests finding the relationship between 2 words in terms of probability, rather than occurrence counts</li>
<li>GloVe looks to find vectors \(w_i\) and \(w_j\) such that
\[
  w_i^T w_j = \log(P_{ij}) = \log(\frac{X_{ij}}{X_i})
  \]</li>
<li>\(\log(X_i)\) is independent of word \(j\) and can be represented as a bias \(b_i\)</li>
<li>Adding a bias term to restore the symmetry for vector \(w_j\) we get:
\[
  w_i^T w_j + b_i + b_j = \log(X_{ij})
  \]</li>
<li><p>
A weighted least squares is used as a cost function for the GloVe model:
\[
  J = \sum_{ij} f(X_{ij})(w_i^T w_j + b_i + b_j - log(X_{ij}))^2
  \]
with:
</p>

\begin{equation}
f(x) =
\begin{cases}
(\frac{x}{x_{\text{max}}})^4 & \text{ if }x < x_{\text{max}} \\
1 & \text{otherwise}
\end{cases}
\end{equation}
<ul class="org-ul">
<li>In original paper, \(\alpha = \frac{3}{4}\) gave the best performance</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgaa06d23" class="outline-4">
<h4 id="orgaa06d23"><span class="section-number-4">5.2.6.</span> GloVe word vectors</h4>
<div class="outline-text-4" id="text-5-2-6">
<ul class="org-ul">
<li>Trained in batches of the training sample with optimizer to <b><b>minimize</b></b> the cost function and hence generate word and context vectors for each word</li>
<li>Each word in the corpus is represented bya dense vector of fixed size length</li>
<li>Word vectors obtained by GloVe showcase the meaning that was captured in these vector representations through similarity and linear structure</li>
<li>Using Euclidean distance or cosine similarity between word vectors represents <b><b>linguistic</b></b> or <b><b>semantic</b></b> similarity of the corresponding words.</li>
<li>E.g. "summer" is most similar to "winter", "spring", "autumn"</li>
</ul>
</div>
</div>
<div id="outline-container-orga2a2d43" class="outline-4">
<h4 id="orga2a2d43"><span class="section-number-4">5.2.7.</span> GloVe conserves linear relationships</h4>
<div class="outline-text-4" id="text-5-2-7">
<ul class="org-ul">
<li>Word vectors by GloVe conserve linear substructures</li>
<li>Vector differences capture as much as possible the meaning specified by two words</li>
<li>E.g.: the underlying concept that differentiates man and woman, i.e. gender, may be equivalently specified by other word pairs such as king and queen:
\[
  w_{\text{man}} - w_{\text{woman}} = w_{\text{king}} - w_{\text{queen}}
  \]</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orga8c0a99" class="outline-2">
<h2 id="orga8c0a99"><span class="section-number-2">6.</span> Week 8: Neural Networks and Word2Vec</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orge4f5863" class="outline-3">
<h3 id="orge4f5863"><span class="section-number-3">6.1.</span> Neural Networks</h3>
<div class="outline-text-3" id="text-6-1">
<p>
M6T1
</p>
</div>
<div id="outline-container-orgf1bc967" class="outline-4">
<h4 id="orgf1bc967"><span class="section-number-4">6.1.1.</span> Inspiration from biological neurons</h4>
<div class="outline-text-4" id="text-6-1-1">
<p>
Neurons: core components of the brain and nervous system.
Consists of:
</p>
<dl class="org-dl">
<dt>Dendrites</dt><dd>collect information from other neurons</dd>
<dt>Axon</dt><dd>generates outgoing spikes</dd>
</dl>
</div>
</div>
<div id="outline-container-org9012322" class="outline-4">
<h4 id="org9012322"><span class="section-number-4">6.1.2.</span> Logistic regression block review</h4>
<div class="outline-text-4" id="text-6-1-2">

<div id="orgfa6b018" class="figure">
<p><img src="./img/neuron-block.png" alt="neuron-block.png" />
</p>
</div>
<ul class="org-ul">
<li><b><b>Summation</b></b> part is a linear combination of features or dimensions - i.e. unique words in the document term matrix
<ul class="org-ul">
<li>Receives a data point as an input, which can be multi-dimensional</li>
<li>Linearly combines them using model parameters, shown as &theta; .</li>
<li>Linear combination of features, the output of the output of summation function, captures the linear relationship between input and output</li>
<li>Linear function may not be sufficient to capture the non-linear and complex relationship between input data points and their output.</li>
</ul></li>
<li>Hence, <b><b>activation</b></b> function is needed, which is fed by output of summation term.
<ul class="org-ul">
<li>Typically chosen to be a non-linear function, which helps the network understand and learn the complex relationship between input and output.</li>
<li>Well-known activation functions:
<dl class="org-dl">
<dt>Linear unit</dt><dd>does not change the output of the summation function. \(z\)</dd>
<dt>Threshold/Sign</dt><dd>used for hard classification algorithm. Positive or negative output for binary classification. Decision line is zero. \(\text{sgn}(z)\)</dd>
<dt>Sigmoid</dt><dd>used to scale output between \([0,1]\). Commonly used for classification problems. If used with 1 learning block, it's a logistic regression algorithm with soft classification. \(\frac{1}{1+\exp(-z)}\)</dd>
<dt>ReLu</dt><dd>Rectified linear unit. Commonly used in deep learning methods because of friendly optimization, back propagation and fast training of predictive model. \(\max(0,z)\)</dd>
<dt>Tangent hyperbolic (Tanh) unit</dt><dd>Scales a real value from -1 to +1. Captures negative values (vs. sigmoid unit), which just scales between \([0,1]\). \(\tanh(z)\)</dd>
</dl></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgcb60de5" class="outline-4">
<h4 id="orgcb60de5"><span class="section-number-4">6.1.3.</span> Connecting blocks to create neural networks</h4>
<div class="outline-text-4" id="text-6-1-3">
<p>
Recap:
</p>
<ul class="org-ul">
<li>a block received a data point or document having features, and linearly combines them in a summation function, that is then fed to activation function \(h(x)\).</li>
<li>New neuron or feature would help network and original features learn more complex relationship between inputs and outputs.</li>
<li>Artificial neural network can solve both regression and classification problems.</li>
<li>Fully connected network: each neuron needs to be connected to all the learning blocks.
<img src="./img/fc-neurons.png" alt="fc-neurons.png" />
<dl class="org-dl">
<dt>1</dt><dd>Bias value</dd>
<dt>\(x_i\)</dt><dd>Feature</dd>
<dt>\(\theta\)</dt><dd>Weights or parameters</dd>
<dt>\(\mu_{21}\)</dt><dd>summation output for layer 2, depth 1</dd>
</dl></li>

<li><span style='background-color: #FFFF00;'>last activation function</span> defines what model / problem we're trying to solve (regression or classification)</li>
<li>Layers not connected to the last learning block are <span style='background-color: #FFFF00;'>hidden layers</span></li>
<li>ANN can have many hidden layers, e.g. \(\theta_0\) till \(\theta_6\)</li>
<li>Changing the "first neuron" (last neuron) to sigmoid changes this to solve a <b><b>classification</b></b> problem</li>
</ul>
</div>
</div>
<div id="outline-container-org5a944f0" class="outline-4">
<h4 id="org5a944f0"><span class="section-number-4">6.1.4.</span> Increasing the depth of each layer</h4>
<div class="outline-text-4" id="text-6-1-4">
<p>
i.e. same number of layers, but more neurons
</p>
<ul class="org-ul">
<li>Can be doen by increasing the num,ber of learning blocks</li>
<li>This generates more O's, increasing the number of parameters</li>
</ul>
</div>
</div>
<div id="outline-container-orgd78ea28" class="outline-4">
<h4 id="orgd78ea28"><span class="section-number-4">6.1.5.</span> Increasing layers</h4>
<div class="outline-text-4" id="text-6-1-5">
<ul class="org-ul">
<li>Add more hidden layers</li>
<li>There can be as many hidden layers as needed</li>
<li>These are hyperparameters
<ul class="org-ul">
<li>Number of neurons</li>
<li>Number of hidden layers</li>
</ul></li>
<li>These hyperparameters need to be tuned for the complexity between inputs and outputs</li>
<li>Need to <span style='background-color: #FFFF00;'>prevent overfitting</span> especially if there's insufficient training or testing data</li>
</ul>
</div>
</div>
<div id="outline-container-org211f8e6" class="outline-4">
<h4 id="org211f8e6"><span class="section-number-4">6.1.6.</span> Forward pass</h4>
<div class="outline-text-4" id="text-6-1-6">
<p>
\[
u_{11} = \sum^d_{i=0} x_i \theta_i = \theta_0 + \theta_1 x_1 + ... + \theta_d x_d \\
O_{11} = \frac{1}{1+e^{-u_{11}}}
\]
</p>
<ul class="org-ul">
<li>In the forward pass, calculate all \(u_{ij}\) and \(o_{ij}\) values from <span style='background-color: #FFFF00;'>left to the right</span> of the network</li>
</ul>
</div>
</div>
<div id="outline-container-orge8beb29" class="outline-4">
<h4 id="orge8beb29"><span class="section-number-4">6.1.7.</span> Backpropagation</h4>
<div class="outline-text-4" id="text-6-1-7">
<ul class="org-ul">
<li>Update all \(\theta_i\) parameters from the right to the left of the network</li>
<li>Optimization can be done using iterative techniques such as gradient descent</li>
<li>Updating parameters depends on the types of loss functions:
<ul class="org-ul">
<li>Regression: RMSE</li>
<li>Classification: Cross-entropy</li>
</ul></li>
<li>Minimize the loss function by taking partial derivatives w.r.t. the model parameters</li>
<li>Last parameters are updated first, and use chaining rule to update the other parameters</li>
<li>Types of approaches
<dl class="org-dl">
<dt>Stochastic gradient descent or iteration</dt><dd>1 document at a time. Process one document at a time, need 1000 iterations to go through all data points. Each time going through all documents = 1 epoch. <span style='background-color: #FFFF00;'>Memory friendly</span> but computationally slow as we are updating parameters for each document one at a time.</dd>
<dt>Batch gradient descent</dt><dd>Pass a sub-portion of documents each time. E.g. if 50 documents / iteration, will reduce the number of iterations from 1000 to 20. Also possible to pass <b><b>all</b></b> data points at once, but usually can't fit in memory.</dd>
</dl></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5e371b6" class="outline-3">
<h3 id="org5e371b6"><span class="section-number-3">6.2.</span> Word2Vec, CBOW and Skipgram</h3>
<div class="outline-text-3" id="text-6-2">
<p>
M6L2
</p>
</div>
<div id="outline-container-org39981f1" class="outline-4">
<h4 id="org39981f1"><span class="section-number-4">6.2.1.</span> Review of One-Hot Encoding</h4>
<div class="outline-text-4" id="text-6-2-1">
<ul class="org-ul">
<li>Simplest word embedding.</li>
<li>Example 1 document: "Apple and orange are fruit". Vectors:
<dl class="org-dl">
<dt>Apple</dt><dd><code>[1, 0, 0, 0]</code></dd>
<dt>Orange</dt><dd><code>[0, 1, 0, 0]</code></dd>
<dt>Are</dt><dd><code>[0, 0, 1, 0]</code></dd>
<dt>Fruit</dt><dd><code>[0, 0, 0, 1]</code></dd>
</dl></li>
<li>Common words are removed first. They don't
provide distinguishing features for the words in the corpus.</li>
</ul>
</div>
</div>
<div id="outline-container-org159708d" class="outline-4">
<h4 id="org159708d"><span class="section-number-4">6.2.2.</span> Issues with One-Hot Embedding</h4>
<div class="outline-text-4" id="text-6-2-2">
<ul class="org-ul">
<li>The size of each word vector = vocabulary size in the corpus. Creates a huge vector if we have millions of words in the vocabulary.</li>
<li>Very long OHE vector wastes storage and computation</li>
<li>Curse of dimensionality can emerge for very large vectors</li>
<li>With a new corpus, the size of each word vector will be different and the model previously trained will be useless (can't transfer learning).</li>
</ul>
</div>
</div>
<div id="outline-container-org7f6dbc6" class="outline-4">
<h4 id="org7f6dbc6"><span class="section-number-4">6.2.3.</span> Contextual meaning of the words</h4>
<div class="outline-text-4" id="text-6-2-3">
<ul class="org-ul">
<li>Word2Vec and GLoVE are context-independent.
<ul class="org-ul">
<li>They output just one vector embedding for each word, combining all different senses of the word into 1 vector.</li>
</ul></li>
<li>E.g. for the example above, do "apple" and "fruit" share some common features since they're all fruit?
<ul class="org-ul">
<li>No, because OHE is just 0 and 1 embedding and does not consider the contextual meaning of words.
<ul class="org-ul">
<li><span style='background-color: #FFFF00;'>There is no correlation</span> between words that have similar meanings or usage.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org399659d" class="outline-4">
<h4 id="org399659d"><span class="section-number-4">6.2.4.</span> What do we want to achieve from word embeddings?</h4>
<div class="outline-text-4" id="text-6-2-4">
<ul class="org-ul">
<li>Can we come up with a word embedding that can capture a numerical similarity value? e.g.
<ul class="org-ul">
<li>Similarity value of (apple, orange) == similarity value of (orange, apple)</li>
<li>Similarity value of (apple, orange) &gt; similarity value of (apple, are)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd437951" class="outline-4">
<h4 id="orgd437951"><span class="section-number-4">6.2.5.</span> Algorithm 1:: Continuous Bag of Words (CBOW)</h4>
<div class="outline-text-4" id="text-6-2-5">
<ul class="org-ul">
<li>Use neural networks to learn the underlying representation of words</li>
<li><span style='background-color: #FFFF00;'>Caveat:</span> neural network model is a supervised algorithm. Needs labels. Must find a way to synthesize the labels from the corpus.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgf0db44a"></a>Neighboring words for CBOW (label creation)<br />
<div class="outline-text-5" id="text-6-2-5-1">
<ul class="org-ul">
<li>i.e., Given the neighbors of a word, can we predict the center word?</li>
<li>Given the context or neighbours of a word, can I predict the blank by a window size?</li>
<li>Simplicity: keep window size of 1, and remove the common words.</li>
<li>Here, we want to find \(P(\text{orange}|\text{context})\), and need to maximize this probability.</li>
</ul>
</div>
</li>
<li><a id="orgcd7184d"></a>Embedding every single word in the corpus using its context<br />
<div class="outline-text-5" id="text-6-2-5-2">
<ul class="org-ul">
<li>Find the embedding representation of the word "orange".</li>
<li>First, all words in the vocabulary need to be encoded using OHE. Each word will have \(d\) dimensions (i.e., the size of the vocabulary).</li>
</ul>
</div>
</li>
<li><a id="org64d9c5b"></a>Using a window size of 1<br />
<div class="outline-text-5" id="text-6-2-5-3">
<p>
[Apple, <span class="underline">_</span>, Are]
<img src="./img/window-size-1.png" alt="window-size-1.png" />
</p>
<ul class="org-ul">
<li>Uses a single hidden layer, which allows the embedding of a word.</li>
<li>Input vectors have the size of context,</li>
<li><span style='background-color: #FFFF00;'>In CHOW</span> the dimensions of the hidden layer and the output layer are always the same.</li>
<li>First weight \(d \times E\):
<dl class="org-dl">
<dt>d</dt><dd>dimension of one-hot encoded word</dd>
<dt>E</dt><dd>embedding size</dd>
</dl></li>
<li>What's the desirable size to vectorize each word of the corpus?</li>
<li>Size of \(E\) is also a hyperparameter. It's much smaller than \(d\).</li>
<li>Use weight matrix to embed word after it's trained by neural network.
<ul class="org-ul">
<li>Multiply word by word matrix W, results gives us the embedded word.</li>
</ul></li>
<li>Hidden layer will be calculated from average element wise-multiplication of each input vector for weights parameter \(w\).</li>
<li>Need to maximize a lot of likelihood by optimizing the parameters via backpropagation</li>
<li>Typically minimize negative log-likelihood instead of maximizing the original likelihood</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org787923f" class="outline-4">
<h4 id="org787923f"><span class="section-number-4">6.2.6.</span> Algorithm 2: Skip-Gram model</h4>
<div class="outline-text-4" id="text-6-2-6">
<ul class="org-ul">
<li>Given a center word, what could be the context (neighbours) of the center word?</li>
<li>Opposite of CBOW.</li>
<li>Example: <code>orange ___ fruit</code></li>
<li>Can I predict the blanks by a window size?
<ul class="org-ul">
<li>Window size is the hyperparameter</li>
</ul></li>
<li>To find and <span style='background-color: #FFFF00;'>maximize</span>: \(P(\text{context | orange})\).</li>
<li>Skipgram
<ul class="org-ul">
<li>Input layer: center word, i.e. "orange"</li>
<li>Output layer: the probability vector that is the prediction of all context words for each output context word.</li>
</ul></li>
<li>Calculate softmax probability</li>
<li>Next, in loss function, <span style='background-color: #FFFF00;'>minimize</span> the negative log likelihood over all softmax context words.</li>
<li>Use a <span style='background-color: #FFFF00;'>cross-entropy loss</span> as we treat this problem as a <b><b>classification</b></b> problem.</li>
</ul>
</div>
</div>
<div id="outline-container-org9371a2f" class="outline-4">
<h4 id="org9371a2f"><span class="section-number-4">6.2.7.</span> Main differences between CBOW and Skip-gram</h4>
<div class="outline-text-4" id="text-6-2-7">
<ul class="org-ul">
<li>CBOW learns better syntactic relationships between words; Skipgram captures better semantic relationships.
<ul class="org-ul">
<li>CBOW would provide cats and cants as similar, while skipgram will provide cats and dogs as similar.</li>
<li>CNOW is trained to predict (i.e., maximize the probaility) of a single word from a fixed window size of context words, while Skip-gram does the opposite and strives to predict several context words from a single input word.</li>
<li><span style='background-color: #FFFF00;'>CBOW is faster to train</span> vs. Skip-gram.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orga145a85" class="outline-2">
<h2 id="orga145a85"><span class="section-number-2">7.</span> Week 9: Convolutional Neural Networks and Recurrent Neural Networks</h2>
<div class="outline-text-2" id="text-7">
<p>
Module 7
</p>
</div>
<div id="outline-container-org2536fa5" class="outline-3">
<h3 id="org2536fa5"><span class="section-number-3">7.1.</span> CNN and Deep Learning I</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>Deep learning is a network that contains several hidden layers</li>
<li>Without convolution, input data (e.g. images) are fed into network in raw format.</li>
<li>If we use fully connected (FC) layers to created hidden layers, it will create a very large number of parameters.
<ul class="org-ul">
<li>Model is thus complex and not generalizable.  &rarr; Over-fitted.</li>
<li><img src="./img/3-layer.png" alt="3-layer.png" /> - There are 3 hidden layers
<ul class="org-ul">
<li>First hidden layer created on the linear combination of input neurons that pass into an activation function.</li>
<li>Typically, <b><b>initial layers</b></b> identify light vs dark pixels, edges, and simple shapes.</li>
<li>Second layer is constructed on linear combinations of the first hidden layer (i.e. linear combination of the input neurons), which helps model to learn a more complex structure.</li>
<li><b><b>Middle layers</b></b> generally responsible for more complex shapes and objects.</li>
<li><b><b>Final layers</b></b> responsible for detecting main objective, or goal of the network. In the pictured example &rarr;  human face.</li>
</ul></li>
</ul></li>
<li>Question:
<ul class="org-ul">
<li>can we reduce number of parameters?</li>
<li>can some parameters be shared between different features or routes?</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org85c66f5" class="outline-4">
<h4 id="org85c66f5"><span class="section-number-4">7.1.1.</span> Example: learning an image</h4>
<div class="outline-text-4" id="text-7-1-1">
<ul class="org-ul">
<li>Some patterns are much smaller than the whole image.</li>
<li>Main goal is to construct network that can learn an bird's beaks.</li>
<li>Model: a type of beak detector in a converged (i.e. in a fully connected layer, the convulational layer).
<ul class="org-ul">
<li>Consider each pixel a neuron. Remove the regional and local dependency among the features</li>
<li>In the big detector network, need to be aware of the surrounding features of pixels to accurately determine if an image has a beak or not.</li>
<li>Looking at one pixel at a time requires network to have many parameters to initially find the local relationship between pixels to figure out whether there's a beak.</li>
<li>Can we help the network by providing the local relationship in advance?</li>
<li><span style='background-color: #FFFF00;'>Simple = using fewer parameters to come up with a beak detector</span></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga9d0162" class="outline-4">
<h4 id="orga9d0162"><span class="section-number-4">7.1.2.</span> Why does CNN work well for images</h4>
<div class="outline-text-4" id="text-7-1-2">
<ul class="org-ul">
<li>Introducing small regions to a network instead of pixel by pixel, how is network going to learn?
<ul class="org-ul">
<li>Small region (or bigger detectors) are commonly known as <span style='background-color: #FFFF00;'>kernels</span>. These are filters.</li>
<li>There will be various filters that detect different shapes or types of bird beaks.</li>
<li>Logically, one filter can't tag all types of bird beaks. E.g. sparrows and doves both have beaks but they are different types of shapes.</li>
<li>Therefore, need to introduce several filters that detect different types, or shapes, of bird beaks.</li>
<li>These filters or kernels will be model parameters that need to be learned.</li>
<li>Can we use CNN for documents too?
<ul class="org-ul">
<li>Yes.</li>
<li>However, need to prepare data and create a matrix that's compatible with CNN model.</li>
</ul></li>
<li>E.g. if we have 3 sample data points:
<ol class="org-ol">
<li><code>Wafa and Mahdi teach NLP class</code></li>
<li><code>NLP is neat</code></li>
<li><code>CNN is a good model</code></li>
</ol></li>
<li>Vocabulary vector has 12 unique words</li>
<li>Longest document has 6 words; CNN needs all datapoints (documents) to have the same size.
<ul class="org-ul">
<li>Use zero-padding to make them the same size.</li>
<li>Use encoding technique to convert every word to vector, e.g. One-Hot Encoding or Word2Vec.</li>
<li>Next, convert the document or sentence to a matrix.</li>
<li>Create matrix with the size of 6 x 12</li>
</ul></li>
<li>Rows are the number of words</li>
<li>Columns are created based on vector encoding technique.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6d92484" class="outline-3">
<h3 id="org6d92484"><span class="section-number-3">7.2.</span> CNN and Deep Learning II</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Module 7 L1 T2
</p>
<ul class="org-ul">
<li>Going over convolutional part and prediction part of the deep learning model</li>
<li>All documents have the same size and are converted to the matrix form</li>
<li>Can start feeding them into CNN model</li>
<li>CNN is a neural network with:
<ul class="org-ul">
<li>Some convolutional layers</li>
<li>And some other layers</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orge9bbf44" class="outline-4">
<h4 id="orge9bbf44"><span class="section-number-4">7.2.1.</span> Convolution vs Fully connected</h4>
<div class="outline-text-4" id="text-7-2-1">
<ul class="org-ul">
<li>A convolutional layer has several filters that do the convolutional operation.</li>
<li>This means that filters need to slide over the input matrix and do a <span style='background-color: #FFFF00;'>dot product</span> operation</li>
<li>E.g. if input data is 6x6:
<ul class="org-ul">
<li>Using ANN approach with fully connected layers, we need to consider each element as a feature or neuron.
<img src="./img/fully-conn.png" alt="fully-conn.png" />
<ul class="org-ul">
<li>Each element of the matrix is a feature or neuron. First hidden layer has the same number of neurons as inputs. Edges between input layer and 1st hidden layer will create 36x36 parameters.</li>
<li><span style='background-color: #FFFF00;'>Increases chance of overfitting</span> as model is more complex.</li>
</ul></li>
<li>To use CNN, we simplify it to 18 parameters.
<img src="./img/conv-vs-fc.png" alt="conv-vs-fc.png" /></li>
<li>Typically, 3x3 or 5x5 filters are used.</li>
<li>The number of filters is a hyperparameter</li>
<li>Field values are randomly initialized at first, then optimized via backpropagation, similar to ANN.</li>
<li>Each iteration of the filter is a sum-product such as:
<img src="./img/sumproduct.png" alt="sumproduct.png" />
Where it starts from the top left and the result is a scalar number.
<ul class="org-ul">
<li>Each neuron is thus connected to 9, instead of 36 neurons.</li>
</ul></li>
<li><span style='background-color: #FFFF00;'>Parameter sharing</span> since the filter (e.g. Filter 1) is shared among all the strides and different 9 squares in the input document</li>
<li><span style='background-color: #FFFF00;'>Max pooling</span>: new matrices are generated from the original one, e.g.:
<img src="./img/max-pooling.png" alt="max-pooling.png" />
Commonly, 2x2 is used for max pooling</li>
<li>6x6 matrix &rarr; convolution &rarr; max pooling &rarr;  new, smaller document is thus generated, a 2x2 document with 2 channels. Each channel is a filter.</li>
</ul></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org5cbf882"></a>Summary<br />
<div class="outline-text-5" id="text-7-2-1-1">
<p>
CNN compresses a fully connected network in 3 ways:
</p>
<ol class="org-ol">
<li>Reduce the number of connections by using filters</li>
<li>Sharing weights on the edges</li>
<li>Max pooling further reduces complexity</li>
</ol>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgf2b1ab7" class="outline-4">
<h4 id="orgf2b1ab7"><span class="section-number-4">7.2.2.</span> The whole CNN</h4>
<div class="outline-text-4" id="text-7-2-2">
<ul class="org-ul">
<li>After initial input, convolution and max pooling, a new smaller image is generated, with the number of channels being the number of filters.</li>
<li>This new image can thus be used again in the next step of the CNN process, and this can be repeated many times.</li>
<li>Convolution is "feature engineering" of CNN process. New smaller image generated creates a richer input that contains more information.</li>
<li>Next step is to flatten the matrix and proceed to the last layer of the neural network which defines the objective of the network.
<ul class="org-ul">
<li>Each 2x2 matrix of 2 channels is then made into a single-column vector and fed into the a <span style='background-color: #FFFF00;'>fully connected feedforward network</span></li>
<li>This can be sentiment analysis, for example.</li>
<li>Backpropagation is also used here.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org9ba1c99" class="outline-4">
<h4 id="org9ba1c99"><span class="section-number-4">7.2.3.</span> CNN in Keras</h4>
<div class="outline-text-4" id="text-7-2-3">
<p>
All these tasks can be done in packages like Keras or Pytorch.
<img src="./img/cnn-keras.png" alt="cnn-keras.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org1943918" class="outline-3">
<h3 id="org1943918"><span class="section-number-3">7.3.</span> Recurrent neural networks Part I</h3>
<div class="outline-text-3" id="text-7-3">
<p>
M7 T2 L1
</p>
<ul class="org-ul">
<li>RNN has concept of sequence, unlike CNN which does not.</li>
<li>CNN only considers the local region of input data, but does not have concept of 'time'.</li>
<li>RNN was created as there were issues in feedforward NN, such as CNN, which cannot handle memorize previous inputs, only considers the current inputs.</li>
<li>Solution is RNN, which can memorize previous input data due to their internal memory.</li>
</ul>
</div>
<div id="outline-container-org9b6e5ee" class="outline-4">
<h4 id="org9b6e5ee"><span class="section-number-4">7.3.1.</span> Name Entity Recognition</h4>
<div class="outline-text-4" id="text-7-3-1">
<ul class="org-ul">
<li>For document: <code>Mahdi and Wafa teach NLP</code></li>
<li>NER model will detect whether each word is a person or not. Model needs to take each word into consideration.</li>
<li>First, encode each word into a vector, using methods such as:
<ul class="org-ul">
<li>GloVe</li>
<li>One Hot Encoding</li>
<li>Word2Vec</li>
</ul></li>
<li>Very important to pay attention to the size of vectors and matrices. Definitions:
<dl class="org-dl">
<dt>x</dt><dd>word. vector of length d.</dd>
<dt>word</dt><dd>d-dimensional vector (\(x \in R^d\))</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org533decc" class="outline-4">
<h4 id="org533decc"><span class="section-number-4">7.3.2.</span> Recap of feed-forward networks</h4>
<div class="outline-text-4" id="text-7-3-2">
<p>
Such as ANN, CNN.
</p>
<ul class="org-ul">
<li>Each word has d elements, input has d neurlas.
<ul class="org-ul">
<li>Add a neuron to handle bias term</li>
<li>Do linear combination in hidden layers with activation function.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org95aa412" class="outline-4">
<h4 id="org95aa412"><span class="section-number-4">7.3.3.</span> Simplifying this for RNN</h4>
<div class="outline-text-4" id="text-7-3-3">

<div id="org3b4a498" class="figure">
<p><img src="./img/rnn1.png" alt="rnn1.png" />
</p>
</div>
<ul class="org-ul">
<li>Need NER for every single word.</li>
<li>Output prediction for each word is
\(\hat{y_t}\)</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org7c184cb" class="outline-3">
<h3 id="org7c184cb"><span class="section-number-3">7.4.</span> Recurrent neural networks Part II</h3>
<div class="outline-text-3" id="text-7-4">
<p>
M7 T2 L2
</p>
<ul class="org-ul">
<li>Previously: created a feed-forward network and represented with compact visualization. Example problem is NER task for a sentence <code>Mahdi and Wafa teach NLP</code>.</li>
</ul>
</div>
<div id="outline-container-orga61f30b" class="outline-4">
<h4 id="orga61f30b"><span class="section-number-4">7.4.1.</span> RNN</h4>
<div class="outline-text-4" id="text-7-4-1">
<ul class="org-ul">
<li>As there are 5 words, we need to construct 5 similar feedforward network for each word.</li>
<li>Compact visualization was previously shown in a horizontal view.</li>
<li>Turning it to a vertical view, we can visualize it for all feed-forward units</li>
<li>\(m\) is the number of hidden neurons, a hyperparameter that needs to be optimized.</li>
<li>2 different sets of parameters:
<ol class="org-ol">
<li>Input to hidden layers \(\theta^1_{d\times m}\)</li>
<li>Hidden layers to output \(\theta^2_{m \times y}\)</li>
</ol></li>
<li>All feedforward networks are isolated and not connected to each other. How to connect them sequentially?
<ul class="org-ul">
<li>Use a hidden neuron, e.g. \(h_0\). this also needs \(\theta^3_{m\times m}\)</li>
<li>We introduced a new set of parameters \(\theta^3_{m\times m}\) which generates a new vector of hidden neurons \(h_t\) with size \(m\).</li>
<li>thetas with 1, 2, 3 are shared among all networks.</li>
<li><span style='background-color: #FFFF00;'>The total number of parameters learned does not depend on the total number of words</span>. Instead,  it is:
\[
    d \times m + d \times y + m \times m
    \]</li>
</ul></li>
<li>As each document may contain a different number of works, it will be inefficient to create a RNN for each document.
<ul class="org-ul">
<li>In practice, zero-padding is used to construct 1 RNN for all input documents with different sizes.</li>
<li>If the difference between max, min length document, we instead use bucketing to make this more efficient, and create RNN for each bucket.</li>
</ul></li>
<li>in Keras, call <code>SimpleRNN</code></li>
</ul>
</div>
</div>
<div id="outline-container-org53c8c04" class="outline-4">
<h4 id="org53c8c04"><span class="section-number-4">7.4.2.</span> Forward pass: how to calculate past memory (h) in RNN</h4>
<div class="outline-text-4" id="text-7-4-2">
<dl class="org-dl">
<dt>\(\theta^1\)</dt><dd>weight (parameter) matrix associated with input data</dd>
<dt>\(\theta^2\)</dt><dd>weight (parameter) matrix associated with output data</dd>
<dt>\(\theta^3\)</dt><dd>weight (parameter) matrix associated with hidden state</dd>
</dl>

<p>
\[
h_t = f(x_t, h_{t-1}, \theta)
\]
</p>

<dl class="org-dl">
<dt>\(h_t\)</dt><dd>activation function such as tanh</dd>
<dt>\(x_t\)</dt><dd>input</dd>
<dt>\(h_{t-1}\)</dt><dd>past memory (previous step)</dd>
<dt>\(\theta\)</dt><dd>model parameters (weight)</dd>
</dl>

<p>
e.g.:
</p>

<p>
\[
h_t = \tanh (x_t\theta^1 + h_{t-1}\theta^3 + b)
\]
</p>

<dl class="org-dl">
<dt>\(x_t \theta^1\)</dt><dd>output is vector of size \(m\)</dd>
<dt>\(h_{t-1} \theta^3\)</dt><dd>output is vector of size \(m\)</dd>
<dt>\(b\)</dt><dd>bias, also vector of size \(m\)</dd>
</dl>
</div>
</div>
<div id="outline-container-orge063e1d" class="outline-4">
<h4 id="orge063e1d"><span class="section-number-4">7.4.3.</span> Forward pass: how to calculate output of each step in RNN</h4>
<div class="outline-text-4" id="text-7-4-3">
<p>
Output of each step is \(\hat{y}\)
\[
\hat{y_t} = \text{softmax}(h_t \theta^2)
\]
where:
</p>
<dl class="org-dl">
<dt>softmax</dt><dd>scales the output between 0 and 1</dd>
<dt>\(h_t \theta^2\)</dt><dd>output will be scalar for the NER problem</dd>
</dl>
</div>
</div>
<div id="outline-container-org3dfd7b9" class="outline-4">
<h4 id="org3dfd7b9"><span class="section-number-4">7.4.4.</span> Backpropagation through time (BPTT)</h4>
<div class="outline-text-4" id="text-7-4-4">
<ul class="org-ul">
<li>5 words means 5 loss functions for each RNN unit.</li>
<li>Total loss comes from both forward pass and backpropagation from each unit locally.</li>
<li>Summation of loss functions = total loss.</li>
<li>Backpropagation: move backwards from each local part to optimize parameters.</li>
<li>When backpropgation paths of 2 local loss functions intersect, they need to be summed up</li>
</ul>
</div>
</div>
<div id="outline-container-orgd1b2062" class="outline-4">
<h4 id="orgd1b2062"><span class="section-number-4">7.4.5.</span> Different RNN models</h4>
<div class="outline-text-4" id="text-7-4-5">
<dl class="org-dl">
<dt>One-to-many</dt><dd>used for text generation, image captions</dd>
<dt>Many-to-one</dt><dd>used for sentiment analysis</dd>
<dt>Many-to-many</dt><dd>used for speech tagging, NER, translation, forecasting</dd>
</dl>
</div>
<ol class="org-ol">
<li><a id="org50d260c"></a>NER<br />
<div class="outline-text-5" id="text-7-4-5-1">
<ul class="org-ul">
<li>Many-to-many is used for NER because need to know the output of whether a word refers to a person or not</li>
</ul>
</div>
</li>
<li><a id="org92f578c"></a>Sentiment analysis<br />
<div class="outline-text-5" id="text-7-4-5-2">
<ul class="org-ul">
<li>Document with multiple words as input, need to detect whether a document as a whole is a positive or negative sentiment</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org98ff65b" class="outline-4">
<h4 id="org98ff65b"><span class="section-number-4">7.4.6.</span> Problems with RNN</h4>
<div class="outline-text-4" id="text-7-4-6">
<p>
Forward pass, backpropagation and repeated gradient computation can lead to 2 issues.
</p>
</div>
<ol class="org-ol">
<li><a id="orgad768fe"></a>Exploding gradient<br />
<div class="outline-text-5" id="text-7-4-6-1">
<p>
High gradient values lead to very different weights in each optimization iteration.
</p>

<p>
Solution: use gradient clipping, where a gradient is clipped when it goes higher than a threshold.
</p>
</div>
</li>
<li><a id="org3fffb30"></a>Vanishing gradient<br />
<div class="outline-text-5" id="text-7-4-6-2">
<p>
Lower gradient values that stall a model from optimizing parameters.
</p>

<p>
May suffer from short-term memory for a long sentence where words of interest may be placed far apart from each other in a sentence.
</p>

<p>
Solution: ReLu activation function, LSTM, GRU (different architectures), better weight initialization.
</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgca93d2c" class="outline-2">
<h2 id="orgca93d2c"><span class="section-number-2">8.</span> Week 10: Long-Short Term Memory and Gated Recurrent Units</h2>
<div class="outline-text-2" id="text-8">
<p>
Module 8
</p>
</div>
<div id="outline-container-org98736ee" class="outline-3">
<h3 id="org98736ee"><span class="section-number-3">8.1.</span> LSTM Part I</h3>
<div class="outline-text-3" id="text-8-1">
</div>
<div id="outline-container-org988975b" class="outline-4">
<h4 id="org988975b"><span class="section-number-4">8.1.1.</span> Why?</h4>
<div class="outline-text-4" id="text-8-1-1">
<ul class="org-ul">
<li>NLP is sequential</li>
<li>RNN suffers from short term memory
<ul class="org-ul">
<li>Due to vanishing gradient</li>
<li>Can't keep track of earlier phrases</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5213781" class="outline-4">
<h4 id="org5213781"><span class="section-number-4">8.1.2.</span> Gated cell: LSTM</h4>
<div class="outline-text-4" id="text-8-1-2">
<ul class="org-ul">
<li>Main concept:
<ul class="org-ul">
<li>Add a gate to RNN unit to control the information that's passed through the network</li>
<li>LSTM augments RNN unit by creating gates
<ul class="org-ul">
<li>that allow some information to be passed through</li>
<li>and some other information to be forgotten</li>
</ul></li>
</ul></li>
<li>LSTM is the leading algorithm used in sequential modelling</li>
</ul>
</div>
</div>
<div id="outline-container-org4617ffa" class="outline-4">
<h4 id="org4617ffa"><span class="section-number-4">8.1.3.</span> Different representation of RNN</h4>
<div class="outline-text-4" id="text-8-1-3">

<div id="orgb7751eb" class="figure">
<p><img src="./img/hidden-state-lstm.png" alt="hidden-state-lstm.png" />
</p>
</div>
<ul class="org-ul">
<li>\(h_t\): hidden state or past memory</li>
<li>Lines are merging into each other
<ul class="org-ul">
<li>i.e. concatenated</li>
</ul></li>
<li>Lines are splitting
<ul class="org-ul">
<li>i.e. copied over</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org32c3326" class="outline-4">
<h4 id="org32c3326"><span class="section-number-4">8.1.4.</span> Simple representation of LSTM</h4>
<div class="outline-text-4" id="text-8-1-4">
<ul class="org-ul">
<li>Each line carries a vector from output of one node to input of another</li>
<li>Example of vector:
<ul class="org-ul">
<li>Word</li>
<li>Hidden state</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org52cb0f2" class="outline-4">
<h4 id="org52cb0f2"><span class="section-number-4">8.1.5.</span> The cell state in LSTM</h4>
<div class="outline-text-4" id="text-8-1-5">
<dl class="org-dl">
<dt>\(c_t\)</dt><dd>cell state vector (from old time stamp to new time stamp). LSTM structure beneath cell state can remove or add information to the cell state.</dd>
</dl>
</div>
</div>
<div id="outline-container-orga56c0db" class="outline-4">
<h4 id="orga56c0db"><span class="section-number-4">8.1.6.</span> How LSTM controls information removal and additional for a cell state</h4>
<div class="outline-text-4" id="text-8-1-6">
<ul class="org-ul">
<li>Gate here is the sigmoid operator (pointwise operation)</li>
<li>Sigmoid layer scales numbers in \([0,1]\).
<ul class="org-ul">
<li>0: nothing is let through</li>
<li>1: all information is let through (i.e. <b><b>addition</b></b>)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org94be7f4" class="outline-3">
<h3 id="org94be7f4"><span class="section-number-3">8.2.</span> LSTM Part II</h3>
<div class="outline-text-3" id="text-8-2">
</div>
<div id="outline-container-org51b1ea0" class="outline-4">
<h4 id="org51b1ea0"><span class="section-number-4">8.2.1.</span> Forget gate</h4>
<div class="outline-text-4" id="text-8-2-1">
<ul class="org-ul">
<li>Determines the information to remove or throw away from the cell state.
<img src="./img/input-gate-layer.png" alt="input-gate-layer.png" /></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org03ca908"></a>Input gate layer<br />
<div class="outline-text-5" id="text-8-2-1-1">
<ul class="org-ul">
<li>Once \(i_t\) and \(\hat{C_t}\) are calculated, multiply them together in a pointwise operation.</li>
<li>The input gate's activation vector (\(i_t\)) decides which information is important to keep from the cell input activation factor (\(\hat{C_t}\)).</li>
</ul>
</div>
</li>
<li><a id="org00ced36"></a>Updating old cell state<br />
<div class="outline-text-5" id="text-8-2-1-2">
<ul class="org-ul">
<li>Calculated \(f_t\) in forget gate layer stack
<ul class="org-ul">
<li>This is how much info is kept or thrown away</li>
</ul></li>
<li>Update cell state based on input vector (\(i_t * \hat{C_t}\)) pointwise</li>
</ul>
</div>
</li>
<li><a id="orgf2006d0"></a>Calculate output of current stack (\(h_t\)) based on new \(C_t\)<br />
<div class="outline-text-5" id="text-8-2-1-3">
<ul class="org-ul">
<li>Pass previous hidden state and current input into sigmoid function (sigmoid is \(O_t\))</li>
<li>Pass updated state to hyperbolic tangent function (pointwise multiplication of tanh and sigmoid output)</li>
<li>Output is new hidden state</li>
<li>New cell state and new hidden are carried to new time step</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgdc3db54" class="outline-4">
<h4 id="orgdc3db54"><span class="section-number-4">8.2.2.</span> Gated Recurrent Units</h4>
<div class="outline-text-4" id="text-8-2-2">
<ul class="org-ul">
<li>GRU</li>
<li>Update to LSTM</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgc1858a7"></a>GRU update and reset gates<br />
<div class="outline-text-5" id="text-8-2-2-1">
<ul class="org-ul">
<li>Update gate
<ul class="org-ul">
<li>Acts similar to forget and input gate of LSTM</li>
<li>Decides what information to throw away and what information to add</li>
</ul></li>
<li>Reset gate
<ul class="org-ul">
<li>Another gate used to decide how much past information to forget</li>
</ul></li>
<li>GRU vs. LSTM
<ul class="org-ul">
<li>GRU has fewer tensor operations; faster to train than LSTM</li>
</ul></li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-org948264f" class="outline-3">
<h3 id="org948264f"><span class="section-number-3">8.3.</span> Attention-based LSTM</h3>
<div class="outline-text-3" id="text-8-3">
<p>
and Encoder-Decoder architecture
</p>
</div>
<div id="outline-container-org6aa772d" class="outline-4">
<h4 id="org6aa772d"><span class="section-number-4">8.3.1.</span> Common uses of encoder-decoder architecture</h4>
<div class="outline-text-4" id="text-8-3-1">
<ul class="org-ul">
<li>Language translation, e.g. from English to Polish
<ul class="org-ul">
<li>e.g. Sequence to sequence models in NLP</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2f566f6" class="outline-4">
<h4 id="org2f566f6"><span class="section-number-4">8.3.2.</span> RNN and LSTM units</h4>
<div class="outline-text-4" id="text-8-3-2">
<ul class="org-ul">
<li>LSTM augments RNN units
<ul class="org-ul">
<li>By creating gates that allow some information to be passed on, and some to be forgotten</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd972010" class="outline-4">
<h4 id="orgd972010"><span class="section-number-4">8.3.3.</span> In language translation</h4>
<div class="outline-text-4" id="text-8-3-3">
<p>
<img src="./img/translation1.png" alt="translation1.png" />
<img src="./img/translation2.png" alt="translation2.png" />
</p>
<ul class="org-ul">
<li>Inputs are provided first, then outputs can be generated</li>
<li>Encoder is single-layer LSTM; has input sequence "how", "are", "you"</li>
<li>Definitions:
<dl class="org-dl">
<dt>\(C_i\)</dt><dd>encoder state</dd>
<dt>\(C\)</dt><dd>final encoder state that is sent to decoder</dd>
<dt>\(S_i\)</dt><dd>decoder state</dd>
<dt>\(y_i\)</dt><dd>network outputs</dd>
</dl></li>
<li>Decoder is also single-layer LSTM</li>
<li>Does \(C\) have access to all information in encoder for all inputs?
<ul class="org-ul">
<li>We need to make sure decoder has all the information that's sent to encoder</li>
<li>Maybe not! Encoding step needs to represent entire input sequence</li>
<li>All information must be compressed to \(C\)</li>
<li>Very complex to ask model to get all information from this single vector \(C\) (which has cell state information passed)
<ul class="org-ul">
<li>Must use very deep LSTM units (many cell state parameters &#x2013; high computation time &#x2013;  or use new structure for such models)</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org56dd024" class="outline-4">
<h4 id="org56dd024"><span class="section-number-4">8.3.4.</span> Attention mechanism</h4>
<div class="outline-text-4" id="text-8-3-4">

<div id="orgbac2916" class="figure">
<p><img src="./img/attention.png" alt="attention.png" />
</p>
</div>
<ul class="org-ul">
<li>In simple encoder-decoder architecture (not attention-based)
<ul class="org-ul">
<li>Decoder makes prediction by looking only at the final output of the encoder step, which has condensed information denoted \(R_c\), or context vector</li>
</ul></li>
<li>Attention-based architecture:
<ul class="org-ul">
<li>Attends every hidden state from each encoder node at every time and step</li>
<li>Then makes predictions after deciding which one is more informative</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgbec419a" class="outline-4">
<h4 id="orgbec419a"><span class="section-number-4">8.3.5.</span> Need to know weights associated with each attention</h4>
<div class="outline-text-4" id="text-8-3-5">

<div id="orgac34b81" class="figure">
<p><img src="./img/attention2.png" alt="attention2.png" />
</p>
</div>
<ul class="org-ul">
<li>Terminology
<dl class="org-dl">
<dt>\(h\) math notation</dt><dd>output of each LSTM unit</dd>
<dt>\(T\)</dt><dd>number of words in sequence</dd>
<dt>\(t\)</dt><dd>current word that we're translating (\(x_t\))</dd>
<dt>\(C_t\)</dt><dd>current word context vector</dd>
<dt>\(\alpha\)</dt><dd>weight vector</dd>
<dt>\(h_t\)</dt><dd>encoder hidden state</dd>
<dt>\(S_{t-1}\)</dt><dd>previous decoder output or hidden state</dd>
</dl></li>
<li>Recall: simple encoder-decoder: the <b><b>last</b></b> state of the encoder is used for the context vector</li>
<li>In attention-based architecture:
<ul class="org-ul">
<li>Embeddings of all the words in the inputs are used instead.</li>
<li>These are represented by hidden states</li>
<li>But, do not want to equally contribute all the hidden states to generate context vector</li>
<li>As some words may be more related to final translation</li>
<li>Hence, calculate \(\alpha\) as weight vector to consider each hidden state's contribution to the current word context vector</li>
</ul></li>
<li>Screenshot shows calculation of only 1 set of state at timestamp \(t\).
<ul class="org-ul">
<li>Need to do the same process for all other timestamps to calculate their cell states, e.g. \(c_{t-1}\), \(c_{t+1}\), etc.</li>
</ul></li>
<li>Raw alignment score (\(e\)) is calculated using feedforward neural network with a single hidden layer
<ul class="org-ul">
<li>Inputs are \(h_t\) and \(S_{t-1}\)</li>
<li>General practice: add or concatenate 2 hidden states, then feed into neural network</li>
<li>Single hidden layer uses \(\tanh\) activation function to produce hidden layer neurons</li>
<li>Finally, a linear layer with 1 neuron (or a linear combination transformation) is used to prduce raw alignment score</li>
<li>Process produces a raw score (\(\alpha_{t,t}\)) for just the current timestamp and the chosen encoder hidden state (\(h_t\)).</li>
<li>Repeat for  all other hidden input states,  to calculate raw score using the same neural network</li>
</ul></li>
<li>Next, all raw scores passed through softmax to produce normalized scores (or $&alpha;$s).</li>
<li>Weighted sum of all hidden state using their \(\alpha\) values will produce the current context vector (\(C_t\))</li>
<li>Now, with the same context vector, we can calculate the output of the encoder-decoder unit (similar to LSTM)</li>
<li>Again, this is only for the current context vector (\(c_t\)). Need to repeat for context vectors for different timestamps e.g. \(c_t\), \(c_{t+1}\) and so on.</li>
<li>This is <span style='background-color: #FFFF00;'>additive attention</span>, and is commonly used in sequence to sequence models like LSTM</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org08efb7d"></a>Other types of models<br />
<div class="outline-text-5" id="text-8-3-5-1">
<ul class="org-ul">
<li>E.g. scaled dot product attention, self-attention (core of transformers, to be discussed later)</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2024-03-29 Fri 12:33</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
