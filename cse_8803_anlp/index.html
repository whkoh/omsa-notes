<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-01-31 Wed 19:59 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>CSE 8803: Applied Natural Language Processing</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">CSE 8803: Applied Natural Language Processing</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgfd47ad7">1. Week 1: Text data preprocessing + Course Intro</a>
<ul>
<li><a href="#orgccecc68">1.1. Why ANLP?</a></li>
<li><a href="#org9bc89d6">1.2. Lots of text and written information</a></li>
<li><a href="#orgc10502d">1.3. Example applications of NLP</a></li>
<li><a href="#org2b18f2d">1.4. Challenges of NLP</a></li>
<li><a href="#org03ca860">1.5. Class overview</a></li>
<li><a href="#org63927f1">1.6. Deliverables</a>
<ul>
<li><a href="#org6aa8a2b">1.6.1. Homework</a></li>
<li><a href="#orgb5a90da">1.6.2. Quizzes (10)</a></li>
</ul>
</li>
<li><a href="#org57eabb8">1.7. Course goals</a></li>
<li><a href="#orga343b41">1.8. Text Preprocessing Techniques</a>
<ul>
<li><a href="#orgd3149bf">1.8.1. Terminology</a></li>
<li><a href="#org478b72f">1.8.2. Preprocessing text data</a></li>
<li><a href="#orgba892d6">1.8.3. Noise removal</a></li>
<li><a href="#org299c1a6">1.8.4. Tokenization</a></li>
<li><a href="#org1504624">1.8.5. Text normalization</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgec7b7cd">2. Week 2: (Discrete) Text Representations</a>
<ul>
<li><a href="#org073a49d">2.1. Why?</a></li>
<li><a href="#orgdf2538d">2.2. Representing Words</a></li>
<li><a href="#org62d70ec">2.3. Representing sentences/documents</a></li>
<li><a href="#org7075b76">2.4. One Hot Encoding</a>
<ul>
<li><a href="#org5144271">2.4.1. Definitions</a></li>
<li><a href="#org35babca">2.4.2. Advantages and disadvantages</a></li>
</ul>
</li>
<li><a href="#org05bd9fc">2.5. Bag of Words (Frequency Counting)</a>
<ul>
<li><a href="#org4da9f38">2.5.1. Advantages and disadvantages</a></li>
</ul>
</li>
<li><a href="#orgbbfb0ca">2.6. TF-IDF (Term Frequency-Inverse Document Frequency)</a>
<ul>
<li><a href="#org1078e50">2.6.1. Why needed?</a></li>
<li><a href="#orgc715916">2.6.2. What is TF-IDF and when to use TF-IDF</a></li>
<li><a href="#orgc56790e">2.6.3. Advantages and disadvantages</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0a1dde2">3. Week 3: Linear Text Classification</a>
<ul>
<li><a href="#orge183140">3.1. Classification introduction</a>
<ul>
<li><a href="#org75fb2f8">3.1.1. Supervised learning: definitions</a></li>
<li><a href="#orga7fe856">3.1.2. Categories of supervised learning</a></li>
<li><a href="#orgf55c064">3.1.3. Regression</a></li>
<li><a href="#orgd4b6979">3.1.4. Classification</a></li>
</ul>
</li>
<li><a href="#org2dc7e51">3.2. Naive Bayes</a>
<ul>
<li><a href="#org18726a4">3.2.1. Method / concepts</a></li>
<li><a href="#org3af57e6">3.2.2. Bayes decision rule</a></li>
<li><a href="#orgff93d02">3.2.3. Generative vs discriminative models</a></li>
<li><a href="#orgab6837f">3.2.4. Details of Naive Bayes</a></li>
<li><a href="#org62c8ff9">3.2.5. Naive Conditional Independence Assumption</a></li>
<li><a href="#org49d3926">3.2.6. Advantages and disadvantages</a></li>
</ul>
</li>
<li><a href="#org7b88169">3.3. Classification Model Evaluation</a>
<ul>
<li><a href="#orgc0ceee2">3.3.1. Common metrics</a></li>
<li><a href="#org6a228bf">3.3.2. Confusion matrix</a></li>
<li><a href="#org439a9fd">3.3.3. Accuracy</a></li>
<li><a href="#org3e6dccc">3.3.4. RoC-AUC curve</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgfd47ad7" class="outline-2">
<h2 id="orgfd47ad7"><span class="section-number-2">1.</span> Week 1: Text data preprocessing + Course Intro</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orgccecc68" class="outline-3">
<h3 id="orgccecc68"><span class="section-number-3">1.1.</span> Why ANLP?</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Text and docs are everywhere</li>
<li>Hundreds of languages in the world</li>
<li>Primary information artifacts</li>
<li>Large volumes of textual data</li>
<li>Big and small companies looking for this skill</li>
</ul>
</div>
</div>
<div id="outline-container-org9bc89d6" class="outline-3">
<h3 id="org9bc89d6"><span class="section-number-3">1.2.</span> Lots of text and written information</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>Internet</li>
<li>Webpages, Facebook, Wikipedia, etc.</li>
<li>Digital libraries: Google Books, ACM, IEEE</li>
<li>Lyrics, subtitles, etc.</li>
<li>Police case reports</li>
<li>Legislation</li>
<li>Reviews</li>
<li>Medical reports</li>
<li>Job descriptions</li>
</ul>
</div>
</div>
<div id="outline-container-orgc10502d" class="outline-3">
<h3 id="orgc10502d"><span class="section-number-3">1.3.</span> Example applications of NLP</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>Establish authenticity, detect plagiarism</li>
<li>Classification of genres</li>
<li>Classification of tone; sentiment analysis</li>
<li>Syntax analysis in code</li>
<li>Machine translation</li>
</ul>
</div>
</div>
<div id="outline-container-org2b18f2d" class="outline-3">
<h3 id="org2b18f2d"><span class="section-number-3">1.4.</span> Challenges of NLP</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>Interdisciplinary field</li>
<li>Ambiguity at many levels of language:
<ol class="org-ol">
<li>Lexical (Word level)</li>
<li>Syntactic: different ways of parsing</li>
<li>Partial information: e.g., how to interpret pronouns</li>
<li>Contextual information: context of sentence may affect meaning of sentence</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org03ca860" class="outline-3">
<h3 id="org03ca860"><span class="section-number-3">1.5.</span> Class overview</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>Preprocessing:
<ul class="org-ul">
<li>Clean text and documents</li>
<li>Tokenization</li>
<li>Reducing inflectional forms of a word:
<ul class="org-ul">
<li>Stemming</li>
<li>Lemmatization</li>
</ul></li>
<li>Normalization</li>
</ul></li>
<li>Text representation
<ul class="org-ul">
<li>One hot encoding</li>
<li>Bag of words (Frequency counting)</li>
<li>Term frequency-Inverse document frequency (TF-IDF)</li>
<li>Embeddings</li>
</ul></li>
<li>Overview of classification methods
<ul class="org-ul">
<li>Naive Bayes</li>
<li>Logistic regression</li>
<li>SVM</li>
<li>Perceptron</li>
<li>Nerual Network</li>
</ul></li>
<li>Overview of Deep Learning
<ul class="org-ul">
<li>Convolutional neural network</li>
<li>Recurrent neural network</li>
<li>Long short-term memory</li>
</ul></li>
<li>Overview of topic modelling
<ul class="org-ul">
<li>Principal component analysis</li>
<li>Singular value decomposition</li>
<li>Latent Dirichlet Allocation</li>
</ul></li>
<li>Overview of Transformer methods
<ul class="org-ul">
<li>Bidirectional Encoder Representations from Transformers</li>
<li>Generative Pre-trained Transformers (GPT)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org63927f1" class="outline-3">
<h3 id="org63927f1"><span class="section-number-3">1.6.</span> Deliverables</h3>
<div class="outline-text-3" id="text-1-6">
</div>
<div id="outline-container-org6aa8a2b" class="outline-4">
<h4 id="org6aa8a2b"><span class="section-number-4">1.6.1.</span> Homework</h4>
<div class="outline-text-4" id="text-1-6-1">
<ul class="org-ul">
<li>HW1: Text preprocessing and classification intro</li>
<li>HW2: Classification methods, dimensionality reduction, SVD</li>
<li>HW3: Deep learning</li>
<li>HW4: Transformers and unsupervised methods</li>
</ul>
</div>
</div>
<div id="outline-container-orgb5a90da" class="outline-4">
<h4 id="orgb5a90da"><span class="section-number-4">1.6.2.</span> Quizzes (10)</h4>
<div class="outline-text-4" id="text-1-6-2">
<ul class="org-ul">
<li>Measure understanding of topic</li>
<li>Mostly conceptual questions</li>
<li>MCQ</li>
<li>Limited time to do the test</li>
<li>Mandatory</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org57eabb8" class="outline-3">
<h3 id="org57eabb8"><span class="section-number-3">1.7.</span> Course goals</h3>
<div class="outline-text-3" id="text-1-7">
<ul class="org-ul">
<li>Demonstrate how to pre-process textual data</li>
<li>Differentiate text representation methods and techniques</li>
<li>Explain different NLP tasks</li>
<li>Develop and assess performance of different NLP models using a variety of techniques</li>
</ul>
</div>
</div>
<div id="outline-container-orga343b41" class="outline-3">
<h3 id="orga343b41"><span class="section-number-3">1.8.</span> Text Preprocessing Techniques</h3>
<div class="outline-text-3" id="text-1-8">
</div>
<div id="outline-container-orgd3149bf" class="outline-4">
<h4 id="orgd3149bf"><span class="section-number-4">1.8.1.</span> Terminology</h4>
<div class="outline-text-4" id="text-1-8-1">
<dl class="org-dl">
<dt>Corpus</dt><dd>collection of text, e.g. Yelp reviews, Wikipedia articles</dd>
<dt>Syntax</dt><dd>Grammatical structure of text</dd>
<dt>Syntactic parsing</dt><dd>process of analyzing natural language with grammatical rules</dd>
<dt>Semantics</dt><dd>meaning of text</dd>
<dt>Tokenization</dt><dd>splitting long pieces of text into smaller pieces (tokens). e.g.: <code>This is a simple sentence</code> -&gt; <code>["This", "is", "a", "simple", "sentence"]</code></dd>
<dt>Stop words</dt><dd>commonly used words, e.g. "the", "a", "an", "is", "are". Do not contribute to overall meaning</dd>
<dt>N-grams</dt><dd>consecutive sequence of words (commonly: 2-5) in a text. 1-gram (unigram), 2-gram (bigram), 3-gram (trigram). Example of bigrams: <code>"This is", "is a", "a simple", "simple sentence"</code></dd>
</dl>
</div>
</div>
<div id="outline-container-org478b72f" class="outline-4">
<h4 id="org478b72f"><span class="section-number-4">1.8.2.</span> Preprocessing text data</h4>
<div class="outline-text-4" id="text-1-8-2">
<ul class="org-ul">
<li>Text is unstructured, so preprocessing is the first step to prepare and clean text data to perform a NLP task</li>
<li>Useful libraries:
<ul class="org-ul">
<li>re: regular expressions</li>
<li>nltk: natural language toolkit</li>
</ul></li>
<li>Common steps:
<ul class="org-ul">
<li>Noise removal</li>
<li>Tokenization</li>
<li>Text normalization</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgba892d6" class="outline-4">
<h4 id="orgba892d6"><span class="section-number-4">1.8.3.</span> Noise removal</h4>
<div class="outline-text-4" id="text-1-8-3">
<p>
Removal of unwanted text formatting information, e.g.:
</p>
<ul class="org-ul">
<li>Punctuation</li>
<li>Accent marks</li>
<li>Special characters</li>
<li>Numeric digits (could be replaced with words)</li>
<li>Leading, ending and vertical whitespace</li>
<li>HTML formatting</li>
</ul>

<p>
Example: <code>This is a 'simple'' sentence !!! 1+ \n</code> -&gt; <code>This is a simple sentence</code>
</p>
</div>
</div>
<div id="outline-container-org299c1a6" class="outline-4">
<h4 id="org299c1a6"><span class="section-number-4">1.8.4.</span> Tokenization</h4>
<div class="outline-text-4" id="text-1-8-4">
<p>
Example:
<code>This is a simple sentence</code> -&gt;
<code>['This', 'is', 'a', 'simple', 'sentence', '.']</code>
</p>
</div>
</div>
<div id="outline-container-org1504624" class="outline-4">
<h4 id="org1504624"><span class="section-number-4">1.8.5.</span> Text normalization</h4>
<div class="outline-text-4" id="text-1-8-5">
<p>
Removing variations in the text to bring it to a standard form.
</p>
<ul class="org-ul">
<li>Case: Convert all letters to upper or lower case</li>
<li>Removing stop words, sparse terms, other special / particular words.</li>
</ul>

<p>
Example of text normalization:
<code>This is a Simple SenTence</code> -&gt;
<code>simple sentence</code>
</p>
<ul class="org-ul">
<li>Stemming: reduce words to word stem, base, or root form.
Example: <code>There are several tytpes of stemming algorithms</code> -&gt; <code>there are sever type fo stem algorithms.</code></li>
<li>Lemmatization: similar to stemming. Reduces inflectional forms to a common base form, <b><b>the lemma</b></b>. Does <b><b>not</b></b> simply chop off inflections. Uses <b><b>lexical knowledge</b></b> to get the correct base form of words.
Example: <code>There are several tytpes of stemming algorithms</code> -&gt; <code>There are several type of stemming algorithms.</code></li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgec7b7cd" class="outline-2">
<h2 id="orgec7b7cd"><span class="section-number-2">2.</span> Week 2: (Discrete) Text Representations</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org073a49d" class="outline-3">
<h3 id="org073a49d"><span class="section-number-3">2.1.</span> Why?</h3>
<div class="outline-text-3" id="text-2-1">
<dl class="org-dl">
<dt>NLP</dt><dd>design algorithms to allow computers to understand natural language, so as to perform some task</dd>
<dt>Required</dt><dd>convert text data to numerical data that can be used in model</dd>
</dl>
</div>
</div>
<div id="outline-container-orgdf2538d" class="outline-3">
<h3 id="orgdf2538d"><span class="section-number-3">2.2.</span> Representing Words</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Can be represented by vectors of 0 &amp; 1 where 1 indicates the position of the word, e.g. lorem = <code>[1, 0]</code>, ipsum = <code>[0, 1]</code>, etc.</li>
</ul>
</div>
</div>
<div id="outline-container-org62d70ec" class="outline-3">
<h3 id="org62d70ec"><span class="section-number-3">2.3.</span> Representing sentences/documents</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>Vectors of vectors eg <code>[[1,0], [0,1]]</code></li>
</ul>
</div>
</div>
<div id="outline-container-org7075b76" class="outline-3">
<h3 id="org7075b76"><span class="section-number-3">2.4.</span> One Hot Encoding</h3>
<div class="outline-text-3" id="text-2-4">
</div>
<div id="outline-container-org5144271" class="outline-4">
<h4 id="org5144271"><span class="section-number-4">2.4.1.</span> Definitions</h4>
<div class="outline-text-4" id="text-2-4-1">
<dl class="org-dl">
<dt>corpus</dt><dd>all texts</dd>
<dt>vocabulary, <span class="underline">V</span></dt><dd>all unique words</dd>
<dt>vocabulary size, <span class="underline">d</span></dt><dd>number of unique words, "dimensions"</dd>
<dt>word, <span class="underline">w</span></dt><dd>represented by vector \(X\)</dd>
</dl>
<p>
\(X^w_i\) = 1 if idw(w) = 1, 0 otherwise
</p>
<dl class="org-dl">
<dt>document</dt><dd>represented by matrix sized \(n \times d\)</dd>
<dt><span class="underline">n</span></dt><dd>number of words in document</dd>
<dt><span class="underline">d</span></dt><dd>a single vector with multiple values of 1 where vocab. words are present</dd>
<dt>Document, <span class="underline">D</span></dt><dd>e.g. <span class="underline">this is a sentence</span></dd>
<dt>Vocabulary, <span class="underline">V</span></dt><dd>e.g. <code>[aardvark, ..., sentence, ..., zither]</code></dd>
<dt>OHE, \(X^D\)</dt><dd><code>[0, ..., 1, ...1]</code></dd>
</dl>
</div>
</div>
<div id="outline-container-org35babca" class="outline-4">
<h4 id="org35babca"><span class="section-number-4">2.4.2.</span> Advantages and disadvantages</h4>
<div class="outline-text-4" id="text-2-4-2">
<ul class="org-ul">
<li>Advantages: easy to implement</li>
<li>Disadvantages:
<ul class="org-ul">
<li>not scalable for large vocabulary</li>
<li>high dimensional sparse matrix results in expensive memory + computation</li>
<li><p>
each word represented individually, hence <b>no notion of similarity or meaning</b>. All vectors are orthogonal
</p>

<p>
\((w^{good})^T \cdot w^{great} = (w^{good})^T \cdot w^{bad} = 0\)
</p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org05bd9fc" class="outline-3">
<h3 id="org05bd9fc"><span class="section-number-3">2.5.</span> Bag of Words (Frequency Counting)</h3>
<div class="outline-text-3" id="text-2-5">
<dl class="org-dl">
<dt>Summary</dt><dd>Represents each document as a bag of words. <b><b>Ignores</b></b> order of words.</dd>
<dt>Document</dt><dd>a column vector of \(X\) word counts</dd>
<dt>Representation</dt><dd>Fixed-length representation</dd>
<dt>Document, <span class="underline">D</span></dt><dd>e.g. <code>It was the best of times, it was the worst of times</code></dd>
<dt>Vocabulary, <span class="underline">V</span></dt><dd>e.g. <code>[aardvark, ..., zither]</code></dd>
<dt>Bag of words: <span class="underline">X</span></dt><dd>[2, &#x2026;, 1]</dd>
<dt>Size of <span class="underline">X</span></dt><dd>\(1 \times d\) (\(d\) = vocabulary size)</dd>
</dl>
<p>
Hence \(n\) documents can be represented by matrix of size \(n \times d\).
</p>
</div>
<div id="outline-container-org4da9f38" class="outline-4">
<h4 id="org4da9f38"><span class="section-number-4">2.5.1.</span> Advantages and disadvantages</h4>
<div class="outline-text-4" id="text-2-5-1">
<ul class="org-ul">
<li>Advantages: easy to implement</li>
<li>Disadvantages:
<ul class="org-ul">
<li>Not scalable for large vocabulary</li>
<li>high dimensional sparse matrix results in expensive memory + computation</li>
<li>Order of words is disregarded; <b><b>no meaning</b></b> from context</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgbbfb0ca" class="outline-3">
<h3 id="orgbbfb0ca"><span class="section-number-3">2.6.</span> TF-IDF (Term Frequency-Inverse Document Frequency)</h3>
<div class="outline-text-3" id="text-2-6">
</div>
<div id="outline-container-org1078e50" class="outline-4">
<h4 id="org1078e50"><span class="section-number-4">2.6.1.</span> Why needed?</h4>
<div class="outline-text-4" id="text-2-6-1">
<ul class="org-ul">
<li>BoW does not provide logical importance
<ul class="org-ul">
<li>i.e., each word is equally important</li>
</ul></li>
<li>TF-IDF assigns more logical importance to words in each document</li>
</ul>
</div>
</div>
<div id="outline-container-orgc715916" class="outline-4">
<h4 id="orgc715916"><span class="section-number-4">2.6.2.</span> What is TF-IDF and when to use TF-IDF</h4>
<div class="outline-text-4" id="text-2-6-2">
<dl class="org-dl">
<dt>Definition of TF-IDF</dt><dd>a word's <b><b>importance score</b></b> in a document among \(N\) documents</dd>
<dt><span class="underline">N</span></dt><dd>total number of documents</dd>
<dt>Word count</dt><dd>likely TF-IDF</dd>
<dt>Term frequency, <span class="underline">TF</span></dt><dd>the number of times a word appears in <b><b>a document</b></b>.
TF is high if word appears many times in document, e.g. <span class="underline">the</span>, <span class="underline">a</span>, etc.</dd>
<dt>Inverse document frequency, <span class="underline">IDF</span></dt><dd>\(\log(\frac{N}{\text{number of docs containing the term}})\).
If all (or most) documents contain that term, then IDF will be <b><b>very small</b></b></dd>
<dt>Word's importance score</dt><dd>\(TF \times IDF\).
Higher score = more "characteristic"</dd>
</dl>
</div>
</div>
<div id="outline-container-orgc56790e" class="outline-4">
<h4 id="orgc56790e"><span class="section-number-4">2.6.3.</span> Advantages and disadvantages</h4>
<div class="outline-text-4" id="text-2-6-3">
<ul class="org-ul">
<li>Advantages:
<ul class="org-ul">
<li>Easy to implement</li>
<li>Higher score = "more characteristic". Common words will have very small scores.</li>
<li>Good technique to search for documents, find similar documents, cluster documents</li>
</ul></li>
<li>Disadvantages
<ul class="org-ul">
<li>Does not consider position of words when creating matrix. Similar problem as with BoW.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org0a1dde2" class="outline-2">
<h2 id="org0a1dde2"><span class="section-number-2">3.</span> Week 3: Linear Text Classification</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orge183140" class="outline-3">
<h3 id="orge183140"><span class="section-number-3">3.1.</span> Classification introduction</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Note: <b><b>classification</b></b>.
</p>
</div>
<div id="outline-container-org75fb2f8" class="outline-4">
<h4 id="org75fb2f8"><span class="section-number-4">3.1.1.</span> Supervised learning: definitions</h4>
<div class="outline-text-4" id="text-3-1-1">
<dl class="org-dl">
<dt>Word count matrix / document term matrix</dt><dd>dataset generated from documents</dd>
<dt>Rows of matrix</dt><dd>each row is 1 document</dd>
<dt>Columns of matrix</dt><dd>each column is 1 unique word</dd>
<dt>Unique words: synonyms</dt><dd>features, dimensions, attributes, variables, columns</dd>
<dt>Documents: synonyms</dt><dd>rows, data points, instances</dd>
<dt>Model weights</dt><dd>= model parameters, i.e. what the model learns</dd>
<dt>Function \(F\)</dt><dd>maps \(X\) to \(Y\)</dd>
<dt>Training data \((x_i, y_i)\)</dt><dd>within set of \({X \times Y}\)</dd>
<dt>Learning - find \(\hat{f}\)</dt><dd>\(\hat{f} \in F\) s.t. \(y_i \approx \hat{f} (x_i)\)</dd>
<dt>New data</dt><dd>\(x\)</dd>
<dt>Prediction \(y\)</dt><dd>\(= \hat{f} (x)\)</dd>
</dl>

<p>
Supervised learning thus takes <b><b>labelled</b></b> training data and <b><b>learns</b></b> or <b><b>derives</b></b> a function \(f(x): y = f(x)\).
</p>
</div>
</div>

<div id="outline-container-orga7fe856" class="outline-4">
<h4 id="orga7fe856"><span class="section-number-4">3.1.2.</span> Categories of supervised learning</h4>
<div class="outline-text-4" id="text-3-1-2">
<dl class="org-dl">
<dt>continuous \(y\)</dt><dd>regression i.e. curve fitting</dd>
<dt>discrete \(y\)</dt><dd>classification i.e. class estimation</dd>
</dl>
</div>
</div>

<div id="outline-container-orgf55c064" class="outline-4">
<h4 id="orgf55c064"><span class="section-number-4">3.1.3.</span> Regression</h4>
<div class="outline-text-4" id="text-3-1-3">
<ul class="org-ul">
<li>Errors represent how much predictions deviate from actual values.</li>
<li>Minimum error = 0, however beware of overfitting, where test errors will be high (trained model cannot generalize).</li>
<li>Example: apartment rent prediction, stock price prediction (difficult due to many predictors, known and unknown).</li>
</ul>
</div>
</div>

<div id="outline-container-orgd4b6979" class="outline-4">
<h4 id="orgd4b6979"><span class="section-number-4">3.1.4.</span> Classification</h4>
<div class="outline-text-4" id="text-3-1-4">
<ul class="org-ul">
<li>Linear classification can be used for spam detection, sentiment analysis, handwriting digit recognition (0.4% error here), etc.</li>
<li>Prepare, clean data, fit a classifier</li>
<li>Retraining is required due to new evolving context, new lingo, etc. Can be implemented into a learning system.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org2dc7e51" class="outline-3">
<h3 id="org2dc7e51"><span class="section-number-3">3.2.</span> Naive Bayes</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<div id="outline-container-org18726a4" class="outline-4">
<h4 id="org18726a4"><span class="section-number-4">3.2.1.</span> Method / concepts</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
Bayes Decision Rule.
</p>
<dl class="org-dl">
<dt>\(x\)</dt><dd>encoded document, e.g. by BoW</dd>
<dt>\(y\)</dt><dd>label of document, i.e. whether document contains positive or negative message</dd>
<dt>Posterior</dt><dd>\(P(y|x)\)</dd>
<dt>Likelihood</dt><dd>\(P(x|y)\)</dd>
<dt>Prior</dt><dd>\(P(y)\)</dd>
<dt>Normalization constant</dt><dd>\(P(x)\)</dd>
</dl>

<p>
\[
P(y|x) = \frac{P(x|y)P(y)}{P(x)} = \frac{P(x,y)}{\sum_y P(x,y)}
\]
</p>
</div>
</div>
<div id="outline-container-org3af57e6" class="outline-4">
<h4 id="org3af57e6"><span class="section-number-4">3.2.2.</span> Bayes decision rule</h4>
<div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li><span style='background-color: #FFFF00;'>important</span>: normalization constant is the same for +ve and -ve labels, hence no need to calculate when predicting sentiment</li>
</ul>
</div>
</div>
<div id="outline-container-orgff93d02" class="outline-4">
<h4 id="orgff93d02"><span class="section-number-4">3.2.3.</span> Generative vs discriminative models</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
Naive Bayes is a generative model
</p>
<ul class="org-ul">
<li>Generative model: able to generate synthetic data points
<ul class="org-ul">
<li><b><b>Need</b></b> to model prior and likelihood distributions.</li>
<li>In Naive Bayes, we normally replace likelihood with the conditional distribution.</li>
<li>Conditional distribution is the pdf/pmf to generate data points.
<ul class="org-ul">
<li>Determining this distribution might be difficult.</li>
</ul></li>
<li>Generative models e.g.: Naive Bayes, Hidden Markov Models</li>
</ul></li>
<li>Discriminative models:
<ul class="org-ul">
<li>Directly estimate posteriors</li>
<li>No need to model prior and likelihood distributions</li>
<li>e.g.: logistic regression, SVM, neural networks</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgab6837f" class="outline-4">
<h4 id="orgab6837f"><span class="section-number-4">3.2.4.</span> Details of Naive Bayes</h4>
<div class="outline-text-4" id="text-3-2-4">
<p>
Bayes decision rule:
\[
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
\]
</p>
<ul class="org-ul">
<li><span style='background-color: #FFFF00;'>assumption</span>: all dimensions (unique words) are independent of each other, i.e. \(p(x|y = 1)\) fully factorized, hence: \(P(x|y=1) = \prod^d_{i=1} P(x_i|y = 1)\)
<ul class="org-ul">
<li>Thus, likelihood can be written in fully factorized way.</li>
<li>It becomes a big joint probability of all unique words (dimensions).</li>
<li>Conditional independence, hence likelihood can be written as multiplication of every dimension given the label.</li>
<li>i.e., the variables corresponding to each dimension are independent given the label.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org62c8ff9" class="outline-4">
<h4 id="org62c8ff9"><span class="section-number-4">3.2.5.</span> Naive Conditional Independence Assumption</h4>
<div class="outline-text-4" id="text-3-2-5">
<p>
\[
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
\]
</p>

<p>
For vocabulary \(V\), <code>[nice, give, us, this, iu, ssn, information, job, a]</code>
</p>

<p>
\(P(\text{document} | y = \text{positive})P(y=\text{positive})\)
</p>

<p>
= \(P(x=\text{nice}) ... P(x=a|y=\text{positive})\) \(\cdot P(y= \text{positive})\)
</p>

<p>
similarly for negatives:
</p>

<p>
\(P(\text{document} | y = \text{negative})P(y=\text{negative})\)
</p>

<p>
= \(P(x=\text{nice}) ... P(x=a|y=\text{negative})\) \(\cdot P(y= \text{negative })\)
</p>
</div>
<ol class="org-ol">
<li><a id="org49a471f"></a>Representing the likelihood<br />
<div class="outline-text-5" id="text-3-2-5-1">
<p>
Common distribution: <b><b>multinomial distribution</b></b>.
</p>

<p>
\[
P(x=\text{nice} | y = \text{positive})
\]
</p>

<p>
\[
= \frac{\text{count of word }\textbf{nice} \text{ in all positive label docs }}{\text{count all words with } \textbf{positive}  \text{  labels}}
\]
</p>

<p>
Then to calc priors:
</p>

<p>
\[
P(y = \text{positive}) = \frac{\text{count # +ve docs}}{\text{count # all docs}}
\]
</p>

<p>
Repeat above for negatives.
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org49d3926" class="outline-4">
<h4 id="org49d3926"><span class="section-number-4">3.2.6.</span> Advantages and disadvantages</h4>
<div class="outline-text-4" id="text-3-2-6">
<ul class="org-ul">
<li>Advantages
<ul class="org-ul">
<li>Simple, easy to implement</li>
<li>No training required</li>
<li>Good results in general</li>
</ul></li>
<li>Disadvantages
<ul class="org-ul">
<li>Position of words do not matter (no semantic meaning) due to BoW approach</li>
<li>Requires / assumes conditional independence</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org7b88169" class="outline-3">
<h3 id="org7b88169"><span class="section-number-3">3.3.</span> Classification Model Evaluation</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<div id="outline-container-orgc0ceee2" class="outline-4">
<h4 id="orgc0ceee2"><span class="section-number-4">3.3.1.</span> Common metrics</h4>
<div class="outline-text-4" id="text-3-3-1">
<ul class="org-ul">
<li>Classification: accuracy, precision, recall, cross-entropy, perplexity, and F1 score</li>
<li>Regression: MSE, MAE</li>
</ul>
</div>
</div>
<div id="outline-container-org6a228bf" class="outline-4">
<h4 id="org6a228bf"><span class="section-number-4">3.3.2.</span> Confusion matrix</h4>
<div class="outline-text-4" id="text-3-3-2">
<ul class="org-ul">
<li>e.g. for multi-label confusion matrix</li>
<li>rows are the actual classes (sport, news politics)</li>
<li>columns are the predicted classes</li>
<li>diagonal elements are number of accurate predictions</li>
<li>off-diagonals: inaccurate predictions</li>
<li>But difficult to parse, can consider using a heat map on the confusion matrix instead of raw #</li>
<li><span style='background-color: #FFFF00;'>meaning of positive and negative in a confusion matrix</span>: not related to sentiment. Only indicator of the label, e.g. sport = positive, news = negative.</li>
</ul>
</div>
</div>
<div id="outline-container-org439a9fd" class="outline-4">
<h4 id="org439a9fd"><span class="section-number-4">3.3.3.</span> Accuracy</h4>
<div class="outline-text-4" id="text-3-3-3">
<ul class="org-ul">
<li>Accuracy = (True Positive + True Negative) / Total observations, i.e. sum of diagonals / count observations.</li>
<li>May not be represent "goodness" since false positives and false negatives have identical treatment.</li>
<li>FP and FN may be important specifically for some fields e.g. medicine.</li>
<li>Another metric, false alarm (false positive, type I error) is easy to remember in security contexts.</li>
</ul>
</div>
</div>
<div id="outline-container-org3e6dccc" class="outline-4">
<h4 id="org3e6dccc"><span class="section-number-4">3.3.4.</span> RoC-AUC curve</h4>
<div class="outline-text-4" id="text-3-3-4">
<ul class="org-ul">
<li>ROC: Receiver Operating Characteristic</li>
<li>Changing thresholds: how to change, what should the new threshold be?</li>
<li>TP (y-axis) vs FP (x-axis)</li>
<li>AUC (area under the curve) represents the how performant the predictive model is. Max is 1.0.</li>
<li>But 0.9 may not be good either.
<ul class="org-ul">
<li>Are there some thresholds where TP = 0? Are these important in the context?</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2024-01-31 Wed 19:59</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
