#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: No BS Guide to Linear Algebra
* Chapter 1: Math Fundamentals
** Completing the square
- Rewrite any quadratic expression $x^2 + BX + C$ as perfect square + some correction factor, i.e. $(x+p)^2 + k$.
- Becomes: $x^2 + Bx + C = (x+\frac{B}{2})^2 + C - (\frac{B}{2})^2$
** Quadratic formula
- $x = \frac{-b\pm \sqrt{b^2-4ac}}{2a}$
- Discriminant: $b^2-4ac$
** Golden ratio
- $x^2-x-1=0$
- $x_1 = (1+\sqrt(5))/2 = \phi$
- $x_2 = (1-\sqrt(5))/2 = -1/\phi$
** Solving systems of linear equations
- Examples
  - $n=1$ : $2x+4=7x$
  - $n=2$ : $x+2y=5$ $3x+9y=21$
- Principles: with $n$ equations and $n$ unknowns, the equations can be solved simultaneously:
  1. By substitution, i.e. isolate $x$ in eq1 to obtain $x=5-2y$, then substitute $x$ into eq2:
     $3(5-2y) + 9y = 21$, then expand to $15-6y+9y=21; 3y=6; y=2$
  2. By subtraction, i.e. multiply eq1 by 3 to obtain: $3x+6y=15 \& 3x+9y=21$, then gets $3y=6$ as the $3x$ terms cancel.
  3. By equating, i.e. isolate $x$ in both eq1 and eq2, then equate them. $x=5-2y; x=\frac{1}{3}(21-9y) = 7-3y$
More advanced techniques will be covered in chapter 3.
*** Geometric solution
Solving system of 2 linear equations in 2 unknowns is equivalent geometrically to finding the point of intersection between 2 lines in the Cartesian plane.
$ax+by=c$ with unknown $x, y$ is actually a *constraint* equation on the set of possible values for $x, y$.
Visualize this by considering the coordinate pairs $(x,y)$ that lie in the Cartesian plane.
Cartesian plane as a whole corresponds to the set $\mathbb{R}^2$ which describes all possible pairs of coordinates.

More precisely:
$$
l : \{(x,y) \in \mathbb{R}^2 | ax+by=c\}
$$

i.e.: line $l$ is defined as the subset of pairs of real numbers $(x,y)$ that satisfy equation $ax+by=c$.

Hence, the geometric approach is to:
1. draw the lines that correspond to each of the equations
2. Find the coordinates of the point where both lines intersect
*** Set notation
And its use in defining possible cases when solving system of 2 linear equations in 2 unknowns
**** One solution
Lines $l_1$ and $l_2$ are non-parallel and intersect at a point.
$$
\{(x,y \in \mathbb{R}^2 | x + 2y=2)\} \cap \{(x,y) \in \mathbb{R}^2 | x=1\} = \{(1, \frac{1}{2})\}
$$
**** No solution
Lines $l_1$ and $l_2$ are parallel and never intersect.
$$
\{(x,y \in \mathbb{R}^2 | x + 2y=2)\} \cap \{(x,y) \in \mathbb{R}^2 | x+2y=4\} = \emptyset
$$
**** Infinitely many solutions
If the lines $l_1$ and $l_2$ are parallel and overlapping.
*** Conclusion
The solution set can be a point, the empty set or a line.
* Chapter 2: Intro to Linear Algebra
Definitions and basic operations:
- Set of n-dimensional vectors with real coefficients :: $\mathbb{R}^n$
- Vector $\vec{v} \in \mathbb{R}^n$ :: n-tuple of real numbers
  - example: $\vec{v} \rightarrow^{def} (v_1, v_2, v_3)$
- Components :: same as coordinates
- Matrix, A :: $A \in \mathbb{R}^{m\times n}$ is a rectangular array of real numbers with $m$ rows and $n$ columns
** Vector operations
1. Addition $+$
2. Subtraction $-$
3. Scaling (implicit)
4. Dot product ($\cdot$)
5. Cross product ($\times$)
** Matrix operations
1. Addition ($A + B$)
2. Subtraction ($A-B$)
3. Scaling by a constant $\alpha$ ($\alpha A$)
4. Matrix product ($AB$)
5. Matrix-vector product ($A \vec{v}$)
6. Matrix inverse ($A^{-1}$)
7. Trace $Tr(A)$
8. Determinant ($det(A)$)
** Matrix-vector product
For matrix $A \in \mathbb{R}^{m\times n}$ and the vector $\vec{v}\in \mathbb{R}^n$.

Matrix-vector product $A \vec{x}$ produces a linear combination of the columns of matrix $A$ with coefficients $\vec{x}$.

E.g.: $\vec{y} = A \vec{x}$

*** Dual interpretation of matrix-vector product
- Row picture: single vector with all operations in them. Compute the dot product of the vector with each of the rows of matrix $A$
- Column picture: multiply each cell of the vector with each column of the matrix. _i.e._, $\vec{y}$ is a linear combination of the columns of $A$.

*** Linear combinations as matrix products
For a set of vectors $\{\vec{e_1}, \vec{e_2}\}$, and a third vector $\vec{y}$ that is a **linear combination** of the vectors $\vec{e_1}$ and $\vec{e_2}$:

$$
\vec{y} = \alpha \vec{e_1} + \beta \vec{e_2}
$$

The numbers $\alpha, \beta \in \mathbb{R}$ are the coefficients in this linear combination.

*** Linear transformations
Matrix-vector product corresponds to the idea of a **linear transformation**.

Multiplication by a matrix $A \in \mathbb{R}^{m\times n}$ is the same as computing a linear transformation $T_A$ that takes $n$ -vectors as inputs and produces $m$ -vectors as outputs:

$$
T_A : \mathbb{R}^n \rightarrow \mathbb{R}^m
$$

Instead of writing $\vec{y} = T_A (\vec{x})$ we can just write $\vec{y} = A \vec{x}$. The result of matrix vector product is an $m$ -vector (as $A$ matrix has $m$ rows).

$T_A$ is represented by the matrix $A$.

*** Inverse
For a square and invertible matrix $A$, there exists an inverse matrix $A^{-1}$ that *undoes* the effect of $A$ to restore the original input vector.

$$
A^{-1} (A\vec{x}) = A^{-1} A\vec{x} = \vec{x}
$$

Cumulative effect of applying $A^{-1}$ after $A$ is the identity matrix $\mathbb{I}$, where $\mathbb{I} \vec{x} = \vec{x}$ for any vector $\vec{x}$.

** Overview of linear algebra
- Functions :: transformations from an input space (i.e. domain) to an output space (i.e. image)
- Linear transformation, T :: $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a function that takes $n$ -vectors as inputs and produces $m$ -vectors as outputs
- Linear function T :: output $\vec{y} = T(\vec{x})$ (i.e. T applied to $\vec{x}$ can be computed as the matrix-vector product $A_T \vec{x}$ for some matrix $A_T \in \mathbb{R}^{m\times n}$)
  - T is represented by matrix $A_T$
  - Equivalently: every matrix $A \in \mathbb{R}^{m\times n}$ corresponds to linear transformation $T_A : \mathbb{R}^n \rightarrow \mathbb{R}^m$
  - Hence, linear algebra is about vectors and linear transformations

*** Correspondence between functions and linear transformations
- function $f: \mathbb{R} \rightarrow  \mathbb{R}$ :: linear transformation $T_A: \mathbb{R}^n \rightarrow \mathbb{R}^m$ represented by matrix $A \in \mathbb{R}^{m\times n}$
- Input $x \in \mathbb{R}$ :: input $\vec{x} \in \mathbb{R}^n$
- Output $f(x) \in \mathbb{R}$ :: output $T_A(\vec{x}) = A \vec{x} \in \mathbb{R}^m$
- $g \circ f(x) = g(f(x))$ :: $T_B(T_A(\vec{x})) = BA\vec{x}$
- Function inverse $f^{-1}$ :: matrix inverse $A^{-1}$
- Roots of $f$ :: kernel of $T_A$ = null space of $A$ = $\mathcal{N}(A)$
- Image of $f$ :: image of $T_A$ = column space of A = $\mathcal{C}(A)$

** Vector operations again
- Addition $\vec{u} + \vec{v}$ :: $(u_1 + v_1, u_2 + v_2, u_3 + v_3)$
- Subtraction $\vec{u} - \vec{v}$ :: $(u_1 - v_1, u_2 - v_2, u_3 - v_3)b$
- Scaling $\alpha \vec{u}$ :: $(\alpha u_1, \alpha u_2, \alpha u_3)$
- Dot product $\vec{u} \cdot \vec{v}$ :: $u_1 v_1 + u_2 v_2 + u_3 v_3$
- Cross product $\vec{u} \times \vec{v}$ :: $\left( u_2 v_3 - u_3 v_2, \; u_3 v_1 - u_1 v_3, \; u_1 v_2 - u_2 v_1 \right)$
- Length $\| \vec{u} \|$ :: $\sqrt{u_1^2 + u_2^2 + u_3^2}$

** Notation
Set of real numbers is denoted $\mathbb{R}$.
$n$ -dimensional real vector has $n$ real numbers in a bracket.
$\mathbb{R}^3 == (\mathbb{R}, \mathbb{R}, \mathbb{R})$.
Similarly, $\mathbb{R}^n$ denotes the set of $n$ -dimensional real vectors.

Should keep track of *type signature* of operations i.e. the types of inputs and types of outputs.

** Addition and subtraction

\begin{equation}
+ : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}^n \\
- : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}^n
\\
\vec{w} = \vec{u} \pm \vec{v} \iff w_i = u_i \pm v_i, \; \forall i \in [1, \ldots, n]
\end{equation}

** Scaling by a constant

$$
\text{scalar-mult} : \mathbb{R} \times \mathbb{R}^n \rightarrow \mathbb{R}^n
\\
\vec{w} = \alpha \vec{u} \iff w_i = \alpha u_i
$$

** Vector products
i.e.: dot product and cross product

*** Dot product
For example:
\begin{equation}
\cdot : \mathbb{R}^3 \times \mathbb{R}^3 \rightarrow \mathbb{R} \\
\vec{v} \cdot \vec{w} = v_x w_x + v_y w_y + v_z w_z \\
\vec{v} \cdot \vec{w} = \| \vec{v} \| \| \vec{w} \| \cos(\varphi)

\end{equation}
