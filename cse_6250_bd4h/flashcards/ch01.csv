What is Big Data?; Big Data refers to extremely large and complex datasets that traditional data-processing software cannot effectively manage or analyze.
What are the “3 Vs” of Big Data?; The three primary characteristics of Big Data are Volume (massive data quantity), Velocity (rapid data generation speed), and Variety (wide range of data types and sources).
Why has Big Data become important in healthcare?; Healthcare organizations collect massive, diverse datasets (EHRs, imaging, genomics, etc.), and Big Data analytics can reveal insights to improve care and efficiency that traditional methods would likely miss.
What are some sources of Big Data in healthcare?; Sources include electronic health records (EHRs), medical imaging (e.g. MRI scans), genomic sequencing data, insurance claims, and patient-generated data from wearable devices.
What challenges do healthcare datasets pose for Big Data analytics?; Healthcare data are heterogeneous (mix of structured and unstructured formats), extremely large in volume, and sensitive (privacy concerns), making it challenging to integrate and analyze them with conventional tools.
Why are new tools needed to analyze Big Data?; The scale and complexity of Big Data demand distributed storage and processing frameworks (like Hadoop and Spark) because traditional single-server databases cannot handle such volume, velocity, and variety efficiently.
What big data technologies are taught in this course?; The course covers big data systems like Hadoop (with tools such as Hive, Pig, HBase), Apache Spark, and even graph databases, focusing on how to use these for large-scale data processing.
Which healthcare analytics applications are emphasized in this course?; It emphasizes applications such as predictive modeling (e.g. risk prediction), computational phenotyping (discovering patient clusters from data), and patient similarity analysis, all in the context of healthcare.
What machine learning techniques are highlighted for Big Data in this course?; The course highlights scalable machine learning techniques, including online learning (models that update with streaming data) and fast similarity search, which are important for handling large, continuously growing datasets.
What background knowledge is recommended for the BD4H course?; A background in machine learning (understanding classification, clustering), proficient programming skills in Python, Scala, or Java, and familiarity with data management (SQL and NoSQL databases) are recommended for success.
What is the focus of the Big Data for Health course?; The focus is on applying big data algorithms and systems to healthcare data – teaching students to handle large-scale medical datasets and derive insights for improving health outcomes and reducing inefficiencies.
What is predictive modeling?; Predictive modeling is a technique that uses historical data to train machine learning models that can predict future or unknown outcomes.
How is predictive modeling used in healthcare?; In healthcare, predictive modeling is used to forecast patient outcomes (such as risk of hospital readmission, disease onset, or treatment response) by learning patterns from past patient data.
What type of learning does predictive modeling typically involve?; It typically involves supervised learning, where models are trained on labeled datasets (with known outcomes) to predict labels or values for new, unseen data (classification or regression tasks).
What are key steps in the predictive modeling process?; Key steps include data preprocessing and feature engineering, selecting and training an appropriate model on a training dataset, and then evaluating the model on validation/test data to ensure it generalizes well.
Why is overfitting a concern in predictive modeling?; Overfitting is when a model learns the training data (including noise) too specifically and fails to generalize to new data. It’s a concern because an overfitted model performs poorly on unseen cases, so techniques like cross-validation and regularization are used to prevent it.
How do we evaluate the performance of a predictive model?; We evaluate a model using appropriate metrics (such as accuracy for overall correctness, or precision/recall for class-specific performance) and by testing it on a separate dataset to see how well it predicts outcomes it hasn’t seen during training.
What is the MapReduce programming model?; MapReduce is a programming model for distributed computing on large data sets, consisting of a Map step that processes and converts input data into intermediate key–value pairs, and a Reduce step that aggregates those intermediate results by key to produce the final output.
How does MapReduce scale to big datasets?; It achieves scalability by dividing the workload across a cluster of machines: the Map phase runs in parallel on many nodes (each handling a portion of the data), and the Reduce phase then aggregates the intermediate results. This parallelism and distribution allow processing of massive datasets efficiently.
What is Hadoop?; Hadoop is an open-source framework for big data processing that implements the MapReduce paradigm. It includes the Hadoop Distributed File System (HDFS) for storing data across a cluster, providing fault tolerance and high-throughput data access.
What does HDFS do in the Hadoop ecosystem?; HDFS (Hadoop Distributed File System) breaks large files into blocks and distributes them across multiple nodes in a cluster. It stores multiple copies of each block for reliability and allows parallel access to data, which is crucial for the high-volume processing in MapReduce jobs.
What is an example of a problem suitable for MapReduce?; A classic example is counting word frequencies in a very large document collection: the Map function emits (word, 1) for each word occurrence in each document, and the Reduce function sums up all counts per word. This task can be split across many machines, with each handling a subset of documents, and then aggregated.
What is a confusion matrix?; A confusion matrix is a table that summarizes a classifier’s performance by showing counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). Each of these entries helps in calculating various evaluation metrics.
How is accuracy calculated for a classifier?; Accuracy = (TP + TN) / (TP + TN + FP + FN). It represents the proportion of all instances that the classifier correctly classified (both positives and negatives combined).
What does precision measure?; Precision = TP / (TP + FP). It measures how reliable positive predictions are – out of all instances predicted as positive by the model, what fraction were actually positive.
What does recall (sensitivity) measure?; Recall (sensitivity) = TP / (TP + FN). It measures the model’s ability to catch positive instances – out of all actual positive cases, how many did the model correctly identify as positive.
What is specificity in classification?; Specificity = TN / (TN + FP). It measures the ability to identify negatives correctly – out of all actual negative cases, what proportion did the model correctly classify as negative.
What is the F1-score?; The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall (it is high only when both are high), which is useful for evaluating performance especially on imbalanced datasets.
What does ROC AUC represent?; ROC AUC (Area Under the ROC Curve) represents the overall ability of the model to discriminate between classes across all classification thresholds. Numerically, it’s the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (AUC = 1.0 indicates a perfect classifier).
What is an ensemble method in machine learning?; An ensemble method combines the predictions of multiple base models (often called “weak learners”) to produce a more powerful model with better performance than any single one.
What is bagging (bootstrap aggregation)?; Bagging trains multiple models on different bootstrapped subsets of the training data (samples drawn with replacement) and then averages their predictions (or takes a majority vote). This approach reduces variance and helps prevent overfitting. Random Forest is a popular ensemble that uses bagging with decision trees.
What is boosting?; Boosting builds an ensemble by training models sequentially, each new model focusing on correcting the errors of the previous ones. The final prediction is a weighted combination of all models. This approach can reduce bias and produce a strong predictor. Examples of boosting algorithms include AdaBoost and Gradient Boosting (e.g. XGBoost).
Why do ensemble methods often perform better than single models?; Ensembles perform better because combining multiple models tends to cancel out individual errors. By aggregating diverse perspectives (different models may capture different patterns), the ensemble reduces the risk of an unlucky guess or overfitting by any one model, leading to higher overall robustness and accuracy.
What is computational phenotyping?; Computational phenotyping is the process of identifying meaningful patient subgroups or patterns (“phenotypes”) from large datasets using algorithms. Often this is done with unsupervised learning methods like clustering or matrix factorization to group patients with similar characteristics.
Why is phenotyping useful in healthcare?; Phenotyping can uncover hidden structure in clinical data, such as subtypes of a disease or distinct patient clusters. Identifying these phenotypes helps clinicians and researchers understand disease variations, tailor treatments to specific patient groups, and improve decision support by recognizing patterns that aren’t obvious on an individual level.
How can clustering be used for phenotyping?; Clustering algorithms can group patients who have similar clinical features (for example, similar lab results, symptoms, and diagnoses). Each cluster may represent a distinct phenotype — for instance, a subgroup of patients who share a common pattern of comorbidities and outcomes.
What is an example of a phenotyping result?; For example, using hospital EHR data, algorithms might reveal distinct phenotypes of diabetes patients: one phenotype could be younger patients with obesity-related diabetes, while another could be older patients whose diabetes is linked with cardiovascular complications. These groupings provide insight into different manifestations of the disease.
How do phenotypes relate to predictive modeling?; The phenotypes discovered can be used to enhance predictive modeling. For instance, the patient clusters (phenotypes) identified via unsupervised learning can serve as features or as segments for building targeted predictive models, thereby potentially improving prediction accuracy by incorporating high-level patient group information.
What is clustering in data analysis?; Clustering is an unsupervised learning approach that automatically groups data points into clusters such that points in the same cluster are more similar to each other than to those in other clusters. It finds structure in data without any predefined labels.
How does the k-means algorithm work?; k-means clustering partitions data into k clusters by iteratively performing two steps: (1) Assign each data point to the nearest cluster centroid (based on distance), and (2) Recompute each centroid as the mean of all points assigned to it. These steps repeat until the cluster assignments stabilize.
What is hierarchical clustering?; Hierarchical clustering creates a hierarchy of nested clusters. In agglomerative (bottom-up) hierarchical clustering, it starts with each data point as its own cluster and then successively merges the closest pairs of clusters until all points belong to one cluster. The result can be visualized as a dendrogram, and you can choose a cutoff to get a desired number of clusters.
How can we determine the appropriate number of clusters?; Methods like the elbow method (plotting the variance explained or clustering cost against number of clusters to find a “bend” or elbow) and the silhouette score (measuring how similar each point is to others in its cluster vs. other clusters) help evaluate clustering results and suggest a good number of clusters k.
What is a use case of clustering in healthcare?; In healthcare, clustering can be used to identify groups of similar patients. For instance, patients might be clustered based on their clinical profiles to find distinct disease subtypes or risk groups, which can then inform personalized treatment plans or target interventions for those specific patient groups.
What is Apache Spark?; Apache Spark is an open-source, fast, in-memory cluster computing framework for big data processing. It provides a unified engine that supports parallel data processing for tasks such as SQL querying, streaming data analysis, machine learning, and graph computations.
How is Spark different from Hadoop’s MapReduce?; Spark keeps data in-memory between operations (when possible) instead of writing intermediate results to disk as Hadoop MapReduce does. This makes Spark much faster for iterative algorithms and interactive data analysis. Spark also offers high-level APIs and libraries which make it easier to develop applications compared to low-level MapReduce code.
What is an RDD in Spark?; An RDD (Resilient Distributed Dataset) is Spark’s core data abstraction: an immutable distributed collection of objects spread across the cluster. RDDs are built to be fault-tolerant (if part of an RDD is lost, it can be recomputed using lineage information) and support parallel operations like transformations (e.g., map, filter) and actions (e.g., reduce, collect).
What are some high-level libraries available in Spark?; Spark comes with powerful libraries including Spark SQL (for structured data and SQL queries, using DataFrames), MLlib (machine learning library providing algorithms for clustering, classification, etc.), GraphX (for graph processing and analysis), and Spark Streaming (for processing real-time streaming data).
In what languages can you write Spark programs?; Spark provides APIs in multiple languages: Scala (its native language), Python (PySpark), Java, and R. Developers can write Spark code in any of these languages to interact with the Spark engine.
What is an advantage of using Spark for big data analytics?; One major advantage of Spark is its speed for complex, iterative workloads due to in-memory processing. Additionally, Spark’s unified platform means you can handle batch processing, real-time streaming, SQL, and machine learning in one framework, simplifying the big data pipeline.
What is a medical ontology?; A medical ontology is a structured representation of medical knowledge that defines concepts (like diseases, symptoms, procedures) and the relationships between them. It provides a standardized vocabulary for healthcare terms, ensuring that different systems and databases refer to concepts in a consistent way.
Give an example of a medical ontology.; One example is SNOMED CT, a comprehensive clinical terminology that covers diagnoses, findings, procedures, etc. Other examples include ICD-10 (a coding system for diagnoses) and LOINC (for lab tests), each providing standard codes for medical concepts in their domain.
How do medical ontologies benefit data analysis?; Ontologies enable integration and more powerful analysis of healthcare data by mapping different terms to the same concept and by leveraging the relationships defined between concepts. For instance, an ontology can help a system recognize that “myocardial infarction” and “heart attack” refer to the same condition or infer that two diagnoses are related (e.g., a type and a subtype), improving data consistency and query capability.
What role do ontologies play in healthcare machine learning?; Ontologies can enrich machine learning models by providing domain knowledge. They can be used to engineer features (for example, grouping lab tests or diagnoses into higher-level categories) or to inform model structure. By incorporating ontology-based features or constraints, models may better capture medical context (e.g., knowing that certain conditions are related) and thus improve predictive accuracy or interpretability.
What is graph analysis?; Graph analysis is the examination of data modeled as a graph (a network of nodes connected by edges). It focuses on understanding relationships and structures such as connectivity, paths, and communities within the graph that might not be apparent from non-relational (tabular) data.
How can graph analysis be applied to healthcare?; In healthcare, graph analysis can model and study networks like patient similarity networks (patients as nodes with edges if patients share similar characteristics), disease co-occurrence networks (diseases linked if they frequently appear together), or physician referral networks. Analyzing these graphs can reveal clusters of related patients or conditions and identify key connection points (such as a disease that links multiple patient groups).
What is PageRank and why is it relevant in graph analysis?; PageRank is an algorithm originally developed to rank webpages by importance, but in graph analysis it measures node importance based on its connections. For example, in a healthcare graph, PageRank could be used to find influential nodes like a central hub in a physician network or a critical symptom that connects many diseases, indicating its importance in the network’s structure.
What are graph databases and how are they used?; Graph databases (e.g., Neo4j) are specialized databases designed to store and query data as a graph. Instead of tables, they use nodes and relationships, allowing efficient retrieval of complex relational information. They are used in healthcare to store data like knowledge graphs (relationships between genes, diseases, drugs) or social networks of patients, and enable queries like “find all diseases connected to a given gene within two hops.”
What is a common graph analysis task besides PageRank?; Community detection is a common task, where algorithms identify groups of nodes that are tightly connected to each other within the graph. In a healthcare context, community detection might find clusters of patients with highly similar profiles or groups of medical concepts that frequently interlink, which can uncover meaningful groupings for further study.
What is the goal of dimensionality reduction?; The goal is to reduce the number of features (dimensions) in a dataset while preserving as much of the important information as possible. This simplifies models (less complex), can improve learning performance by avoiding overfitting, and makes it easier to visualize high-dimensional data.
What is Principal Component Analysis (PCA)?; PCA is a linear dimensionality reduction technique that transforms the original variables into a new set of uncorrelated variables called principal components. Each principal component is a linear combination of the original features, and they are ordered by how much variance in the data they explain (with the first component capturing the most variance).
Why is dimensionality reduction useful for high-dimensional data?; In very high-dimensional data, many features may be redundant or noisy. Dimensionality reduction techniques help address the curse of dimensionality by eliminating irrelevant features and consolidating information, which can improve model performance and reduce computational cost. It also helps in understanding and visualizing the underlying structure of data.
What is t-SNE used for?; t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear dimensionality reduction method mainly used for visualization. It embeds high-dimensional data into 2 or 3 dimensions, aiming to preserve local structure (points that are similar in high dimensions stay close in the low-dimensional visualization). This is useful for exploring complex datasets (like gene expression or image data) to visually detect clusters or patterns.
How does feature selection differ from PCA?; Feature selection involves choosing a subset of the original features (for example, picking the top 10 most informative variables) and discarding the rest, whereas PCA creates new features (principal components) as combinations of original features. Both reduce dimensionality, but feature selection keeps the original feature meanings, while PCA yields transformed features that may be less interpretable.
What is patient similarity analysis?; Patient similarity analysis is the process of comparing patients based on their data (such as diagnoses, lab results, demographics, etc.) to find those who are most alike. It involves creating a representation of each patient (for instance, as a vector of features) and then using a similarity measure to identify patient pairs or groups that are similar in the feature space.
How is patient similarity measured?; It’s typically measured by representing each patient as a feature vector and then computing a distance or similarity metric between these vectors. Common choices include Euclidean distance and cosine similarity for numeric feature vectors, or Jaccard index for comparing sets of items like diagnosis codes – the closer or more similar the metrics, the more alike the patients are considered.
What is an example of using patient similarity in healthcare?; One example is a clinical decision support tool that finds patients similar to a current patient to aid in treatment decisions. For instance, given a patient with certain conditions and demographics, the system might retrieve past patients with very similar profiles and show how those patients responded to various treatments, helping doctors personalize the current patient’s care.
Why is patient similarity useful for predictive modeling?; Patient similarity can be used to implement nearest neighbor approaches or to create patient cohorts for prediction. For example, to predict an outcome for a patient, one might look at outcomes of the most similar patients (k-nearest neighbors). It also enhances interpretability – instead of a black-box model, a clinician can see references to similar past cases, making predictions more understandable via those analogies.
What are challenges in computing patient similarity?; Challenges include the heterogeneity of patient data (mixing different types of information like numeric labs, categorical diagnoses, free-text notes), deciding how to weight features (not all clinical features are equally important for similarity), and ensuring that the chosen similarity metric aligns with clinical relevance (so that patients deemed “similar” truly have comparable clinical profiles or prognoses).