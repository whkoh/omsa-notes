<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-03-29 Sat 11:32 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>BD4H</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">BD4H</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org6723db6">1. Introduction to Big Data</a>
<ul>
<li><a href="#org1167bec">1.1. Introduction</a></li>
<li><a href="#orge3c8d2b">1.2. About BDAH</a></li>
<li><a href="#orge90366d">1.3. Learning Goals</a></li>
<li><a href="#org8ccd522">1.4. Current Problems in Healthcare</a></li>
<li><a href="#org339b589">1.5. The Four Vs</a></li>
<li><a href="#orgaff04e8">1.6. More About The Four Vs</a></li>
<li><a href="#org6da076e">1.7. Data Science is Sexy</a></li>
<li><a href="#org6c6aa6e">1.8. BDAH Quiz 1 Question</a></li>
<li><a href="#org0834867">1.9. BDAH Quiz 1 Solution</a></li>
<li><a href="#org10820e0">1.10. BDAH Quiz 2 Question</a></li>
<li><a href="#orge3a8a65">1.11. BDAH Quiz 2 Solution</a></li>
<li><a href="#orgc6a3f3d">1.12. BDAH Quiz 3 Question</a></li>
<li><a href="#orgec28fe5">1.13. BDAH Quiz 3 Solution</a></li>
</ul>
</li>
<li><a href="#org2ee9367">2. Big Data Course Overview</a>
<ul>
<li><a href="#org1733604">2.1. Introduction to Course Overview</a></li>
<li><a href="#org1bf3ae2">2.2. Big Data Big Picture</a></li>
<li><a href="#org8ff1e68">2.3. Healthcare Applications</a></li>
<li><a href="#orgc7fee3a">2.4. Predictive Modeling Quiz Question</a></li>
<li><a href="#orgc6d8033">2.5. Predictive Modeling Quiz Solution</a></li>
<li><a href="#org4322eaf">2.6. Predictive Modeling Challenges</a></li>
<li><a href="#orgba3c3c6">2.7. Computational Phenotyping</a></li>
<li><a href="#orgdfd3945">2.8. Computational Phenotyping Quiz Question</a></li>
<li><a href="#org705f0fa">2.9. Computational Phenotyping Quiz Solution</a></li>
<li><a href="#org85e8905">2.10. Phenotyping Algorithm</a></li>
<li><a href="#orge917f2c">2.11. Patient Similarity Quiz Question</a></li>
<li><a href="#orgfe101ba">2.12. Patient Similarity Quiz Solution</a></li>
<li><a href="#orgeb936d0">2.13. Patient Similarity</a></li>
<li><a href="#orgfac0a18">2.14. Algorithms</a></li>
<li><a href="#org1d3cfed">2.15. Systems</a></li>
<li><a href="#orgfb29f0c">2.16. Summary</a></li>
</ul>
</li>
<li><a href="#orgbe3a660">3. Predictive Modeling</a>
<ul>
<li><a href="#org1a9d80e">3.1. Introduction to Predictive Modeling</a></li>
<li><a href="#org2784924">3.2. Predictive Modeling vs EHR</a></li>
<li><a href="#org5996ce6">3.3. Predictive Modeling Pipeline</a></li>
<li><a href="#org0382f36">3.4. Prediction Target</a></li>
<li><a href="#org99aeabd">3.5. Heart Failure Quiz</a></li>
<li><a href="#org65994be">3.6. Motivations for Early Detection</a></li>
<li><a href="#orgfa0ae97">3.7. Cohort Construction</a></li>
<li><a href="#orge761112">3.8. Prospective vs. Retrospective Studies</a></li>
<li><a href="#org8de3cc1">3.9. Prospective vs. Retrospective Quiz</a></li>
<li><a href="#org43f4ceb">3.10. Cohort Study</a></li>
<li><a href="#orgd4671e5">3.11. Case-Control Study</a></li>
<li><a href="#org2360ff3">3.12. Feature Construction</a></li>
<li><a href="#org66e506b">3.13. Feature Construction Quizzes</a></li>
<li><a href="#org01c10a2">3.14. Prediction Performance on Windows</a></li>
<li><a href="#org263e920">3.15. Feature Selection</a></li>
<li><a href="#org6a7cc51">3.16. Predictive Model</a></li>
<li><a href="#orgd846d17">3.17. Performance Evaluation</a></li>
<li><a href="#orgdbfb7fb">3.18. Cross-Validation</a></li>
<li><a href="#orgf34eabf">3.19. Conclusion</a></li>
</ul>
</li>
<li><a href="#org53f2f62">4. MapReduce</a>
<ul>
<li><a href="#org56dc554">4.1. Introduction to MapReduce</a></li>
<li><a href="#orgd7c2b80">4.2. What is MapReduce</a></li>
<li><a href="#orgc7b1b0a">4.3. Computational Process</a></li>
<li><a href="#org7931de1">4.4. Learning Via Aggregation Statistics</a></li>
<li><a href="#orgfe91dcf">4.5. MapReduce Abstraction</a></li>
<li><a href="#orga10fc35">4.6. MapReduce System</a></li>
<li><a href="#org71eba8b">4.7. MapReduce Fault Recovery</a></li>
<li><a href="#org543f08b">4.8. Distributed File Systems</a></li>
<li><a href="#org64510ce">4.9. MapReduce Design Choice</a></li>
<li><a href="#org7a4e4c7">4.10. Analytics with MapReduce</a></li>
<li><a href="#org79ae1a8">4.11. MapReduce KNN</a></li>
<li><a href="#org51a0b1d">4.12. Linear Regression</a></li>
<li><a href="#org8b8c648">4.13. MapReduce for Linear Regression Quiz Question</a></li>
<li><a href="#orgf6b3462">4.14. MapReduce for Linear Regression Quiz Solution</a></li>
<li><a href="#orgee7cb81">4.15. Limitations of MapReduce</a></li>
<li><a href="#org94bf2e0">4.16. MapReduce Summary Quiz Question</a></li>
<li><a href="#orgecf0cb5">4.17. MapReduce Summary Quiz Solution</a></li>
</ul>
</li>
<li><a href="#org2be23ff">5. Classification Model Metrics</a>
<ul>
<li><a href="#org4898d91">5.1. Predictive Model review</a></li>
<li><a href="#orgee7391e">5.2. Confusion matrix</a></li>
<li><a href="#org8bf44df">5.3. Accuracy metrics</a>
<ul>
<li><a href="#orge27c10d">5.3.1. Other notes</a></li>
</ul>
</li>
<li><a href="#org85564b1">5.4. Predictive metrics</a></li>
<li><a href="#org66103e5">5.5. F1 score</a></li>
<li><a href="#org23b56a6">5.6. Classifier quiz</a></li>
<li><a href="#org2709acb">5.7. Reversing predictions</a></li>
<li><a href="#orge442cc5">5.8. Receiver operating characteristic</a></li>
<li><a href="#org24b184f">5.9. "Best classifier threshold" quiz</a></li>
<li><a href="#org4681613">5.10. Regression metrics</a>
<ul>
<li><a href="#org570f8c6">5.10.1. \(R^2\)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga94a02e">6. Ensemble methods</a>
<ul>
<li><a href="#orgdc69930">6.1. Gradient Descent Method (GDM) for Linear Regression</a></li>
<li><a href="#org67bbd7c">6.2. Stochastic Gradient Descent (SGD) Method</a></li>
<li><a href="#orge9b5e1f">6.3. SGD for Linear Regression</a></li>
<li><a href="#orgaf89357">6.4. Ensemble Method Pt 1</a></li>
<li><a href="#orgfffe665">6.5. Ensemble Method Pt 2</a></li>
<li><a href="#orgd472803">6.6. Bias Variance Tradeoff</a></li>
<li><a href="#org5aa9a7c">6.7. Bias Variance Tradeoff Quiz Question</a></li>
<li><a href="#orgbc26924">6.8. Bias Variance Tradeoff Quiz Solution</a></li>
<li><a href="#org398f01e">6.9. Bias Variance Tradeoff Quiz 2 Question</a></li>
<li><a href="#orge9de2bb">6.10. Bias Variance Tradeoff Quiz 2 Solution</a></li>
<li><a href="#orgaf36110">6.11. Bagging</a></li>
<li><a href="#orgb327892">6.12. Random Forest</a></li>
<li><a href="#org96ccff9">6.13. Why Bagging Works</a></li>
<li><a href="#orge3864db">6.14. Boosting</a></li>
<li><a href="#org833a08c">6.15. Bagging vs Boosting Quiz Question</a></li>
<li><a href="#org8540423">6.16. Bagging vs Boosting Quiz Solution</a></li>
<li><a href="#org8b72fe9">6.17. Summary for Ensemble Methods</a></li>
<li><a href="#orgebddb45">6.18. Introduction to Ensemble Method</a></li>
<li><a href="#org96bb029">6.19. Gradient Descent Method</a></li>
</ul>
</li>
<li><a href="#org0fc1db0">7. Computational Phenotyping</a>
<ul>
<li><a href="#org17edfa4">7.1. Introduction to Phenotyping</a></li>
<li><a href="#orgf4c90f9">7.2. Computational Phenotyping</a></li>
<li><a href="#org9c9549a">7.3. Phenotyping Algorithm</a></li>
<li><a href="#org9a3c6b8">7.4. Applications of Phenotyping</a></li>
<li><a href="#org5436c8b">7.5. Genomic Wide Association Study</a></li>
<li><a href="#org6fa820d">7.6. Why Do We Care About Phenotyping</a></li>
<li><a href="#org7bf1ccc">7.7. Clinical Predictive Modeling</a></li>
<li><a href="#org2e87679">7.8. Pragmatic Clinical Trials</a></li>
<li><a href="#org4033822">7.9. Healthcare Quality Measurement</a></li>
<li><a href="#org938a978">7.10. Phenotyping Methods Part 1</a></li>
<li><a href="#org9171dc4">7.11. Phenotyping Methods Part 2</a></li>
<li><a href="#orgaabde3d">7.12. Phenotyping Quiz Question</a></li>
<li><a href="#org70d0a6b">7.13. Phenotyping Quiz Solution</a></li>
</ul>
</li>
<li><a href="#orge5397cc">8. Clustering</a>
<ul>
<li><a href="#orgc84ba51">8.1. Introduction to Clustering</a></li>
<li><a href="#org9ea8c69">8.2. Healthcare Applications</a></li>
<li><a href="#org398754e">8.3. What is Clustering</a></li>
<li><a href="#org1c9e94e">8.4. Algorithm Overview</a></li>
<li><a href="#orgcb1c48c">8.5. K Means</a></li>
<li><a href="#org3924687">8.6. K Means Quiz Question</a></li>
<li><a href="#org6b31646">8.7. K Means Quiz Solution</a></li>
<li><a href="#org0c41fe6">8.8. Hierarchical Clustering</a></li>
<li><a href="#orga5b1625">8.9. Agglomerative Clustering</a></li>
<li><a href="#orga5287a7">8.10. Gaussian Mixture Model</a></li>
<li><a href="#org40f09e3">8.11. GMM Expectation Maximization</a></li>
<li><a href="#orga61bfad">8.12. GMM Steps</a></li>
<li><a href="#org4726f7e">8.13. GMM Visual Illustration</a></li>
<li><a href="#org2b43591">8.14. K Means Vs GMM</a></li>
<li><a href="#org14b9c7c">8.15. Mini Batch K Means</a></li>
<li><a href="#orgbc36b2b">8.16. Mini Batch K Means Quiz Question</a></li>
<li><a href="#orgfb1a4cb">8.17. Mini Batch K Means Quiz Solution</a></li>
<li><a href="#orgb7c35ee">8.18. DBScan</a></li>
<li><a href="#orgf41e607">8.19. DBScan Key Concepts</a></li>
<li><a href="#org9b8ba6e">8.20. DBScan Algorithm</a></li>
<li><a href="#org15bfcab">8.21. DBScan Example</a></li>
<li><a href="#org43e9dbd">8.22. DBScan Quiz Question</a></li>
<li><a href="#org10fd5d5">8.23. DBScan Quiz Solution</a></li>
<li><a href="#org3ab9568">8.24. Clustering Evaluation Metrics</a></li>
<li><a href="#orgc0a6ab3">8.25. Rand Index</a></li>
<li><a href="#org7a1a36b">8.26. Mutual Information</a></li>
<li><a href="#orgc6a647f">8.27. Summary of RI and MI</a></li>
<li><a href="#orgd11d8b2">8.28. Silhouette Coefficient</a></li>
<li><a href="#org849008c">8.29. Silhouette Coefficient Pros and Cons</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org6723db6" class="outline-2">
<h2 id="org6723db6"><span class="section-number-2">1.</span> Introduction to Big Data</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org1167bec" class="outline-3">
<h3 id="org1167bec"><span class="section-number-3">1.1.</span> Introduction</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Course: Big Data Analytics for Healthcare (BDAH)</li>
<li>Instructor: Jimeng Sun, associate professor at Georgia Tech</li>
<li>Background: Expertise in healthcare analytics and data mining, previous work at IBM TJ Watson Research Center</li>
<li>Course focus: What students will learn and why it matters</li>
</ul>
</div>
</div>

<div id="outline-container-orge3c8d2b" class="outline-3">
<h3 id="orge3c8d2b"><span class="section-number-3">1.2.</span> About BDAH</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>Intersection of healthcare and big data</li>
<li>Covers healthcare applications, data, analytics, and big data processing</li>
<li>Focus on how data science is applied to healthcare</li>
</ul>
</div>
</div>

<div id="outline-container-orge90366d" class="outline-3">
<h3 id="orge90366d"><span class="section-number-3">1.3.</span> Learning Goals</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>Understand healthcare data</li>
<li>Learn analytics algorithms</li>
<li>Work with big data systems</li>
<li>Application: Build models for disease risk prediction, treatment recommendation, patient clustering, and similarity analysis</li>
<li>Assessments: Homework using big data tools, project with system building, report writing, and presentations</li>
</ul>
</div>
</div>

<div id="outline-container-org8ccd522" class="outline-3">
<h3 id="org8ccd522"><span class="section-number-3">1.4.</span> Current Problems in Healthcare</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>High costs: $3.8 trillion/year in the U.S.</li>
<li>Massive waste: $765 billion/year</li>
<li>Poor quality: 200,000-400,000 preventable deaths annually</li>
<li>Preventable deaths ranked as the third leading cause of death</li>
<li>Hope: Big data can improve care and reduce costs</li>
</ul>
</div>
</div>

<div id="outline-container-org339b589" class="outline-3">
<h3 id="org339b589"><span class="section-number-3">1.5.</span> The Four Vs</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>Volume: Large healthcare data amounts</li>
<li>Variety: Different types of healthcare data sources</li>
<li>Velocity: Real-time data processing needs</li>
<li>Veracity: Issues with data quality (noise, missing data, errors)</li>
</ul>
</div>
</div>

<div id="outline-container-orgaff04e8" class="outline-3">
<h3 id="orgaff04e8"><span class="section-number-3">1.6.</span> More About The Four Vs</h3>
<div class="outline-text-3" id="text-1-6">
<ul class="org-ul">
<li>Examples of data volume:
<ul class="org-ul">
<li>Genomic data: 200 GB per genome</li>
<li>fMRI scan: ~300 GB per scan</li>
<li>US medical imaging data per year: ~100 petabytes</li>
</ul></li>
<li>Data variety:
<ul class="org-ul">
<li>Clinical (demographics, diagnosis, procedures, etc.)</li>
<li>Patient-generated (wearables, sensors)</li>
<li>Real-time (ICU monitoring data)</li>
</ul></li>
<li>Focus on managing diverse data types in the course</li>
</ul>
</div>
</div>

<div id="outline-container-org6da076e" class="outline-3">
<h3 id="org6da076e"><span class="section-number-3">1.7.</span> Data Science is Sexy</h3>
<div class="outline-text-3" id="text-1-7">
<ul class="org-ul">
<li>Data science is a critical field</li>
<li>Harvard Business Review: "Data Scientist, The Sexiest Job in the 21st Century"</li>
<li>Data scientists:
<ul class="org-ul">
<li>Capitalize on big data</li>
<li>Overcome technical limitations</li>
<li>Develop key tools like Hadoop and Spark</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org6c6aa6e" class="outline-3">
<h3 id="org6c6aa6e"><span class="section-number-3">1.8.</span> BDAH Quiz 1 Question</h3>
<div class="outline-text-3" id="text-1-8">
<ul class="org-ul">
<li>True/False: A graduate-level degree is necessary to become a data scientist.</li>
</ul>
</div>
</div>

<div id="outline-container-org0834867" class="outline-3">
<h3 id="org0834867"><span class="section-number-3">1.9.</span> BDAH Quiz 1 Solution</h3>
<div class="outline-text-3" id="text-1-9">
<ul class="org-ul">
<li>Answer: True</li>
</ul>
</div>
</div>

<div id="outline-container-org10820e0" class="outline-3">
<h3 id="org10820e0"><span class="section-number-3">1.10.</span> BDAH Quiz 2 Question</h3>
<div class="outline-text-3" id="text-1-10">
<ul class="org-ul">
<li>Question: What is the average salary of a data scientist?</li>
</ul>
</div>
</div>

<div id="outline-container-orge3a8a65" class="outline-3">
<h3 id="orge3a8a65"><span class="section-number-3">1.11.</span> BDAH Quiz 2 Solution</h3>
<div class="outline-text-3" id="text-1-11">
<ul class="org-ul">
<li>Answer: $120,000 per year (experienced data scientists earn over $150,000)</li>
</ul>
</div>
</div>

<div id="outline-container-orgc6a3f3d" class="outline-3">
<h3 id="orgc6a3f3d"><span class="section-number-3">1.12.</span> BDAH Quiz 3 Question</h3>
<div class="outline-text-3" id="text-1-12">
<ul class="org-ul">
<li>Question: What skills do data scientists need?</li>
</ul>
</div>
</div>

<div id="outline-container-orgec28fe5" class="outline-3">
<h3 id="orgec28fe5"><span class="section-number-3">1.13.</span> BDAH Quiz 3 Solution</h3>
<div class="outline-text-3" id="text-1-13">
<ul class="org-ul">
<li>Answer:
<ul class="org-ul">
<li>Math and statistics</li>
<li>Domain knowledge</li>
<li>Programming and databases</li>
<li>Communication and visualization</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org2ee9367" class="outline-2">
<h2 id="org2ee9367"><span class="section-number-2">2.</span> Big Data Course Overview</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org1733604" class="outline-3">
<h3 id="org1733604"><span class="section-number-3">2.1.</span> Introduction to Course Overview</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Overview of big data analytics for healthcare.</li>
<li>Topics covered:
<ul class="org-ul">
<li>Healthcare applications of big data.</li>
<li>Algorithms used in applications.</li>
<li>Software systems for implementation.</li>
</ul></li>
<li>Course structure alternates among these topics.</li>
</ul>
</div>
</div>

<div id="outline-container-org1bf3ae2" class="outline-3">
<h3 id="org1bf3ae2"><span class="section-number-3">2.2.</span> Big Data Big Picture</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Course focuses on three key areas:
<ul class="org-ul">
<li>Big data systems.</li>
<li>Scalable machine learning algorithms.</li>
<li>Healthcare applications.</li>
</ul></li>
<li>Integration of these elements to solve healthcare problems.</li>
</ul>
</div>
</div>

<div id="outline-container-org8ff1e68" class="outline-3">
<h3 id="org8ff1e68"><span class="section-number-3">2.3.</span> Healthcare Applications</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>Three types of healthcare applications:
<ol class="org-ol">
<li><b><b>Predictive Modeling</b></b>: Uses historical data to predict future outcomes.</li>
<li><b><b>Computational Phenotyping</b></b>: Converts electronic health records (EHRs) into meaningful clinical concepts.</li>
<li><b><b>Patient Similarity</b></b>: Identifies groups of patients with similar characteristics.</li>
</ol></li>
<li>Starts with predictive modeling.</li>
</ul>
</div>
</div>

<div id="outline-container-orgc7fee3a" class="outline-3">
<h3 id="orgc7fee3a"><span class="section-number-3">2.4.</span> Predictive Modeling Quiz Question</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>Predictive modeling example: predicting treatment effectiveness for epilepsy patients.</li>
<li>Quiz: Estimate percentage of patients responding to different treatments.</li>
</ul>
</div>
</div>

<div id="outline-container-orgc6d8033" class="outline-3">
<h3 id="orgc6d8033"><span class="section-number-3">2.5.</span> Predictive Modeling Quiz Solution</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>Solution breakdown:
<ul class="org-ul">
<li><b><b>Group A</b></b> (responded within 2 years): 32%.</li>
<li><b><b>Group B</b></b> (responded between 2-5 years): 24%.</li>
<li><b><b>Group C</b></b> (did not respond after 5 years): 44%.</li>
</ul></li>
<li>Goal of predictive modeling:
<ul class="org-ul">
<li>Improve early response rates.</li>
<li>Identify non-responders for alternative treatments.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org4322eaf" class="outline-3">
<h3 id="org4322eaf"><span class="section-number-3">2.6.</span> Predictive Modeling Challenges</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>Challenges in predictive modeling:
<ul class="org-ul">
<li>Handling large patient datasets with various types of data.</li>
<li>Managing multiple models in a complex computational pipeline.</li>
<li>Evaluating and comparing multiple predictive pipelines.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgba3c3c6" class="outline-3">
<h3 id="orgba3c3c6"><span class="section-number-3">2.7.</span> Computational Phenotyping</h3>
<div class="outline-text-3" id="text-2-7">
<ul class="org-ul">
<li>Converts raw patient data (e.g., demographics, diagnoses, medications, procedures, lab tests, clinical notes) into clinical concepts or phenotypes.</li>
</ul>
</div>
</div>

<div id="outline-container-orgdfd3945" class="outline-3">
<h3 id="orgdfd3945"><span class="section-number-3">2.8.</span> Computational Phenotyping Quiz Question</h3>
<div class="outline-text-3" id="text-2-8">
<ul class="org-ul">
<li>Identifying data issues in phenotyping.</li>
<li>Quiz: List potential "waste products" in raw data.</li>
</ul>
</div>
</div>

<div id="outline-container-org705f0fa" class="outline-3">
<h3 id="org705f0fa"><span class="section-number-3">2.9.</span> Computational Phenotyping Quiz Solution</h3>
<div class="outline-text-3" id="text-2-9">
<ul class="org-ul">
<li>Common data issues:
<ul class="org-ul">
<li><b><b>Missing values</b></b>: Some important data may be absent.</li>
<li><b><b>Duplicates</b></b>: Patient records may appear multiple times.</li>
<li><b><b>Irrelevant data</b></b>: Not all raw information is useful.</li>
<li><b><b>Redundant information</b></b>: Different records may indicate the same condition (e.g., diagnosis and medication for diabetes).</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org85e8905" class="outline-3">
<h3 id="org85e8905"><span class="section-number-3">2.10.</span> Phenotyping Algorithm</h3>
<div class="outline-text-3" id="text-2-10">
<ul class="org-ul">
<li>Example: Identifying Type 2 Diabetes from EHR data.</li>
<li>Decision process:
<ul class="org-ul">
<li>Check for Type 1 Diabetes diagnosis.</li>
<li>Check for Type 2 Diabetes diagnosis.</li>
<li>Verify medication records and abnormal lab results.</li>
</ul></li>
<li>Importance:
<ul class="org-ul">
<li>EHR data is often unreliable.</li>
<li>Multiple data sources improve diagnostic accuracy.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orge917f2c" class="outline-3">
<h3 id="orge917f2c"><span class="section-number-3">2.11.</span> Patient Similarity Quiz Question</h3>
<div class="outline-text-3" id="text-2-11">
<ul class="org-ul">
<li>Different types of reasoning doctors use:
<ul class="org-ul">
<li>Flowchart-based reasoning (like phenotyping algorithms).</li>
<li>Instinct and intuition.</li>
<li>Case-based reasoning (comparing patients to past cases).</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgfe101ba" class="outline-3">
<h3 id="orgfe101ba"><span class="section-number-3">2.12.</span> Patient Similarity Quiz Solution</h3>
<div class="outline-text-3" id="text-2-12">
<ul class="org-ul">
<li>Correct answer: <b><b>Case-based reasoning</b></b>.</li>
<li>Doctors often compare current patients to previous similar cases.</li>
</ul>
</div>
</div>

<div id="outline-container-orgeb936d0" class="outline-3">
<h3 id="orgeb936d0"><span class="section-number-3">2.13.</span> Patient Similarity</h3>
<div class="outline-text-3" id="text-2-13">
<ul class="org-ul">
<li>Simulating case-based reasoning using algorithms.</li>
<li>Process:
<ul class="org-ul">
<li>Search for similar patients in a database.</li>
<li>Identify treatment outcomes for similar cases.</li>
<li>Recommend best treatment based on historical data.</li>
</ul></li>
<li>Objective: Utilize full database knowledge instead of relying on a single doctor's experience.</li>
</ul>
</div>
</div>

<div id="outline-container-orgfac0a18" class="outline-3">
<h3 id="orgfac0a18"><span class="section-number-3">2.14.</span> Algorithms</h3>
<div class="outline-text-3" id="text-2-14">
<ul class="org-ul">
<li>Introduction to machine learning algorithms in healthcare.</li>
<li>Covered topics:
<ol class="org-ol">
<li><b><b>Classification</b></b>: Mapping patient data to target variables (e.g., predicting heart attack risk).</li>
<li><b><b>Clustering</b></b>: Grouping similar patients based on health conditions.</li>
<li><b><b>Dimensionality Reduction</b></b>: Reducing large patient datasets to essential features.</li>
<li><b><b>Graph Analysis</b></b>: Analyzing relationships between patients and diseases.</li>
</ol></li>
</ul>
</div>
</div>

<div id="outline-container-org1d3cfed" class="outline-3">
<h3 id="org1d3cfed"><span class="section-number-3">2.15.</span> Systems</h3>
<div class="outline-text-3" id="text-2-15">
<ul class="org-ul">
<li>Introduction to big data systems for healthcare applications.</li>
<li>Covered systems:
<ul class="org-ul">
<li><b><b>Hadoop</b></b>: Disk-based distributed system.</li>
<li><b><b>Spark</b></b>: In-memory distributed system (faster than Hadoop).</li>
</ul></li>
<li>Topics include:
<ul class="org-ul">
<li>Hadoop infrastructure (MapReduce, HDFS, Pig, Hive, HBase).</li>
<li>Spark infrastructure (Spark SQL, Spark Streaming, MLlib, GraphX).</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgfb29f0c" class="outline-3">
<h3 id="orgfb29f0c"><span class="section-number-3">2.16.</span> Summary</h3>
<div class="outline-text-3" id="text-2-16">
<ul class="org-ul">
<li>Recap of three key areas:
<ul class="org-ul">
<li>Healthcare applications.</li>
<li>Machine learning algorithms.</li>
<li>Big data systems.</li>
</ul></li>
<li>Course integrates these areas:
<ul class="org-ul">
<li>Example: Using logistic regression on Hadoop to predict heart failure.</li>
</ul></li>
<li>Next step: Begin applying concepts.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgbe3a660" class="outline-2">
<h2 id="orgbe3a660"><span class="section-number-2">3.</span> Predictive Modeling</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org1a9d80e" class="outline-3">
<h3 id="org1a9d80e"><span class="section-number-3">3.1.</span> Introduction to Predictive Modeling</h3>
<div class="outline-text-3" id="text-3-1">
<dl class="org-dl">
<dt>Predictive modeling</dt><dd>using historical data to predict future events.

<ul class="org-ul">
<li>Example: Using electronic health records (EHR) to model heart failure</li>
<li>Key Goal: Develop a good predictive model using EHR efficiently.</li>
</ul></dd>
</dl>
</div>
</div>

<div id="outline-container-org2784924" class="outline-3">
<h3 id="org2784924"><span class="section-number-3">3.2.</span> Predictive Modeling vs EHR</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>Importance: Increased research interest in using EHR for clinical predictive modeling.</li>
<li>Data Sources: EHR has become a major source for predictive modeling research.</li>
</ul>
</div>
</div>

<div id="outline-container-org5996ce6" class="outline-3">
<h3 id="org5996ce6"><span class="section-number-3">3.3.</span> Predictive Modeling Pipeline</h3>
<div class="outline-text-3" id="text-3-3">
<ul class="org-ul">
<li>Predictive modeling is a multi-step computational process:
<ol class="org-ol">
<li>Define the prediction target.</li>
<li>Construct the relevant patient cohort.</li>
<li>Identify potentially relevant features.</li>
<li>Select the most relevant features.</li>
<li>Compute the predictive model.</li>
<li>Evaluate the predictive model.</li>
</ol></li>
<li>Iterative process until a satisfactory model is obtained.</li>
</ul>
</div>
</div>

<div id="outline-container-org0382f36" class="outline-3">
<h3 id="org0382f36"><span class="section-number-3">3.4.</span> Prediction Target</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>Investigators may have many targets, but only some are feasible.</li>
<li>Selection criteria: The target should be interesting and possible with the available data.</li>
<li>Example: Predicting the onset of heart failure.</li>
</ul>
</div>
</div>

<div id="outline-container-org99aeabd" class="outline-3">
<h3 id="org99aeabd"><span class="section-number-3">3.5.</span> Heart Failure Quiz</h3>
<div class="outline-text-3" id="text-3-5">
<ul class="org-ul">
<li>Question: How many new heart failure cases occur annually in the U.S.?</li>
<li>Options: 17,000; 260,000; 550,000; 1,250,000.</li>
<li>Answer: 550,000 cases per year.</li>
</ul>
</div>
</div>

<div id="outline-container-org65994be" class="outline-3">
<h3 id="org65994be"><span class="section-number-3">3.6.</span> Motivations for Early Detection</h3>
<div class="outline-text-3" id="text-3-6">
<ul class="org-ul">
<li>Heart failure is complex with diverse symptoms and subsets.</li>
<li>Early detection can:
<ul class="org-ul">
<li>Reduce hospitalization costs.</li>
<li>Introduce early interventions to slow progression.</li>
<li>Improve clinical guidelines for heart failure prevention.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgfa0ae97" class="outline-3">
<h3 id="orgfa0ae97"><span class="section-number-3">3.7.</span> Cohort Construction</h3>
<div class="outline-text-3" id="text-3-7">
<ul class="org-ul">
<li>Defines the study population for predictive modeling.</li>
<li>Study population is a subset of all patients.</li>
<li>Four study designs based on two axes:
<ul class="org-ul">
<li>Prospective vs. Retrospective studies.</li>
<li>Cohort vs. Case-Control studies.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orge761112" class="outline-3">
<h3 id="orge761112"><span class="section-number-3">3.8.</span> Prospective vs. Retrospective Studies</h3>
<div class="outline-text-3" id="text-3-8">
<ul class="org-ul">
<li><b><b>Prospective Study</b></b>: Define cohort first, then collect data.</li>
<li><b><b>Retrospective Study</b></b>: Use existing data from past records.</li>
</ul>
</div>
</div>

<div id="outline-container-org8de3cc1" class="outline-3">
<h3 id="org8de3cc1"><span class="section-number-3">3.9.</span> Prospective vs. Retrospective Quiz</h3>
<div class="outline-text-3" id="text-3-9">
<ul class="org-ul">
<li>Comparison of study properties:
<ul class="org-ul">
<li>Retrospective studies have more noise.</li>
<li>Prospective studies are more expensive and time-consuming.</li>
<li>Retrospective studies can handle larger datasets.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org43f4ceb" class="outline-3">
<h3 id="org43f4ceb"><span class="section-number-3">3.10.</span> Cohort Study</h3>
<div class="outline-text-3" id="text-3-10">
<ul class="org-ul">
<li>Includes patients exposed to a particular risk (e.g., heart failure readmission).</li>
<li>Defines inclusion and exclusion criteria.</li>
</ul>
</div>
</div>

<div id="outline-container-orgd4671e5" class="outline-3">
<h3 id="orgd4671e5"><span class="section-number-3">3.11.</span> Case-Control Study</h3>
<div class="outline-text-3" id="text-3-11">
<ul class="org-ul">
<li>Compares patients with a condition (cases) to similar patients without it (controls).</li>
<li>Matching criteria: Age, gender, clinic visits.</li>
<li>Cases are rarer than controls in real-world data.</li>
</ul>
</div>
</div>

<div id="outline-container-org2360ff3" class="outline-3">
<h3 id="org2360ff3"><span class="section-number-3">3.12.</span> Feature Construction</h3>
<div class="outline-text-3" id="text-3-12">
<ul class="org-ul">
<li>Defines patient features for predicting outcomes.</li>
<li>Data sequences from EHR:
<ul class="org-ul">
<li>Events (diagnoses, medications, lab results).</li>
<li>Observation window (used for feature extraction).</li>
<li>Prediction window (future period to predict).</li>
</ul></li>
<li>Impact of window sizes on model accuracy.</li>
</ul>
</div>
</div>

<div id="outline-container-org66e506b" class="outline-3">
<h3 id="org66e506b"><span class="section-number-3">3.13.</span> Feature Construction Quizzes</h3>
<div class="outline-text-3" id="text-3-13">
<ul class="org-ul">
<li>Quiz 1: Best timeline for modeling → Large observation, small prediction window.</li>
<li>Quiz 2: Most useful model → Small observation, large prediction window (ideal but difficult).</li>
</ul>
</div>
</div>

<div id="outline-container-org01c10a2" class="outline-3">
<h3 id="org01c10a2"><span class="section-number-3">3.14.</span> Prediction Performance on Windows</h3>
<div class="outline-text-3" id="text-3-14">
<ul class="org-ul">
<li><b><b>Prediction Window</b></b>: Longer window reduces accuracy.</li>
<li><b><b>Observation Window</b></b>: Longer window improves model performance until plateau.</li>
</ul>
</div>
</div>

<div id="outline-container-org263e920" class="outline-3">
<h3 id="org263e920"><span class="section-number-3">3.15.</span> Feature Selection</h3>
<div class="outline-text-3" id="text-3-15">
<ul class="org-ul">
<li>Identifies relevant features from EHR data.</li>
<li>EHR provides a large number of potential features (e.g., demographics, vitals).</li>
<li>Different targets require different feature subsets.</li>
</ul>
</div>
</div>

<div id="outline-container-org6a7cc51" class="outline-3">
<h3 id="org6a7cc51"><span class="section-number-3">3.16.</span> Predictive Model</h3>
<div class="outline-text-3" id="text-3-16">
<ul class="org-ul">
<li>Maps input features to predicted outcomes.</li>
<li>Regression models for continuous targets (e.g., healthcare costs).</li>
<li>Classification models for categorical targets (e.g., heart failure).</li>
<li>Common methods: Logistic regression, decision trees, random forests.</li>
</ul>
</div>
</div>

<div id="outline-container-orgd846d17" class="outline-3">
<h3 id="orgd846d17"><span class="section-number-3">3.17.</span> Performance Evaluation</h3>
<div class="outline-text-3" id="text-3-17">
<ul class="org-ul">
<li>Key metric: Testing error (not training error).</li>
<li>Models must generalize to unseen data.</li>
</ul>
</div>
</div>

<div id="outline-container-orgdbfb7fb" class="outline-3">
<h3 id="orgdbfb7fb"><span class="section-number-3">3.18.</span> Cross-Validation</h3>
<div class="outline-text-3" id="text-3-18">
<ul class="org-ul">
<li>Splits data into training and validation sets.</li>
<li>Common methods:
<ul class="org-ul">
<li>Leave-One-Out Cross-Validation.</li>
<li>K-Fold Cross-Validation.</li>
<li>Randomized Cross-Validation.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgf34eabf" class="outline-3">
<h3 id="orgf34eabf"><span class="section-number-3">3.19.</span> Conclusion</h3>
<div class="outline-text-3" id="text-3-19">
<ul class="org-ul">
<li>Summary of the predictive modeling pipeline:
<ul class="org-ul">
<li>Define the prediction target.</li>
<li>Construct the patient cohort.</li>
<li>Generate relevant features.</li>
<li>Select important features.</li>
<li>Train the predictive model.</li>
<li>Evaluate model performance.</li>
</ul></li>
<li>Goal: Design high-level predictive modeling studies using EHR data.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org53f2f62" class="outline-2">
<h2 id="org53f2f62"><span class="section-number-2">4.</span> MapReduce</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org56dc554" class="outline-3">
<h3 id="org56dc554"><span class="section-number-3">4.1.</span> Introduction to MapReduce</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Overview of MapReduce as a big data processing tool.</li>
<li>Utilizes distributed computation and storage.</li>
<li>Covers:
<ul class="org-ul">
<li>What MapReduce is</li>
<li>Fault tolerance in distributed environments</li>
<li>Analytical use cases and limitations</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgd7c2b80" class="outline-3">
<h3 id="orgd7c2b80"><span class="section-number-3">4.2.</span> What is MapReduce</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>Hadoop/MapReduce is:
<ul class="org-ul">
<li>A programming model for parallel computation</li>
<li>An execution environment (Hadoop with HDFS)</li>
<li>A software package with various tools</li>
</ul></li>
<li>Capabilities:
<ul class="org-ul">
<li>Distributed storage via HDFS</li>
<li>Distributed computation via MapReduce</li>
<li>Built-in fault tolerance</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgc7b1b0a" class="outline-3">
<h3 id="orgc7b1b0a"><span class="section-number-3">4.3.</span> Computational Process</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>Originated at Google (2004), open-sourced via Apache Hadoop.</li>
<li>Java-based platform.</li>
<li>Programming constrained to Map and Reduce functions for scalability.</li>
<li>Designed for parallel, fault-tolerant processing.</li>
<li>Emphasis on mastering computational patterns for analytics.</li>
</ul>
</div>
</div>

<div id="outline-container-org7931de1" class="outline-3">
<h3 id="org7931de1"><span class="section-number-3">4.4.</span> Learning Via Aggregation Statistics</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>Core design: express ML algorithms as aggregation tasks.</li>
<li>Example: Heart failure risk factor frequency analysis.</li>
<li>Map: extract risk factors per patient.</li>
<li>Reduce: aggregate frequencies across population.</li>
<li>Leads into deeper abstraction needs and trade-offs.</li>
</ul>
</div>
</div>

<div id="outline-container-orgfe91dcf" class="outline-3">
<h3 id="orgfe91dcf"><span class="section-number-3">4.5.</span> MapReduce Abstraction</h3>
<div class="outline-text-3" id="text-4-5">
<ul class="org-ul">
<li>Example task: count disease cases from patient records.</li>
<li>Map:
<ul class="org-ul">
<li>Emit (disease, 1) for each mention</li>
</ul></li>
<li>Shuffle:
<ul class="org-ul">
<li>Group by disease</li>
</ul></li>
<li>Reduce:
<ul class="org-ul">
<li>Sum values for each disease</li>
</ul></li>
<li>Emphasizes two-phase logic for scalability.</li>
</ul>
</div>
</div>

<div id="outline-container-orga10fc35" class="outline-3">
<h3 id="orga10fc35"><span class="section-number-3">4.6.</span> MapReduce System</h3>
<div class="outline-text-3" id="text-4-6">
<ul class="org-ul">
<li>Real-world data is too large for single machines.</li>
<li>Data is partitioned and processed by multiple mappers.</li>
<li>Intermediate results are shuffled and passed to reducers.</li>
<li>Final results computed from reduce function.</li>
<li>Three stages:
<ul class="org-ul">
<li>Map</li>
<li>Shuffle</li>
<li>Reduce</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org71eba8b" class="outline-3">
<h3 id="org71eba8b"><span class="section-number-3">4.7.</span> MapReduce Fault Recovery</h3>
<div class="outline-text-3" id="text-4-7">
<ul class="org-ul">
<li>Fault tolerance is a built-in system feature.</li>
<li>On failure:
<ul class="org-ul">
<li>Mappers/reducers are restarted with minimal recomputation.</li>
</ul></li>
<li>Goal: Only failed components are recomputed.</li>
<li>System handles all failure recovery.</li>
</ul>
</div>
</div>

<div id="outline-container-org543f08b" class="outline-3">
<h3 id="org543f08b"><span class="section-number-3">4.8.</span> Distributed File Systems</h3>
<div class="outline-text-3" id="text-4-8">
<ul class="org-ul">
<li>HDFS: Hadoop's storage system.</li>
<li>Splits large files into partitions.</li>
<li>Partitions stored on multiple machines with redundancy.</li>
<li>Benefits:
<ul class="org-ul">
<li>Faster concurrent access</li>
<li>Fault tolerance (recover from worker failures)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org64510ce" class="outline-3">
<h3 id="org64510ce"><span class="section-number-3">4.9.</span> MapReduce Design Choice</h3>
<div class="outline-text-3" id="text-4-9">
<ul class="org-ul">
<li>Design principle: minimal functionality for reliability and scalability.</li>
<li>Restricted computation model (e.g., aggregation queries).</li>
<li>Map: operate on individual records</li>
<li>Reduce: aggregate results</li>
<li>Supports straggler mitigation (slow mappers duplicated)</li>
</ul>
</div>
</div>

<div id="outline-container-org7a4e4c7" class="outline-3">
<h3 id="org7a4e4c7"><span class="section-number-3">4.10.</span> Analytics with MapReduce</h3>
<div class="outline-text-3" id="text-4-10">
<ul class="org-ul">
<li>Application examples:
<ul class="org-ul">
<li>K-Nearest Neighbors (KNN)</li>
<li>Linear Regression</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org79ae1a8" class="outline-3">
<h3 id="org79ae1a8"><span class="section-number-3">4.11.</span> MapReduce KNN</h3>
<div class="outline-text-3" id="text-4-11">
<ul class="org-ul">
<li>KNN implementation:
<ul class="org-ul">
<li>Map:
<ul class="org-ul">
<li>Find K nearest neighbors per partition</li>
</ul></li>
<li>Reduce:
<ul class="org-ul">
<li>Combine local results to find global nearest neighbors</li>
</ul></li>
</ul></li>
<li>Patient data partitioned, processed in parallel</li>
</ul>
</div>
</div>

<div id="outline-container-org51a0b1d" class="outline-3">
<h3 id="org51a0b1d"><span class="section-number-3">4.12.</span> Linear Regression</h3>
<div class="outline-text-3" id="text-4-12">
<ul class="org-ul">
<li>Goal: map patient features to heart disease risk</li>
<li>Normal equation:
<ul class="org-ul">
<li>β = (XᵗX)⁻¹Xᵗy</li>
</ul></li>
<li>MapReduce:
<ul class="org-ul">
<li>Map f1: compute xi * xiᵗ</li>
<li>Map f2: compute xi * yi</li>
<li>Reduce: aggregate sums</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org8b8c648" class="outline-3">
<h3 id="org8b8c648"><span class="section-number-3">4.13.</span> MapReduce for Linear Regression Quiz Question</h3>
<div class="outline-text-3" id="text-4-13">
<ul class="org-ul">
<li>Quiz: specify map/reduce pseudo code for computing Xᵗy</li>
</ul>
</div>
</div>

<div id="outline-container-orgf6b3462" class="outline-3">
<h3 id="orgf6b3462"><span class="section-number-3">4.14.</span> MapReduce for Linear Regression Quiz Solution</h3>
<div class="outline-text-3" id="text-4-14">
<ul class="org-ul">
<li>Solution:
<ul class="org-ul">
<li>Map: compute xi * yi</li>
<li>Reduce: aggregate results</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgee7cb81" class="outline-3">
<h3 id="orgee7cb81"><span class="section-number-3">4.15.</span> Limitations of MapReduce</h3>
<div class="outline-text-3" id="text-4-15">
<ul class="org-ul">
<li>Example: Logistic Regression via Gradient Descent</li>
<li>Challenge: Requires iterative computation
<ul class="org-ul">
<li>Each iteration loads data twice</li>
</ul></li>
<li>MapReduce not efficient for:
<ul class="org-ul">
<li>Iterative, multi-stage computation</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org94bf2e0" class="outline-3">
<h3 id="org94bf2e0"><span class="section-number-3">4.16.</span> MapReduce Summary Quiz Question</h3>
<div class="outline-text-3" id="text-4-16">
<ul class="org-ul">
<li>Quiz on ideal conditions for MapReduce:
<ul class="org-ul">
<li>Single vs. multiple passes</li>
<li>Skewed vs. uniform key distribution</li>
<li>Synchronization needs</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgecf0cb5" class="outline-3">
<h3 id="orgecf0cb5"><span class="section-number-3">4.17.</span> MapReduce Summary Quiz Solution</h3>
<div class="outline-text-3" id="text-4-17">
<ul class="org-ul">
<li>Best suited for:
<ul class="org-ul">
<li>Single-pass jobs (e.g., histograms)</li>
<li>Both skewed and uniform key distributions</li>
<li>Minimal synchronization (only between Map and Reduce phases)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2be23ff" class="outline-2">
<h2 id="org2be23ff"><span class="section-number-2">5.</span> Classification Model Metrics</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org4898d91" class="outline-3">
<h3 id="org4898d91"><span class="section-number-3">5.1.</span> Predictive Model review</h3>
<div class="outline-text-3" id="text-5-1">
<dl class="org-dl">
<dt>Predictive model</dt><dd>mapping function between model inputs and outputs (pred's)</dd>
<dt>Model metrics</dt><dd>needed to know how well they're performing</dd>
</dl>
</div>
</div>
<div id="outline-container-orgee7391e" class="outline-3">
<h3 id="orgee7391e"><span class="section-number-3">5.2.</span> Confusion matrix</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>True positive/negative also known as "Condition" positive/negative</li>
<li>Predicted positive/negative also known as "Prediction Outcome" positive/negative</li>
</ul>
</div>
</div>
<div id="outline-container-org8bf44df" class="outline-3">
<h3 id="org8bf44df"><span class="section-number-3">5.3.</span> Accuracy metrics</h3>
<div class="outline-text-3" id="text-5-3">
<p>
All divided by ground truth values.
</p>
<dl class="org-dl">
<dt>Accuracy</dt><dd>(TP+TN)/TP. Not most useful for imbalanced class</dd>
<dt>True positive rate</dt><dd>TP/CP (Sensitivity, Recall)</dd>
<dt>False positive rate</dt><dd>FP/CN</dd>
<dt>False negative</dt><dd>FN/CP</dd>
<dt>True negative rate</dt><dd>TN/CN (Specificity)</dd>
</dl>
</div>
<div id="outline-container-orge27c10d" class="outline-4">
<h4 id="orge27c10d"><span class="section-number-4">5.3.1.</span> Other notes</h4>
<div class="outline-text-4" id="text-5-3-1">
<ul class="org-ul">
<li>FP is a type I error.</li>
<li>FN is a type II error.</li>
<li>Hard to perform well on all metrics</li>
<li>Important to choose the right metrics</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org85564b1" class="outline-3">
<h3 id="org85564b1"><span class="section-number-3">5.4.</span> Predictive metrics</h3>
<div class="outline-text-3" id="text-5-4">
<p>
All divided by prediction outcomes.
</p>
<dl class="org-dl">
<dt>Prevalence</dt><dd>CP/Total population. How likely disease occurs in population</dd>
<dt>Positive predictive value (Precision)</dt><dd>TP/Pred outcome positive.</dd>
<dt>False discovery rate</dt><dd>FP/Pred outcome positive</dd>
<dt>Negative predictive value</dt><dd>TN/Pred outcome negative</dd>
<dt>False omission rate</dt><dd>FN/Pred outcome negative</dd>
</dl>
</div>
</div>
<div id="outline-container-org66103e5" class="outline-3">
<h3 id="org66103e5"><span class="section-number-3">5.5.</span> F1 score</h3>
<div class="outline-text-3" id="text-5-5">
<p>
Harmonic mean of PPV and TPR
\[
F_1 = 2 \times \frac{PPV \times TPR}{PPV + TPR}
\]
</p>
</div>
</div>
<div id="outline-container-org23b56a6" class="outline-3">
<h3 id="org23b56a6"><span class="section-number-3">5.6.</span> Classifier quiz</h3>
<div class="outline-text-3" id="text-5-6">
<p>
Which is the best classifier?
</p>
<ul class="org-ul">
<li>Highest F1, PPV and Accuracy is the best classifier</li>
</ul>
</div>
</div>
<div id="outline-container-org2709acb" class="outline-3">
<h3 id="org2709acb"><span class="section-number-3">5.7.</span> Reversing predictions</h3>
<div class="outline-text-3" id="text-5-7">
<p>
It's always possible to reverse the predictions so 0.21 might perform better than 0.69/0.50.
</p>
</div>
</div>
<div id="outline-container-orge442cc5" class="outline-3">
<h3 id="orge442cc5"><span class="section-number-3">5.8.</span> Receiver operating characteristic</h3>
<div class="outline-text-3" id="text-5-8">
<ul class="org-ul">
<li>Predictive models generally output continuous score.</li>
<li>Threshold is needed as precision bound to force to a certain category</li>
<li>ROC provides a way to compare performance of classifiers as the decision boundary is varied</li>
<li>ROC curve is the plot of TP rate vs FP rate at various threshold values
<ul class="org-ul">
<li>We sort by prediction score (highest first)</li>
<li>Use prediction score as threshold values</li>
<li>Plot on the chart and see how many are misclassified (needs True value to be known)</li>
</ul></li>
<li>AUROC does not depend on the choice of threshold.</li>
<li>AUROC is the most popular metric for classification</li>
</ul>
</div>
</div>
<div id="outline-container-org24b184f" class="outline-3">
<h3 id="org24b184f"><span class="section-number-3">5.9.</span> "Best classifier threshold" quiz</h3>
<div class="outline-text-3" id="text-5-9">
<ul class="org-ul">
<li>There isn't a standard answer, it depends on whether TP, TN or other metric is prioritized</li>
</ul>
</div>
</div>
<div id="outline-container-org4681613" class="outline-3">
<h3 id="org4681613"><span class="section-number-3">5.10.</span> Regression metrics</h3>
<div class="outline-text-3" id="text-5-10">
<dl class="org-dl">
<dt>MAE</dt><dd>average of absolute errors, harder to work with since the absolute value is not differentiable</dd>
<dt>MSE</dt><dd>average of squared errors, easier to work with as the derivative of squared term is linear. Increases a lot faster than MAE</dd>
<dt>\(R^2\)</dt><dd>bounded by \((-\infty,1)\). AKA coefficient of determination.</dd>
</dl>
</div>
<div id="outline-container-org570f8c6" class="outline-4">
<h4 id="org570f8c6"><span class="section-number-4">5.10.1.</span> \(R^2\)</h4>
<div class="outline-text-4" id="text-5-10-1">
<p>
\[
R^2 = 1-\frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i(y_i - \bar{y})^2}
\]
i.e. 1-MSE/Variance
</p>

<p>
Negative \(R^2\) values means they perform worse than a simple average of raw data.
As noise increases, \(R^2\) decreases.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orga94a02e" class="outline-2">
<h2 id="orga94a02e"><span class="section-number-2">6.</span> Ensemble methods</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgdc69930" class="outline-3">
<h3 id="orgdc69930"><span class="section-number-3">6.1.</span> Gradient Descent Method (GDM) for Linear Regression</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>Illustrates gradient descent using linear regression.</li>
<li>Dataset: each row = patient; features in X, outcome Y (e.g., hospital cost).</li>
<li>Objective: learn linear mapping from features X to outcome Y.</li>
<li>Assumes Gaussian distribution → log-likelihood leads to squared error minimization.</li>
<li>Gradient: derived by taking derivative of log-likelihood w.r.t. coefficients β.</li>
<li>Update rule: move β in direction of gradient with step size η.</li>
<li>Requires multiple iterations over full dataset → computationally expensive.</li>
</ul>
</div>
</div>

<div id="outline-container-org67bbd7c" class="outline-3">
<h3 id="org67bbd7c"><span class="section-number-3">6.2.</span> Stochastic Gradient Descent (SGD) Method</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>SGD is a scalable variant of gradient descent for large datasets.</li>
<li>Instead of using full dataset, computes gradients on random subsets (mini-batches).</li>
<li>One-point update = SGD; larger batches = mini-batch gradient descent.</li>
<li>Faster per-iteration updates compared to full batch gradient descent.</li>
</ul>
</div>
</div>

<div id="outline-container-orge9b5e1f" class="outline-3">
<h3 id="orge9b5e1f"><span class="section-number-3">6.3.</span> SGD for Linear Regression</h3>
<div class="outline-text-3" id="text-6-3">
<ul class="org-ul">
<li>Applies SGD to same regression problem.</li>
<li>Each update uses one patient:
<ul class="org-ul">
<li>Compute individual log-likelihood and gradient.</li>
<li>Update β using gradient.</li>
</ul></li>
<li>More efficient than full gradient computation.</li>
<li>Key idea: updates based on single or small group of samples.</li>
</ul>
</div>
</div>

<div id="outline-container-orgaf89357" class="outline-3">
<h3 id="orgaf89357"><span class="section-number-3">6.4.</span> Ensemble Method Pt 1</h3>
<div class="outline-text-3" id="text-6-4">
<ul class="org-ul">
<li>Ensemble methods combine multiple models for improved performance.</li>
<li>Can use same (e.g., Random Forest) or different base models.</li>
<li>Real-world example: Netflix Prize → ensemble models won.</li>
<li>Many teams merged, forming more powerful ensembles.</li>
</ul>
</div>
</div>

<div id="outline-container-orgfffe665" class="outline-3">
<h3 id="orgfffe665"><span class="section-number-3">6.5.</span> Ensemble Method Pt 2</h3>
<div class="outline-text-3" id="text-6-5">
<ul class="org-ul">
<li>General ensemble steps:
<ol class="org-ol">
<li>Generate multiple datasets (via bagging or boosting).</li>
<li>Train separate models on each.</li>
<li>Aggregate outputs using functions like averaging or weighted sum.</li>
</ol></li>
<li>Outcome: different ensemble strategies.</li>
</ul>
</div>
</div>

<div id="outline-container-orgd472803" class="outline-3">
<h3 id="orgd472803"><span class="section-number-3">6.6.</span> Bias Variance Tradeoff</h3>
<div class="outline-text-3" id="text-6-6">
<ul class="org-ul">
<li>Key insight: model error = bias² + variance.</li>
<li>Bias: error from wrong assumptions (e.g., assuming linear when it's not).</li>
<li>Variance: error from sensitivity to training data.</li>
<li>Ideal model: low bias and low variance.</li>
<li>Graphical example: dartboard showing different combinations.</li>
<li>As model complexity increases:
<ul class="org-ul">
<li>Bias ↓</li>
<li>Variance ↑</li>
<li>Total error = minimum at an optimal complexity.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org5aa9a7c" class="outline-3">
<h3 id="org5aa9a7c"><span class="section-number-3">6.7.</span> Bias Variance Tradeoff Quiz Question</h3>
<div class="outline-text-3" id="text-6-7">
<ul class="org-ul">
<li>Task: rank four models (flat, linear, smooth curve, wiggly) by complexity.</li>
<li>A → D: D (flat) &lt; A (linear) &lt; B (curve) &lt; C (wiggly)</li>
</ul>
</div>
</div>

<div id="outline-container-orgbc26924" class="outline-3">
<h3 id="orgbc26924"><span class="section-number-3">6.8.</span> Bias Variance Tradeoff Quiz Solution</h3>
<div class="outline-text-3" id="text-6-8">
<ul class="org-ul">
<li>Explanation:
<ul class="org-ul">
<li>D: lowest complexity (flat line).</li>
<li>A: next, simple linear model.</li>
<li>B: more complex curve.</li>
<li>C: most complex (wiggly).</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org398f01e" class="outline-3">
<h3 id="org398f01e"><span class="section-number-3">6.9.</span> Bias Variance Tradeoff Quiz 2 Question</h3>
<div class="outline-text-3" id="text-6-9">
<ul class="org-ul">
<li>Given same models, asked which best fits data.</li>
</ul>
</div>
</div>

<div id="outline-container-orge9de2bb" class="outline-3">
<h3 id="orge9de2bb"><span class="section-number-3">6.10.</span> Bias Variance Tradeoff Quiz 2 Solution</h3>
<div class="outline-text-3" id="text-6-10">
<ul class="org-ul">
<li>Answer: B (smooth curve)</li>
<li>Balances bias and variance well.</li>
</ul>
</div>
</div>

<div id="outline-container-orgaf36110" class="outline-3">
<h3 id="orgaf36110"><span class="section-number-3">6.11.</span> Bagging</h3>
<div class="outline-text-3" id="text-6-11">
<ul class="org-ul">
<li>Bagging = Bootstrap Aggregation.</li>
<li>Method:
<ul class="org-ul">
<li>Sample data with replacement.</li>
<li>Train models on each sample.</li>
<li>Combine predictions via majority vote or averaging.</li>
</ul></li>
<li>Reduces variance of the final model.</li>
</ul>
</div>
</div>

<div id="outline-container-orgb327892" class="outline-3">
<h3 id="orgb327892"><span class="section-number-3">6.12.</span> Random Forest</h3>
<div class="outline-text-3" id="text-6-12">
<ul class="org-ul">
<li>Special case of bagging with decision trees.</li>
<li>Steps:
<ul class="org-ul">
<li>Randomly sample both rows (patients) and columns (features).</li>
<li>Train simple decision trees on these subsets.</li>
</ul></li>
<li>Final prediction = average of all trees.</li>
<li>Simple trees help maintain diversity and lower computation.</li>
</ul>
</div>
</div>

<div id="outline-container-org96ccff9" class="outline-3">
<h3 id="org96ccff9"><span class="section-number-3">6.13.</span> Why Bagging Works</h3>
<div class="outline-text-3" id="text-6-13">
<ul class="org-ul">
<li>Reduces variance without increasing bias.</li>
<li>Variance reduction from averaging independent model predictions.</li>
<li>Based on statistical principle: variance of mean = variance / T for T models.</li>
</ul>
</div>
</div>

<div id="outline-container-orge3864db" class="outline-3">
<h3 id="orge3864db"><span class="section-number-3">6.14.</span> Boosting</h3>
<div class="outline-text-3" id="text-6-14">
<ul class="org-ul">
<li>Builds models sequentially.</li>
<li>Each new model focuses on mistakes of previous ones.</li>
<li>Final model = weighted combination of all models.</li>
<li>Pros: better accuracy.</li>
<li>Cons: prone to overfitting.</li>
<li>Example: AdaBoost is most popular.</li>
</ul>
</div>
</div>

<div id="outline-container-org833a08c" class="outline-3">
<h3 id="org833a08c"><span class="section-number-3">6.15.</span> Bagging vs Boosting Quiz Question</h3>
<div class="outline-text-3" id="text-6-15">
<ul class="org-ul">
<li>Quiz to compare characteristics:
<ul class="org-ul">
<li>Combination method (simple vs weighted average)</li>
<li>Parallelism</li>
<li>Noise sensitivity</li>
<li>Accuracy</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org8540423" class="outline-3">
<h3 id="org8540423"><span class="section-number-3">6.16.</span> Bagging vs Boosting Quiz Solution</h3>
<div class="outline-text-3" id="text-6-16">
<ul class="org-ul">
<li>Bagging:
<ul class="org-ul">
<li>Simple average, parallel-friendly, less sensitive to noise, reliable.</li>
</ul></li>
<li>Boosting:
<ul class="org-ul">
<li>Weighted average, sequential (hard to parallelize), sensitive to noise, potentially higher accuracy but less reliable.</li>
</ul></li>
<li>Analogy:
<ul class="org-ul">
<li>Bagging = reliable Japanese car.</li>
<li>Boosting = high-performance but risky sports car.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org8b72fe9" class="outline-3">
<h3 id="org8b72fe9"><span class="section-number-3">6.17.</span> Summary for Ensemble Methods</h3>
<div class="outline-text-3" id="text-6-17">
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>Simple, flexible, few parameters, theoretical backing.</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Computational cost (training + inference).</li>
<li>Harder interpretation (due to model complexity).</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgebddb45" class="outline-3">
<h3 id="orgebddb45"><span class="section-number-3">6.18.</span> Introduction to Ensemble Method</h3>
<div class="outline-text-3" id="text-6-18">
<ul class="org-ul">
<li>Sets the stage:
<ul class="org-ul">
<li>Recap on model evaluation.</li>
<li>Overview: SGD, ensemble methods, bias-variance tradeoff, bagging &amp; boosting.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org96bb029" class="outline-3">
<h3 id="org96bb029"><span class="section-number-3">6.19.</span> Gradient Descent Method</h3>
<div class="outline-text-3" id="text-6-19">
<ul class="org-ul">
<li>General overview of gradient descent for optimization.</li>
<li>Applies to regression/classification.</li>
<li>Process:
<ol class="org-ol">
<li>Define likelihood (or log-likelihood).</li>
<li>Compute gradient w.r.t. parameters.</li>
<li>Update parameters iteratively using gradient.</li>
</ol></li>
<li>Requires step size (learning rate).</li>
<li>Can be extended using more advanced techniques (e.g., conjugate gradient).</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org0fc1db0" class="outline-2">
<h2 id="org0fc1db0"><span class="section-number-2">7.</span> Computational Phenotyping</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org17edfa4" class="outline-3">
<h3 id="org17edfa4"><span class="section-number-3">7.1.</span> Introduction to Phenotyping</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>Introduction to clustering in healthcare, specifically "phenotyping."</li>
<li>Phenotypes refer to diseases or conditions; known phenotypes exist, but many more are undiscovered.</li>
<li>Computational phenotyping helps discover novel phenotypes using available data.</li>
<li>Applications include disease diagnosis, cost prediction, readmission risk, and genomic studies.</li>
</ul>
</div>
</div>

<div id="outline-container-orgf4c90f9" class="outline-3">
<h3 id="orgf4c90f9"><span class="section-number-3">7.2.</span> Computational Phenotyping</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li>Converts raw EHR data into meaningful phenotypes using algorithms.</li>
<li>EHR data includes demographics, diagnosis codes, medications, procedures, labs, notes.</li>
<li>Challenges:
<ul class="org-ul">
<li>Noisy and incomplete data.</li>
<li>Primarily designed for clinical use, not research.</li>
<li>Redundancy across data sources.</li>
</ul></li>
<li>Goal: derive research-grade phenotypes from raw clinical data.</li>
</ul>
</div>
</div>

<div id="outline-container-org9c9549a" class="outline-3">
<h3 id="org9c9549a"><span class="section-number-3">7.3.</span> Phenotyping Algorithm</h3>
<div class="outline-text-3" id="text-7-3">
<ul class="org-ul">
<li>Example: Type 2 diabetes phenotyping algorithm.</li>
<li>Sequential logic:
<ol class="org-ol">
<li>Exclude Type 1 diagnosis.</li>
<li>Check for Type 2 diagnosis.</li>
<li>Check medication and lab results.</li>
</ol></li>
<li>Multiple confirmation paths possible.</li>
<li>Developed manually by clinical experts.</li>
</ul>
</div>
</div>

<div id="outline-container-org9a3c6b8" class="outline-3">
<h3 id="org9a3c6b8"><span class="section-number-3">7.4.</span> Applications of Phenotyping</h3>
<div class="outline-text-3" id="text-7-4">
<ul class="org-ul">
<li>Genomic studies: link genomic and phenotypic data.</li>
<li>Clinical predictive modeling: forecast disease onset, hospitalizations.</li>
<li>Pragmatic clinical trials: assess treatment effectiveness in real-world settings.</li>
<li>Healthcare quality measurement: compare care quality across institutions.</li>
<li>All rely on robust phenotyping algorithms.</li>
</ul>
</div>
</div>

<div id="outline-container-org5436c8b" class="outline-3">
<h3 id="org5436c8b"><span class="section-number-3">7.5.</span> Genomic Wide Association Study</h3>
<div class="outline-text-3" id="text-7-5">
<ul class="org-ul">
<li>GWAS scans SNPs to associate genetic variations with diseases.</li>
<li>Steps:
<ol class="org-ol">
<li>Identify phenotypes.</li>
<li>Divide subjects into cases/controls.</li>
<li>Collect DNA and scan for SNPs.</li>
<li>Calculate odds ratios and p-values.</li>
</ol></li>
<li>Statistically significant SNPs may indicate disease relevance.</li>
<li>High-quality phenotyping critical for accurate GWAS.</li>
</ul>
</div>
</div>

<div id="outline-container-org6fa820d" class="outline-3">
<h3 id="org6fa820d"><span class="section-number-3">7.6.</span> Why Do We Care About Phenotyping</h3>
<div class="outline-text-3" id="text-7-6">
<ul class="org-ul">
<li>Genomic data is rapidly becoming cheaper and more abundant.</li>
<li>Phenotypic data remains complex and costly to acquire.</li>
<li>Demand for better phenotyping algorithms to support scalable, high-quality genomic research.</li>
</ul>
</div>
</div>

<div id="outline-container-org7bf1ccc" class="outline-3">
<h3 id="org7bf1ccc"><span class="section-number-3">7.7.</span> Clinical Predictive Modeling</h3>
<div class="outline-text-3" id="text-7-7">
<ul class="org-ul">
<li>Predictive modeling using raw EHR data is problematic (noise, complexity, lack of portability).</li>
<li>Phenotyping transforms raw data into simpler, meaningful concepts.</li>
<li>Benefits:
<ul class="org-ul">
<li>Better model accuracy.</li>
<li>Easier interpretation.</li>
<li>Generalizability across hospitals.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org2e87679" class="outline-3">
<h3 id="org2e87679"><span class="section-number-3">7.8.</span> Pragmatic Clinical Trials</h3>
<div class="outline-text-3" id="text-7-8">
<ul class="org-ul">
<li>Traditional vs. pragmatic trials:
<ul class="org-ul">
<li>Traditional: controlled, single condition/drug, strict selection, randomized.</li>
<li>Pragmatic: real-world, multiple conditions/drugs, minimal selection, often not randomized.</li>
</ul></li>
<li>High-quality phenotyping essential for identifying patient conditions and treatments in pragmatic settings.</li>
</ul>
</div>
</div>

<div id="outline-container-org4033822" class="outline-3">
<h3 id="org4033822"><span class="section-number-3">7.9.</span> Healthcare Quality Measurement</h3>
<div class="outline-text-3" id="text-7-9">
<ul class="org-ul">
<li>Challenge: hospitals use diverse EHR formats.</li>
<li>Centralized processing of raw data is complex.</li>
<li>Solution: local phenotyping at hospitals → send standardized phenotype data to central site.</li>
<li>Enables scalable and fair quality comparison across institutions.</li>
</ul>
</div>
</div>

<div id="outline-container-org938a978" class="outline-3">
<h3 id="org938a978"><span class="section-number-3">7.10.</span> Phenotyping Methods Part 1</h3>
<div class="outline-text-3" id="text-7-10">
<ul class="org-ul">
<li>Two main approaches:
<ul class="org-ul">
<li>Supervised learning: uses labeled data (function approximation).</li>
<li>Unsupervised learning: identifies patterns/structures without labels (description/summarization).</li>
</ul></li>
<li>Illustrative examples and playful dialogue explore these ideas in depth.</li>
</ul>
</div>
</div>

<div id="outline-container-org9171dc4" class="outline-3">
<h3 id="org9171dc4"><span class="section-number-3">7.11.</span> Phenotyping Methods Part 2</h3>
<div class="outline-text-3" id="text-7-11">
<ul class="org-ul">
<li>Supervised: expert-defined rules (Boolean logic, decision trees), or machine learning classifiers.
<ul class="org-ul">
<li>Pros: interpretable, low validation effort.</li>
<li>Cons: high development cost, not good for unknown phenotypes, potential poor transferability.</li>
</ul></li>
<li>Unsupervised: clustering, dimensionality reduction.
<ul class="org-ul">
<li>Pros: less manual effort, no need for labels.</li>
<li>Cons: hard to validate, large data requirements.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgaabde3d" class="outline-3">
<h3 id="orgaabde3d"><span class="section-number-3">7.12.</span> Phenotyping Quiz Question</h3>
<div class="outline-text-3" id="text-7-12">
<ul class="org-ul">
<li>Quiz:
<ul class="org-ul">
<li>Which approach requires more human effort during evaluation?</li>
<li>Which is easier to interpret?</li>
<li>Choices: expert-defined rules vs. classification models.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org70d0a6b" class="outline-3">
<h3 id="org70d0a6b"><span class="section-number-3">7.13.</span> Phenotyping Quiz Solution</h3>
<div class="outline-text-3" id="text-7-13">
<ul class="org-ul">
<li>Answer:
<ul class="org-ul">
<li>Classification models require more evaluation effort (need more labeled data).</li>
<li>Expert-defined rules are easier to interpret (clinician-derived, intuitive).</li>
</ul></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-orge5397cc" class="outline-2">
<h2 id="orge5397cc"><span class="section-number-2">8.</span> Clustering</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-orgc84ba51" class="outline-3">
<h3 id="orgc84ba51"><span class="section-number-3">8.1.</span> Introduction to Clustering</h3>
<div class="outline-text-3" id="text-8-1">
<ul class="org-ul">
<li>Introduction of clustering as a method distinct from classification.</li>
<li>Classification: Grouping by known labels.</li>
<li>Clustering: Discovering groups from raw data.</li>
<li>Topics covered:
<ul class="org-ul">
<li>Definition of clustering.</li>
<li>Algorithms: K-means, Gaussian Mixture Models (GMM).</li>
<li>Scalable algorithms for large datasets.</li>
<li>Applications in healthcare.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org9ea8c69" class="outline-3">
<h3 id="org9ea8c69"><span class="section-number-3">8.2.</span> Healthcare Applications</h3>
<div class="outline-text-3" id="text-8-2">
<ul class="org-ul">
<li>Clustering in healthcare:
<ul class="org-ul">
<li>Patient stratification: group patients with similar characteristics.</li>
<li>Disease hierarchy discovery: learn structure/relationships among diseases.</li>
<li>Phenotyping: convert raw data into meaningful phenotypes (clinical concepts).</li>
<li>Phenotypes = clusters of similar patients.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org398754e" class="outline-3">
<h3 id="org398754e"><span class="section-number-3">8.3.</span> What is Clustering</h3>
<div class="outline-text-3" id="text-8-3">
<ul class="org-ul">
<li>Clustering explained via patient-disease matrix:
<ul class="org-ul">
<li>Rows = patients, Columns = diseases.</li>
<li>Apply clustering to rows → patient clusters (e.g., P1, P2, P3).</li>
<li>Apply clustering to columns → disease clusters (e.g., D1, D2, D3).</li>
<li>Supports applications like phenotyping and disease discovery.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org1c9e94e" class="outline-3">
<h3 id="org1c9e94e"><span class="section-number-3">8.4.</span> Algorithm Overview</h3>
<div class="outline-text-3" id="text-8-4">
<ul class="org-ul">
<li>Two categories of clustering algorithms:
<ul class="org-ul">
<li>Classical: K-means, Hierarchical, Gaussian Mixture Model.</li>
<li>Scalable: Mini-batch K-means, DBSCAN.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgcb1c48c" class="outline-3">
<h3 id="orgcb1c48c"><span class="section-number-3">8.5.</span> K Means</h3>
<div class="outline-text-3" id="text-8-5">
<ul class="org-ul">
<li>Inputs: data points X₁ to Xₙ, number of clusters K.</li>
<li>Outputs: K clusters S₁ to Sₖ.</li>
<li>Objective: Minimize distance of data points to cluster centers (µᵢ).</li>
<li>Algorithm steps:
<ol class="org-ol">
<li>Initialize K centers.</li>
<li>Assign points to nearest center.</li>
<li>Update centers based on mean of assigned points.</li>
<li>Repeat until convergence.</li>
</ol></li>
<li>Example illustrated using K=2.</li>
</ul>
</div>
</div>

<div id="outline-container-org3924687" class="outline-3">
<h3 id="org3924687"><span class="section-number-3">8.6.</span> K Means Quiz Question</h3>
<div class="outline-text-3" id="text-8-6">
<ul class="org-ul">
<li>Quiz: Given n, k, d (dimensions), i (iterations), what's the complexity?</li>
</ul>
</div>
</div>

<div id="outline-container-org6b31646" class="outline-3">
<h3 id="org6b31646"><span class="section-number-3">8.7.</span> K Means Quiz Solution</h3>
<div class="outline-text-3" id="text-8-7">
<ul class="org-ul">
<li>Complexity: O(n × k × d × i)
<ul class="org-ul">
<li>Cost driven by assignment and update operations.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org0c41fe6" class="outline-3">
<h3 id="org0c41fe6"><span class="section-number-3">8.8.</span> Hierarchical Clustering</h3>
<div class="outline-text-3" id="text-8-8">
<ul class="org-ul">
<li>Builds a hierarchy of data points.</li>
<li>Two approaches:
<ul class="org-ul">
<li>Agglomerative (bottom-up): Merge smallest clusters until one remains.</li>
<li>Divisive (top-down): Split one big cluster into smaller ones.</li>
</ul></li>
<li>Agglomerative more commonly used.</li>
</ul>
</div>
</div>

<div id="outline-container-orga5b1625" class="outline-3">
<h3 id="orga5b1625"><span class="section-number-3">8.9.</span> Agglomerative Clustering</h3>
<div class="outline-text-3" id="text-8-9">
<ul class="org-ul">
<li>Steps:
<ol class="org-ol">
<li>Compute n×n distance matrix.</li>
<li>Initialize each point as its own cluster.</li>
<li>Iteratively merge closest clusters and update distance matrix.</li>
<li>Stop when one cluster remains (produces a hierarchy).</li>
</ol></li>
</ul>
</div>
</div>

<div id="outline-container-orga5287a7" class="outline-3">
<h3 id="orga5287a7"><span class="section-number-3">8.10.</span> Gaussian Mixture Model</h3>
<div class="outline-text-3" id="text-8-10">
<ul class="org-ul">
<li>Soft clustering: points belong to multiple clusters with probabilities.</li>
<li>Each cluster is a Gaussian (normal distribution).</li>
<li>Parameters: mean (µₖ), variance (σₖ), mixing coefficient (πₖ).</li>
<li>Goal: Estimate GMM parameters from data.</li>
</ul>
</div>
</div>

<div id="outline-container-org40f09e3" class="outline-3">
<h3 id="org40f09e3"><span class="section-number-3">8.11.</span> GMM Expectation Maximization</h3>
<div class="outline-text-3" id="text-8-11">
<ul class="org-ul">
<li>Uses Expectation-Maximization (EM) algorithm:
<ul class="org-ul">
<li>Initialize parameters.</li>
<li>E-step: Compute assignment scores (γₙₖ).</li>
<li>M-step: Update parameters using γₙₖ.</li>
<li>Iterate until convergence.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orga61bfad" class="outline-3">
<h3 id="orga61bfad"><span class="section-number-3">8.12.</span> GMM Steps</h3>
<div class="outline-text-3" id="text-8-12">
<ul class="org-ul">
<li>Details of EM:
<ul class="org-ul">
<li>Initialization: K-means can be used for better results.</li>
<li>E-step: Compute γₙₖ using cluster probabilities.</li>
<li>M-step:
<ul class="org-ul">
<li>Compute cluster size (Nₖ), mean (µₖ), covariance (σₖ), mixing coefficient (πₖ).</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org4726f7e" class="outline-3">
<h3 id="org4726f7e"><span class="section-number-3">8.13.</span> GMM Visual Illustration</h3>
<div class="outline-text-3" id="text-8-13">
<ul class="org-ul">
<li>Visual demo of GMM clustering.</li>
<li>Iteratively updates Gaussians until convergence.</li>
<li>Final model reflects soft cluster assignments.</li>
</ul>
</div>
</div>

<div id="outline-container-org2b43591" class="outline-3">
<h3 id="org2b43591"><span class="section-number-3">8.14.</span> K Means Vs GMM</h3>
<div class="outline-text-3" id="text-8-14">
<ul class="org-ul">
<li>Comparison:
<ul class="org-ul">
<li>K-means: hard assignment, single center (µₖ).</li>
<li>GMM: soft assignment, parameters include µₖ, σₖ, πₖ.</li>
<li>Both use iterative updates.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org14b9c7c" class="outline-3">
<h3 id="org14b9c7c"><span class="section-number-3">8.15.</span> Mini Batch K Means</h3>
<div class="outline-text-3" id="text-8-15">
<ul class="org-ul">
<li>Scalable version of K-means.</li>
<li>Works with small batches (M) instead of full dataset.</li>
<li>Updates centers incrementally.</li>
<li>Reduces computational cost for large data.</li>
</ul>
</div>
</div>

<div id="outline-container-orgbc36b2b" class="outline-3">
<h3 id="orgbc36b2b"><span class="section-number-3">8.16.</span> Mini Batch K Means Quiz Question</h3>
<div class="outline-text-3" id="text-8-16">
<ul class="org-ul">
<li>Quiz: Given k, t (iterations), b (batch size), d (dimensionality), compute complexity.</li>
</ul>
</div>
</div>

<div id="outline-container-orgfb1a4cb" class="outline-3">
<h3 id="orgfb1a4cb"><span class="section-number-3">8.17.</span> Mini Batch K Means Quiz Solution</h3>
<div class="outline-text-3" id="text-8-17">
<ul class="org-ul">
<li>Complexity: O(t × b × k × d)</li>
</ul>
</div>
</div>

<div id="outline-container-orgb7c35ee" class="outline-3">
<h3 id="orgb7c35ee"><span class="section-number-3">8.18.</span> DBScan</h3>
<div class="outline-text-3" id="text-8-18">
<ul class="org-ul">
<li>Density-Based Spatial Clustering.</li>
<li>Clusters = dense areas, separated by sparse regions.</li>
<li>Can identify clusters of arbitrary shape.</li>
<li>Effective for noisy data.</li>
</ul>
</div>
</div>

<div id="outline-container-orgf41e607" class="outline-3">
<h3 id="orgf41e607"><span class="section-number-3">8.19.</span> DBScan Key Concepts</h3>
<div class="outline-text-3" id="text-8-19">
<ul class="org-ul">
<li>Density = number of points within ε-distance.</li>
<li>Key concepts:
<ul class="org-ul">
<li>Core points: high-density.</li>
<li>Border points: near core points.</li>
<li>Noise: not in dense region.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org9b8ba6e" class="outline-3">
<h3 id="org9b8ba6e"><span class="section-number-3">8.20.</span> DBScan Algorithm</h3>
<div class="outline-text-3" id="text-8-20">
<ul class="org-ul">
<li>Steps:
<ol class="org-ol">
<li>Build graph connecting core points to neighbors.</li>
<li>Identify connected components as clusters.</li>
<li>Remove clustered points and repeat.</li>
</ol></li>
</ul>
</div>
</div>

<div id="outline-container-org15bfcab" class="outline-3">
<h3 id="org15bfcab"><span class="section-number-3">8.21.</span> DBScan Example</h3>
<div class="outline-text-3" id="text-8-21">
<ul class="org-ul">
<li>Visual example: finds 6 clusters, some noise points remain.</li>
</ul>
</div>
</div>

<div id="outline-container-org43e9dbd" class="outline-3">
<h3 id="org43e9dbd"><span class="section-number-3">8.22.</span> DBScan Quiz Question</h3>
<div class="outline-text-3" id="text-8-22">
<ul class="org-ul">
<li>Quiz: How many clusters can a point belong to in DBSCAN?</li>
</ul>
</div>
</div>

<div id="outline-container-org10fd5d5" class="outline-3">
<h3 id="org10fd5d5"><span class="section-number-3">8.23.</span> DBScan Quiz Solution</h3>
<div class="outline-text-3" id="text-8-23">
<ul class="org-ul">
<li>Answer: 0 or 1 (noise = 0, core/border = 1)</li>
</ul>
</div>
</div>

<div id="outline-container-org3ab9568" class="outline-3">
<h3 id="org3ab9568"><span class="section-number-3">8.24.</span> Clustering Evaluation Metrics</h3>
<div class="outline-text-3" id="text-8-24">
<ul class="org-ul">
<li>Metrics:
<ul class="org-ul">
<li>Rand Index (RI)</li>
<li>Mutual Information (MI)</li>
<li>Silhouette Coefficient</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgc0a6ab3" class="outline-3">
<h3 id="orgc0a6ab3"><span class="section-number-3">8.25.</span> Rand Index</h3>
<div class="outline-text-3" id="text-8-25">
<ul class="org-ul">
<li>Measures agreement between predicted and true clusters.</li>
<li>RI = (number of correct pairs) / (total pairs)</li>
<li>Value between 0 (bad) and 1 (perfect).</li>
</ul>
</div>
</div>

<div id="outline-container-org7a1a36b" class="outline-3">
<h3 id="org7a1a36b"><span class="section-number-3">8.26.</span> Mutual Information</h3>
<div class="outline-text-3" id="text-8-26">
<ul class="org-ul">
<li>From information theory: measures shared info between predicted and true clusters.</li>
<li>Normalized Mutual Information (NMI) between 0 and 1.</li>
</ul>
</div>
</div>

<div id="outline-container-orgc6a647f" class="outline-3">
<h3 id="orgc6a647f"><span class="section-number-3">8.27.</span> Summary of RI and MI</h3>
<div class="outline-text-3" id="text-8-27">
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>Bounded values, interpretable.</li>
<li>No assumptions about cluster shapes.</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Require ground truth, which may be unavailable.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgd11d8b2" class="outline-3">
<h3 id="orgd11d8b2"><span class="section-number-3">8.28.</span> Silhouette Coefficient</h3>
<div class="outline-text-3" id="text-8-28">
<ul class="org-ul">
<li>Measures how similar a point is to its own cluster vs. others.</li>
<li>Formula: (b - a) / max(a, b)</li>
<li>Does not require ground truth.</li>
</ul>
</div>
</div>

<div id="outline-container-org849008c" class="outline-3">
<h3 id="org849008c"><span class="section-number-3">8.29.</span> Silhouette Coefficient Pros and Cons</h3>
<div class="outline-text-3" id="text-8-29">
<ul class="org-ul">
<li>Range: -1 (bad) to 1 (good), 0 = overlapping clusters.</li>
<li>Assumes spherical clusters → less effective for complex shapes (e.g. DBSCAN).</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2025-03-29 Sat 11:32</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
