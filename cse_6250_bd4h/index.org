#+AUTHOR: W
#+SETUPFILE:/Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: BD4H
* Introduction to Big Data
** Introduction
   - Course: Big Data Analytics for Healthcare (BDAH)
   - Instructor: Jimeng Sun, associate professor at Georgia Tech
   - Background: Expertise in healthcare analytics and data mining, previous work at IBM TJ Watson Research Center
   - Course focus: What students will learn and why it matters

** About BDAH
   - Intersection of healthcare and big data
   - Covers healthcare applications, data, analytics, and big data processing
   - Focus on how data science is applied to healthcare

** Learning Goals
   - Understand healthcare data
   - Learn analytics algorithms
   - Work with big data systems
   - Application: Build models for disease risk prediction, treatment recommendation, patient clustering, and similarity analysis
   - Assessments: Homework using big data tools, project with system building, report writing, and presentations

** Current Problems in Healthcare
   - High costs: $3.8 trillion/year in the U.S.
   - Massive waste: $765 billion/year
   - Poor quality: 200,000-400,000 preventable deaths annually
   - Preventable deaths ranked as the third leading cause of death
   - Hope: Big data can improve care and reduce costs

** The Four Vs
   - Volume: Large healthcare data amounts
   - Variety: Different types of healthcare data sources
   - Velocity: Real-time data processing needs
   - Veracity: Issues with data quality (noise, missing data, errors)

** More About The Four Vs
   - Examples of data volume:
     - Genomic data: 200 GB per genome
     - fMRI scan: ~300 GB per scan
     - US medical imaging data per year: ~100 petabytes
   - Data variety:
     - Clinical (demographics, diagnosis, procedures, etc.)
     - Patient-generated (wearables, sensors)
     - Real-time (ICU monitoring data)
   - Focus on managing diverse data types in the course

** Data Science is Sexy
   - Data science is a critical field
   - Harvard Business Review: "Data Scientist, The Sexiest Job in the 21st Century"
   - Data scientists:
     - Capitalize on big data
     - Overcome technical limitations
     - Develop key tools like Hadoop and Spark

** BDAH Quiz 1 Question
   - True/False: A graduate-level degree is necessary to become a data scientist.

** BDAH Quiz 1 Solution
   - Answer: True

** BDAH Quiz 2 Question
   - Question: What is the average salary of a data scientist?

** BDAH Quiz 2 Solution
   - Answer: $120,000 per year (experienced data scientists earn over $150,000)

** BDAH Quiz 3 Question
   - Question: What skills do data scientists need?

** BDAH Quiz 3 Solution
   - Answer:
     - Math and statistics
     - Domain knowledge
     - Programming and databases
     - Communication and visualization

* Big Data Course Overview
** Introduction to Course Overview
   - Overview of big data analytics for healthcare.
   - Topics covered:
     - Healthcare applications of big data.
     - Algorithms used in applications.
     - Software systems for implementation.
   - Course structure alternates among these topics.

** Big Data Big Picture
   - Course focuses on three key areas:
     - Big data systems.
     - Scalable machine learning algorithms.
     - Healthcare applications.
   - Integration of these elements to solve healthcare problems.

** Healthcare Applications
   - Three types of healthcare applications:
     1. **Predictive Modeling**: Uses historical data to predict future outcomes.
     2. **Computational Phenotyping**: Converts electronic health records (EHRs) into meaningful clinical concepts.
     3. **Patient Similarity**: Identifies groups of patients with similar characteristics.
   - Starts with predictive modeling.

** Predictive Modeling Quiz Question
   - Predictive modeling example: predicting treatment effectiveness for epilepsy patients.
   - Quiz: Estimate percentage of patients responding to different treatments.

** Predictive Modeling Quiz Solution
   - Solution breakdown:
     - **Group A** (responded within 2 years): 32%.
     - **Group B** (responded between 2-5 years): 24%.
     - **Group C** (did not respond after 5 years): 44%.
   - Goal of predictive modeling:
     - Improve early response rates.
     - Identify non-responders for alternative treatments.

** Predictive Modeling Challenges
   - Challenges in predictive modeling:
     - Handling large patient datasets with various types of data.
     - Managing multiple models in a complex computational pipeline.
     - Evaluating and comparing multiple predictive pipelines.

** Computational Phenotyping
   - Converts raw patient data (e.g., demographics, diagnoses, medications, procedures, lab tests, clinical notes) into clinical concepts or phenotypes.

** Computational Phenotyping Quiz Question
   - Identifying data issues in phenotyping.
   - Quiz: List potential "waste products" in raw data.

** Computational Phenotyping Quiz Solution
   - Common data issues:
     - **Missing values**: Some important data may be absent.
     - **Duplicates**: Patient records may appear multiple times.
     - **Irrelevant data**: Not all raw information is useful.
     - **Redundant information**: Different records may indicate the same condition (e.g., diagnosis and medication for diabetes).

** Phenotyping Algorithm
   - Example: Identifying Type 2 Diabetes from EHR data.
   - Decision process:
     - Check for Type 1 Diabetes diagnosis.
     - Check for Type 2 Diabetes diagnosis.
     - Verify medication records and abnormal lab results.
   - Importance:
     - EHR data is often unreliable.
     - Multiple data sources improve diagnostic accuracy.

** Patient Similarity Quiz Question
   - Different types of reasoning doctors use:
     - Flowchart-based reasoning (like phenotyping algorithms).
     - Instinct and intuition.
     - Case-based reasoning (comparing patients to past cases).

** Patient Similarity Quiz Solution
   - Correct answer: **Case-based reasoning**.
   - Doctors often compare current patients to previous similar cases.

** Patient Similarity
   - Simulating case-based reasoning using algorithms.
   - Process:
     - Search for similar patients in a database.
     - Identify treatment outcomes for similar cases.
     - Recommend best treatment based on historical data.
   - Objective: Utilize full database knowledge instead of relying on a single doctor's experience.

** Algorithms
   - Introduction to machine learning algorithms in healthcare.
   - Covered topics:
     1. **Classification**: Mapping patient data to target variables (e.g., predicting heart attack risk).
     2. **Clustering**: Grouping similar patients based on health conditions.
     3. **Dimensionality Reduction**: Reducing large patient datasets to essential features.
     4. **Graph Analysis**: Analyzing relationships between patients and diseases.

** Systems
   - Introduction to big data systems for healthcare applications.
   - Covered systems:
     - **Hadoop**: Disk-based distributed system.
     - **Spark**: In-memory distributed system (faster than Hadoop).
   - Topics include:
     - Hadoop infrastructure (MapReduce, HDFS, Pig, Hive, HBase).
     - Spark infrastructure (Spark SQL, Spark Streaming, MLlib, GraphX).

** Summary
   - Recap of three key areas:
     - Healthcare applications.
     - Machine learning algorithms.
     - Big data systems.
   - Course integrates these areas:
     - Example: Using logistic regression on Hadoop to predict heart failure.
   - Next step: Begin applying concepts.

* Predictive Modeling
** Introduction to Predictive Modeling
  - Predictive modeling ::  using historical data to predict future events.

    - Example: Using electronic health records (EHR) to model heart failure
    - Key Goal: Develop a good predictive model using EHR efficiently.

** Predictive Modeling vs EHR
  - Importance: Increased research interest in using EHR for clinical predictive modeling.
  - Data Sources: EHR has become a major source for predictive modeling research.

** Predictive Modeling Pipeline
  - Predictive modeling is a multi-step computational process:
    1. Define the prediction target.
    2. Construct the relevant patient cohort.
    3. Identify potentially relevant features.
    4. Select the most relevant features.
    5. Compute the predictive model.
    6. Evaluate the predictive model.
  - Iterative process until a satisfactory model is obtained.

** Prediction Target
  - Investigators may have many targets, but only some are feasible.
  - Selection criteria: The target should be interesting and possible with the available data.
  - Example: Predicting the onset of heart failure.

** Heart Failure Quiz
  - Question: How many new heart failure cases occur annually in the U.S.?
  - Options: 17,000; 260,000; 550,000; 1,250,000.
  - Answer: 550,000 cases per year.

** Motivations for Early Detection
  - Heart failure is complex with diverse symptoms and subsets.
  - Early detection can:
    - Reduce hospitalization costs.
    - Introduce early interventions to slow progression.
    - Improve clinical guidelines for heart failure prevention.

** Cohort Construction
  - Defines the study population for predictive modeling.
  - Study population is a subset of all patients.
  - Four study designs based on two axes:
    - Prospective vs. Retrospective studies.
    - Cohort vs. Case-Control studies.

** Prospective vs. Retrospective Studies
  - **Prospective Study**: Define cohort first, then collect data.
  - **Retrospective Study**: Use existing data from past records.

** Prospective vs. Retrospective Quiz
  - Comparison of study properties:
    - Retrospective studies have more noise.
    - Prospective studies are more expensive and time-consuming.
    - Retrospective studies can handle larger datasets.

** Cohort Study
  - Includes patients exposed to a particular risk (e.g., heart failure readmission).
  - Defines inclusion and exclusion criteria.

** Case-Control Study
  - Compares patients with a condition (cases) to similar patients without it (controls).
  - Matching criteria: Age, gender, clinic visits.
  - Cases are rarer than controls in real-world data.

** Feature Construction
  - Defines patient features for predicting outcomes.
  - Data sequences from EHR:
    - Events (diagnoses, medications, lab results).
    - Observation window (used for feature extraction).
    - Prediction window (future period to predict).
  - Impact of window sizes on model accuracy.

** Feature Construction Quizzes
  - Quiz 1: Best timeline for modeling → Large observation, small prediction window.
  - Quiz 2: Most useful model → Small observation, large prediction window (ideal but difficult).

** Prediction Performance on Windows
  - **Prediction Window**: Longer window reduces accuracy.
  - **Observation Window**: Longer window improves model performance until plateau.

** Feature Selection
  - Identifies relevant features from EHR data.
  - EHR provides a large number of potential features (e.g., demographics, vitals).
  - Different targets require different feature subsets.

** Predictive Model
  - Maps input features to predicted outcomes.
  - Regression models for continuous targets (e.g., healthcare costs).
  - Classification models for categorical targets (e.g., heart failure).
  - Common methods: Logistic regression, decision trees, random forests.

** Performance Evaluation
  - Key metric: Testing error (not training error).
  - Models must generalize to unseen data.

** Cross-Validation
  - Splits data into training and validation sets.
  - Common methods:
    - Leave-One-Out Cross-Validation.
    - K-Fold Cross-Validation.
    - Randomized Cross-Validation.

** Conclusion
  - Summary of the predictive modeling pipeline:
    - Define the prediction target.
    - Construct the patient cohort.
    - Generate relevant features.
    - Select important features.
    - Train the predictive model.
    - Evaluate model performance.
  - Goal: Design high-level predictive modeling studies using EHR data.

* MapReduce
** Introduction to MapReduce
- Overview of MapReduce as a big data processing tool.
- Utilizes distributed computation and storage.
- Covers:
  - What MapReduce is
  - Fault tolerance in distributed environments
  - Analytical use cases and limitations

** What is MapReduce
- Hadoop/MapReduce is:
  - A programming model for parallel computation
  - An execution environment (Hadoop with HDFS)
  - A software package with various tools
- Capabilities:
  - Distributed storage via HDFS
  - Distributed computation via MapReduce
  - Built-in fault tolerance

** Computational Process
- Originated at Google (2004), open-sourced via Apache Hadoop.
- Java-based platform.
- Programming constrained to Map and Reduce functions for scalability.
- Designed for parallel, fault-tolerant processing.
- Emphasis on mastering computational patterns for analytics.

** Learning Via Aggregation Statistics
- Core design: express ML algorithms as aggregation tasks.
- Example: Heart failure risk factor frequency analysis.
- Map: extract risk factors per patient.
- Reduce: aggregate frequencies across population.
- Leads into deeper abstraction needs and trade-offs.

** MapReduce Abstraction
- Example task: count disease cases from patient records.
- Map:
  - Emit (disease, 1) for each mention
- Shuffle:
  - Group by disease
- Reduce:
  - Sum values for each disease
- Emphasizes two-phase logic for scalability.

** MapReduce System
- Real-world data is too large for single machines.
- Data is partitioned and processed by multiple mappers.
- Intermediate results are shuffled and passed to reducers.
- Final results computed from reduce function.
- Three stages:
  - Map
  - Shuffle
  - Reduce

** MapReduce Fault Recovery
- Fault tolerance is a built-in system feature.
- On failure:
  - Mappers/reducers are restarted with minimal recomputation.
- Goal: Only failed components are recomputed.
- System handles all failure recovery.

** Distributed File Systems
- HDFS: Hadoop's storage system.
- Splits large files into partitions.
- Partitions stored on multiple machines with redundancy.
- Benefits:
  - Faster concurrent access
  - Fault tolerance (recover from worker failures)

** MapReduce Design Choice
- Design principle: minimal functionality for reliability and scalability.
- Restricted computation model (e.g., aggregation queries).
- Map: operate on individual records
- Reduce: aggregate results
- Supports straggler mitigation (slow mappers duplicated)

** Analytics with MapReduce
- Application examples:
  - K-Nearest Neighbors (KNN)
  - Linear Regression

** MapReduce KNN
- KNN implementation:
  - Map:
    - Find K nearest neighbors per partition
  - Reduce:
    - Combine local results to find global nearest neighbors
- Patient data partitioned, processed in parallel

** Linear Regression
- Goal: map patient features to heart disease risk
- Normal equation:
  - β = (XᵗX)⁻¹Xᵗy
- MapReduce:
  - Map f1: compute xi * xiᵗ
  - Map f2: compute xi * yi
  - Reduce: aggregate sums

** MapReduce for Linear Regression Quiz Question
- Quiz: specify map/reduce pseudo code for computing Xᵗy

** MapReduce for Linear Regression Quiz Solution
- Solution:
  - Map: compute xi * yi
  - Reduce: aggregate results

** Limitations of MapReduce
- Example: Logistic Regression via Gradient Descent
- Challenge: Requires iterative computation
  - Each iteration loads data twice
- MapReduce not efficient for:
  - Iterative, multi-stage computation

** MapReduce Summary Quiz Question
- Quiz on ideal conditions for MapReduce:
  - Single vs. multiple passes
  - Skewed vs. uniform key distribution
  - Synchronization needs

** MapReduce Summary Quiz Solution
- Best suited for:
  - Single-pass jobs (e.g., histograms)
  - Both skewed and uniform key distributions
  - Minimal synchronization (only between Map and Reduce phases)
* Classification Model Metrics
** Predictive Model review
- Predictive model :: mapping function between model inputs and outputs (pred's)
- Model metrics :: needed to know how well they're performing
** Confusion matrix
- True positive/negative also known as "Condition" positive/negative
- Predicted positive/negative also known as "Prediction Outcome" positive/negative
** Accuracy metrics
All divided by ground truth values.
- Accuracy :: (TP+TN)/TP. Not most useful for imbalanced class
- True positive rate :: TP/CP (Sensitivity, Recall)
- False positive rate :: FP/CN
- False negative :: FN/CP
- True negative rate :: TN/CN (Specificity)
*** Other notes
- FP is a type I error.
- FN is a type II error.
- Hard to perform well on all metrics
- Important to choose the right metrics
** Predictive metrics
All divided by prediction outcomes.
- Prevalence :: CP/Total population. How likely disease occurs in population
- Positive predictive value (Precision) :: TP/Pred outcome positive.
- False discovery rate :: FP/Pred outcome positive
- Negative predictive value :: TN/Pred outcome negative
- False omission rate :: FN/Pred outcome negative
** F1 score
Harmonic mean of PPV and TPR
$$
F_1 = 2 \times \frac{PPV \times TPR}{PPV + TPR}
$$
** Classifier quiz
Which is the best classifier?
- Highest F1, PPV and Accuracy is the best classifier
** Reversing predictions
It's always possible to reverse the predictions so 0.21 might perform better than 0.69/0.50.
** Receiver operating characteristic
- Predictive models generally output continuous score.
- Threshold is needed as precision bound to force to a certain category
- ROC provides a way to compare performance of classifiers as the decision boundary is varied
- ROC curve is the plot of TP rate vs FP rate at various threshold values
  - We sort by prediction score (highest first)
  - Use prediction score as threshold values
  - Plot on the chart and see how many are misclassified (needs True value to be known)
- AUROC does not depend on the choice of threshold.
- AUROC is the most popular metric for classification
** "Best classifier threshold" quiz
- There isn't a standard answer, it depends on whether TP, TN or other metric is prioritized
** Regression metrics
- MAE :: average of absolute errors, harder to work with since the absolute value is not differentiable
- MSE :: average of squared errors, easier to work with as the derivative of squared term is linear. Increases a lot faster than MAE
- $R^2$ :: bounded by $(-\infty,1)$. AKA coefficient of determination.
*** $R^2$
$$
R^2 = 1-\frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i(y_i - \bar{y})^2}
$$
i.e. 1-MSE/Variance

Negative $R^2$ values means they perform worse than a simple average of raw data.
As noise increases, $R^2$ decreases.
* Ensemble methods
** Introduction to ensemble methods
- Algorithms used in Big Data, e.g. SGD
- Ensemble methods
- Bias, variance, tradeoff
- Bagging
- Boosting
** Gradient descent for classification
- Basic optimization method
- Procedure for gradient descent
  1. Training dataset $D$ has $n$ pairs of $(x,y)$ data points
  2. Outputs are some parameters
  3. Find joint probability / likelihood $P(D|\theta)$
  4. Find gradient g = $dp(D|\theta)\over d\theta$
  5. Update gradient

Notes:
- Other methods can be used as long as gradient can be calculated.
- These can even be put in as "black box" inputs (by just plugging in the gradient computation as the function)
** GDM for linear regression
-
