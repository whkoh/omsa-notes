<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-03-30 Sat 15:11 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ISYE 7406: Data Mining and Statistical Learning</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ISYE 7406: Data Mining and Statistical Learning</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgee850bd">1. Bootstrapping algorithm</a>
<ul>
<li><a href="#orgece2f32">1.1. Motivating example</a></li>
<li><a href="#org2a81047">1.2. Idea in Bootstrapping algorithm</a></li>
<li><a href="#orgbe178bd">1.3. Resample with replacement</a></li>
<li><a href="#org07ba9f3">1.4. New use of bootstrapping</a></li>
<li><a href="#orgf3c058a">1.5. Bootstrapping in R</a></li>
<li><a href="#orgd253c64">1.6. More efficient implementation</a></li>
</ul>
</li>
<li><a href="#org13cc5f6">2. Bagging algorithm</a>
<ul>
<li><a href="#orgd4012bc">2.1. Improve tree methods</a></li>
<li><a href="#org184a760">2.2. Variance reduction</a></li>
<li><a href="#orgae3145c">2.3. Ideas in bagging</a></li>
<li><a href="#orgb5280a7">2.4. Bagging algorithm</a></li>
<li><a href="#org4022b78">2.5. Remarks on bagging</a></li>
<li><a href="#orgb7a57ef">2.6. Theoretical issues</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgee850bd" class="outline-2">
<h2 id="orgee850bd"><span class="section-number-2">1.</span> Bootstrapping algorithm</h2>
<div class="outline-text-2" id="text-1">
<p>
9.1.3
</p>
</div>
<div id="outline-container-orgece2f32" class="outline-3">
<h3 id="orgece2f32"><span class="section-number-3">1.1.</span> Motivating example</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Data: \(z = (z_1, ..., z_n)\) where \(z_i = (Y_i, x_{i1}, ..., x_{ip}), i=1,...,n\)</li>
<li>Parameter estimation: we derived real-valued summary statistics
\(S(z) = S(z_1, ..., z_n)\) which is en estimator of population parameter \(\theta\) (e.g. mean, median, correlation coefficient, regression coefficient, etc.)</li>
<li>Objective: derive a <b><b>robust</b></b> estimator of the confidence interval of \(\theta\) or the standard error of \(S(z)\)</li>
<li>Challenges: we don't know the distribution of data, and are unable to obtain additional training datasets</li>
</ul>
</div>
</div>
<div id="outline-container-org2a81047" class="outline-3">
<h3 id="org2a81047"><span class="section-number-3">1.2.</span> Idea in Bootstrapping algorithm</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>Intuitive idea:
<ul class="org-ul">
<li>Estimate the standard error of \(S(z) = S(z_1, ..., z_n)\) based on the sample standard deviation if we have many values of \(S(z)\) or many independent copies of training data</li>
<li><span style='background-color: #FFFF00;'>Resample with replacement</span> from the original training dataset (\(z= z_1, z_2, ..., z_n\)) to generate many copies of "new" training dataset. This allows us to compute many values of \(S(z)\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgbe178bd" class="outline-3">
<h3 id="orgbe178bd"><span class="section-number-3">1.3.</span> Resample with replacement</h3>
<div class="outline-text-3" id="text-1-3">

<div id="org73cd001" class="figure">
<p><img src="./img/bootstrap-resample.png" alt="bootstrap-resample.png" />
</p>
</div>
<ul class="org-ul">
<li>High level:
<ul class="org-ul">
<li>Input Data: \(z = (z_1, ..., z_n)\) where \(z_i = (Y_i, x_{i1}, ..., x_{ip}), i=1,...,n\) and estimator of \(S(z)\) for \(\theta\)</li>
<li>For \(b=1,...,B\)
<ul class="org-ul">
<li><b><b>Sample with replacement</b></b> to get bootstrap sample \(z^{*b} = (z_1^{*b}, ..., z_n^{*b})\)</li>
<li>Compute the value \(S(z^{*b}) = S(z_1^{*b}, ..., z_n^{*b})\) for this bootstrap sample</li>
</ul></li>
<li>Once the \(B\) values of \(S(z^{*b})\) have been computed,
<ul class="org-ul">
<li>The quantiles of \(S(z^{*b})\)'s provide an empirical distribution of \(S(z)\)</li>
<li>They can be used to provide confidence intervals of \(\theta\)</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org07ba9f3" class="outline-3">
<h3 id="org07ba9f3"><span class="section-number-3">1.4.</span> New use of bootstrapping</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>Bootstrapping was originally developed by Efron (1979) to estimate the variance in the classical statistics context</li>
<li>Breiman (1996) found a surprising used of bootstrapping to improve the predictive performance of tree-based method and model:
<ul class="org-ul">
<li>Bagging (bootstrap aggregating)</li>
<li>Random forest</li>
</ul></li>
<li>Since then, bootstrapping algorithm has been used to combine other algorithms in machine learning and data mining</li>
</ul>
</div>
</div>
<div id="outline-container-orgf3c058a" class="outline-3">
<h3 id="orgf3c058a"><span class="section-number-3">1.5.</span> Bootstrapping in R</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>Given vector \(Z\), how to write your own functions to generate the bootstrap sample in R?</li>
<li>Naively:
<ol class="org-ol">
<li>Extract sample size of <code>Z</code> with <code>length()</code> and store it in <code>n</code></li>
<li>Copy <code>Z</code> to <code>sample</code></li>
<li>Draw randomly an integer between 1 and n with <code>ceiling(runif())</code></li>
<li>Corresponding value of <code>Z</code> is extracted and stored in <code>sample</code> which is finally returned</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgd253c64" class="outline-3">
<h3 id="orgd253c64"><span class="section-number-3">1.6.</span> More efficient implementation</h3>
<div class="outline-text-3" id="text-1-6">
<ul class="org-ul">
<li>Avoid loops to generate data index simultaneously</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org13cc5f6" class="outline-2">
<h2 id="org13cc5f6"><span class="section-number-2">2.</span> Bagging algorithm</h2>
<div class="outline-text-2" id="text-2">
<p>
9.2.1
</p>
</div>
<div id="outline-container-orgd4012bc" class="outline-3">
<h3 id="orgd4012bc"><span class="section-number-3">2.1.</span> Improve tree methods</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Observed (training) dataset: \((Y_i, x_{i1}, ..., x_{ip})\) for \(i=1,...,n\)</li>
<li>Objective: find an function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset</li>
<li>Tree-based method is easy to interpret but might have poor predictive performance due to <span style='background-color: #FFFF00;'>high variance</span></li>
<li>Can we improve the tree-based model and methods?</li>
</ul>
</div>
</div>
<div id="outline-container-org184a760" class="outline-3">
<h3 id="org184a760"><span class="section-number-3">2.2.</span> Variance reduction</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>In statistics, sample mean can reduce the variance:
if \(U_1, ..., U_m\) are i.i.d. with variance \(\sigma^2\) then the sample mean \(\bar{U} = \frac{U_1+...+U_m}{m}\) has the variance \(\frac{\sigma^2}{m}\)</li>
<li>Thus we can reduce the variance of the tree-based method if we can use the average of predictions from several independent trees</li>
<li>This would require us to have <b><b>several independent</b></b> datasets, but in most applications, we only have 1 training dataset</li>
</ul>
</div>
</div>
<div id="outline-container-orgae3145c" class="outline-3">
<h3 id="orgae3145c"><span class="section-number-3">2.3.</span> Ideas in bagging</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Bagging (bootstrap aggregating) was proposed by Breiman (1994), and is a popular ensemble method.
</p>

<p>
Key ideas:
</p>
<ul class="org-ul">
<li>Use the training dataset to generate many bootstrapping samples</li>
<li>Use each bootstrap sample to build a distinct tree</li>
<li>Our final prediction is the <span style='background-color: #FFFF00;'>average</span> of predictions from these bootstrapping trees, which will have smaller variance and thus lead to better predictive performance</li>
</ul>
</div>
</div>
<div id="outline-container-orgb5280a7" class="outline-3">
<h3 id="orgb5280a7"><span class="section-number-3">2.4.</span> Bagging algorithm</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>Given training dataset, fit a base model (e.g. Tree) that predicts the response as \(\hat{h}(x)\) at the new datapoint \(x\).</li>
<li>For \(b=1,...,B\):
<ol class="org-ol">
<li>Draw a bootstrap sample from the training dataset</li>
<li>Fit a model \(\hat{h}^{*b} (x)\) to each bootstrap sample</li>
<li>Then the bagging prediction at the new datapoint \(x\) for regression:
\[
     \widehat{\boldsymbol{h}}_{bag} ( \boldsymbol{x} )=\frac{1} {B} \sum_{b=1}^{B} \widehat{\boldsymbol{h}}^{* b} ( \boldsymbol{x} )
     \]
<ul class="org-ul">
<li>for classification: majority vote</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org4022b78" class="outline-3">
<h3 id="org4022b78"><span class="section-number-3">2.5.</span> Remarks on bagging</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>Bagging can improve a good, but unstable procedure (e.g. Tree, neural networks, best subset selections in linear regression, etc.)</li>
<li>Bagging generally reduces higher-order variation: if one uses a decomposition into linear and higher-order terms, bagging affects the variability of higher-order terms</li>
<li>Bagging <span style='background-color: #FFFF00;'>re-used the data</span> to get more information in them but at the same time introduces a new variability, that of the model. Strangely, more variability can be an improvement at times.</li>
</ul>
</div>
</div>
<div id="outline-container-orgb7a57ef" class="outline-3">
<h3 id="orgb7a57ef"><span class="section-number-3">2.6.</span> Theoretical issues</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>Given a training dataset of \(n\) distinct observations, and suppose we generate a bootstrap sample,
<ul class="org-ul">
<li>What is the chance of a given observation not being selected in the bootstrap sample?</li>
</ul></li>
</ul>
<p>
\[
p_{1}=\frac{( n-1 )^{n}} {n^{n}}=\left( 1-\frac{1} {n} \right)^{n} \ \ \to\ \frac{1} {e}=3 6. 8 9 / \ \ a s \, n \to\infty
\]
</p>

<ul class="org-ul">
<li><p>
Given a training dataset of \(n\) distinct observations, suppose there are \(B=100\) bootstrap samples
</p>
<ul class="org-ul">
<li>Let \(W\) be the number of bootstrap samples that do not include the given observation. What is the distribution of \(W\)?</li>
</ul>
<p>
\[
  \text{Binomial}(B=100, p=p1 \approx \frac{1}{e})
  \]
</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2024-03-30 Sat 15:11</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
