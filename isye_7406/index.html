<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-02-07 Wed 07:31 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ISYE 7406: Data Mining and Statistical Learning</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ISYE 7406: Data Mining and Statistical Learning</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0bac7e0">1. Week 3: Linear Regression (II)</a>
<ul>
<li><a href="#orgecf6280">1.1. James-Stein Estimator</a>
<ul>
<li><a href="#org5d95848">1.1.1. It's a special case of linear regression</a></li>
<li><a href="#org3948bcf">1.1.2. Simultaneous estimation</a></li>
<li><a href="#orga3dbdbf">1.1.3. JS estimator</a></li>
<li><a href="#org94e8ebf">1.1.4. Baseball example</a></li>
<li><a href="#org962aeb7">1.1.5. Comparing MLE and JS</a></li>
</ul>
</li>
<li><a href="#org6a29547">1.2. Shrinkage Methods</a>
<ul>
<li><a href="#orge0fbab4">1.2.1. Setting up shrinkage method</a></li>
<li><a href="#orgc491f6d">1.2.2. Alternative formulation</a></li>
<li><a href="#orgc68fad3">1.2.3. Bayesian interpretation</a></li>
<li><a href="#orgefebd8a">1.2.4. Choices of priors</a></li>
<li><a href="#org6173cbe">1.2.5. Ridge regression</a></li>
<li><a href="#org41aef01">1.2.6. LASSO estimator</a></li>
</ul>
</li>
<li><a href="#orgf63e542">1.3. Ridge Regression</a>
<ul>
<li><a href="#org8854fcf">1.3.1. Ridge Regression Estimator</a></li>
<li><a href="#orgc2fe815">1.3.2. Mathematical solution</a></li>
<li><a href="#org2bf9c45">1.3.3. Properties of Ridge Regression</a></li>
<li><a href="#orgd418b21">1.3.4. Computational issues</a></li>
<li><a href="#org9d0e07e">1.3.5. Example of SVD</a></li>
<li><a href="#org3eec225">1.3.6. SVD Example (I): \(XX^T\)</a></li>
<li><a href="#org4acb162">1.3.7. SVD Example (II): \(X^T X\)</a></li>
<li><a href="#org517ff30">1.3.8. SVD verification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org0bac7e0" class="outline-2">
<h2 id="org0bac7e0"><span class="section-number-2">1.</span> Week 3: Linear Regression (II)</h2>
<div class="outline-text-2" id="text-1">
<p>
M2T2
</p>
</div>
<div id="outline-container-orgecf6280" class="outline-3">
<h3 id="orgecf6280"><span class="section-number-3">1.1.</span> James-Stein Estimator</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-org5d95848" class="outline-4">
<h4 id="org5d95848"><span class="section-number-4">1.1.1.</span> It's a special case of linear regression</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
in LR model with
\[
Y_{n\times 1} = X_{n\times p} \beta_{p\times 1} + \epsilon_{n\times 1}, \text{s.t.} \epsilon \sim N(0, \sigma^2 I_{n\times n})
\]
</p>

<p>
Special case:
</p>
<ul class="org-ul">
<li>\(n=p\)</li>
<li>\(X_{n\times p} = I_{p\times p}\)</li>
</ul>

<p>
OLS yields the estimator of:
</p>

<p>
\[
\hat{\beta_{ols}} = (X^T X)^{-1} X^T Y =
(I^T_{p\times p} I_{p\times p})^{-1}
I^T_{p\times p} Y_{p\times 1} =
\bf{Y_{p\times 1}}
\]
</p>

<p>
When \(\bf{p\ge 3}\), is it possible to do better than OLS?
</p>
</div>
</div>
<div id="outline-container-org3948bcf" class="outline-4">
<h4 id="org3948bcf"><span class="section-number-4">1.1.2.</span> Simultaneous estimation</h4>
<div class="outline-text-4" id="text-1-1-2">
<ul class="org-ul">
<li><p>
Problem of estimating \(p\) # of parameters \(\beta_i\)'s simultaneously from \(p\) observations (\(Y_i\)'s) under model:
</p>

<p>
\[
  \bf{Y_i} \sim N(\beta_i, \sigma^2), \text{for }i = 1, 2, ..., p
  \]
</p></li>
<li><p>
OLS (a.k.a. Maximum Likelihood Estimator, MLE) yields estimator:
</p>

<p>
\[
  \hat{\beta_i} = Y_i \text{ for }i = 1, 2, ..., p
  \]
</p>

<p>
Is it possible to do better here?
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orga3dbdbf" class="outline-4">
<h4 id="orga3dbdbf"><span class="section-number-4">1.1.3.</span> JS estimator</h4>
<div class="outline-text-4" id="text-1-1-3">
<ul class="org-ul">
<li><p>
Showed MLS/MLE estimator inadmissible for \(p\ge 3\); dominated by JS estimator.
</p>

<p>
\[
  \hat{\beta_i^{MLE}} = Y_i \text{ for }i = 1, 2, ..., p
  \]
</p>

<p>
\[
  \hat{\beta_i^{(JS)}} = w Y_i + (1-w) \bar{Y} \text{ for }i = 1, 2, ..., p;
  \]
</p>

<p>
\[
  w = 1 - \frac{(p-3)\sigma^2}{\sum^p_i(Y_i-\bar{Y})^2}
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org94e8ebf" class="outline-4">
<h4 id="org94e8ebf"><span class="section-number-4">1.1.4.</span> Baseball example</h4>
<div class="outline-text-4" id="text-1-1-4">
<ul class="org-ul">
<li>Observe \(Y_1, Y_2, ... Y_p\) batting averages (where \(Y_i\) is the batting average for p=18 players), 45 AB</li>
<li>"True" values \(\mu_i\) are the averages over remainder of seasons, 370 AB</li>
<li>Qn: how to predict season averages \(\mu_i\) from early statistics \(Y_i\)?</li>
<li>Estimators: MLE and JS</li>
</ul>
</div>
</div>
<div id="outline-container-org962aeb7" class="outline-4">
<h4 id="org962aeb7"><span class="section-number-4">1.1.5.</span> Comparing MLE and JS</h4>
<div class="outline-text-4" id="text-1-1-5">
<ul class="org-ul">
<li>JS has lower predictive squared error than MLS (by 50%)</li>
<li>JS estimator is <b><b>shrinkage</b></b> estimator.
<ul class="org-ul">
<li>Each MLE value shrunken towards <b><b>grand mean</b></b></li>
<li>Data-based estimator, compromises between:
<ul class="org-ul">
<li>null hypothesis: all means the same</li>
<li>MLE assumption: no relationship between all \(\mu_i\) values</li>
</ul></li>
<li>Difficult to estimate \(p\ge 3\) parameters simultaneously.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6a29547" class="outline-3">
<h3 id="org6a29547"><span class="section-number-3">1.2.</span> Shrinkage Methods</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>Estimation in linear regression: JS works in only specific cases (<a href="#org5d95848">1.1.1</a>) when \(p\ge 3\)</li>
<li>How to do better <i>generally</i>?</li>
<li>Shrinkage methods (penalized, regularized)
<ul class="org-ul">
<li>Based on subtracting penalty from log-likelihood</li>
<li>Penalty is a function of <i>decay parameter</i></li>
<li>Sort the variables to be included by size of <i>decay parameter</i></li>
<li>This reduces to a nested case</li>
<li>After estimating <i>decay parameter</i>, variable or model selection is complete!</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orge0fbab4" class="outline-4">
<h4 id="orge0fbab4"><span class="section-number-4">1.2.1.</span> Setting up shrinkage method</h4>
<div class="outline-text-4" id="text-1-2-1">
<ul class="org-ul">
<li>Needs: \(Y_1, x_{11}, x_{12}, ..., x_{1,p}, \text{such that } i = 1,2,...,n\)</li>
<li>Assume all X &amp; Y are standardized i.e.:
\[
  \sum^n_{i=1}Y_i = 0,
  \sum^n_{i=i}x_{ij} = 0,
  \sum^n_{i=1}x^2_{ij} = 1
  \]</li>
<li><p>
If not standardized, do linear transformations:
\[
  Y^{*}_i = Y_i - \bar{Y}
  \]
</p>

<p>
\[
  x^{*}_{ij} = \frac{x_{ij}-\bar{x_j}}{\sqrt{\text{Var}_j}}
  \]
</p></li>
<li>With this assumption, \(\beta_0 = 0\) in the model, i.e.
\[
  Y_i = \bf{0 } \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{i,p} + \epsilon_i
  \]</li>
<li><p>
<b>The shrinkage method solves this optimization problem</b>
</p>

<p>
\[
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  \]
</p>
<dl class="org-dl">
<dt>penalty function</dt><dd>\(J(|\beta_j)\)</dd>
<dt>decay or tuning parameter</dt><dd>\(\lambda \ge 0\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-orgc491f6d" class="outline-4">
<h4 id="orgc491f6d"><span class="section-number-4">1.2.2.</span> Alternative formulation</h4>
<div class="outline-text-4" id="text-1-2-2">
<ul class="org-ul">
<li>Shrinkage method solves the <b>unconstrained</b> optimization problem
\[
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  \]</li>
<li><p>
Alternative formulation solves a <b>constrained</b> optimization problem
</p>

<p>
\[
  \min_{\beta} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2, \text{ subject to: }
  \sum^p_{j=1} J(|B_j|) \le s
  \]
</p>
<dl class="org-dl">
<dt>tuning parameter</dt><dd>\(s \gt 0\)</dd>
</dl></li>

<li>The alternative formulation may greatly facilitate computation <b>at times</b>, e.g. in LASSO which is piecewise linear in \(s\).</li>
</ul>
</div>
</div>
<div id="outline-container-orgc68fad3" class="outline-4">
<h4 id="orgc68fad3"><span class="section-number-4">1.2.3.</span> Bayesian interpretation</h4>
<div class="outline-text-4" id="text-1-2-3">
<ul class="org-ul">
<li>For LR model (<a href="#org5d95848">1.1.1</a>):
<dl class="org-dl">
<dt>prior on &beta;</dt><dd>\(\pi(\beta)\)</dd>
<dt>independent prior on \(\sigma^2\)</dt><dd>\(\pi(\sigma^2)\)</dd>
<dt>posterior for \((\beta, sigma^2)\)</dt><dd>proportional to
\[
    \pi (\sigma^2)(\sigma^2)^{(n-1)/2}\exp\{-\frac{1}{2\sigma^2} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \log \pi(\beta) \}
    \]</dd>
</dl></li>
<li><p>
<b>Posterior maximization method</b> yields shrinkage estimator
</p>

<p>
\[
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \textbf{Pen}(\beta)
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgefebd8a" class="outline-4">
<h4 id="orgefebd8a"><span class="section-number-4">1.2.4.</span> Choices of priors</h4>
<div class="outline-text-4" id="text-1-2-4">
<p>
i.e. choice of prior \(\pi(\beta)\)
</p>
<dl class="org-dl">
<dt>Normal prior</dt><dd>yields <b>ridge regression</b> etsimator</dd>
<dt>Laplace prior</dt><dd>yields <b>LASSO</b> estimator</dd>
</dl>
</div>
</div>
<div id="outline-container-org6173cbe" class="outline-4">
<h4 id="org6173cbe"><span class="section-number-4">1.2.5.</span> Ridge regression</h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
<b>Normal prior</b> assumes \(\beta_1 ... \beta_p\) are i.i.d. \(N(0, \tau^2)\) with prior density
\[
\pi(\beta) = \prod^p_{i=1} \frac{1}{\sqrt{2\pi}\tau} \exp\left
(-\frac{1}{2\tau^2}\beta_i^2\right)
\]
</p>

<p>
Yields <b>ridge regression</b> estimator, which minimizes
</p>

<p>
\[
\parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
\]
</p>
</div>
</div>
<div id="outline-container-org41aef01" class="outline-4">
<h4 id="org41aef01"><span class="section-number-4">1.2.6.</span> LASSO estimator</h4>
<div class="outline-text-4" id="text-1-2-6">
<ul class="org-ul">
<li><b>Laplace Prior</b>, assume  \(\beta_1 ... \beta_p\) are i.i.d. double-exponential (Laplace) \(\sim \text{Lapalce} (,\tau)\) with prior density
\[
  \pi(\beta) = \prod^p_{i=1} \frac{1}{2\tau} \exp \left(- \frac{1}{\tau} |\beta_i| \right)
  \]</li>
<li><p>
Yields <b>LASSO</b> estimator that minimizes
</p>

<p>
\[
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}|\beta_i|
  \]
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf63e542" class="outline-3">
<h3 id="orgf63e542"><span class="section-number-3">1.3.</span> Ridge Regression</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-org8854fcf" class="outline-4">
<h4 id="org8854fcf"><span class="section-number-4">1.3.1.</span> Ridge Regression Estimator</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
Assume these are observed: \(Y_i, x_{i1}, ..., x_{ip}\), and all are standardized:
\[
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
\]
</p>

<p>
In linear regression model without intercepts (<a href="#org5d95848">1.1.1</a>)
</p>

<p>
The ridge regression estimator is defined as:
</p>

<p>
\[
\hat{\beta^{\text{ridge}}} = \min_{\beta}
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
\]
</p>
</div>
</div>
<div id="outline-container-orgc2fe815" class="outline-4">
<h4 id="orgc2fe815"><span class="section-number-4">1.3.2.</span> Mathematical solution</h4>
<div class="outline-text-4" id="text-1-3-2">
<ul class="org-ul">
<li>Explicit expression is thus
\[
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  \]</li>
<li>Ridge regression <b>estimator or prediction</b>:
\[
  \hat{Y}^{\text{ridge}} = X_{n\times p}  \hat{\beta^{\text{ridge}}}
  \]</li>
<li>Requires <b>choosing</b> the tuning parameter \(\lambda\), based on data, usually <b>by cross-validation</b></li>
</ul>
</div>
</div>
<div id="outline-container-org2bf9c45" class="outline-4">
<h4 id="org2bf9c45"><span class="section-number-4">1.3.3.</span> Properties of Ridge Regression</h4>
<div class="outline-text-4" id="text-1-3-3">
<ul class="org-ul">
<li>Ridge regression <b>most useful</b> when \(X_{n\times p}\) is <b>non-singular</b>, but has <b>high collinearity</b>
<ul class="org-ul">
<li>i.e. \(X^T_{n\times p} X_{n\times p}\) has eigenvalue close to 0</li>
</ul></li>
<li>\(\hat{\beta^{\text{ridge}}}\) is biased, with bias \(\rightarrow\) 0 as \(\lambda \rightarrow 0\)</li>
<li>As \(\lambda\) increases, \(\hat{\beta^{\text{ridge}}}\) \(\rightarrow 0\), though rarely = 0.</li>
<li>Despite the bias, \(\text{Var}(\hat{\beta^{\text{ridge}}})\) will usually be smaller than OLS
<ul class="org-ul">
<li>Therefore better prediction than OLS.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd418b21" class="outline-4">
<h4 id="orgd418b21"><span class="section-number-4">1.3.4.</span> Computational issues</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li>How to compute ridge regression efficiently for any \(\lambda\)?
\[
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  \]</li>
<li>It is highly non-trivial to compute the inverse of a large \(p\times p\) matrix.</li>
<li><b>Singular Value Decomposition</b> (SVD) algorithm:
<ul class="org-ul">
<li>Write the matrix \(X_{n\times p}\) in its SVD form
\[
    X_{n\times p} = U_{n\times p} D_{p\times p} V^T_{p\times p}
    \]
where: \(U\) and $V are orthogonal; D = diag(\(d_1, ..., d_p\)) is diagonal.</li>
</ul></li>
<li>Then: ridge regression estimator becomes the matrix product:
\[
   \hat{\beta^{\text{ridge}}} = V_{p\times p} \text{diag} \left(\frac{d_1}{d^2_1 + \lambda}, ..., \frac{d_p}{d^2_p + \lambda} \right) U^T_{p\times n} Y_{n\times 1}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org9d0e07e" class="outline-4">
<h4 id="org9d0e07e"><span class="section-number-4">1.3.5.</span> Example of SVD</h4>
<div class="outline-text-4" id="text-1-3-5">
<ul class="org-ul">
<li>Find SVD of matrix
\[
  X_{3 \times 2}
  = \begin{pmatrix}
  1 & 0 \\
  0 & 1 \\
  1 & 1 \\
  \end{pmatrix}
  = U_{n\times p} D_{p\times p} V^T_{p\times p}
  \]</li>
<li>Steps (required: \(p \leq n\)):
<ol class="org-ol">
<li>\(U_{n\times p}\) is the normalized \(p\) (largest) eigenvectors of \(XX^T\)</li>
<li>\(V_{p\times p}\) is the normalized eigenvectors of \(X^T X\)</li>
<li>Matrix \(D = \text{diag}(d_1, ..., d_p)\) with \(d_j\) being the square root of \(p\) (largest) eigenvalues of \(XX^T\) or \(X^T X\).</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org3eec225" class="outline-4">
<h4 id="org3eec225"><span class="section-number-4">1.3.6.</span> SVD Example (I): \(XX^T\)</h4>
<div class="outline-text-4" id="text-1-3-6">
<p>
For matrix <a href="#org9d0e07e">1.3.5</a>:
we have
\[
XX^T = \begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 1 \\
1 & 1 &2\end{pmatrix}
\]
</p>
<ul class="org-ul">
<li>Characteristic polynomial is
\(-\lambda^3 + 4\lambda^2 - 3\lambda = -\lambda(\lambda-1)(\lambda-3)\)</li>
<li>The eigenvalues of \(XX^T\) are \(\lambda = 3, 1, 0\)</li>
<li>Corresponding eigenvectors are:
\[
  u'_1 = \begin{pmatrix}
  1 \\
  1 \\
  2\end{pmatrix},
  u'_2 = \begin{pmatrix}
  1 \\
  -1 \\
  0\end{pmatrix},
  u'_3 = \begin{pmatrix}
  1 \\
  1 \\
  -1\end{pmatrix},
  \]</li>
<li>Normalizing yields
\[
  U_{3\times 2} = \begin{pmatrix}
  1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
  1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
  2\over{\sqrt{6}} & 0 \end{pmatrix},
  \]</li>
<li>\(d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1\)</li>
</ul>
</div>
</div>

<div id="outline-container-org4acb162" class="outline-4">
<h4 id="org4acb162"><span class="section-number-4">1.3.7.</span> SVD Example (II): \(X^T X\)</h4>
<div class="outline-text-4" id="text-1-3-7">
<p>
For matrix <a href="#org9d0e07e">1.3.5</a>:
\[
X^T X
= \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix}
\]
</p>

<ul class="org-ul">
<li>Characteristic polynomial is
\(\lambda^2 - 4\lambda + 3 = (\lambda -1)(\lambda -3)\)</li>
<li>Eigenvalues of \(XX^T\) are: \(\lambda = 3, 1\).</li>
<li>Corresponding eigenvalues are:
\[
  v'_1 = \begin{pmatrix}
  1 \\
  1\end{pmatrix},
  v'_2 = \begin{pmatrix}
  1 \\
  -1\end{pmatrix}
  \]</li>
<li>Normalizing them yields:
\[
  V_{2\times 2} = (v_1, v_2) =
  \begin{pmatrix}
  1\over\sqrt{2} & 1\over\sqrt{2} \\
  1\over\sqrt{2} & -1\over\sqrt{2}\end{pmatrix},
  \]</li>
<li>\(d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1\)</li>
</ul>
</div>
</div>
<div id="outline-container-org517ff30" class="outline-4">
<h4 id="org517ff30"><span class="section-number-4">1.3.8.</span> SVD verification</h4>
<div class="outline-text-4" id="text-1-3-8">
<p>
Might need to multiply some eigenvectors by -1.
</p>

<p>
\[
X_{n\times p} =
\]
</p>

\begin{pmatrix}
1 & 0 \\
0 & 1 \\
1 & 1\end{pmatrix}

<p>
\(=\)
</p>


\begin{pmatrix}
1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
2\over{\sqrt{6}} & 0\end{pmatrix}
\begin{pmatrix}
\sqrt{3} & 0 \\
0 & 1\end{pmatrix}
\begin{pmatrix}
1\over{\sqrt{2}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{2}} & -1\over{\sqrt{2}}\end{pmatrix}

<p>
\(= U_{n\times p} D_{p\times p}V^T_{p\times p}\)
</p>

<p>
\[
= \lambda_1 u_1 v^T_1 + \lambda_2 u_2 v^T_2 =
\]
</p>

<p>
0.5
</p>
\begin{pmatrix}
1 & 1 \\
1 & 1 \\
2 & 2 \end{pmatrix} +
0.5
\begin{pmatrix}
1 & -1 \\
-1 & 1 \\
0 & 0 \end{pmatrix}
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2024-02-07 Wed 07:31</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
