<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-04-04 Thu 22:02 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ISYE 7406: Data Mining and Statistical Learning</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ISYE 7406: Data Mining and Statistical Learning</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb5b536d">1. Week 12: Advanced Supervised Learning</a>
<ul>
<li><a href="#org3b55367">1.1. Introduction to Support Vector Machines</a>
<ul>
<li><a href="#orgbfabdd5">1.1.1. Binary classification problem</a></li>
<li><a href="#orgef4212e">1.1.2. Toy motivating example</a></li>
<li><a href="#orgf74e79b">1.1.3. Good separate line?</a></li>
<li><a href="#org29f8e53">1.1.4. Poor separation</a></li>
<li><a href="#orgcff4e1e">1.1.5. Margin maximization</a></li>
<li><a href="#org4ad13ee">1.1.6. Distance in 2 dimensions</a></li>
<li><a href="#orgd6044bc">1.1.7. Distance from point to hyperplane</a></li>
<li><a href="#orga464f19">1.1.8. Distance between hyperplanes</a></li>
</ul>
</li>
<li><a href="#org5f99ca6">1.2. Maximum Margin Optimization for SVM</a>
<ul>
<li><a href="#orgae8ff5e">1.2.1. Linear classification</a></li>
<li><a href="#org3bf9955">1.2.2. Linear SVM</a></li>
<li><a href="#org0da973f">1.2.3. Margin computation</a></li>
<li><a href="#org82ed34a">1.2.4. Maximum margin optimization</a></li>
<li><a href="#org131596a">1.2.5. Constraints for SVM</a></li>
<li><a href="#org4069f05">1.2.6. Mathematical formulation</a></li>
<li><a href="#orgaba2d79">1.2.7. A simplified formulation</a></li>
<li><a href="#orga7be53c">1.2.8. Constrained optimization</a></li>
</ul>
</li>
<li><a href="#org516774c">1.3. SVM for Linearly Separable</a>
<ul>
<li><a href="#org62a9841">1.3.1. Linear SVM</a></li>
<li><a href="#org320a412">1.3.2. Maximum margin optimization</a></li>
<li><a href="#org93b9e79">1.3.3. Unconstrained optimization</a></li>
<li><a href="#org8c4a28d">1.3.4. Local minimum</a></li>
<li><a href="#orgbf520a5">1.3.5. KKT necessary condition</a></li>
<li><a href="#org7d62a68">1.3.6. Objective function in dual space</a></li>
<li><a href="#orge28e8c1">1.3.7. Dual space formulation of SVM</a></li>
<li><a href="#org3076baa">1.3.8. The SVM classifier</a></li>
</ul>
</li>
<li><a href="#org16af93a">1.4. Slack variables for linearly non-separable</a>
<ul>
<li><a href="#org891a8cd">1.4.1. Linear classification</a></li>
<li><a href="#org2c98400">1.4.2. Linearly separable scenario</a></li>
<li><a href="#org504f0d2">1.4.3. Linearly non-separable scenario</a></li>
<li><a href="#orge2e5180">1.4.4. Slack variable</a></li>
<li><a href="#orgcc9f290">1.4.5. Implication of slack variables</a></li>
<li><a href="#org8928dd6">1.4.6. Equality scenario</a></li>
<li><a href="#org7d5f5da">1.4.7. Linearly non-separable scenarios</a></li>
<li><a href="#orgb0976ee">1.4.8. Comparison of two scenarios</a></li>
</ul>
</li>
<li><a href="#org266b2fc">1.5. Optimization problem for linearly non-separable</a>
<ul>
<li><a href="#orgbb2e220">1.5.1. Three criteria in SVM</a></li>
<li><a href="#org314e8d7">1.5.2. Possible formulation?</a></li>
<li><a href="#org08dbf1d">1.5.3. Key idea of new formulation</a></li>
<li><a href="#org6b05862">1.5.4. Constrained optimization</a></li>
</ul>
</li>
<li><a href="#org365d305">1.6. SVM for linearly non-separable</a>
<ul>
<li><a href="#org353a647">1.6.1. Linear SVM classification</a></li>
<li><a href="#orgb318cf1">1.6.2. Optimization problem</a></li>
<li><a href="#org4ea6529">1.6.3. Constrained optimization</a></li>
<li><a href="#org78dc882">1.6.4. Derivatives on \((\beta_0,\beta)\)</a></li>
<li><a href="#orgea4195c">1.6.5. Derivatives on \(\xi_i\)</a></li>
<li><a href="#org5cbbbac">1.6.6. New objective function?</a></li>
<li><a href="#orgfe908bf">1.6.7. Dual space formulation of SVM</a></li>
<li><a href="#org2b1c8d7">1.6.8. The SVM Classifier</a></li>
<li><a href="#org73b3a9b">1.6.9. Optional</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgb5b536d" class="outline-2">
<h2 id="orgb5b536d"><span class="section-number-2">1.</span> Week 12: Advanced Supervised Learning</h2>
<div class="outline-text-2" id="text-1">
<p>
Support Vector Machine in Linear Scenario.
(Week 11 is Spring Break)
</p>
</div>
<div id="outline-container-org3b55367" class="outline-3">
<h3 id="org3b55367"><span class="section-number-3">1.1.</span> Introduction to Support Vector Machines</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-orgbfabdd5" class="outline-4">
<h4 id="orgbfabdd5"><span class="section-number-4">1.1.1.</span> Binary classification problem</h4>
<div class="outline-text-4" id="text-1-1-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) where \(Y_i\) is <b><b>binary</b></b></li>
<li>Linear classifier: classify binary response \(Y\) depending on whether \(h(x) = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p = \beta_0 + \beta^T x\) is large or not</li>
<li>Question: how to estimate \((\beta_0, \beta_1, ..., \beta_p)\) from the training data?
<dl class="org-dl">
<dt>Linear discriminant analysis</dt><dd><b><b>testing hypothesis</b></b> after modelling \(x_{i1}, x_{i2}, ..., x_{ip}\) as multivariate normal distribution \(\text{MVN}(\mu_i, \Sigma)\)</dd>
<dt>Logistic regression</dt><dd><b><b>maximum likelihood estimator</b></b> after modelling the relationship between \(P(Y_i=1)\) and the function \(h(x)\)</dd>
<dt>Support vector machine</dt><dd><b><b>optimization-based</b></b>, with \(Y_i \in {-1,1}\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-orgef4212e" class="outline-4">
<h4 id="orgef4212e"><span class="section-number-4">1.1.2.</span> Toy motivating example</h4>
<div class="outline-text-4" id="text-1-1-2">
<ul class="org-ul">
<li>Data: \((Y_i, X_{i1}, X_{i2})\) for \(i=1, ..., n=60\). Here, \(Y_i \in {-1,1}\) are class labels</li>
<li><p>
True generative model:
</p>
\begin{equation}
X_{i2} =
\begin{cases}
2X_{i1} + 1 + \epsilon_i & \text{ if } Y_i=1 \\
2X_{i1} - 1 + \epsilon_i & \text{ if } Y_i=-1 \\
\end{cases}
\text{where }\epsilon_i \text{ are iid N}(0, 0.3^2)
\end{equation}</li>
<li>Objective: Suppose we don't know the true generative model, how do we use the training data to predict the class?</li>
</ul>
</div>
</div>
<div id="outline-container-orgf74e79b" class="outline-4">
<h4 id="orgf74e79b"><span class="section-number-4">1.1.3.</span> Good separate line?</h4>
<div class="outline-text-4" id="text-1-1-3">

<div id="org93b497b" class="figure">
<p><img src="./img/good-sep.png" alt="good-sep.png" />
</p>
</div>
<ul class="org-ul">
<li>If we want to separate two classes along the line \(X_2 + 2X_1 + c\), this yields the classifier of form \(\text{sign}(X_2 - 2X_1 - c)\)
<ul class="org-ul">
<li>The two boundary lines are:
\[
    X_2 = 2X_1 + 0.607 \\
    X_2 = 2X_1 - 0.590
    \]</li>
<li>The distance between the two boundary lines is
\[
    (0.607 + 0.590) / \sqrt{2^2+1} = 0.535
    \]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org29f8e53" class="outline-4">
<h4 id="org29f8e53"><span class="section-number-4">1.1.4.</span> Poor separation</h4>
<div class="outline-text-4" id="text-1-1-4">
<p>
What if we change the line direction; and instead, consider classifier of the form:
\[
\text{sign}(X_2 - X_1 -c)
\]
</p>
<ul class="org-ul">
<li>The two boundary lines are thus:
\[
  X_2 = X_1 + 0.725 \\
  X_2 = X_1 + 0.198
  \]</li>
<li>The <b><b>distance</b></b> between these 2 boundary lines is thus
\[
  (0.725-0.198) / \sqrt {1^2+1} = 0.373
  \]</li>
<li>The separation is thus <b><b>smaller</b></b> and likely leads to <b><b>larger misclassification error</b></b> on testing data</li>
</ul>
</div>
</div>
<div id="outline-container-orgcff4e1e" class="outline-4">
<h4 id="orgcff4e1e"><span class="section-number-4">1.1.5.</span> Margin maximization</h4>
<div class="outline-text-4" id="text-1-1-5">
<ul class="org-ul">
<li>Support vector machine: main idea is to find a line direction that <b><b>maximizes the margin</b></b> of the training data</li>
<li>Margin: the distance between the two boundary lines of two classes</li>
</ul>
</div>
</div>
<div id="outline-container-org4ad13ee" class="outline-4">
<h4 id="org4ad13ee"><span class="section-number-4">1.1.6.</span> Distance in 2 dimensions</h4>
<div class="outline-text-4" id="text-1-1-6">

<div id="org6b3420e" class="figure">
<p><img src="./img/distance-in-2d.png" alt="distance-in-2d.png" />
</p>
</div>
<ul class="org-ul">
<li>In the 2-dimensional plane, the distance from a point \((x_{1, \text{new}}, x_{2,\text{new}})\) is:
\[
  d = \frac{|\beta_0 - \beta_0^{*}|}{\sqrt{\beta_1^2 + \beta^2_2}}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgd6044bc" class="outline-4">
<h4 id="orgd6044bc"><span class="section-number-4">1.1.7.</span> Distance from point to hyperplane</h4>
<div class="outline-text-4" id="text-1-1-7">
<p>
In the \(R^{\mathbb{p}}\) space, the <b><b>distance</b></b> from a point
\[
x_\text{new} = (x_{1, \text{new}}, x_{2, \text{new}}, ..., x_{\text{p, new}})^T
\]
to thy hyperplane
\[
\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = 0 ~ ~ ~ \text{i.e.: }\beta_0 + \beta^T x = 0
\]
is:
\[
d = \frac{|\beta_0 + \beta_1 x_{1, \text{new}} + \beta_2 x_{2,\text{new}} + ... + \beta_p x_{p, \text{new}}|}{\sqrt{\beta^2_1 + \beta^2_2 + ... + \beta^2_p}} \\
= \frac{|\beta_0 + \beta^T x_{\text{new}}}{|\beta|}
\]
</p>
</div>
</div>
<div id="outline-container-orga464f19" class="outline-4">
<h4 id="orga464f19"><span class="section-number-4">1.1.8.</span> Distance between hyperplanes</h4>
<div class="outline-text-4" id="text-1-1-8">
<ul class="org-ul">
<li>In the \(R^{\mathbb{p}}\) space, the distance between two hyperplanes
\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = c_1 ~ ~ ~ \text{i.e.: }\beta_0 - c_1 + \beta^T x = 0 \\
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = c_2 ~ ~ ~ \text{i.e.: }\beta_0 - c_2 + \beta^T x = 0
  \]
is
\[
  d = \frac{|c_1 - c_2|}{\sqrt{\beta^2_1 + \beta^2_2 + ... + \beta^2_p}} = \frac{|c_1 - c_2|}{|\beta|}
  \]</li>
<li>In the special case of \(c_1 = -1\) and \(c_2 = 1\), the margin or distance of the two hyperplanes is
\[
  d = \frac{|-1-1}{\sqrt{\beta^2_1 + \beta^2_2 + ... + \beta^2_p}} = \frac{2}{|\beta|}
  \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5f99ca6" class="outline-3">
<h3 id="org5f99ca6"><span class="section-number-3">1.2.</span> Maximum Margin Optimization for SVM</h3>
<div class="outline-text-3" id="text-1-2">
<p>
11.1.2
</p>
</div>
<div id="outline-container-orgae8ff5e" class="outline-4">
<h4 id="orgae8ff5e"><span class="section-number-4">1.2.1.</span> Linear classification</h4>
<div class="outline-text-4" id="text-1-2-1">
<ul class="org-ul">
<li>Training dataset: - Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1}\)</li>
<li><p>
Linear classification: find the linear function
\[
  h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta_0 + \beta^T x
  \]
and predict the binary response of any new data by:
</p>
\begin{equation}
\text{sign} \big( h ( x ) \big) = \text{sign} \big( \beta_{0}+\beta^{T} x \big)=
\begin{cases} -1 & \text{ if } \beta_0+\beta^{T} x \leq 0 \\
1 & \text{ if } \beta_0+\beta^{T} x > 0
\end{cases}
\end{equation}</li>
<li>Different methods to estimate \(\beta_0\) and \(\beta\) from training data:
<ul class="org-ul">
<li>Linear discriminant analysis</li>
<li>Logistic regression</li>
<li>Support vector machine (when <span style='background-color: #FFFF00;'>linearly separable</span>)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3bf9955" class="outline-4">
<h4 id="org3bf9955"><span class="section-number-4">1.2.2.</span> Linear SVM</h4>
<div class="outline-text-4" id="text-1-2-2">

<div id="org05db531" class="figure">
<p><img src="./img/svm-2-lines.png" alt="svm-2-lines.png" />
</p>
</div>
<ul class="org-ul">
<li>Linear SVM in the <span style='background-color: #FFFF00;'>linearly separable scenario</span></li>
<li>Given the direction \(\beta = (\beta_1, \beta_2, ..., \beta_p)\), there are 3 lines / hyperplanes of the form
\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = 0
  \]
Not unique, e.g. multiplying 3 both sides</li>
<li>For uniqueness, we assume <b><b>two boundary lines</b></b> of two classes are defined by
\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = - 1 \\
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = 1
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org0da973f" class="outline-4">
<h4 id="org0da973f"><span class="section-number-4">1.2.3.</span> Margin computation</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
<img src="./img/svm-margin-compute.png" alt="svm-margin-compute.png" />
Margin =
\[
\frac{2}{\parallel\beta\parallel} =
\frac{2}{\sqrt{\beta^2_1 + ... + \beta^2_p}}
\]
</p>
</div>
</div>
<div id="outline-container-org82ed34a" class="outline-4">
<h4 id="org82ed34a"><span class="section-number-4">1.2.4.</span> Maximum margin optimization</h4>
<div class="outline-text-4" id="text-1-2-4">
<p>
Consider the scenario when the training data are linearly separable.
</p>
<ul class="org-ul">
<li>Linear SVM classifier:
\[
  \hat{Y}   = \text{sign}(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p)
  \]</li>
<li>Objective: maximize the margin
\[
  \frac{2}{\parallel\beta\parallel} =
  \frac{2}{\sqrt{\beta^2_1 + ... + \beta^2_p}}
  \]</li>
<li>This translates to <span style='background-color: #FFFF00;'>low prediction error</span> on new data in test data</li>
</ul>
</div>
</div>
<div id="outline-container-org131596a" class="outline-4">
<h4 id="org131596a"><span class="section-number-4">1.2.5.</span> Constraints for SVM</h4>
<div class="outline-text-4" id="text-1-2-5">
<ul class="org-ul">
<li>Linear SVM classifier:
<ul class="org-ul">
<li>Constraints: classify all training data correctly in the scenario when the training data is linearly separable:
<ul class="org-ul">
<li>When \(Y_i = -1 \rightarrow \beta_0 + \beta^T x_i \le -1\)</li>
<li>When \(Y_I = 1 \rightarrow \beta_0 + \beta^T x_i \ge 1\)</li>
</ul></li>
<li>The constraints can be written simply as:
\[
    Y_i(\beta_0 + \beta^T x_i) \ge 1 \text{ for all }i=1,2,...,n
    \]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org4069f05" class="outline-4">
<h4 id="org4069f05"><span class="section-number-4">1.2.6.</span> Mathematical formulation</h4>
<div class="outline-text-4" id="text-1-2-6">
<p>
Formula of SVM in the scenario when training data are linearly separable:
</p>
<ul class="org-ul">
<li>Problem 0: find the linear function, or hyperplane below
\[
  h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta_0 + \beta^T x
  \]
that <span style='background-color: #FFFF00;'>maximizes</span> margin:
\[
  \max_{\beta, \beta_0} \frac{2}{\parallel \beta \parallel}
  \]
subject to the constraint on classifying training data well:
\[
  Y_i(\beta_0 + \beta^T x_i) \ge 1 \text{ for all} i=1,2,...,n
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgaba2d79" class="outline-4">
<h4 id="orgaba2d79"><span class="section-number-4">1.2.7.</span> A simplified formulation</h4>
<div class="outline-text-4" id="text-1-2-7">
<p>
In the scenario when training data are linearly separable:
</p>
<ul class="org-ul">
<li>Problem 1 (Primal form): Find the linear function, or hyperplane, that <span style='background-color: #FFFF00;'>minimizes the quadratic objective function</span>
\[
  \min_{\beta, \beta_0} (\frac{1}{2} \parallel \beta \parallel^2)
  \]
subject to the linear constraints
\[
  Y_i(\beta_0 + \beta^T x_i) \ge 1 \text{ for all} i=1,2,...,n
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orga7be53c" class="outline-4">
<h4 id="orga7be53c"><span class="section-number-4">1.2.8.</span> Constrained optimization</h4>
<div class="outline-text-4" id="text-1-2-8">
<p>
By <span style='background-color: #FFFF00;'>Lagrange multipliers</span>, we reformulate the constrained primal problem into (unconstrained) dual space.
</p>
<ul class="org-ul">
<li>Problem 2: find \((\beta_0, \beta)\) that minimizes the objective function
\[
  g = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
where \(\lambda_i \ge 0\)  are the lagrange multipliers</li>
<li>This is a quadratic optimizaton problem which can be solved conveniently. This gives the SVM in the linearly separable scenario.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org516774c" class="outline-3">
<h3 id="org516774c"><span class="section-number-3">1.3.</span> SVM for Linearly Separable</h3>
<div class="outline-text-3" id="text-1-3">
<p>
11.1.3
</p>
</div>
<div id="outline-container-org62a9841" class="outline-4">
<h4 id="org62a9841"><span class="section-number-4">1.3.1.</span> Linear SVM</h4>
<div class="outline-text-4" id="text-1-3-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1}\)</li>
<li>Support vector machine: predict \(\hat{Y}
  = \text{sign}(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p)\)</li>
<li>In the linearly separable scenario, we assume two boundary hyperplanes are
\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = - 1 \\
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = 1
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org320a412" class="outline-4">
<h4 id="org320a412"><span class="section-number-4">1.3.2.</span> Maximum margin optimization</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
Maximum margin optimization formulation of SVM when the training data are linearly separable.
</p>
<ul class="org-ul">
<li>Problem 1 (Primal form): Find the linear function (or hyperplane) that achieves
\[
  \max{{\beta_0, \beta}} \frac{2}{\parallel \beta \parallel}
  \]
i.e.
\[
  \min_{\beta_0, \beta} (\frac{1}{2} \parallel \beta \parallel^2)
  \]
subject to the constraints on classifying the training data well:
\[
  Y_i (\beta_0 + \beta^T x_i) \ge 1 \text{ for all }i=1,2,...,n
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org93b9e79" class="outline-4">
<h4 id="org93b9e79"><span class="section-number-4">1.3.3.</span> Unconstrained optimization</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
By Lagrange multipliers, we reformulate the constrained primal problem into unconstrained dual space.
</p>
<ul class="org-ul">
<li>Problem 2: find \(\beta_0, \beta\) that minimizes the objective function
\[
  g = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
where \(\alpha_i \ge 0\) are the Lagrange multipliers</li>
<li>Let us present some technical details on this unconstrained quadratic optimization problem</li>
</ul>
</div>
</div>
<div id="outline-container-org8c4a28d" class="outline-4">
<h4 id="org8c4a28d"><span class="section-number-4">1.3.4.</span> Local minimum</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li>Taking derivatives of
\[
  g = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
yields
\[
  \frac{\partial g}{\partial \beta_0} = -\sum^n_{i=1} \alpha_i Y_i \\
  \frac{\partial g}{\partial \beta} = \beta - \sum^n_{i=1} \alpha_i Y_i x_i
  \]</li>
<li>Thus a local minimum must satisfy
\[
  \sum^n_{i=1} \alpha_i Y_i = 0 \text{ and } \beta = \sum^n_{i=1} \alpha_i Y_i x_i
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgbf520a5" class="outline-4">
<h4 id="orgbf520a5"><span class="section-number-4">1.3.5.</span> KKT necessary condition</h4>
<div class="outline-text-4" id="text-1-3-5">
<ul class="org-ul">
<li>Karush-Kuhn-Tucker (or KKT) necessary condition for local minima: tat the point of the solutions the product between dual variables and constraints has to vanish</li>
<li>For our objective function
\[
  g = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
by KKT, there exists \((\alpha_1^* ..., \alpha_n^*)\), called KKT multiplier, such that:
<ul class="org-ul">
<li>\(\alpha_i^{*} = 0\) whenever \(Y_i(\beta_0 + \beta^T x_i) \gt 1\)</li>
<li>\(\alpha_i^{*} \gt 0\) only if \(Y_i (\beta_0 + \beta^T x_i) = 1\) i.e. \(\beta_0 + \beta^T x_i = Y_i \in \pm 1\)</li>
<li>The vectors \(x_i\) for which \(\alpha_i^{*} > 0\) are called support vectors as they belong to the boundary hyperplanes</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org7d62a68" class="outline-4">
<h4 id="org7d62a68"><span class="section-number-4">1.3.6.</span> Objective function in dual space</h4>
<div class="outline-text-4" id="text-1-3-6">
<ul class="org-ul">
<li><p>
Plugging \(\sum^n{i=1} \alpha_i Y_i = 0\) and \(\beta = \sum^n_{i=1} \alpha_i Y_i x_i\) into the objective function
\[
  g_{\text{dual}} = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
</p>

<p>
\[
	= {\frac{1} {2}} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}-\beta_{0} \sum_{i=1}^{n} \alpha_{i} Y_{i}-\sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}+\sum_{i=1}^{n} \alpha_{i}
	\]
</p>

<p>
\[
	= \sum_{i=1}^{n} \alpha_{i}-\frac{1} {2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}
	\]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orge28e8c1" class="outline-4">
<h4 id="orge28e8c1"><span class="section-number-4">1.3.7.</span> Dual space formulation of SVM</h4>
<div class="outline-text-4" id="text-1-3-7">
<ul class="org-ul">
<li>Problem 3: Find the \(\alpha_i\) that minimizes
\[
	g=\frac{1} {2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}-\sum_{i=1}^{n} \alpha_{i}
	\]
subject to these 2 families of constraints
\[
	\sum_{i=1}^{n} \alpha_{i} Y_{i}=0 \quad \text{and} \quad\alpha_{i} \geq0, \forall i=1, \cdots, n
	\]
<ul class="org-ul">
<li>Standard quadratic programming algorithms can solve the \(\alpha_i\) 's</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3076baa" class="outline-4">
<h4 id="org3076baa"><span class="section-number-4">1.3.8.</span> The SVM classifier</h4>
<div class="outline-text-4" id="text-1-3-8">
<p>
Once we determine the \(\alpha_i\), we have
</p>
<ul class="org-ul">
<li>\(\widehat{\beta} = \sum^n_{i=1} \alpha_i Y_i x_i\) which is a p-dimensional vector</li>
<li>By KKT, for \(\alpha_i \ne 0\), we have \(Y_i (\beta_0 + \beta^T x_i)=1\). Multiplying both sides by \(Y_i \in {-1,1}\) we have \(\widehat{\beta_0} = Y_i - \beta^T x_i\)</li>
<li><p>
Since most \(\alpha_i\) are 0, the SVM classifier can be rewritten as
</p>

<p>
\[
	h(x)=\text{sign}\big( \widehat{\beta}_{0}+\widehat{\beta}^{T} \boldsymbol{x} \big)=\text{sign} \left( \widehat{\beta}_{0}+\sum_{i=1}^{n} \alpha_{i} Y_{i} x_{i}^{T} \boldsymbol{x} \right)
	\]
</p>

<p>
\[
	\widehat{\beta}_{0}+\sum_{j=1}^{| s |} \alpha_{s_{j}} Y_{s_{j}} x_{s_{j}}^{T} x
	\]
</p>

<p>
if we eliminate the non-zero terms
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org16af93a" class="outline-3">
<h3 id="org16af93a"><span class="section-number-3">1.4.</span> Slack variables for linearly non-separable</h3>
<div class="outline-text-3" id="text-1-4">
<p>
11.2.1
</p>
</div>
<div id="outline-container-org891a8cd" class="outline-4">
<h4 id="org891a8cd"><span class="section-number-4">1.4.1.</span> Linear classification</h4>
<div class="outline-text-4" id="text-1-4-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1}\)</li>
<li><p>
Linear classification: find the linear function
\[
  h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta_0 + \beta^T x
  \]
where we classify new data based on
</p>
\begin{equation}
\text{sign} \big( h ( x ) \big) = \text{sign} \big( \beta_{0}+\beta^{T} x \big)=
\begin{cases} -1 & \text{ if } \beta_0+\beta^{T} x \leq 0 \\
1 & \text{ if } \beta_0+\beta^{T} x > 0
\end{cases}
\end{equation}</li>
<li>It's desirable to find a classifier that <span style='background-color: #FFFF00;'>maximizes margin</span>, which translates to a <span style='background-color: #FFFF00;'>lower prediction error</span> on new test data</li>
</ul>
</div>
</div>
<div id="outline-container-org2c98400" class="outline-4">
<h4 id="org2c98400"><span class="section-number-4">1.4.2.</span> Linearly separable scenario</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
See <a href="#org0da973f">1.2.3</a>
</p>
</div>
</div>
<div id="outline-container-org504f0d2" class="outline-4">
<h4 id="org504f0d2"><span class="section-number-4">1.4.3.</span> Linearly non-separable scenario</h4>
<div class="outline-text-4" id="text-1-4-3">
<p>
<img src="./img/non-sep.png" alt="non-sep.png" />
What if we cannot perfectly classify training data?
</p>
</div>
</div>
<div id="outline-container-orge2e5180" class="outline-4">
<h4 id="orge2e5180"><span class="section-number-4">1.4.4.</span> Slack variable</h4>
<div class="outline-text-4" id="text-1-4-4">
<ul class="org-ul">
<li>When the data are linearly non-separable, we can <b><b>soften</b></b> the margins by introducing <span style='background-color: #FFFF00;'>slack</span> variables in constraints</li>
<li>To be more specific, when the hard constraints
\[
  Y_i (\beta_0 + \beta^T x_i) \ge 1
  \]
cannot be satisfied for all \(i\), we replace them by the soft constraint
\[
  Y_i (\beta_0 + \beta^T x_i) - + \xi_i \ge 0, ~ ~ \xi_i \ge 0
  \]
where the new \(\xi_i\) are the <span style='background-color: #FFFF00;'>slack variables</span></li>
</ul>
</div>
</div>
<div id="outline-container-orgcc9f290" class="outline-4">
<h4 id="orgcc9f290"><span class="section-number-4">1.4.5.</span> Implication of slack variables</h4>
<div class="outline-text-4" id="text-1-4-5">
<p>
Let the slack variable \(\xi_i \ge 0\)  be the <span style='background-color: #FFFF00;'>smallest value</span> satisfying the constraints
\[
Y_i (\beta_0 + \beta^T x_i) - + \xi_i \ge 0
\]
There are two scenarios:
</p>
<dl class="org-dl">
<dt>Inequality scenario</dt><dd>\(Y_i (\beta_0 + \beta^T x_i) -1 + \xi_i \gt 0\) for <span style='background-color: #FFFF00;'>all</span> \(\xi_i \ge 0\). The smallest slack variable \(\xi_i =0\) and \(Y_i(\beta_0 + \beta^T x_i)>1\)</dd>
<dt>Equality scenario</dt><dd>\(Y_i(\beta_0 + \beta^T x_i) -1 + \xi_i = 0\) for <span style='background-color: #FFFF00;'>some</span> \(\xi_i \ge 0\). Then, \(Y_i(\beta_0+\beta^T x_i)=1-\xi_i\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org8928dd6" class="outline-4">
<h4 id="org8928dd6"><span class="section-number-4">1.4.6.</span> Equality scenario</h4>
<div class="outline-text-4" id="text-1-4-6">
<p>
There are 3 subcases for equality scenario
\[
Y_i(\beta_0 + \beta^T x_i) -1 + \xi_i = 0
\]
depending on the values of slack variables \(\xi_i \ge 0\):
</p>
<dl class="org-dl">
<dt>Margin support vectors (\(\xi_i=0\))</dt><dd>we have \(Y_i(\beta_0+\beta^T x_i) =1 ~ \text{i.e. } \beta_0+\beta^Tx_i=Y_i\in\pm1\)</dd>
<dt>Non-margin support vectors (\(0<\xi_i\le1\))</dt><dd>we have \(Y_i(\beta_0+\beta^Tx_i)=1-\xi_i\ge0 ~ \text{i.e. }Y_i=\text{sign }(\beta_0+\beta^Tx_i)\)</dd>
<dt>Misclassification errors (\(\xi_i\gt1\))</dt><dd>since
\(Y_i(\beta_0+\beta^T x_i)=1-\xi_i <0 ~ \text{i.e. } Y_i\ne \text{sign}(\beta_0+\beta^Tx_i)\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org7d5f5da" class="outline-4">
<h4 id="org7d5f5da"><span class="section-number-4">1.4.7.</span> Linearly non-separable scenarios</h4>
<div class="outline-text-4" id="text-1-4-7">
<p>
<img src="./img/non-sep-types.png" alt="non-sep-types.png" />
\(Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0\)
</p>
</div>
</div>
<div id="outline-container-orgb0976ee" class="outline-4">
<h4 id="orgb0976ee"><span class="section-number-4">1.4.8.</span> Comparison of two scenarios</h4>
<div class="outline-text-4" id="text-1-4-8">
<p>
Comparison of SVM classifier between linearly separable and linearly non-separable scenarios:
</p>
<ul class="org-ul">
<li>Hard constraint \(Y_i(\beta_0+\beta^Tx_i)\ge 1\) is replaced by soft constraint \(Y(\beta_0+\beta^Tx_i)-1+\xi_i\ge 0\) and \(\xi_i\ge 0\)</li>
<li>Perfect classification for training data is replaced by <span style='background-color: #FFFF00;'>misclassification error</span> for training data \(\sum^n_{i=1}(\xi_i > 1)\)</li>
<li>Mathematical formulation for SVM classifier in the <span style='background-color: #FFFF00;'>linearly non-separable</span> scenario needs to adjust the training error</li>
<li>Optimization algorithms are similar.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org266b2fc" class="outline-3">
<h3 id="org266b2fc"><span class="section-number-3">1.5.</span> Optimization problem for linearly non-separable</h3>
<div class="outline-text-3" id="text-1-5">
<p>
11.2.2
</p>
</div>
<div id="outline-container-orgbb2e220" class="outline-4">
<h4 id="orgbb2e220"><span class="section-number-4">1.5.1.</span> Three criteria in SVM</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
There are three criteria when evaluating linear SVM based on the linear function/hyperplane \(h(x)=\beta_0+\beta_1x_1+...+\beta_px_p = \beta_0+\beta^Tx\)
</p>
<dl class="org-dl">
<dt>Margin maximization</dt><dd>\[
  \max_{\beta,\beta_0}\frac{2}{\parallel\beta\parallel} ~ \text{ i.e. }\min_{\beta,\beta_0}(\frac{1}{2}\parallel\beta\parallel^2)
  \]</dd>
<dt>Soft constraints on training data</dt><dd>\[
  Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0, ~ , \xi_i \ge 0, ~ \forall i=1,2,...,n
  \]</dd>
<dt>Misclassification error on training data</dt><dd>\[
  \sum^n_{i=1}I(\xi_i>1)
  \]</dd>
</dl>
</div>
</div>
<div id="outline-container-org314e8d7" class="outline-4">
<h4 id="org314e8d7"><span class="section-number-4">1.5.2.</span> Possible formulation?</h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
(P1): find the linear function or hyperplane
\[
h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta_0 + \beta^T x
\]
that minimizes an objective function to account for training error
\[
\frac{1}{2} \parallel\beta\parallel^2 + C\sum^n_{i=1}I(\xi_i\gt 1)
\]
for some \(C>0\) subject to the soft constraints on the training data
\[
Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0, ~ , \xi_i \ge 0, ~ \forall i=1,2,...,n
\]
</p>
<ul class="org-ul">
<li>Unfortunately, this is difficult to optimize as it is <span style='background-color: #FFFF00;'>non-convex</span></li>
<li>We can motivate another useful formulation</li>
</ul>
</div>
</div>
<div id="outline-container-org08dbf1d" class="outline-4">
<h4 id="org08dbf1d"><span class="section-number-4">1.5.3.</span> Key idea of new formulation</h4>
<div class="outline-text-4" id="text-1-5-3">
<p>
New formulation of SVM for linearly non-separable:
</p>
<ul class="org-ul">
<li>(P2): find the linear function \(h(x)=\beta_0+\beta^Tx\) and \((\xi_1,...,\xi_n)\) that minimizes the objective function
\[
  \frac{1}{2} \parallel\beta\parallel^2 + C\sum^n_{i=1}\xi_i
  \]
for some \(C>0\), subject to the soft constraints on training data
\[
  Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0, ~ , \xi_i \ge 0, ~ \forall i=1,2,...,n
  \]
This is a constrained quadratic optimization data that is solvable</li>
</ul>
</div>
</div>
<div id="outline-container-org6b05862" class="outline-4">
<h4 id="org6b05862"><span class="section-number-4">1.5.4.</span> Constrained optimization</h4>
<div class="outline-text-4" id="text-1-5-4">
<ul class="org-ul">
<li>By <span style='background-color: #FFFF00;'>Lagrange multipliers</span>, we reformulate the constrained primal problem into the unconstrained dual space with the objective function
\[
	g=\frac{1} {2} \| \beta\|^{2}+C \sum_{i=1}^{n} \xi_{i}-\sum_{i=1}^{n} \alpha_{i} \big[ Y_{i} \big( \beta_{0}+\beta^{T} x_{i} \big)-1+\xi_{i} \big]-\sum_{i=1}^{n} \delta_{i} \xi_{i}
	\]
where \(\alpha_i \ge 0, \delta_i \ge 0\) are the <b><b>Lagrange multipliers</b></b></li>
<li>The optimal solution gives the SVM classifier in the linearly non-separable scenario</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org365d305" class="outline-3">
<h3 id="org365d305"><span class="section-number-3">1.6.</span> SVM for linearly non-separable</h3>
<div class="outline-text-3" id="text-1-6">
<p>
11.2.3
</p>
</div>
<div id="outline-container-org353a647" class="outline-4">
<h4 id="org353a647"><span class="section-number-4">1.6.1.</span> Linear SVM classification</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
Question: how does SVM classifier estimate the coefficients \((\beta_0,\beta)\) when the training data are <b><b>linearly non-separable</b></b>?
</p>
</div>
</div>
<div id="outline-container-orgb318cf1" class="outline-4">
<h4 id="orgb318cf1"><span class="section-number-4">1.6.2.</span> Optimization problem</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
In the linearly non-separable problem, the SVM classifier solves the following optimization problem:
Problem (P2): find the linear function \(h(x) = \beta_0 + \beta^Tx\) and the slack variables \((\xi_1, ..., \xi_n)\) that minimizes the objective function
\[
\frac{1}{2}\parallel\beta\parallel^2 + C\sum^n_{i=1}\xi_i
\]
for some \(C>0\) subject to the soft constraints
\[
Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0, ~ , \xi_i \ge 0, ~ \forall i=1,2,...,n
\]
How to solve this optimization problem?
</p>
</div>
</div>
<div id="outline-container-org4ea6529" class="outline-4">
<h4 id="org4ea6529"><span class="section-number-4">1.6.3.</span> Constrained optimization</h4>
<div class="outline-text-4" id="text-1-6-3">
<ul class="org-ul">
<li>By <span style='background-color: #FFFF00;'>Lagrange multipliers</span>, we reformulate the constrained primal problem into unconstrained dual space with the objective function
\[
	g=\frac{1} {2} \| \beta\|^{2}+C \sum_{i=1}^{n} \xi_{i}-\sum_{i=1}^{n} \alpha_{i} \big[ Y_{i} \big( \beta_{0}+\beta^{T} x_{i} \big)-1+\xi_{i} \big]-\sum_{i=1}^{n} \delta_{i} \xi_{i}
	\]
where \(\alpha_i\ge0, \delta_i\ge0\) are the Lagrange multipliers</li>
</ul>
</div>
</div>
<div id="outline-container-org78dc882" class="outline-4">
<h4 id="org78dc882"><span class="section-number-4">1.6.4.</span> Derivatives on \((\beta_0,\beta)\)</h4>
<div class="outline-text-4" id="text-1-6-4">
<p>
\[
g=\frac{1} {2} \| \beta\|^{2}+C \sum_{i=1}^{n} \xi_{i}-\sum_{i=1}^{n} \alpha_{i} \big[ Y_{i} \big( \beta_{0}+\beta^{T} x_{i} \big)-1+\xi_{i} \big]-\sum_{i=1}^{n} \delta_{i} \xi_{i}
\]
</p>
<ul class="org-ul">
<li>Taking derivatives w.r.t. \((\beta_0,\beta)\) yields
\[
	\frac{\partial g} {\partial\beta_{0}}=-\sum_{i=1}^{n} \alpha_{i} Y_{i} \,, \qquad\frac{\partial g} {\partial\beta}=\beta-\sum_{i=1}^{n} \alpha_{i} Y_{i} x_{i}
	\]</li>
<li>Thus a local minimum must satisfy
\[
	\sum_{i=1}^{n} \alpha_{i} Y_{i}=0 \quad{\text{and}} \ \ \beta=\sum_{i=1}^{n} \alpha_{i} Y_{i} x_{i}
	\]
This is the same as those in the linearly separable scenario!</li>
</ul>
</div>
</div>
<div id="outline-container-orgea4195c" class="outline-4">
<h4 id="orgea4195c"><span class="section-number-4">1.6.5.</span> Derivatives on \(\xi_i\)</h4>
<div class="outline-text-4" id="text-1-6-5">
<p>
\[
g=\frac{1} {2} \| \beta\|^{2}+C \sum_{i=1}^{n} \xi_{i}-\sum_{i=1}^{n} \alpha_{i} \big[ Y_{i} \big( \beta_{0}+\beta^{T} x_{i} \big)-1+\xi_{i} \big]-\sum_{i=1}^{n} \delta_{i} \xi_{i}
\]
</p>
<ul class="org-ul">
<li>Taking derivatives w.r.t. \(\xi_i\) yields
\[
  \frac{\partial g}{\partial\xi_i} = C-\alpha_i-\delta_i
  \]</li>
<li>Thus a local optimum satisfies \(\alpha_i + \delta_i = C \forall i=1,2,...,n\)</li>
<li>This leads to a crucial new constraint on \(\alpha_i\) in the linearly non-separable scenario:
\[
  0 \le \alpha_i \le C, ~ \forall i = 1, ..., n
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org5cbbbac" class="outline-4">
<h4 id="org5cbbbac"><span class="section-number-4">1.6.6.</span> New objective function?</h4>
<div class="outline-text-4" id="text-1-6-6">
<p>
\[
g=\frac{1} {2} \| \beta\|^{2}+C \sum_{i=1}^{n} \xi_{i}-\sum_{i=1}^{n} \alpha_{i} \big[ Y_{i} \big( \beta_{0}+\beta^{T} x_{i} \big)-1+\xi_{i} \big]-\sum_{i=1}^{n} \delta_{i} \xi_{i}
\]
</p>
<ul class="org-ul">
<li>Putting the relation \(\delta_i = C-\alpha_i \forall i=1,...,n\), the objective function is reduced to the form:
\[
	g=\frac{1} {2} \| \beta\|^{2}-\sum_{i=1}^{n} \alpha_{i} \big[ Y_{i} \big( \beta_{0}+\beta^{T} x_{i} \big)-1 \big]
	\]
which is the same as before in the linearly separable scenario</li>
<li>Plugging the fact \(\beta=\sum^n_{o=1} \alpha_i Y_i x_i\) yields the dual space formulation</li>
</ul>
</div>
</div>
<div id="outline-container-orgfe908bf" class="outline-4">
<h4 id="orgfe908bf"><span class="section-number-4">1.6.7.</span> Dual space formulation of SVM</h4>
<div class="outline-text-4" id="text-1-6-7">
<p>
The dual space formulation of SVM in the linearly non-separable scenario:
Problem (P3): Find the \(\alpha_i\) that minimizes
\[
g=\frac{1} {2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}-\sum_{i=1}^{n} \alpha_{i}
\]
subject to the two families of constraints
\[
\sum^n_{i=1} \alpha_i Y_i = 0 ~ \text{and} ~ 0 \le \alpha_i \le C, ~ \forall i=1,...,n
\]
</p>
<ul class="org-ul">
<li>This is the same as those of of the linearly separable scenario, except that there is an upper bound \(C\) on the \(\alpha_i\)</li>
<li>Quadratic programming algorithms can be used to solve the \(\alpha_i\)</li>
</ul>
</div>
</div>
<div id="outline-container-org2b1c8d7" class="outline-4">
<h4 id="org2b1c8d7"><span class="section-number-4">1.6.8.</span> The SVM Classifier</h4>
<div class="outline-text-4" id="text-1-6-8">
<p>
Once we determined the \(\alpha_i\) for the linearly non-separable, we have
</p>
<ul class="org-ul">
<li>\(\widehat{\beta} = \sum^n_{i=1} \alpha_i Y_i x_i\) and compute \(\widehat{\beta_0} = Y_i - \widehat{\beta}^T x_i\) from margin support</li>
<li>Since most \(\alpha_i\) are 0, the SVM classifier can be rewritten as
\[
  \hat{Y} = \text{sign}(\hat{\beta_0}+\hat{\beta}^T x) = \text{sign}(\hat{\beta_0}+\sum^n_{i=1}\alpha_iY_ix_i^Tx) \\
  = \text{sign}(\hat{\beta_0} + \sum^{|s|}_{j=1} \alpha_{s_j} Y_{s_j} x_{s_j}^T x) \]
if we eliminate non-zero terms</li>
<li>Only training data with \(\alpha_i\ne0\) plays a role in the SVM classifier with either linearly separable or linearly non-separable scenarios</li>
</ul>
</div>
</div>
<div id="outline-container-org73b3a9b" class="outline-4">
<h4 id="org73b3a9b"><span class="section-number-4">1.6.9.</span> Optional</h4>
<div class="outline-text-4" id="text-1-6-9">
<ul class="org-ul">
<li>KKT necessary condition</li>
<li>Three cases
<img src="./img/three-cases-final.png" alt="three-cases-final.png" /></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2024-04-04 Thu 22:02</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
