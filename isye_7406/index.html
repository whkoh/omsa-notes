<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-02-09 Fri 18:40 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ISYE 7406: Data Mining and Statistical Learning</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ISYE 7406: Data Mining and Statistical Learning</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org372ced5">1. Week 3: Linear Regression (II)</a>
<ul>
<li><a href="#org433ec01">1.1. James-Stein Estimator</a>
<ul>
<li><a href="#orgd2e3766">1.1.1. It's a special case of linear regression</a></li>
<li><a href="#orgab86526">1.1.2. Simultaneous estimation</a></li>
<li><a href="#org17509ac">1.1.3. JS estimator</a></li>
<li><a href="#orgc7fbce3">1.1.4. Baseball example</a></li>
<li><a href="#org30a623a">1.1.5. Comparing MLE and JS</a></li>
</ul>
</li>
<li><a href="#orge3000ef">1.2. Shrinkage Methods</a>
<ul>
<li><a href="#orgd69492e">1.2.1. Setting up shrinkage method</a></li>
<li><a href="#orgb1a3e45">1.2.2. Alternative formulation</a></li>
<li><a href="#org91684c9">1.2.3. Bayesian interpretation</a></li>
<li><a href="#org442cad8">1.2.4. Choices of priors</a></li>
<li><a href="#orgb021faa">1.2.5. Ridge regression</a></li>
<li><a href="#org9b92d65">1.2.6. LASSO estimator</a></li>
</ul>
</li>
<li><a href="#org334af2e">1.3. Ridge Regression</a>
<ul>
<li><a href="#org234ed2c">1.3.1. Ridge Regression Estimator</a></li>
<li><a href="#orgb26b9a7">1.3.2. Mathematical solution</a></li>
<li><a href="#org73b94b3">1.3.3. Properties of Ridge Regression</a></li>
<li><a href="#orgedcacd9">1.3.4. Computational issues</a></li>
<li><a href="#org9d4d4c6">1.3.5. Example of SVD</a></li>
<li><a href="#org2b15aa0">1.3.6. SVD Example (I): \(XX^T\)</a></li>
<li><a href="#org6ec2f30">1.3.7. SVD Example (II): \(X^T X\)</a></li>
<li><a href="#org4546b56">1.3.8. SVD verification</a></li>
</ul>
</li>
<li><a href="#org81ecaed">1.4. LASSO (3.2.1)</a>
<ul>
<li><a href="#org239a1e1">1.4.1. LASSO estimator</a></li>
<li><a href="#orgb488f31">1.4.2. L2-norm vs L1-norm</a></li>
<li><a href="#org3c8a29a">1.4.3. Mathematical solution for LASSO estimator</a></li>
<li><a href="#orge3b4b8f">1.4.4. Properties of LASSO</a></li>
<li><a href="#org02111e3">1.4.5. LASSO weaknesses</a></li>
<li><a href="#org73208fe">1.4.6. Computation issues of LASSO</a></li>
<li><a href="#org8519d7e">1.4.7. LASSO is piecewise linear</a></li>
<li><a href="#orgb284cf0">1.4.8. Standard error of LASSO</a></li>
<li><a href="#orge0becd7">1.4.9. Variants of L1-norm</a></li>
</ul>
</li>
<li><a href="#org64a0ef4">1.5. Principal Components (3.2.2)</a>
<ul>
<li><a href="#org71203a2">1.5.1. Dimension reduction</a></li>
<li><a href="#org4b9f9b8">1.5.2. Motivation of PCA</a></li>
<li><a href="#orge8f1b5d">1.5.3. Find the PC's:  Population version</a></li>
<li><a href="#org03f6900">1.5.4. Eigenvectors lead to PC's</a></li>
<li><a href="#org14fcde0">1.5.5. Proof by Lagrange Multipliers</a></li>
<li><a href="#orgdfbc1d5">1.5.6. PCs in Empirical Version</a></li>
<li><a href="#orgb2bad38">1.5.7. Principal component regression</a></li>
</ul>
</li>
<li><a href="#orgafffd1c">1.6. Partial least squares (3.2.3)</a>
<ul>
<li><a href="#org85f38b7">1.6.1. Dimension reduction</a></li>
<li><a href="#org363c5c9">1.6.2. Partial least squares</a></li>
<li><a href="#org4e4aac5">1.6.3. 2 versions of PLS</a></li>
<li><a href="#orgbcc911d">1.6.4. Simple PLS algorithm</a></li>
<li><a href="#orgc2b0919">1.6.5. The PLS model</a></li>
<li><a href="#org808f74f">1.6.6. Key idea in The PLS Model</a></li>
<li><a href="#orga5764e0">1.6.7. PLS for linear regression</a></li>
<li><a href="#orgf3ca7b4">1.6.8. Canonical correlation analysis (CCA)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org86e4513">2. Week 4: Linear Classification</a>
<ul>
<li><a href="#orgdcf0964">2.1. Overview of Linear Discriminant Analysis (4.1.1)</a>
<ul>
<li><a href="#orga622f90">2.1.1. Classification problem</a></li>
<li><a href="#org29105d4">2.1.2. Popular classification methods and R packages</a></li>
<li><a href="#orgd3bcd37">2.1.3. Discriminant analysis</a></li>
<li><a href="#orgbdc7cbd">2.1.4. Example for 3-class problem</a></li>
<li><a href="#org5ce0ed6">2.1.5. Discriminant functions</a></li>
<li><a href="#org866bef0">2.1.6. Multi-class vs. Binary</a></li>
</ul>
</li>
<li><a href="#orgaf5d8c5">2.2. Linear discrimination analysis continued (4.1.2)</a>
<ul>
<li><a href="#orge50db44">2.2.1. Bayes classifier</a></li>
<li><a href="#org8cb019d">2.2.2. Normality asumption</a></li>
<li><a href="#org67e775b">2.2.3. Bayes classifier for normal distribution</a></li>
<li><a href="#orga8605a1">2.2.4. LDA in practice</a></li>
<li><a href="#org2802fca">2.2.5. LDA for K=2 classes</a></li>
<li><a href="#orge715ea2">2.2.6. Fisher's distance-based approach</a></li>
</ul>
</li>
<li><a href="#orga001928">2.3. Quadratic Discrimination Analysis classifier (4.1.3)</a>
<ul>
<li><a href="#orgf409618">2.3.1. Bayes classifier</a></li>
<li><a href="#org1e7498c">2.3.2. Normal distribution</a></li>
<li><a href="#org3bc0dba">2.3.3. Normal model for \(f_k\)</a></li>
<li><a href="#org8520a2b">2.3.4. Three approaches to estimate</a></li>
<li><a href="#org8cd3fb9">2.3.5. QDA classifier</a></li>
<li><a href="#org4703aec">2.3.6. Naive Bayes classifier</a></li>
<li><a href="#org645cba5">2.3.7. General Naive Bayes classifier</a></li>
</ul>
</li>
<li><a href="#orge6236e4">2.4. Logistic Regression: Estimation (4.2.1)</a>
<ul>
<li><a href="#org48c0295">2.4.1. Classification methods</a></li>
<li><a href="#orgf16f3b8">2.4.2. Binary logistic regression</a></li>
<li><a href="#org076258d">2.4.3. Conditional probability</a></li>
<li><a href="#org65b0fb1">2.4.4. Statistical inference</a></li>
<li><a href="#orgbe2ede9">2.4.5. Maximum likelihood estimation</a></li>
<li><a href="#org8cd4c1c">2.4.6. Asymptotic properties of MLE</a></li>
<li><a href="#org81f5797">2.4.7. Other link function</a></li>
<li><a href="#orgc11d619">2.4.8. Logistic or LDA?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org372ced5" class="outline-2">
<h2 id="org372ced5"><span class="section-number-2">1.</span> Week 3: Linear Regression (II)</h2>
<div class="outline-text-2" id="text-1">
<p>
M2T2
</p>
</div>
<div id="outline-container-org433ec01" class="outline-3">
<h3 id="org433ec01"><span class="section-number-3">1.1.</span> James-Stein Estimator</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-orgd2e3766" class="outline-4">
<h4 id="orgd2e3766"><span class="section-number-4">1.1.1.</span> It's a special case of linear regression</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
in LR model with
\[
Y_{n\times 1} = X_{n\times p} \beta_{p\times 1} + \epsilon_{n\times 1}, \text{s.t.} \epsilon \sim N(0, \sigma^2 I_{n\times n})
\]
</p>

<p>
Special case:
</p>
<ul class="org-ul">
<li>\(n=p\)</li>
<li>\(X_{n\times p} = I_{p\times p}\)</li>
</ul>

<p>
OLS yields the estimator of:
</p>

<p>
\[
\hat{\beta_{ols}} = (X^T X)^{-1} X^T Y =
(I^T_{p\times p} I_{p\times p})^{-1}
I^T_{p\times p} Y_{p\times 1} =
\bf{Y_{p\times 1}}
\]
</p>

<p>
When \(\bf{p\ge 3}\), is it possible to do better than OLS?
</p>
</div>
</div>
<div id="outline-container-orgab86526" class="outline-4">
<h4 id="orgab86526"><span class="section-number-4">1.1.2.</span> Simultaneous estimation</h4>
<div class="outline-text-4" id="text-1-1-2">
<ul class="org-ul">
<li><p>
Problem of estimating \(p\) # of parameters \(\beta_i\)'s simultaneously from \(p\) observations (\(Y_i\)'s) under model:
</p>

<p>
\[
  \bf{Y_i} \sim N(\beta_i, \sigma^2), \text{for }i = 1, 2, ..., p
  \]
</p></li>
<li><p>
OLS (a.k.a. Maximum Likelihood Estimator, MLE) yields estimator:
</p>

<p>
\[
  \hat{\beta_i} = Y_i \text{ for }i = 1, 2, ..., p
  \]
</p>

<p>
Is it possible to do better here?
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org17509ac" class="outline-4">
<h4 id="org17509ac"><span class="section-number-4">1.1.3.</span> JS estimator</h4>
<div class="outline-text-4" id="text-1-1-3">
<ul class="org-ul">
<li><p>
Showed MLS/MLE estimator inadmissible for \(p\ge 3\); dominated by JS estimator.
</p>

<p>
\[
  \hat{\beta_i^{MLE}} = Y_i \text{ for }i = 1, 2, ..., p
  \]
</p>

<p>
\[
  \hat{\beta_i^{(JS)}} = w Y_i + (1-w) \bar{Y} \text{ for }i = 1, 2, ..., p;
  \]
</p>

<p>
\[
  w = 1 - \frac{(p-3)\sigma^2}{\sum^p_i(Y_i-\bar{Y})^2}
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgc7fbce3" class="outline-4">
<h4 id="orgc7fbce3"><span class="section-number-4">1.1.4.</span> Baseball example</h4>
<div class="outline-text-4" id="text-1-1-4">
<ul class="org-ul">
<li>Observe \(Y_1, Y_2, ... Y_p\) batting averages (where \(Y_i\) is the batting average for p=18 players), 45 AB</li>
<li>"True" values \(\mu_i\) are the averages over remainder of seasons, 370 AB</li>
<li>Qn: how to predict season averages \(\mu_i\) from early statistics \(Y_i\)?</li>
<li>Estimators: MLE and JS</li>
</ul>
</div>
</div>
<div id="outline-container-org30a623a" class="outline-4">
<h4 id="org30a623a"><span class="section-number-4">1.1.5.</span> Comparing MLE and JS</h4>
<div class="outline-text-4" id="text-1-1-5">
<ul class="org-ul">
<li>JS has lower predictive squared error than MLS (by 50%)</li>
<li>JS estimator is <b><b>shrinkage</b></b> estimator.
<ul class="org-ul">
<li>Each MLE value shrunken towards <b><b>grand mean</b></b></li>
<li>Data-based estimator, compromises between:
<ul class="org-ul">
<li>null hypothesis: all means the same</li>
<li>MLE assumption: no relationship between all \(\mu_i\) values</li>
</ul></li>
<li>Difficult to estimate \(p\ge 3\) parameters simultaneously.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge3000ef" class="outline-3">
<h3 id="orge3000ef"><span class="section-number-3">1.2.</span> Shrinkage Methods</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>Estimation in linear regression: JS works in only specific cases (<a href="#orgd2e3766">1.1.1</a>) when \(p\ge 3\)</li>
<li>How to do better <i>generally</i>?</li>
<li>Shrinkage methods (penalized, regularized)
<ul class="org-ul">
<li>Based on subtracting penalty from log-likelihood</li>
<li>Penalty is a function of <i>decay parameter</i></li>
<li>Sort the variables to be included by size of <i>decay parameter</i></li>
<li>This reduces to a nested case</li>
<li>After estimating <i>decay parameter</i>, variable or model selection is complete!</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgd69492e" class="outline-4">
<h4 id="orgd69492e"><span class="section-number-4">1.2.1.</span> Setting up shrinkage method</h4>
<div class="outline-text-4" id="text-1-2-1">
<ul class="org-ul">
<li>Needs: \(Y_1, x_{11}, x_{12}, ..., x_{1,p}, \text{such that } i = 1,2,...,n\)</li>
<li>Assume all X &amp; Y are standardized i.e.:
\[
  \sum^n_{i=1}Y_i = 0,
  \sum^n_{i=i}x_{ij} = 0,
  \sum^n_{i=1}x^2_{ij} = 1
  \]</li>
<li><p>
If not standardized, do linear transformations:
\[
  Y^{*}_i = Y_i - \bar{Y}
  \]
</p>

<p>
\[
  x^{*}_{ij} = \frac{x_{ij}-\bar{x_j}}{\sqrt{\text{Var}_j}}
  \]
</p></li>
<li>With this assumption, \(\beta_0 = 0\) in the model, i.e.
\[
  Y_i = \bf{0 } \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{i,p} + \epsilon_i
  \]</li>
<li><p>
<b>The shrinkage method solves this optimization problem</b>
</p>

<p>
\[
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  \]
</p>
<dl class="org-dl">
<dt>penalty function</dt><dd>\(J(|\beta_j)\)</dd>
<dt>decay or tuning parameter</dt><dd>\(\lambda \ge 0\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-orgb1a3e45" class="outline-4">
<h4 id="orgb1a3e45"><span class="section-number-4">1.2.2.</span> Alternative formulation</h4>
<div class="outline-text-4" id="text-1-2-2">
<ul class="org-ul">
<li>Shrinkage method solves the <b>unconstrained</b> optimization problem
\[
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  \]</li>
<li><p>
Alternative formulation solves a <b>constrained</b> optimization problem
</p>

<p>
\[
  \min_{\beta} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2, \text{ subject to: }
  \sum^p_{j=1} J(|B_j|) \le s
  \]
</p>
<dl class="org-dl">
<dt>tuning parameter</dt><dd>\(s \gt 0\)</dd>
</dl></li>

<li>The alternative formulation may greatly facilitate computation <b>at times</b>, e.g. in LASSO which is piecewise linear in \(s\).</li>
</ul>
</div>
</div>
<div id="outline-container-org91684c9" class="outline-4">
<h4 id="org91684c9"><span class="section-number-4">1.2.3.</span> Bayesian interpretation</h4>
<div class="outline-text-4" id="text-1-2-3">
<ul class="org-ul">
<li>For LR model (<a href="#orgd2e3766">1.1.1</a>):
<dl class="org-dl">
<dt>prior on &beta;</dt><dd>\(\pi(\beta)\)</dd>
<dt>independent prior on \(\sigma^2\)</dt><dd>\(\pi(\sigma^2)\)</dd>
<dt>posterior for \((\beta, sigma^2)\)</dt><dd>proportional to
\[
    \pi (\sigma^2)(\sigma^2)^{(n-1)/2}\exp\{-\frac{1}{2\sigma^2} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \log \pi(\beta) \}
    \]</dd>
</dl></li>
<li><p>
<b>Posterior maximization method</b> yields shrinkage estimator
</p>

<p>
\[
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \textbf{Pen}(\beta)
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org442cad8" class="outline-4">
<h4 id="org442cad8"><span class="section-number-4">1.2.4.</span> Choices of priors</h4>
<div class="outline-text-4" id="text-1-2-4">
<p>
i.e. choice of prior \(\pi(\beta)\)
</p>
<dl class="org-dl">
<dt>Normal prior</dt><dd>yields <b>ridge regression</b> etsimator</dd>
<dt>Laplace prior</dt><dd>yields <b>LASSO</b> estimator</dd>
</dl>
</div>
</div>
<div id="outline-container-orgb021faa" class="outline-4">
<h4 id="orgb021faa"><span class="section-number-4">1.2.5.</span> Ridge regression</h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
<b>Normal prior</b> assumes \(\beta_1 ... \beta_p\) are i.i.d. \(N(0, \tau^2)\) with prior density
\[
\pi(\beta) = \prod^p_{i=1} \frac{1}{\sqrt{2\pi}\tau} \exp\left
(-\frac{1}{2\tau^2}\beta_i^2\right)
\]
</p>

<p>
Yields <b>ridge regression</b> estimator, which minimizes
</p>

<p>
\[
\parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
\]
</p>
</div>
</div>
<div id="outline-container-org9b92d65" class="outline-4">
<h4 id="org9b92d65"><span class="section-number-4">1.2.6.</span> LASSO estimator</h4>
<div class="outline-text-4" id="text-1-2-6">
<ul class="org-ul">
<li><b>Laplace Prior</b>, assume  \(\beta_1 ... \beta_p\) are i.i.d. double-exponential (Laplace) \(\sim \text{Lapalce} (,\tau)\) with prior density
\[
  \pi(\beta) = \prod^p_{i=1} \frac{1}{2\tau} \exp \left(- \frac{1}{\tau} |\beta_i| \right)
  \]</li>
<li><p>
Yields <b>LASSO</b> estimator that minimizes
</p>

<p>
\[
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}|\beta_i|
  \]
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org334af2e" class="outline-3">
<h3 id="org334af2e"><span class="section-number-3">1.3.</span> Ridge Regression</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-org234ed2c" class="outline-4">
<h4 id="org234ed2c"><span class="section-number-4">1.3.1.</span> Ridge Regression Estimator</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
Assume these are observed: \(Y_i, x_{i1}, ..., x_{ip}\), and all are standardized:
\[
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
\]
</p>

<p>
In linear regression model without intercepts (<a href="#orgd2e3766">1.1.1</a>)
</p>

<p>
The ridge regression estimator is defined as:
</p>

<p>
\[
\hat{\beta^{\text{ridge}}} = \min_{\beta}
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
\]
</p>
</div>
</div>
<div id="outline-container-orgb26b9a7" class="outline-4">
<h4 id="orgb26b9a7"><span class="section-number-4">1.3.2.</span> Mathematical solution</h4>
<div class="outline-text-4" id="text-1-3-2">
<ul class="org-ul">
<li>Explicit expression is thus
\[
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  \]</li>
<li>Ridge regression <b>estimator or prediction</b>:
\[
  \hat{Y}^{\text{ridge}} = X_{n\times p}  \hat{\beta^{\text{ridge}}}
  \]</li>
<li>Requires <b>choosing</b> the tuning parameter \(\lambda\), based on data, usually <b>by cross-validation</b></li>
</ul>
</div>
</div>
<div id="outline-container-org73b94b3" class="outline-4">
<h4 id="org73b94b3"><span class="section-number-4">1.3.3.</span> Properties of Ridge Regression</h4>
<div class="outline-text-4" id="text-1-3-3">
<ul class="org-ul">
<li>Ridge regression <b>most useful</b> when \(X_{n\times p}\) is <b>non-singular</b>, but has <b>high collinearity</b>
<ul class="org-ul">
<li>i.e. \(X^T_{n\times p} X_{n\times p}\) has eigenvalue close to 0</li>
</ul></li>
<li>\(\hat{\beta^{\text{ridge}}}\) is biased, with bias \(\rightarrow\) 0 as \(\lambda \rightarrow 0\)</li>
<li>As \(\lambda\) increases, \(\hat{\beta^{\text{ridge}}}\) \(\rightarrow 0\), though rarely = 0.</li>
<li>Despite the bias, \(\text{Var}(\hat{\beta^{\text{ridge}}})\) will usually be smaller than OLS
<ul class="org-ul">
<li>Therefore better prediction than OLS.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgedcacd9" class="outline-4">
<h4 id="orgedcacd9"><span class="section-number-4">1.3.4.</span> Computational issues</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li>How to compute ridge regression efficiently for any \(\lambda\)?
\[
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  \]</li>
<li>It is highly non-trivial to compute the inverse of a large \(p\times p\) matrix.</li>
<li><b>Singular Value Decomposition</b> (SVD) algorithm:
<ul class="org-ul">
<li>Write the matrix \(X_{n\times p}\) in its SVD form
\[
    X_{n\times p} = U_{n\times p} D_{p\times p} V^T_{p\times p}
    \]
where: \(U\) and $V are orthogonal; D = diag(\(d_1, ..., d_p\)) is diagonal.</li>
</ul></li>
<li>Then: ridge regression estimator becomes the matrix product:
\[
   \hat{\beta^{\text{ridge}}} = V_{p\times p} \text{diag} \left(\frac{d_1}{d^2_1 + \lambda}, ..., \frac{d_p}{d^2_p + \lambda} \right) U^T_{p\times n} Y_{n\times 1}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org9d4d4c6" class="outline-4">
<h4 id="org9d4d4c6"><span class="section-number-4">1.3.5.</span> Example of SVD</h4>
<div class="outline-text-4" id="text-1-3-5">
<ul class="org-ul">
<li>Find SVD of matrix
\[
  X_{3 \times 2}
  = \begin{pmatrix}
  1 & 0 \\
  0 & 1 \\
  1 & 1 \\
  \end{pmatrix}
  = U_{n\times p} D_{p\times p} V^T_{p\times p}
  \]</li>
<li>Steps (required: \(p \leq n\)):
<ol class="org-ol">
<li>\(U_{n\times p}\) is the normalized \(p\) (largest) eigenvectors of \(XX^T\)</li>
<li>\(V_{p\times p}\) is the normalized eigenvectors of \(X^T X\)</li>
<li>Matrix \(D = \text{diag}(d_1, ..., d_p)\) with \(d_j\) being the square root of \(p\) (largest) eigenvalues of \(XX^T\) or \(X^T X\).</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org2b15aa0" class="outline-4">
<h4 id="org2b15aa0"><span class="section-number-4">1.3.6.</span> SVD Example (I): \(XX^T\)</h4>
<div class="outline-text-4" id="text-1-3-6">
<p>
For matrix <a href="#org9d4d4c6">1.3.5</a>:
we have
\[
XX^T = \begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 1 \\
1 & 1 &2\end{pmatrix}
\]
</p>
<ul class="org-ul">
<li>Characteristic polynomial is
\(-\lambda^3 + 4\lambda^2 - 3\lambda = -\lambda(\lambda-1)(\lambda-3)\)</li>
<li>The eigenvalues of \(XX^T\) are \(\lambda = 3, 1, 0\)</li>
<li>Corresponding eigenvectors are:
\[
  u'_1 = \begin{pmatrix}
  1 \\
  1 \\
  2\end{pmatrix},
  u'_2 = \begin{pmatrix}
  1 \\
  -1 \\
  0\end{pmatrix},
  u'_3 = \begin{pmatrix}
  1 \\
  1 \\
  -1\end{pmatrix},
  \]</li>
<li>Normalizing yields
\[
  U_{3\times 2} = \begin{pmatrix}
  1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
  1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
  2\over{\sqrt{6}} & 0 \end{pmatrix},
  \]</li>
<li>\(d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1\)</li>
</ul>
</div>
</div>

<div id="outline-container-org6ec2f30" class="outline-4">
<h4 id="org6ec2f30"><span class="section-number-4">1.3.7.</span> SVD Example (II): \(X^T X\)</h4>
<div class="outline-text-4" id="text-1-3-7">
<p>
For matrix <a href="#org9d4d4c6">1.3.5</a>:
\[
X^T X
= \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix}
\]
</p>

<ul class="org-ul">
<li>Characteristic polynomial is
\(\lambda^2 - 4\lambda + 3 = (\lambda -1)(\lambda -3)\)</li>
<li>Eigenvalues of \(XX^T\) are: \(\lambda = 3, 1\).</li>
<li>Corresponding eigenvalues are:
\[
  v'_1 = \begin{pmatrix}
  1 \\
  1\end{pmatrix},
  v'_2 = \begin{pmatrix}
  1 \\
  -1\end{pmatrix}
  \]</li>
<li>Normalizing them yields:
\[
  V_{2\times 2} = (v_1, v_2) =
  \begin{pmatrix}
  1\over\sqrt{2} & 1\over\sqrt{2} \\
  1\over\sqrt{2} & -1\over\sqrt{2}\end{pmatrix},
  \]</li>
<li>\(d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1\)</li>
</ul>
</div>
</div>
<div id="outline-container-org4546b56" class="outline-4">
<h4 id="org4546b56"><span class="section-number-4">1.3.8.</span> SVD verification</h4>
<div class="outline-text-4" id="text-1-3-8">
<p>
Might need to multiply some eigenvectors by -1.
</p>

\begin{equation}
X_{n\times p}
=
\begin{pmatrix}
1 & 0 \\
0 & 1 \\
1 & 1\end{pmatrix}
=
\begin{pmatrix}
1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
2\over{\sqrt{6}} & 0\end{pmatrix}
\begin{pmatrix}
\sqrt{3} & 0 \\
0 & 1\end{pmatrix}
\begin{pmatrix}
1\over{\sqrt{2}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{2}} & -1\over{\sqrt{2}}\end{pmatrix}

= U_{n\times p} D_{p\times p}V^T_{p\times p}
\end{equation}

\begin{equation}
= \lambda_1 u_1 v^T_1 + \lambda_2 u_2 v^T_2 =
0.5\begin{pmatrix}
1 & 1 \\
1 & 1 \\
2 & 2 \end{pmatrix} +
0.5\begin{pmatrix}
1 & -1 \\
-1 & 1 \\
0 & 0 \end{pmatrix}
\end{equation}
</div>
</div>
</div>
<div id="outline-container-org81ecaed" class="outline-3">
<h3 id="org81ecaed"><span class="section-number-3">1.4.</span> LASSO (3.2.1)</h3>
<div class="outline-text-3" id="text-1-4">
</div>
<div id="outline-container-org239a1e1" class="outline-4">
<h4 id="org239a1e1"><span class="section-number-4">1.4.1.</span> LASSO estimator</h4>
<div class="outline-text-4" id="text-1-4-1">
<ul class="org-ul">
<li>Assume these are observed: \(Y_i, x_{i1}, ..., x_{ip}\), and all are standardized:</li>
</ul>
<p>
\[
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
\]
</p>

<p>
In linear regression model without intercepts (<a href="#orgd2e3766">1.1.1</a>)
</p>

<dl class="org-dl">
<dt>LASSO</dt><dd>Least Absolute Selection and Shrinkage Operator</dd>

<dt>Definition</dt><dd>\[
  \hat{\beta}^{\text{lasso}} = \min_\beta \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1} |\beta_j|
  \]</dd>
<dt>s.t.</dt><dd>tuning parameter \(\lambda > 0\)</dd>
</dl>
</div>
</div>
<div id="outline-container-orgb488f31" class="outline-4">
<h4 id="orgb488f31"><span class="section-number-4">1.4.2.</span> L2-norm vs L1-norm</h4>
<div class="outline-text-4" id="text-1-4-2">

<div id="org1a356ea" class="figure">
<p><img src="./img/l2-l1-norm.png" alt="l2-l1-norm.png" />
</p>
</div>
<ul class="org-ul">
<li>L1-norm: <b>sparse</b>, as boundary points of the L1-norm ball have lower dimensions (are in lower-dimensional space, \(x_1 = 0, or x_2=0\))</li>
</ul>
</div>
</div>
<div id="outline-container-org3c8a29a" class="outline-4">
<h4 id="org3c8a29a"><span class="section-number-4">1.4.3.</span> Mathematical solution for LASSO estimator</h4>
<div class="outline-text-4" id="text-1-4-3">
<ul class="org-ul">
<li>In the LASSO optimization, there is <b>no explicit</b> mathematical solution to \(\hat{\beta}^{\text{lasso}}\)</li>
<li>Hence, need to use computational algorithms to get solution</li>
<li>Explicit solution only available when \(X^T X = I_{n\times n}\)
<ul class="org-ul">
<li><p>
In this case, LASSO estimator is:
</p>

\begin{equation}
\hat{\beta}^{\text{lasso}}_j =
\begin{cases}
  \hat{\beta}_j^{ols} - \frac{\lambda}{2} & \text{if }\hat{\beta}_j^{ols}> \frac{\lambda}{2}\\
  0 & \text{if }|\hat{\beta}_j^{ols}| \leq \frac{\lambda}{2}\\
  \hat{\beta}_j^{ols} + \frac{\lambda}{2} & \text{if }\hat{\beta}_j^{ols} < -\frac{\lambda}{2}\\
\end{cases}
\end{equation}</li>
</ul></li>
<li><p>
As: LASSO can be simplified to 1-dimensional optimization problem
</p>

<p>
\[
  \min_{-\infty < x < \infty} (x - \hat{\beta}_j^{ols})^2 + \lambda |x|
  \]
</p></li>
</ul>

<p>
since:
</p>

<p>
\[
\parallel Y - X\beta \parallel^2 = \parallel Y-X\hat{\beta}^{ols} \parallel^2 + (\beta - \hat{\beta}^{ols})^T X^T X(\beta - \hat{\beta}^{ols})
\]
</p>
</div>
</div>
<div id="outline-container-orge3b4b8f" class="outline-4">
<h4 id="orge3b4b8f"><span class="section-number-4">1.4.4.</span> Properties of LASSO</h4>
<div class="outline-text-4" id="text-1-4-4">
<ul class="org-ul">
<li><b>Good empirical performance when true model is sparse</b>
<ul class="org-ul">
<li>If so, outperforms AIC, BIC, stepwise, ridge</li>
</ul></li>
<li>Nice theoretical properties, i.e. high probability of the following under certain regularity conditions:
<dl class="org-dl">
<dt>parameter recovery</dt><dd>when \(|\hat{\beta}_j^{lasso}-\hat{\beta}^{true}|^2\) is small</dd>
<dt>variable selection</dt><dd>\(\textbf{supp}(\hat{\beta}_j^{lasso}) = \textbf{supp}(\beta^{true})\)</dd>
<dt>prediction error bound</dt><dd>\(|X\hat{\beta}_j^{lasso} - X\beta^{true}|^2\) is small</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org02111e3" class="outline-4">
<h4 id="org02111e3"><span class="section-number-4">1.4.5.</span> LASSO weaknesses</h4>
<div class="outline-text-4" id="text-1-4-5">
<ul class="org-ul">
<li>Not always consistent</li>
<li>Tends to select over-parameterized model</li>
<li>Does poorly when
<ol class="org-ol">
<li>True model is <b>not sparse</b></li>
<li>When few X variables are highly correlated (LASSO picks 1 randomly)</li>
<li>When the design \(X\) matrix is too correlated (Ridge outperforms)</li>
<li>When there are outliers in responses</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org73208fe" class="outline-4">
<h4 id="org73208fe"><span class="section-number-4">1.4.6.</span> Computation issues of LASSO</h4>
<div class="outline-text-4" id="text-1-4-6">
<ul class="org-ul">
<li>Computation algorithms include:
<ul class="org-ul">
<li>Coordinate descent</li>
<li>Sub-gradient methods</li>
<li>Proximal gradient methods</li>
</ul></li>
<li>Would be ideal to compute entire solution path all at once, i.e. for all values for \(\lambda\) simultaneously.</li>
</ul>
</div>
</div>
<div id="outline-container-org8519d7e" class="outline-4">
<h4 id="org8519d7e"><span class="section-number-4">1.4.7.</span> LASSO is piecewise linear</h4>
<div class="outline-text-4" id="text-1-4-7">
<ul class="org-ul">
<li>The number of linear pieces in LASSO path is approximately \(p\),</li>
<li>The computational complexity of getting whole LASSO path is \(O(np^2)\)
<ul class="org-ul">
<li>i.e. same cost as computing least-squares fit</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb284cf0" class="outline-4">
<h4 id="orgb284cf0"><span class="section-number-4">1.4.8.</span> Standard error of LASSO</h4>
<div class="outline-text-4" id="text-1-4-8">
<ul class="org-ul">
<li>How to estimate standard error of LASSO estimator i.e.
\[
  \hat{\beta}^{\text{lasso}} = \min_\beta \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1} |\beta_j|
  \]</li>
<li>Answer: bootstrapping:
<ol class="org-ol">
<li>Fix \(\lambda\), generate a set of bootstrap samples</li>
<li>Obtain corresponding \(\hat{\beta}^{lasso}(\lambda)\)</li>
<li>Repeat for \(L\) times and use them to estimate standard error</li>
<li>If not determined/fixed, \(\lambda\) can be estimated by cross-validation (e.g. 5 fold CV).</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orge0becd7" class="outline-4">
<h4 id="orge0becd7"><span class="section-number-4">1.4.9.</span> Variants of L1-norm</h4>
<div class="outline-text-4" id="text-1-4-9">
<ul class="org-ul">
<li>Elastic net
\[
  \hat{\beta}^{enet} = \min_\beta \parallel Y_{n\times 1} - X_{n \times p} \beta_{p \times 1} \parallel^2 + \lambda_1 \sum^p_{j=1} |\beta_j| + \lambda_2 \sum^p_j (\beta_j)^2
  \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org64a0ef4" class="outline-3">
<h3 id="org64a0ef4"><span class="section-number-3">1.5.</span> Principal Components (3.2.2)</h3>
<div class="outline-text-3" id="text-1-5">
<p>
Assume these are observed: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)
</p>

<p>
Classical datasets: mostly small values of \(p\); modern datasets: large \(p\).
</p>

<p>
Essential to conduct <b>dimension reduction</b> to reduce the number of variables.
</p>
</div>
<div id="outline-container-org71203a2" class="outline-4">
<h4 id="org71203a2"><span class="section-number-4">1.5.1.</span> Dimension reduction</h4>
<div class="outline-text-4" id="text-1-5-1">
<ul class="org-ul">
<li>2 approaches:
<ol class="org-ol">
<li><b>variable selection</b>, i.e.: AIC, BIC, stepwise algorithm, LASSO, etc.</li>
<li><b>feature extraction</b>, i.e. identify which functions of data are most important. <b>no restricted</b> to using existing features/variables. Options are:
<ul class="org-ul">
<li>Principal component analysis</li>
<li>Partial least squares</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org4b9f9b8" class="outline-4">
<h4 id="org4b9f9b8"><span class="section-number-4">1.5.2.</span> Motivation of PCA</h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
"Obtain more variance by transforming axes"
</p>
<ul class="org-ul">
<li>Find <b>linear combinations</b> of \((x_1, ..., x_p)\) that express as much variability in \(X\) as possible.
<ul class="org-ul">
<li>A linear combination with <b>high</b> variance will likely affect the response the most</li>
<li>If most variation of \(X\) comes from the first few PCs then: enough to build models.</li>
<li>Other linear combination vary so little among different observations &rarr; can be ignored</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge8f1b5d" class="outline-4">
<h4 id="orge8f1b5d"><span class="section-number-4">1.5.3.</span> Find the PC's:  Population version</h4>
<div class="outline-text-4" id="text-1-5-3">
<ul class="org-ul">
<li><b>Optimization problem for PC's</b>: Given a \(p\) -dim random vector
\[
  \textbf{X} = (X_1, ..., X_p)^T, \text{ with covariance } \Sigma = \text{Cov}(X)
  \]</li>
<li>PC1: Find \(U_1 = \alpha_1 X_1 + ... + \alpha_p X_p\) that maximizes
\[
  \textbf{Var}(\alpha_1 X_1 + ... + \alpha_p X_p) = \textbf{Var}(\alpha^T \textbf{X}) = \alpha^T \Sigma \alpha
  \]
subject to:
\[
  \alpha^2_1 + ... \alpha^2_p = 1, \text{i.e. } \alpha^T \alpha = 1, \text{where } \alpha = (\alpha_1, ..., \alpha_p)^T
  \]</li>
<li>PC2: Find \(U_2 = \alpha_1 X_1 + ... + \alpha_p X_p\) that maximizes \text{Var}(&alpha;<sup>T</sup> X) = &alpha;<sup>T</sup> &Sigma;&alpha;$, subject to <b>constraints</b>: - \(\alpha^T \alpha = 1\)
<ul class="org-ul">
<li>\(\text{Cov}(U_1, U_2) = 0\)</li>
</ul></li>
<li>Other (later) PC are defined analogously and uncorrelated with all previous PC's</li>
</ul>
</div>
</div>
<div id="outline-container-org03f6900" class="outline-4">
<h4 id="org03f6900"><span class="section-number-4">1.5.4.</span> Eigenvectors lead to PC's</h4>
<div class="outline-text-4" id="text-1-5-4">
<ul class="org-ul">
<li>Theorem: Let covariance matrix \(\bf{\Sigma} = \text{Cov}(\bf{X})\) have eignvectors \(e_1, ..., e_p\) with corresponding eigenvalues \(\lambda_1 \ge ... \ge \lambda_p \ge 0\). For \(j=1,2,...,p\),
<ul class="org-ul">
<li>\(j\) -th PC is
\[
    U_j = e_j^T X = e_{j1}X_1 + ... + e_{jp}X_p
    \]</li>
<li>Variance of \(j\) -th PC is
\[
    \text{Var}(U_j) = \bf{e_j^T\Sigma e_j} = \lambda_j
    \]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org14fcde0" class="outline-4">
<h4 id="org14fcde0"><span class="section-number-4">1.5.5.</span> Proof by Lagrange Multipliers</h4>
<div class="outline-text-4" id="text-1-5-5">
<ul class="org-ul">
<li>The Lagrange multiplier is to maximize
\[
  \phi(\alpha) = \alpha^T \Sigma\alpha - \lambda(\alpha^T \alpha -1)
  \]</li>
<li>Setting derivatives qual = 0 gives:
\[
  \frac{\partial \phi(\alpha)}{\partial\alpha} = 2\Sigma\alpha - 2\lambda\alpha = 0
  \]</li>
<li>Thus: \(\Sigma\alpha = \lambda\alpha\) &rarr;
&lambda; is an eigenvalue of &Sigma; and &alpha; is the corresponding normalized eigenvector.</li>
<li>For \(U = \alpha^T X\), we have \(\text{Var}(U) = \alpha^T \Sigma\alpha = \alpha^T (\lambda\alpha) = \lambda\).</li>
<li>For PC1, we need to find largest eigenvector of &Sigma;.</li>
<li>Proofs of other Pcs are similar.</li>
</ul>
</div>
</div>
<div id="outline-container-orgdfbc1d5" class="outline-4">
<h4 id="orgdfbc1d5"><span class="section-number-4">1.5.6.</span> PCs in Empirical Version</h4>
<div class="outline-text-4" id="text-1-5-6">
<p>
In many real world applications, only given dataset
\(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)
How to find PC's?
</p>
<ul class="org-ul">
<li>Key idea: estimate the unknown &Sigma; by \(\hat{\Sigma}_{p\times p}\) from the data, then find the PC's by the eigenvalues and eigenvectors of \(\hat{\Sigma}_{p \times p}\)</li>
<li><b>Empirical covariance matrix</b> \(\hat{\Sigma}_{p\times p}\) is widely used when \(p<n\)
<ul class="org-ul">
<li>Here, the \((r,s)\) entry of \(\hat{\Sigma}_{p\times p}\) is defined as:
\[
    \hat{\Sigma}_{rs} = \frac{1}{n} \sum^n_i(x_{ir}-\bar{x}_r)(x_{is}-\bar{x}_s)
    \]</li>
<li>Research tbd on how to estimate \(\Sigma\) effectively when \(p>>n\).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb2bad38" class="outline-4">
<h4 id="orgb2bad38"><span class="section-number-4">1.5.7.</span> Principal component regression</h4>
<div class="outline-text-4" id="text-1-5-7">
<ul class="org-ul">
<li>Original data: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>After we extract all PC's, raw data can be written as new format
\((Y_i, u_{i1}, ..., u_{ip})\) for \(i=1,2,...,n\)</li>
<li>Principal component regression: linear regression by using only first \(k\) PC's:
\[
  Y_i = \beta_0 + \beta_1 u_{i1} + ... + \beta_k u_{ik} + \epsilon_i
  \]</li>
<li>Choosing \(k\): done by <b>cross-validation</b>.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgafffd1c" class="outline-3">
<h3 id="orgafffd1c"><span class="section-number-3">1.6.</span> Partial least squares (3.2.3)</h3>
<div class="outline-text-3" id="text-1-6">
</div>
<div id="outline-container-org85f38b7" class="outline-4">
<h4 id="org85f38b7"><span class="section-number-4">1.6.1.</span> Dimension reduction</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
There are 2 kinds of dimension reduction algorithms:
</p>
<ol class="org-ol">
<li>Unsupervised dimension reduction, e.g. PCA. Criticisms: it explains \(X\) but no reason to be sure that the result also explains a response \(Y\).</li>
<li>Supervised dimension reduction, i.e. conduct reduction on \(X\) by using the extra information in \(Y\).
<ul class="org-ul">
<li>Reasonable to believe that supervised techniques will <b>do better</b></li>
<li><b>Partial least squares</b> is one such technique</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-org363c5c9" class="outline-4">
<h4 id="org363c5c9"><span class="section-number-4">1.6.2.</span> Partial least squares</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
Collection of techniques with 2 common properties:
</p>
<ol class="org-ol">
<li><b>Maximizes correlation between \(Y \& X\)</b>, rather than maximizing variance of \(Y\) only.</li>
<li>Can be interpreted as finding the underlying factors of \(X\) that are also underlying factors of \(Y\).</li>
</ol>
</div>
</div>
<div id="outline-container-org4e4aac5" class="outline-4">
<h4 id="org4e4aac5"><span class="section-number-4">1.6.3.</span> 2 versions of PLS</h4>
<div class="outline-text-4" id="text-1-6-3">
<ol class="org-ol">
<li>Simple PLS algorithm: variant of PC's but using <b>correlation</b> instead of variance</li>
<li>PLS model: identify common factors of \(X \& Y\)</li>
</ol>
</div>
</div>
<div id="outline-container-orgbcc911d" class="outline-4">
<h4 id="orgbcc911d"><span class="section-number-4">1.6.4.</span> Simple PLS algorithm</h4>
<div class="outline-text-4" id="text-1-6-4">
<ul class="org-ul">
<li>Given \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>Let \(x_i = (x_{i1}, ..., x_{ip})^T\).</li>
<li>First PLS, \(V_1 = \alpha_1 X_1 + ... + \alpha_p X_p\) is defined as finding \(\alpha = (\alpha_1, ..., \alpha_p)^T\) that maximizes <b>covariance</b>
\[
  \hat{\textbf{CoV}}(Y, V_1) = \frac{1}{n} \sum^n_i (Y_i-\bar{Y})(v_i-\bar{v}),
  \]
when
\(v_i = \alpha_1 x_{i1} + ... + \alpha_p x_{ip}\) for \(i=1,2,...n\) subject to \(\alpha^T \alpha = 1\)</li>
<li>Later PLSs are defined analogously to maximize the covariance and are <b>assumed to be uncorrelated</b> with all previous PLSs.</li>
<li><b>Solution</b>: the \(\alpha\)'s are the eigenvectors of the \(p \times p\) matrix \(X^T Y Y^T X\) when the data matrices \(X \& Y\) have column mean zero.</li>
</ul>
</div>
</div>
<div id="outline-container-orgc2b0919" class="outline-4">
<h4 id="orgc2b0919"><span class="section-number-4">1.6.5.</span> The PLS model</h4>
<div class="outline-text-4" id="text-1-6-5">
<ul class="org-ul">
<li>Data: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>Assume data have mean 0.</li>
<li>Write data matrix as \((Y_{n \times  q}, X_{n \times  p})\)</li>
<li>Goal: find \(\ell\) linear combinations from \(X \& Y\) to use as new dimensions.</li>
<li><p>
The PLS model: noniterative iterative partial least squares (NIPALS):
</p>

<p>
\[
  X_{n \times  p} = T_{n \times \ell} P_{\ell \times  p} + E,
  Y_{n \times  q} = U_{n \times  \ell} Q_{\ell \times  q} + F
  \]
where:
</p>
<ul class="org-ul">
<li>\(T_{n \times \ell}\) and \(U_{n \times \ell}\) represent \(\ell\) factors</li>
<li>\(P_{\ell \times  p}\) and \(Q_{\ell \times q}\) are loadings.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org808f74f" class="outline-4">
<h4 id="org808f74f"><span class="section-number-4">1.6.6.</span> Key idea in The PLS Model</h4>
<div class="outline-text-4" id="text-1-6-6">
<ul class="org-ul">
<li>How to estimate the &ell; factors, or the \(T_{n \times \ell}\) and \(U_{n \times \ell}\) matrices?</li>
<li>Answer:
<ul class="org-ul">
<li>Write the first column of  \(T_{n \times \ell}\) and \(U_{n \times \ell}\)  as \(t=\bf{X} r\) and \(u=\bf{Y} s\) for two unit vectors, \(\parallel r \parallel = \parallel s \parallel = 1\)</li>
<li>Find \(r\) and \(s\) that maximizes the <b>covariance-squared</b>: \(\textbf{Cov}^2(Xr, Ys)\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga5764e0" class="outline-4">
<h4 id="orga5764e0"><span class="section-number-4">1.6.7.</span> PLS for linear regression</h4>
<div class="outline-text-4" id="text-1-6-7">
<ul class="org-ul">
<li>Original data: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>After extracting all PLS's, raw data can be written as new formats \((Y_i, v_{i1}, ..., v_{ip})\) for \(i=1,2,...,n\)</li>
<li>Partial least squares regression: linear regression using only <b>first k</b> PLSs:
\[
  Y_i = \beta_0 + \beta_1 v_[i1] + ... + \beta_k v_{ik} + \epsilon_i
  \]</li>
<li>Choosing \(k\): by <b>cross-validation</b></li>
</ul>
</div>
</div>
<div id="outline-container-orgf3ca7b4" class="outline-4">
<h4 id="orgf3ca7b4"><span class="section-number-4">1.6.8.</span> Canonical correlation analysis (CCA)</h4>
<div class="outline-text-4" id="text-1-6-8">
<ul class="org-ul">
<li>CCA: find the unit vectors \((r, s)\) that maximizes the <b>correlation coefficient</b>
\[
  \text{Corr}(Xr, Ys) = \frac{r^T \Sigma_{XY}s}{\sqrt{r^T \Sigma_{XX}r}\sqrt{s^T \Sigma_{YY}s}}
  \]</li>
<li><p>
Solution: in the population(?) version with \((X_1, ..., X_p)\) and \((Y_1, ..., Y_q)\), consider \(r^T X = r_1 X_1 + ... + r_p X_p\) and \(s^T Y = s_1 Y_1 + ... + s_q Y_q\), the optimal \(r, s\) values are the respective eigenvectors of
</p>

<p>
\[
  \Sigma^{-1}_{XX} \Sigma^{-1}_{YY} \Sigma_{YX} \text{ and }
  \Sigma^{-1}_{YY} \Sigma^{-1}_{XX} \Sigma_{XY}
  \]
</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org86e4513" class="outline-2">
<h2 id="org86e4513"><span class="section-number-2">2.</span> Week 4: Linear Classification</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-orgdcf0964" class="outline-3">
<h3 id="orgdcf0964"><span class="section-number-3">2.1.</span> Overview of Linear Discriminant Analysis (4.1.1)</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Supervised learning recap
</p>
<ul class="org-ul">
<li>Data: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>Objective: predict \(Y\) for given new input \(x_{\text{new}} = (x_1, ..., x_p)\)</li>
<li>Types of tasks:
<ol class="org-ol">
<li>Regression: why response \(Y\) is continuous</li>
<li>Regression: When response $Y is binary, or discrete values representing classes</li>
</ol></li>
</ul>
</div>
<div id="outline-container-orga622f90" class="outline-4">
<h4 id="orga622f90"><span class="section-number-4">2.1.1.</span> Classification problem</h4>
<div class="outline-text-4" id="text-2-1-1">
<ul class="org-ul">
<li>For \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\) where \(Y_i \in {1, 2, ...K}\) is the class label</li>
<li>Objective: find a decision function to <b>discriminate</b> among data form the \(K\) different classes.
<ul class="org-ul">
<li>Learn a decision rule \(h(x) \in {1, 2, ...k}\) used to separate the \(K\) classes</li>
<li>Predict the class label for new input \(x_{\text{new}}\)</li>
</ul></li>
<li>Example: classifying patients in hospital ER into classes (low, med, high risk) by age, gender, weight, BP, insurance, etc.
<ul class="org-ul">
<li>Assign treatment priority to high-risk patients?</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org29105d4" class="outline-4">
<h4 id="org29105d4"><span class="section-number-4">2.1.2.</span> Popular classification methods and R packages</h4>
<div class="outline-text-4" id="text-2-1-2">
<dl class="org-dl">
<dt>Discriminant analysis</dt><dd>?</dd>
</dl>
<p>
a- Tree-based classifiers :: <code>rpart</code>
</p>
<dl class="org-dl">
<dt>Boosting</dt><dd><code>gbm</code></dd>
<dt>Random forest</dt><dd><code>randomForest</code></dd>
<dt>Neural networks</dt><dd><code>nnet</code></dd>
<dt>svm</dt><dd><code>e1071</code></dd>
</dl>
</div>
</div>
<div id="outline-container-orgd3bcd37" class="outline-4">
<h4 id="orgd3bcd37"><span class="section-number-4">2.1.3.</span> Discriminant analysis</h4>
<div class="outline-text-4" id="text-2-1-3">
<ul class="org-ul">
<li>A multi-classifier is associated with a \(K\) - dim vector
\[
  d(x) = (d_1 (x), ..., d_K (x))
  \]
where \(d_k (x)\) represents the strength of evidence that \(x\) is in class \(k\).</li>
<li><p>
For given new input \(x_{\text{new}}\), predict the class label
</p>

<p>
\[
  \hat{k} = \text{argmax}_{k=1, 2, ..., K} d_k (x_\text{new})
  \]
</p></li>
<li>Discriminant functions \(d_k (x)\) are <b>linear</b> functions of \(x = (x_1, ..., x_p)\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgbdc7cbd" class="outline-4">
<h4 id="orgbdc7cbd"><span class="section-number-4">2.1.4.</span> Example for 3-class problem</h4>
<div class="outline-text-4" id="text-2-1-4">
<p>
<img src="./img/3-class.png" alt="3-class.png" />
Predict \(Y \in {1,2,3}\) based on \((X_1, X_2)\):
</p>
<ul class="org-ul">
<li>Class 1 if \(d_1 > \max(d_2, d_3)\)</li>
<li>Class 2 if \(d_2 > \max(d_1, d_3)\)</li>
<li><p>
Class 3 if \(d_3 > \max(d_1, d_2)\)
</p>

<p>
In this example, boundaries \({(x_1, x_2): d_i (x_1, x_2) = d_j(x_1, x_2)}\) are linear.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org5ce0ed6" class="outline-4">
<h4 id="org5ce0ed6"><span class="section-number-4">2.1.5.</span> Discriminant functions</h4>
<div class="outline-text-4" id="text-2-1-5">
<p>
How to construct discriminant functions \(d_k (x)\) 's?
3 approaches are available:
</p>
<ol class="org-ol">
<li>Distance-based discriminant analysis, e.g. \(d_k\) is the distance between the sample mean of the \(k\) -th class and the new input \(x\).</li>
<li>Bayes Rules</li>
<li>Probability-based rule, i.e. Logistic Regression</li>
</ol>
</div>
</div>
<div id="outline-container-org866bef0" class="outline-4">
<h4 id="org866bef0"><span class="section-number-4">2.1.6.</span> Multi-class vs. Binary</h4>
<div class="outline-text-4" id="text-2-1-6">
<p>
If the number \(K\) of classes is not too large, can simplify the multi-class problems into <b>series of</b> binary problems via two approaches:
</p>
<dl class="org-dl">
<dt>one vs rest</dt><dd>training binary classifiers with &part;<sub>k</sub>(X) separating class \(k\) from the rest. It's easy to implement but poor performance if no dominating class. Binary problems are unbalanced.</dd>
<dt>pairwise comparison approach</dt><dd>train \(K(K-1)/2\) binary classifiers. The final class prediction is decided by a voting scheme among all classifiers.</dd>
</dl>
</div>
</div>
</div>
<div id="outline-container-orgaf5d8c5" class="outline-3">
<h3 id="orgaf5d8c5"><span class="section-number-3">2.2.</span> Linear discrimination analysis continued (4.1.2)</h3>
<div class="outline-text-3" id="text-2-2">
<p>
See: <a href="#orgd3bcd37">2.1.3</a>.
</p>

<p>
LDA was first developed as distance-based classification (Fisher, 1936). Alternate interpretation: Bayes.
</p>
</div>
<div id="outline-container-orge50db44" class="outline-4">
<h4 id="orge50db44"><span class="section-number-4">2.2.1.</span> Bayes classifier</h4>
<div class="outline-text-4" id="text-2-2-1">
<ul class="org-ul">
<li>Under \(0-1\) loss, Bayes classifier is
\[
  \text{argmax}_k  (P(Y=k | x))
  = \text{argmax}_k (\pi_k f_k (x))
  = \text{argmax}_k (\log(\pi_k) + \log(f_k (x)))
  \]
where:
<dl class="org-dl">
<dt>\(\pi_k\)</dt><dd>a prior</dd>
<dt>\(f_k\)</dt><dd>the density function of the \(k\) -th class</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org8cb019d" class="outline-4">
<h4 id="org8cb019d"><span class="section-number-4">2.2.2.</span> Normality asumption</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
LDA is the Bayes Rule under assumption that <b>densities</b> \(f_k\) are <b>multivariate normal</b> with the <b>common covariance</b>, i.e. \(N(\mu_k, \Sigma)\).
</p>

<p>
Mathematically, LDA assumes:
\[
f_k(x) = \frac{1}{\sqrt{2\pi}|\text{det}(\Sigma)} \exp\left({-\frac{1}{2} (x-\mu_k)^T \Sigma^{-1}(x-\mu_k)}\right)
\]
for \(k=1,2,...,K\)
</p>
</div>
</div>
<div id="outline-container-org67e775b" class="outline-4">
<h4 id="org67e775b"><span class="section-number-4">2.2.3.</span> Bayes classifier for normal distribution</h4>
<div class="outline-text-4" id="text-2-2-3">
<ul class="org-ul">
<li><p>
If the \(f_k\) are normal with common variance, i.e. \(N(\mu_k, \Sigma)\), then the Bayes classification rule is
\[
  \text{argmax}_k \left(
  -\frac{1}{2} \text{logdet}(\Sigma) - \frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) + \log \pi_k
  \right)
  \]
</p>

<p>
\[
  = \text{argmax}_k \left(
  x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu^T_k \Sigma^{-1} \mu_k + \log \pi_k
  \right)
  \]
</p>

<p>
since the common variance term drops out.
</p></li>

<li>Leads to linear discrimination function:
\[
  d_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1} \mu_k + \log \pi_k
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orga8605a1" class="outline-4">
<h4 id="orga8605a1"><span class="section-number-4">2.2.4.</span> LDA in practice</h4>
<div class="outline-text-4" id="text-2-2-4">
<ul class="org-ul">
<li>Data: for \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\) where \(Y_i \in {1, 2, ...K}\) is the class label</li>
<li>LDA uses the  discrimination function:
\[
  d_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1} \mu_k + \log \pi_k
  \]</li>
<li>In practice, parameters are estimated from training data as follows:
<dl class="org-dl">
<dt>\(\hat{\pi_k}\)</dt><dd>\(\frac{n_k}{n}\)</dd>
<dt>\(\hat{\mu_k}\)</dt><dd>\(\frac{1}{n_k} \sum_{y_i=k} x_i\) where \(n_k\) is the no. of observations in class \(k\)</dd>
<dt>\(\hat{\Sigma}\)</dt><dd>\(\frac{1}{\sum^K_{k=1}(n_k -1)} \sum^K_{k=1} \sum_{y_i=k}(x_i - \hat{\mu}_k)(x_i-\hat{\mu}_k)^T\) which is the within-class sample variance</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org2802fca" class="outline-4">
<h4 id="org2802fca"><span class="section-number-4">2.2.5.</span> LDA for K=2 classes</h4>
<div class="outline-text-4" id="text-2-2-5">
<ul class="org-ul">
<li><p>
Classifies into class 2 if and only if
</p>
\begin{equation}
x_T \hat{\Sigma}^{-1} - \frac{1}{2} \hat{\mu}^T_2 \hat{\Sigma}^{-1} \hat{\mu}_2 + \log \hat{\pi}_2 >
x^T \Sigma^{-1}\hat{\mu}_1 - \frac{1}{2} \hat{\mu}_1^T \hat{\Sigma}^{-1} \hat{\Sigma}_1 + \log \hat{\pi}_1
\end{equation}
<p>
or
</p>
 \begin{equation}
 x^T \Sigma^{-1}(\hat{\mu}_2- \hat{\mu}_1) >
  \frac{1}{2} \hat{\mu}_2^T \hat{\Sigma}^{-1} \hat{\mu}_2 -
  \frac{1}{2}\hat{\mu}_1^T \Sigma^{-1} \hat{\mu}_1 + \log{\frac{n_1}{n}} - \log{\frac{n_2}{n}}
\end{equation}

<ul class="org-ul">
<li>let \(w=\Sigma^{-1} (\hat{\mu}_2 - \hat{\mu}_1)\), the LHS = \(x^T w = w \cdot x = w_1 x_1 + ... + w_p x_p\), is the projection of the \(p\) -th dimensional vector \(x\) to a real-valued number</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge715ea2" class="outline-4">
<h4 id="orge715ea2"><span class="section-number-4">2.2.6.</span> Fisher's distance-based approach</h4>
<div class="outline-text-4" id="text-2-2-6">
<ul class="org-ul">
<li>for \(k=2\) classes:
<ul class="org-ul">
<li>projects the p-dimensional vector \(\textbf{x}\) to a real-valued number
\[
    L = \textbf{w} \cdot \textbf{x} = x^T w = w_1x_1 + ... + w_p x_p
    \]</li>
<li>Find the optimal direction \(\textbf{w}\) that best separates two classes on the projection line, using <b>training data</b></li>
<li>Assign new point \(x\) to class 2 if and only if:
\[
    \textbf{w} \cdot \textbf{x} > \textbf{w} \cdot \frac{\hat{\mu}_1 + \hat{\mu}_2}{2}
    \]</li>
<li><p>
Equivalently: assign \(x\) to class 2 if
</p>
\begin{equation}
x^T \Sigma^{-1}(\hat{\mu}_2- \hat{\mu}_1) > \frac{1}{2} \hat{\mu}_2^T \hat{\Sigma}^{-1} \hat{\mu}_2 - \frac{1}{2}\hat{\mu}_1^T \Sigma^{-1} \hat{\mu}_1
\end{equation}

<p>
which is the same as Bayes-based LDA when the classes have equal numbers of observations.
</p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga001928" class="outline-3">
<h3 id="orga001928"><span class="section-number-3">2.3.</span> Quadratic Discrimination Analysis classifier (4.1.3)</h3>
<div class="outline-text-3" id="text-2-3">
<p>
See <a href="#orgd3bcd37">2.1.3</a>.
How to choose suitable discriminant functions \(d_k(x)\)?
</p>
</div>
<div id="outline-container-orgf409618" class="outline-4">
<h4 id="orgf409618"><span class="section-number-4">2.3.1.</span> Bayes classifier</h4>
<div class="outline-text-4" id="text-2-3-1">
<ul class="org-ul">
<li>The discriminant functions \(d_k(x)\) are based on <b>posterior</b> distributions</li>
<li><p>
The Bayes classifier is defined as:
</p>
\begin{equation}
\text{argmax}_k (P(Y=k|x))
= \text{argmax}_k (\pi_k f_k (x))
= \text{argmax}_k (\log \pi_k + \log f_k (x))
\end{equation}
<p>
where:
</p>
<ul class="org-ul">
<li>\(\pi_k\) is a <b>prior</b></li>
<li>\(f_k\) is the density function of the \(k\) th class</li>
</ul>
<p>
Question: how to model the density functions \(f_k\)? <b>Normal distribution!</b>
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org1e7498c" class="outline-4">
<h4 id="org1e7498c"><span class="section-number-4">2.3.2.</span> Normal distribution</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
Univariate normal distribution \(N(\mu, \sigma^2)\)
</p>
<ul class="org-ul">
<li>Probability density function is given by:
\[
  p(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp \left( -\frac{1}{2\sigma^2} (x-\mu)^2\right)
  \]</li>
<li>parameter estimation of \((\mu, \sigma^2)\) from training data:
<dl class="org-dl">
<dt>sample mean, \(\hat{\mu}\)</dt><dd>\(\bar{x} = \frac{1}{n} \sum^n_{i=1}x_i\)</dd>
<dt>sample variance, \(\hat{\sigma}^2\)</dt><dd>\(\frac{1}{n-1} \sum^n_{i=1} (x_i - \bar{x})^2\)</dd>
</dl></li>
<li>properties: the components of \(X\) are independent iif \(\Sigma\) is diagonal!</li>
</ul>
</div>
</div>
<div id="outline-container-org3bc0dba" class="outline-4">
<h4 id="org3bc0dba"><span class="section-number-4">2.3.3.</span> Normal model for \(f_k\)</h4>
<div class="outline-text-4" id="text-2-3-3">
<ul class="org-ul">
<li>See <a href="#orgd3bcd37">2.1.3</a></li>
<li>Bayes classifier:
\(\text{argmax}_k (\log \pi_k + \log f_k (x))\) where \(\pi_k\) is a prior and \(f_k\) is the pdf of the \(k\) th class.</li>
<li>Model: assume pdfs \(f_k(x) = f_k(x_1, ..., x_p) \sim N(\mu_k, \Sigma_k)\)</li>
<li>Question: how to estimate \((\mu_k, \Sigma_k)\)?</li>
</ul>
</div>
</div>
<div id="outline-container-org8520a2b" class="outline-4">
<h4 id="org8520a2b"><span class="section-number-4">2.3.4.</span> Three approaches to estimate</h4>
<div class="outline-text-4" id="text-2-3-4">
<p>
Different assumptions:
</p>
<dl class="org-dl">
<dt>Linear discriminant analysis</dt><dd>When \(\Sigma_k \equiv \Sigma\) (common variance), estimated by within-sample covariance</dd>
<dt>Quadratic discriminant analysis</dt><dd>when \(\Sigma_k\) is estimated by the sample covariance of the \(k\) -th class</dd>
<dt>Naive Bayes</dt><dd>when each component of \(\textbf{X}\) is independent, i.e. when \(\Sigma_k \equiv \Sigma = \text{diag}(\sigma^2_{k1}, ..., \sigma^2_{kp})\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org8cd3fb9" class="outline-4">
<h4 id="org8cd3fb9"><span class="section-number-4">2.3.5.</span> QDA classifier</h4>
<div class="outline-text-4" id="text-2-3-5">
<ul class="org-ul">
<li>Assumes that \(f_k\) are normal \(N(\mu_k, \Sigma_k)\)</li>
<li>Assigns data \(x\) to the class:
\[
  \text{argmax}_k \left(
  -\frac{1}{2} \log \text{det}(\Sigma_k) -
  \frac{1}{2}(x-\mu_k)^T \Sigma^{-1}_k (x-\mu_k) + \log \pi_k
  \right)
  \]</li>
<li>In practice, estimated from training data:
<dl class="org-dl">
<dt>\(\hat{\pi}_k\)</dt><dd>\(\frac{n_k}{n}\)</dd>
<dt>\(\hat{\mu}_k\)</dt><dd>\(\frac{1}{n_k} \sum_{y_i = k} x_i\) where n<sub>k</sub> is the no. of observations in class \(k\)</dd>
<dt>\(\hat{\Sigma}_k\)</dt><dd>\(\frac{1}{n_k-1} \sum_{y_i=k} (x_i-\hat{\mu}_k)(x_i - \hat{\mu}_k)^T\) which is the sample covariance of class \(k\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org4703aec" class="outline-4">
<h4 id="org4703aec"><span class="section-number-4">2.3.6.</span> Naive Bayes classifier</h4>
<div class="outline-text-4" id="text-2-3-6">
<ul class="org-ul">
<li>Assumes that \(\Sigma_k \equiv \Sigma = \text{diag}(\sigma^2_{k1}, ..., \sigma^2_{kp})\), or equivalently:
\[
  f_k(x) = f_k (x_1, ..., x_p) = \prod^p_{j=1} f_{kj}(x_j),
  f_{kj} \sim N(\mu_{kj}, \sigma^2_{kj})
  \]</li>
<li>Assigns \(x\) to class \(\text{argmax}_k (\pi_k \prod^p_{j=1}) f_{k,j}(x_j)\)</li>
<li>In practice, parameters \((\mu_{kj}, \sigma^2_{kj})\) estimated from the \(j\) -th component of \(X\) variables for \(k\) -th class in training data:
<dl class="org-dl">
<dt>\(\hat{\pi}\)</dt><dd>\(\frac{n_k}{n}\)</dd>
<dt>\(\hat{\mu}_{kj}\)</dt><dd>\(\frac{1}{n_k} \sum_{y_i = k} x_{ij}\)</dd>
<dt>\(\hat{\sigma^2_{kj}}\)</dt><dd>\(\frac{1}{n_k-1} \sum_{y_i = k} (x_{ij}-\bar{x}_j)^2\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org645cba5" class="outline-4">
<h4 id="org645cba5"><span class="section-number-4">2.3.7.</span> General Naive Bayes classifier</h4>
<div class="outline-text-4" id="text-2-3-7">
<ul class="org-ul">
<li><b>Ignore</b> any dependence between explanatory variables and assume that \(X_j\)'s are independent.</li>
<li>Thus, corresponding classifier is
\[
  \text{argmax}_k (\pi_k \prod^p_{j=1}f_{k,j}(x_j))
  \]</li>
<li>In practice, marginal densities \(f_{k,j} (\cdot)\) are usually assumed to be parameterized by some parameters, which must be estimated, e.g.:
<ul class="org-ul">
<li>Gaussian Naive Bayes</li>
<li>Bernoulli Naive Bayes</li>
<li>Multinomial Naive Bayes</li>
<li>Poisson Naive Bayes</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge6236e4" class="outline-3">
<h3 id="orge6236e4"><span class="section-number-3">2.4.</span> Logistic Regression: Estimation (4.2.1)</h3>
<div class="outline-text-3" id="text-2-4">
</div>
<div id="outline-container-org48c0295" class="outline-4">
<h4 id="org48c0295"><span class="section-number-4">2.4.1.</span> Classification methods</h4>
<div class="outline-text-4" id="text-2-4-1">
<ul class="org-ul">
<li>With data \((Y_i, X_i)\) for \(i=1,...,n\), where \(Y_i\) is the class label</li>
<li>Two approaches to develop classifiers:
<ol class="org-ol">
<li>Model conditional densities \(f_k = p(\textbf{X}|Y=k)\) at the given \(k\) -th class
<ul class="org-ul">
<li><b>Normality assumption on X</b>: LDA, QDA, Naive Bayes</li>
</ul></li>
<li>Model the conditional density \(P(Y=k|\textbf{X})\) directly:
<ul class="org-ul">
<li><b>Bernoulli assumption on Y</b>: Logistic Regression</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgf16f3b8" class="outline-4">
<h4 id="orgf16f3b8"><span class="section-number-4">2.4.2.</span> Binary logistic regression</h4>
<div class="outline-text-4" id="text-2-4-2">
<ul class="org-ul">
<li>With data: \((Y_i, x_{i1}, ..., x_{i,p-1})\) for \(i=1,...,n\), where \(Y_i \in {0,1}\) is the class label</li>
<li>Logistic regression defined as 2 components:
<ol class="org-ol">
<li>Model the response \(\textbf{Y}\) as Bernoulli distribution:
\[
     P(Y_i=1) = \pi_i \\ P(Y_i = 0) = 1-\pi_i
     \]</li>
<li>Link the model parameters to the independent \(\textbf{X}\) variables:
\[
     \log \frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 x_{i1} + ... + \beta_{p-1} x_{i,p-1}
     \]
with \(p\) = number of \(\beta\) coefficients</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org076258d" class="outline-4">
<h4 id="org076258d"><span class="section-number-4">2.4.3.</span> Conditional probability</h4>
<div class="outline-text-4" id="text-2-4-3">
<ul class="org-ul">
<li>Under logistic regression, at given \(X=(x_1, ..., x_{p-1})\), the conditional probabilities are
\[
  P(Y=1|X) = \pi = \frac{e^{\beta_0 + \beta_1 x_1 + ... + \beta_{p-1} x_{p-1}}}{1+ e^{\beta_0 + \beta_1 x_1 + ... + \beta_{p-1} x_{p-1}}} \\
  P(Y=0|X) = 1-\pi = \frac{1}{1+ e^{\beta_0 + \beta_1 x_1 + ... + \beta_{p-1} x_{p-1}}}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org65b0fb1" class="outline-4">
<h4 id="org65b0fb1"><span class="section-number-4">2.4.4.</span> Statistical inference</h4>
<div class="outline-text-4" id="text-2-4-4">
<p>
Statistical questions in logistic regression:
</p>
<ul class="org-ul">
<li>How to estimate the \(\beta\) parameters in the logistic regression model from training data?</li>
<li>How to conduct hypothesis testing or get confidence interval?</li>
<li>How to use logistic regression model for prediction?</li>
</ul>
</div>
</div>
<div id="outline-container-orgbe2ede9" class="outline-4">
<h4 id="orgbe2ede9"><span class="section-number-4">2.4.5.</span> Maximum likelihood estimation</h4>
<div class="outline-text-4" id="text-2-4-5">
<ul class="org-ul">
<li>The likelihood function of the logistic regression model is
\[
  L(\beta) = \prod^n_{i=1} \pi_i^{y_i} (1-\pi_i)^{1-y_i} \\
  = ... \\
  = \prod^n_{i=1} \frac{e^{y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i,p-1})}}{1+e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i,p-1}}}
  \]</li>
<li>MLE \(\hat{\beta}\) of the \(\beta_i\) 's can be found by maximizing \(L(\beta)\).</li>
</ul>
</div>
</div>
<div id="outline-container-org8cd4c1c" class="outline-4">
<h4 id="org8cd4c1c"><span class="section-number-4">2.4.6.</span> Asymptotic properties of MLE</h4>
<div class="outline-text-4" id="text-2-4-6">
<ul class="org-ul">
<li><p>
MLE \(\hat{\beta}\) has nice asymptotic properties:
\[
  \hat{\beta} \sim N(\beta, I^{-1}_{p\times p})
  \]
where \(I_{p\times p}\) is the observed Fisher Information Matrix defined by the negative values of the <b>2nd order derivatives</b> of the log-likelihood function (\(\log L(\beta)\)) at \(\hat{\beta}\), i.e.
</p>

<p>
\[
  I_{p\times p} = (-\frac{\partial^2\log L}{\partial \beta_i \partial \beta_j})|_{\hat{\beta}}
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org81f5797" class="outline-4">
<h4 id="org81f5797"><span class="section-number-4">2.4.7.</span> Other link function</h4>
<div class="outline-text-4" id="text-2-4-7">
<ul class="org-ul">
<li>With data: \((Y_i, x_{i1}, ..., x_{i,p-1})\) for \(i=1,...,n\), where \(Y_i \in {0,1}\) is the class label</li>
<li>Generalized linear model: two steps
<ol class="org-ol">
<li>\[
     P(Y_i = 1) = \pi_i, P(Y_i = 0) = 1-\pi_i
     \]</li>
<li>\[
     g(\pi_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i, p-1}
     \]
where \(g(\cdot): (0,1) \rightarrow (-\infty, infty)\) is called a <b>link function</b></li>
</ol></li>
<li>Other link functions available:
<ol class="org-ol">
<li>Normal/probit link: \(g=\phi^{-1}\) where \(\phi(t) = P(N(0,1) \le t)\) is the c.d.f. of the normal distribution</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgc11d619" class="outline-4">
<h4 id="orgc11d619"><span class="section-number-4">2.4.8.</span> Logistic or LDA?</h4>
<div class="outline-text-4" id="text-2-4-8">
<p>
Comparison:
</p>
<ul class="org-ul">
<li>Similar: both have discriminant functions that are linear combinations of independent \(X\) variables</li>
<li>Difference: how to estimate linear coefficients
<ul class="org-ul">
<li>LDA: assume \(X|Y = k\) is <b>Gaussian</b></li>
<li>Logistic regression: <b>ignore</b> \(\textbf{P(X)}\)</li>
</ul></li>
<li>Generally, <b>logistic regression</b> is thought to be <b>safer &amp; more robust</b> than LDA
<ul class="org-ul">
<li>Often, same results</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2024-02-09 Fri 18:40</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
