<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-03-31 Sun 09:17 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ISYE 7406: Data Mining and Statistical Learning</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ISYE 7406: Data Mining and Statistical Learning</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb036919">1. Week 10: Tree-Based and Ensemble Methods (Cont'd)</a>
<ul>
<li><a href="#org7c729c4">1.1. Introduction to Boosting</a>
<ul>
<li><a href="#org50ab7e3">1.1.1. Improving Tree-Based Methods</a></li>
<li><a href="#org94beb7b">1.1.2. Ensemble Methods</a></li>
<li><a href="#org1489edf">1.1.3. Boosting history</a></li>
<li><a href="#org115f5a5">1.1.4. AdaBoost motivation</a></li>
<li><a href="#org806fbc2">1.1.5. Idea in AdaBoost</a></li>
<li><a href="#org39964d7">1.1.6. Modern viewpoint of boosting</a></li>
</ul>
</li>
<li><a href="#org40cc58f">1.2. AdaBoost for Binary classification</a>
<ul>
<li><a href="#orgf702985">1.2.1. Problem setup</a></li>
<li><a href="#orgd11990f">1.2.2. AdaBoost algorithm</a></li>
<li><a href="#orgf776edf">1.2.3. Example</a></li>
<li><a href="#org1584306">1.2.4. 3 weaker learners</a></li>
<li><a href="#orge8cd427">1.2.5. AdaBoost round 1</a></li>
<li><a href="#org3971565">1.2.6. AdaBoost round 2</a></li>
<li><a href="#org3778fbe">1.2.7. AdaBoost round 3</a></li>
<li><a href="#org4dba677">1.2.8. Output classifier</a></li>
<li><a href="#orgdae530e">1.2.9. AdaBoost output</a></li>
</ul>
</li>
<li><a href="#org5686f84">1.3. Statistical view of AdaBoost</a>
<ul>
<li><a href="#orgdc1b5a9">1.3.1. Problem setup</a></li>
<li><a href="#org68cf1f6">1.3.2. AdaBoost algorithm</a></li>
<li><a href="#orgd2ddf7b">1.3.3. Statistical properties of AdaBoost</a></li>
<li><a href="#orgbd5fe53">1.3.4. Exponential loss function</a></li>
<li><a href="#org1b9cabe">1.3.5. Optimization problem</a></li>
<li><a href="#orge0282fb">1.3.6. Optimization in function</a></li>
<li><a href="#org76017e8">1.3.7. Optimization in coefficient</a></li>
<li><a href="#org49b3fd7">1.3.8. Implementation update</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgb036919" class="outline-2">
<h2 id="orgb036919"><span class="section-number-2">1.</span> Week 10: Tree-Based and Ensemble Methods (Cont'd)</h2>
<div class="outline-text-2" id="text-1">
<p>
Ensemble Methods (Cont'd)
</p>
</div>
<div id="outline-container-org7c729c4" class="outline-3">
<h3 id="org7c729c4"><span class="section-number-3">1.1.</span> Introduction to Boosting</h3>
<div class="outline-text-3" id="text-1-1">
<p>
10.1.1
</p>
</div>
<div id="outline-container-org50ab7e3" class="outline-4">
<h4 id="org50ab7e3"><span class="section-number-4">1.1.1.</span> Improving Tree-Based Methods</h4>
<div class="outline-text-4" id="text-1-1-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\)</li>
<li>Objective: find a function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset.</li>
<li>Tree-based method is easy to interpret but might have poor predictive performance</li>
<li>How to improve tree?</li>
</ul>
</div>
</div>
<div id="outline-container-org94beb7b" class="outline-4">
<h4 id="org94beb7b"><span class="section-number-4">1.1.2.</span> Ensemble Methods</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
2 types of ensemble methods can improve trees:
</p>
<dl class="org-dl">
<dt>Re-sampling</dt><dd>bagging, random forest
<ul class="org-ul">
<li>Create many trees <span style='background-color: #FFFF00;'>in parallel</span></li>
<li>Each tree built in bootstrap dataset, independent of other trees</li>
<li><b><b>average</b></b> of trees reduces variance</li>
</ul></dd>
<dt>Re-weighting</dt><dd>boosting
<ul class="org-ul">
<li>Create trees <span style='background-color: #FFFF00;'>sequentially</span></li>
<li>Each tree built to correct misclassification of previous trees</li>
<li><b><b>Weighted average</b></b> of trees improves training error</li>
</ul></dd>
</dl>
</div>
</div>
<div id="outline-container-org1489edf" class="outline-4">
<h4 id="org1489edf"><span class="section-number-4">1.1.3.</span> Boosting history</h4>
<div class="outline-text-4" id="text-1-1-3">
<dl class="org-dl">
<dt>Adaboost for binary classification</dt><dd>Freund and Schapire, 1997</dd>
<dt>Statistical view of boosting</dt><dd>Friendman, Hastie and Tibshirani, 2000</dd>
<dt>gbm (gradient boosting machine) algorithm</dt><dd>Friedman, 2001</dd>
<dt>XGBoost algorithm</dt><dd>Chen and Guestrin, 2016</dd>
</dl>
</div>
</div>
<div id="outline-container-org115f5a5" class="outline-4">
<h4 id="org115f5a5"><span class="section-number-4">1.1.4.</span> AdaBoost motivation</h4>
<div class="outline-text-4" id="text-1-1-4">
<ul class="org-ul">
<li>Developed for classification, when responses \(Y_i \in {-1,1}\)</li>
<li>Original motivation is to combine a set of "weak learners" to create a single "strong learner"
<dl class="org-dl">
<dt>Weaker learner</dt><dd>classifier that is only slightly better than <b><b>random guess</b></b></dd>
<dt>Strong learner</dt><dd>a classifier that has very good classification performance</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org806fbc2" class="outline-4">
<h4 id="org806fbc2"><span class="section-number-4">1.1.5.</span> Idea in AdaBoost</h4>
<div class="outline-text-4" id="text-1-1-5">
<p>
<span style='background-color: #FFFF00;'>Re-weighting</span>:
</p>
<ul class="org-ul">
<li>Weights of training data is <b><b>readjusted</b></b> after a weaker learner is added, i.e. put more weights to misclassified data</li>
<li>Future weak learners are applied to the same training data with different weights
<ul class="org-ul">
<li>i.e., focus more on the data that the previous weak learners misclassified</li>
</ul></li>
<li><p>
Final classifier is based on <b><b>weighted average</b></b> of all weaker learners i.e.
</p>

<p>
\[
  \text{sign}(\sum^M_{m=1} w_m f_m (x))
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org39964d7" class="outline-4">
<h4 id="org39964d7"><span class="section-number-4">1.1.6.</span> Modern viewpoint of boosting</h4>
<div class="outline-text-4" id="text-1-1-6">
<ul class="org-ul">
<li>Researchers have spent significant time and effort to understand why it works, and how to extend</li>
<li>Modern viewpoint of boosting is to <b><b>sequentially</b></b> fit the <span style='background-color: #FFFF00;'>Forward Stagewise Additive Regression Model</span> of the form
\[
  h(x) = \sum^m_{m=1} w_m f(x, \gamma_m)
  \]</li>
<li>The choice of \(f(x, \gamma_m)\) is based on <span style='background-color: #FFFF00;'>gradient descent</span></li>
<li>Prefer to choose <span style='background-color: #FFFF00;'>small coefficients</span> of \(w_m\) with large \(M\)
<ul class="org-ul">
<li>i.e., prefer to <b><b>learn slowly</b></b></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org40cc58f" class="outline-3">
<h3 id="org40cc58f"><span class="section-number-3">1.2.</span> AdaBoost for Binary classification</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-orgf702985" class="outline-4">
<h4 id="orgf702985"><span class="section-number-4">1.2.1.</span> Problem setup</h4>
<div class="outline-text-4" id="text-1-2-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1E}\)</li>
<li>Objective: find a function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset.</li>
<li>Weak learners:
<ul class="org-ul">
<li>we have many tree classifiers \({h_m(x)}\) available to predict \(Y\) but each learner is only slightly better than random guessing</li>
</ul></li>
<li>How to use weak learners to construct a better classifier?</li>
</ul>
</div>
</div>
<div id="outline-container-orgd11990f" class="outline-4">
<h4 id="orgd11990f"><span class="section-number-4">1.2.2.</span> AdaBoost algorithm</h4>
<div class="outline-text-4" id="text-1-2-2">
<ol class="org-ol">
<li>Start with equal weights for all training data i.e.
\[
   w_i = 1/n, i=1,...,n
   \]</li>
<li>Repeat for \(m=1,2,...,M\):
<ol class="org-ol">
<li>Fit the best classifier \(h_m (x)\) to the training data with weights \(w_i\)</li>
<li>Compute constants:
\[
      \text{err}_m = E_w I(Y \neq h_m (x)) \\
      c_m = \log(\frac{1-\text{err}_m}{\text{err}_m})
      \]</li>
<li>Set weights \(w_i \leftarrow  w_i \exp(c_m I(Y_i \neq h_m (x_i)))\) and re-normalize \(\sum_i w_i = 1\)</li>
</ol></li>
<li>Output the final classifier \(h(x) = \text{sign}(\sum^M_{m=1} c_m h_m (x))\)</li>
</ol>
</div>
</div>
<div id="outline-container-orgf776edf" class="outline-4">
<h4 id="orgf776edf"><span class="section-number-4">1.2.3.</span> Example</h4>
<div class="outline-text-4" id="text-1-2-3">

<div id="org4167336" class="figure">
<p><img src="./img/ada1.png" alt="ada1.png" />
</p>
</div>
<ul class="org-ul">
<li>Training data: \(n=3, p=1\)
\[
  (x_1, Y_1) = (-1,1) \\
  (x_2, Y_2) = (0,-1) \\
  (x_3, Y_3) = (1,1)
  \]</li>
<li>Weaker learners: only consider tree stump (i.e., two-node tree) of the form \(x<v\) or \(x \ge v\)</li>
</ul>
</div>
</div>
<div id="outline-container-org1584306" class="outline-4">
<h4 id="org1584306"><span class="section-number-4">1.2.4.</span> 3 weaker learners</h4>
<div class="outline-text-4" id="text-1-2-4">
<ol class="org-ol">
<li>Learner 1 misclassifies \((x_1, Y_1) = (-1,1)\) and has loss \(w_1\)
\[
   h_{1} ( x )=\left\{\begin{matrix} {{1 ~ ~ ~ ~ i f ~ ~ x \geq0. 5}} \\ {{-1 ~ ~ i f ~ ~ x < 0. 5}} \\ \end{matrix} \right.
   \]</li>
<li>Learner 2 misclassifies \((x_2, Y_2) = (0,-1)\) and has loss \(w_2\)
\[
   h_2 (x) = 1 \text{for all }x
   \]</li>
<li>Learner 3 misclassifies \((x_3, Y_3) = (1,1)\) and has loss \(w_3\)
\[
	 h_{3} ( x )=\left\{\begin{matrix} {{-1 ~ ~ ~ ~ i f ~ ~ x \geq-0. 5}} \\ {{1 ~ ~ i f ~ ~ x <- 0. 5}} \\ \end{matrix} \right.
	 \]</li>
</ol>
</div>
</div>
<div id="outline-container-orge8cd427" class="outline-4">
<h4 id="orge8cd427"><span class="section-number-4">1.2.5.</span> AdaBoost round 1</h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
\(m=1\)
</p>
<ul class="org-ul">
<li>Original weights
\[
  w_1 = \frac{1}{3}, w_2 = \frac{1}{3}, w_3 = \frac{1}{3}
  \]</li>
<li>Fit a best classifier. Without loss of generality, consider \(h_2 (x) =1\) for all x</li>
<li>Compute constants \(\text{err}\) and \(c=\log((1-\text{err})/\text{err})\)
\[
  \text{err} = w_2 = \frac{1}{3} \\
  c = \log \frac{1-1/3}{1/3} = \log 2
  \]</li>
<li>Output: \(h(x) = \text{sign}(\sum^M_{m=1}c_m h_m (x)) = \text{sign}(\log(2) h_2 (x)) = 1\) for all \(x\)</li>
</ul>
</div>
</div>
<div id="outline-container-org3971565" class="outline-4">
<h4 id="org3971565"><span class="section-number-4">1.2.6.</span> AdaBoost round 2</h4>
<div class="outline-text-4" id="text-1-2-6">
<p>
\(m=2\)
</p>
<ul class="org-ul">
<li>Update weights
\[
  w_1 \leftarrow \frac{1}{3},
  w_2 \leftarrow \frac{1}{3} e^{\log(2)} = \frac{2}{3},
  w_3 \leftarrow \frac{1}{3}
  \]
Renormalizing we have \(w_1 = \frac{1}{4}, w_2 = \frac{1}{2}, w_3 = \frac{1}{4}\)</li>
<li>Fit a best classifier: WLOG, consider
\[
	h_{1} ( x )=\left\{\begin{matrix} {{1 ~ ~ ~ ~ i f ~ ~ x \geq0. 5}} \\ {{-1 ~ ~ i f ~ ~ x < 0. 5}} \\ \end{matrix} \right.
	\]</li>
<li>Compute constants: \(\text{err}\) and \(c=\log((1-\text{err})/\text{err})\)
\[
  \text{err} = w_1 = \frac{1}{4}, \\
  c = \log(\frac{1-1/4}{1/4}) = \log(3)
  \]</li>
<li>Output:
\[
  h(x) = \text{sign}(\log(2)h_2 (x) + \log(3) h_1 (x)) = h_1 (x)
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org3778fbe" class="outline-4">
<h4 id="org3778fbe"><span class="section-number-4">1.2.7.</span> AdaBoost round 3</h4>
<div class="outline-text-4" id="text-1-2-7">
<p>
\(m=3\)
</p>
<ul class="org-ul">
<li>Update weights:
\[
  w_1 \leftarrow \frac{1}{4} e^{\log(3)} = \frac{3}{4},
  w_2 \leftarrow \frac{1}{2},
  w_3 \leftarrow \frac{1}{4}
  \]
Renormalizing we have \(w_1 = \frac{1}{2}, w_2 = \frac{1}{3}, w_3 = \frac{1}{6}\)</li>
<li>Fit a best classifier
\[
	h_{3} ( x )=\left\{\begin{matrix} {{-1 ~ ~ ~ ~ i f ~ ~ x \geq-0. 5}} \\ {{1 ~ ~ i f ~ ~ x <- 0. 5}} \\ \end{matrix} \right.
	\]</li>
<li>Compute constants: \(\text{err}\) and $c=log((1-\text{err})/\text{err})
\[
  \text{err} = w_3 = \frac{1}{6} \\
  c = \log(\frac{1-1/6}{1/6}) = \log(5)
  \]</li>
<li>Output: \(h(x) = \text{sign}(\log(2) h_2 (x) + \log(3) h_1 (x) + \log(5) h_3 (x))\)</li>
</ul>
</div>
</div>
<div id="outline-container-org4dba677" class="outline-4">
<h4 id="org4dba677"><span class="section-number-4">1.2.8.</span> Output classifier</h4>
<div class="outline-text-4" id="text-1-2-8">
<p>
At end of round 3,
</p>

\begin{equation}
h(x) =
\text{sign of}
\begin{cases}
        \log(2) - \log(3) + \log(5) & \text{if } x < -0.5 \\
        \log(2) - \log(3) - \log(5) & \text{if } -0.5 \leq x < 0.5 \\
        \log(2) + \log(3) + \log(5) & \text{if } x \geq 0.5 \\
\end{cases}
\end{equation}

<p>
\[
\left\{\begin{matrix} {{{1}}} & {{{i f x <-0. 5}}} \\ {{{-1}}} & {{{i f-0. 5 \leq x < 0. 5}}} \\ {{{1}}} & {{{i f x \geq0. 5}}} \\ \end{matrix} \right.
\]
</p>
</div>
</div>
<div id="outline-container-orgdae530e" class="outline-4">
<h4 id="orgdae530e"><span class="section-number-4">1.2.9.</span> AdaBoost output</h4>
<div class="outline-text-4" id="text-1-2-9">
<p>
<img src="./img/ada3.png" alt="ada3.png" />
The <b><b>strong learner</b></b> at \(M=3\):
\[
h(x) = \text{sign}(\sum^M_{m=1} c_m h_m (x)) =
\]
\[
\left\{\begin{matrix} {{{1}}} & {{{i f x <-0. 5}}} \\ {{{-1}}} & {{{i f-0. 5 \leq x < 0. 5}}} \\ {{{1}}} & {{{i f x \geq0. 5}}} \\ \end{matrix} \right.
\]
correctly classifies all 3 training data points.
</p>
</div>
</div>
</div>
<div id="outline-container-org5686f84" class="outline-3">
<h3 id="org5686f84"><span class="section-number-3">1.3.</span> Statistical view of AdaBoost</h3>
<div class="outline-text-3" id="text-1-3">
<p>
10.1.3
</p>
</div>
<div id="outline-container-orgdc1b5a9" class="outline-4">
<h4 id="orgdc1b5a9"><span class="section-number-4">1.3.1.</span> Problem setup</h4>
<div class="outline-text-4" id="text-1-3-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1}\)</li>
<li>Objective: find a function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset.</li>
<li>Trees: we have many tree classifiers \({h_m (x)}\) available to predict the response \(Y\) but each tree is only slightly better than random guessing</li>
<li>AdaBoost uses those weak learners to construct a very good classifier</li>
</ul>
</div>
</div>
<div id="outline-container-org68cf1f6" class="outline-4">
<h4 id="org68cf1f6"><span class="section-number-4">1.3.2.</span> AdaBoost algorithm</h4>
<div class="outline-text-4" id="text-1-3-2">
<ul class="org-ul">
<li>Final classifier: \(h(x) = \text{sign}(H_M (x))\) where</li>
</ul>

<p>
\[
H_{M} ( x )=\sum_{m=1}^{M} c_{m} h_{m} ( x )=H_{M-1} ( x )+c_{m} h_{m} ( x )
\]
</p>
<ul class="org-ul">
<li>At the m-th round, the function \(h_m(x)\) is chosen to minimize the error rate \(\text{err} = E_w I(Y\ne h(x))\)
<ul class="org-ul">
<li>And the coefficient \(c_m\) is chosen as \(c_m = \log(\frac{1- \text{err}_m}{\text{err}_m})\)</li>
</ul></li>
<li>What is the statistical interpretation of the choice of \((h_m (x), c_m)\)?</li>
</ul>
</div>
</div>
<div id="outline-container-orgd2ddf7b" class="outline-4">
<h4 id="orgd2ddf7b"><span class="section-number-4">1.3.3.</span> Statistical properties of AdaBoost</h4>
<div class="outline-text-4" id="text-1-3-3">
<ul class="org-ul">
<li>Theorem: AdaBoost builds an additive logistic regression model via gradient descent when minimizing the exponential loss function
<dl class="org-dl">
<dt>Additive logistic regression model</dt><dd>\[
		\log\frac{P ( Y=1 )} {1-P ( Y=1 )}=H ( \boldsymbol{x} )=\sum_{m=1}^{M} c_{m} h_{m} ( \boldsymbol{x} )
		\]</dd>
<dt>Exponential loss function</dt><dd>\[
		L ( Y, H )=e^{-Y H ( x )}
		\]</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-orgbd5fe53" class="outline-4">
<h4 id="orgbd5fe53"><span class="section-number-4">1.3.4.</span> Exponential loss function</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li><p>
When predicting \(Y\in{-1,1}\) by the \(\hat{Y} = \text{sign}(H(x))\) the standard 0-1 loss for classification is
</p>
\begin{equation}
L^{*}(Y,H) = I(Y\neq\hat{Y})
= \begin{cases}
0 & \text{if }YH(x) \ge 0 \\
1 & \text{if }YH(x) \lt 0
\end{cases}
\end{equation}</li>
<li>In AdaBoost, we consider the exponential loss, which is a surrogate function
\[
  L ( Y, H )=e^{-Y H ( x )}
  \]</li>
</ul>

<div id="orgcadc88e" class="figure">
<p><img src="./img/ada-loss.png" alt="ada-loss.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org1b9cabe" class="outline-4">
<h4 id="org1b9cabe"><span class="section-number-4">1.3.5.</span> Optimization problem</h4>
<div class="outline-text-4" id="text-1-3-5">
<p>
Given the current \(H(x)\) we want to add \(ch(x)\) that minimizes the exponential loss function
\[
E \left( e^{-Y \left( H ( x )+c h ( x ) \right)} \right)
\]
</p>
<dl class="org-dl">
<dt>What is the optimal function \(h(x)\)</dt><dd>gradient descent</dd>
<dt>Given \(H(x) \text{and } h(x)\), what is the optimal value for \(c\)?</dt><dd>Direct optimization over the re-weighted data</dd>
</dl>
</div>
</div>
<div id="outline-container-orge0282fb" class="outline-4">
<h4 id="orge0282fb"><span class="section-number-4">1.3.6.</span> Optimization in function</h4>
<div class="outline-text-4" id="text-1-3-6">
<p>
Given the current \(H(x)\), by Taylor Series expansion, when adding \(ch(x)\),
</p>
\begin{aligned} {{{\mathbf{E} \left( e^{-Y \left( H ( x )+c h ( x ) \right)} \right)}}} & {{} {{} {\approx \mathbf{E} \left( e^{-Y H ( x )} \left( 1-c Y h ( x )+{\frac{c^{2} Y^{2} h ( x )^{2}} {2}} \right) \right)}}} \\ {{{}}} & {{} {{} {{}=\mathbf{E} \left( e^{-Y H ( x )} \left( 1-c Y h ( x )+{\frac{c^{2}} {2}} \right) \right) \quad\mathbf{s i n c e} \ Y h ( x )=\pm1}}} \\ {{{}}} & {{} {{} {{}=\mathbf{E}_{\mathbf{w}} \left( 1-c Y h ( x )+{\frac{c^{2}} {2}} \right)}}} \\ \end{aligned}
<ul class="org-ul">
<li>Gradient descent finds \(h(x) \in \pm 1\) that minimizes
\[
  \frac{1} {2} \mathbf{E}_{\mathbf{w}} \big( 1-Y h ( x ) \big)=\mathbf{P}_{\mathbf{w}} ( Y h ( x )=-1 )=\mathbf{P}_{\mathbf{w}} \big( Y \neq h ( x ) \big)= \text{err}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org76017e8" class="outline-4">
<h4 id="org76017e8"><span class="section-number-4">1.3.7.</span> Optimization in coefficient</h4>
<div class="outline-text-4" id="text-1-3-7">
<ul class="org-ul">
<li><p>
Given the \(H(x) \text{ and } h(x)\), we can directly find the coefficient \(c\) that minimizes the risk
</p>
\begin{array} {c} {{{\mathbf{E} \bigl( e^{-Y \bigl( H ( \boldsymbol{x} )+c h ( \boldsymbol{x} ) \bigr)} \bigr)=\mathbf{E}_{\mathbf{w}} ( e^{-c Y h ( \boldsymbol{x} )} )}}} \\ {{{=e^{c} \, \mathbf{P}_{\mathbf{w}} ( Y h ( \boldsymbol{x} )=-1 )+e^{-c} \mathbf{P}_{\mathbf{w}} ( Y h ( \boldsymbol{x} )=1 )}}} \\ {{{=e^{c} \, * \, \text{err}+e^{-c} \, * \, ( 1-\text{err} )}}} \\ \end{array}</li>
<li>Taking derivative with respect to \(c\) and setting to \(0\), the risk is minimized at
\[
  c = \frac{1}{2}\log\frac{1-\text{err}}{\text{err}}
  \]</li>
<li>This value is similar to \(c_m\) in AdaBoost, with the exception of adding factor \(\frac{1}{2}\)</li>
</ul>
</div>
</div>
<div id="outline-container-org49b3fd7" class="outline-4">
<h4 id="org49b3fd7"><span class="section-number-4">1.3.8.</span> Implementation update</h4>
<div class="outline-text-4" id="text-1-3-8">
<ul class="org-ul">
<li>By gradient descent, the optimal update should be
\[
	H ( x ) \gets H ( x )+{\frac{1} {2}} \text{log} {\frac{1-\text{err}} {\text{err}}} \, h ( x ) \,, \qquad\quad w \gets w e^{-c Y h ( x )}
	\]</li>
<li>A simpler implementation:
<ul class="org-ul">
<li>Since \(-Yh(x) = 2I(Y\ne h(x))-1\), the update is equivalent to
\[
    w \leftarrow w \exp[\log\frac{1-\text{err}}{\text{err}} I(Y\ne h(x))]
    \]</li>
<li>The sign function is the same if
\(H(x) \leftarrow H(x) + \log \frac{1-\text{err}}{\text{err}} h(x)\)</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2024-03-31 Sun 09:17</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
