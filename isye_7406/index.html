<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-04-04 Thu 21:16 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ISYE 7406: Data Mining and Statistical Learning</title>
<meta name="author" content="W" />
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../src/readtheorg_theme/css/readtheorg.css"/>
<script type="text/javascript" src="../src/lib/js/jquery.min.js"></script>
<script type="text/javascript" src="../src/lib/js/bootstrap.min.js"></script>
<script type="text/javascript" src="../src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="../src/readtheorg_theme/js/readtheorg.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">ISYE 7406: Data Mining and Statistical Learning</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgd101c46">1. Week 3: Linear Regression (II)</a>
<ul>
<li><a href="#orgeea1f8f">1.1. James-Stein Estimator</a>
<ul>
<li><a href="#orgc8c650c">1.1.1. It's a special case of linear regression</a></li>
<li><a href="#org8c3f412">1.1.2. Simultaneous estimation</a></li>
<li><a href="#org65dcaf9">1.1.3. JS estimator</a></li>
<li><a href="#org8b6bef3">1.1.4. Baseball example</a></li>
<li><a href="#orgbdc5c28">1.1.5. Comparing MLE and JS</a></li>
</ul>
</li>
<li><a href="#org8093d88">1.2. Shrinkage Methods</a>
<ul>
<li><a href="#orgffdc9b1">1.2.1. Setting up shrinkage method</a></li>
<li><a href="#org07c4a63">1.2.2. Alternative formulation</a></li>
<li><a href="#org4e351ea">1.2.3. Bayesian interpretation</a></li>
<li><a href="#orgd839dcf">1.2.4. Choices of priors</a></li>
<li><a href="#orgb3942f7">1.2.5. Ridge regression</a></li>
<li><a href="#org54754bf">1.2.6. LASSO estimator</a></li>
</ul>
</li>
<li><a href="#orga0a1645">1.3. Ridge Regression</a>
<ul>
<li><a href="#orga3b7cc8">1.3.1. Ridge Regression Estimator</a></li>
<li><a href="#org3f2c8c0">1.3.2. Mathematical solution</a></li>
<li><a href="#orgeb44451">1.3.3. Properties of Ridge Regression</a></li>
<li><a href="#orgdbea4a0">1.3.4. Computational issues</a></li>
<li><a href="#org6affc92">1.3.5. Example of SVD</a></li>
<li><a href="#org1098e83">1.3.6. SVD Example (I): \(XX^T\)</a></li>
<li><a href="#org1dac47d">1.3.7. SVD Example (II): \(X^T X\)</a></li>
<li><a href="#org28c4dfa">1.3.8. SVD verification</a></li>
</ul>
</li>
<li><a href="#orgfbfb14b">1.4. LASSO (3.2.1)</a>
<ul>
<li><a href="#org46aadb3">1.4.1. LASSO estimator</a></li>
<li><a href="#orga817a0c">1.4.2. L2-norm vs L1-norm</a></li>
<li><a href="#org86c32f7">1.4.3. Mathematical solution for LASSO estimator</a></li>
<li><a href="#orgd4d0d7d">1.4.4. Properties of LASSO</a></li>
<li><a href="#org19ea55c">1.4.5. LASSO weaknesses</a></li>
<li><a href="#org1422a25">1.4.6. Computation issues of LASSO</a></li>
<li><a href="#org4164dfb">1.4.7. LASSO is piecewise linear</a></li>
<li><a href="#org77d27b3">1.4.8. Standard error of LASSO</a></li>
<li><a href="#orge17186f">1.4.9. Variants of L1-norm</a></li>
</ul>
</li>
<li><a href="#org5270205">1.5. Principal Components (3.2.2)</a>
<ul>
<li><a href="#org69b70a8">1.5.1. Dimension reduction</a></li>
<li><a href="#org66f6942">1.5.2. Motivation of PCA</a></li>
<li><a href="#org76e89d4">1.5.3. Find the PC's:  Population version</a></li>
<li><a href="#org8a81b99">1.5.4. Eigenvectors lead to PC's</a></li>
<li><a href="#orga4a3bbc">1.5.5. Proof by Lagrange Multipliers</a></li>
<li><a href="#orge85a72b">1.5.6. PCs in Empirical Version</a></li>
<li><a href="#orgf922d42">1.5.7. Principal component regression</a></li>
</ul>
</li>
<li><a href="#orge1c0c62">1.6. Partial least squares (3.2.3)</a>
<ul>
<li><a href="#org1ea3cb8">1.6.1. Dimension reduction</a></li>
<li><a href="#orgfe707e3">1.6.2. Partial least squares</a></li>
<li><a href="#org7d45fda">1.6.3. 2 versions of PLS</a></li>
<li><a href="#orgb8849ab">1.6.4. Simple PLS algorithm</a></li>
<li><a href="#orged797a5">1.6.5. The PLS model</a></li>
<li><a href="#orgc6c3db7">1.6.6. Key idea in The PLS Model</a></li>
<li><a href="#orgfb12769">1.6.7. PLS for linear regression</a></li>
<li><a href="#org401e9ae">1.6.8. Canonical correlation analysis (CCA)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc5b56bc">2. Week 4: Linear Classification</a>
<ul>
<li><a href="#org928369e">2.1. Overview of Linear Discriminant Analysis (4.1.1)</a>
<ul>
<li><a href="#org6c9aa8f">2.1.1. Classification problem</a></li>
<li><a href="#org5b31ffe">2.1.2. Popular classification methods and R packages</a></li>
<li><a href="#org22a46c7">2.1.3. Discriminant analysis</a></li>
<li><a href="#orgda33d6a">2.1.4. Example for 3-class problem</a></li>
<li><a href="#org4e98900">2.1.5. Discriminant functions</a></li>
<li><a href="#orgff608e9">2.1.6. Multi-class vs. Binary</a></li>
</ul>
</li>
<li><a href="#orgd66c131">2.2. Linear discrimination analysis continued (4.1.2)</a>
<ul>
<li><a href="#org3064f6b">2.2.1. Bayes classifier</a></li>
<li><a href="#org676ed39">2.2.2. Normality asumption</a></li>
<li><a href="#orga63d38c">2.2.3. Bayes classifier for normal distribution</a></li>
<li><a href="#orgb6ef57f">2.2.4. LDA in practice</a></li>
<li><a href="#org1081a1f">2.2.5. LDA for K=2 classes</a></li>
<li><a href="#org87919a7">2.2.6. Fisher's distance-based approach</a></li>
</ul>
</li>
<li><a href="#org8053979">2.3. Quadratic Discrimination Analysis classifier (4.1.3)</a>
<ul>
<li><a href="#org1a1a690">2.3.1. Bayes classifier</a></li>
<li><a href="#org35788bb">2.3.2. Normal distribution</a></li>
<li><a href="#org3a67e24">2.3.3. Normal model for \(f_k\)</a></li>
<li><a href="#org0140fca">2.3.4. Three approaches to estimate</a></li>
<li><a href="#org0141512">2.3.5. QDA classifier</a></li>
<li><a href="#orge5c1e61">2.3.6. Naive Bayes classifier</a></li>
<li><a href="#orge131716">2.3.7. General Naive Bayes classifier</a></li>
</ul>
</li>
<li><a href="#org4f17f65">2.4. Logistic Regression: Estimation (4.2.1)</a>
<ul>
<li><a href="#orge430750">2.4.1. Classification methods</a></li>
<li><a href="#org5959986">2.4.2. Binary logistic regression</a></li>
<li><a href="#orga802dfd">2.4.3. Conditional probability</a></li>
<li><a href="#org3515b81">2.4.4. Statistical inference</a></li>
<li><a href="#org2b22e03">2.4.5. Maximum likelihood estimation</a></li>
<li><a href="#org8c4af41">2.4.6. Asymptotic properties of MLE</a></li>
<li><a href="#org921ed4b">2.4.7. Other link function</a></li>
<li><a href="#org788f8a0">2.4.8. Logistic or LDA?</a></li>
</ul>
</li>
<li><a href="#orgbbff3dd">2.5. Optimizations in Logistic Regression (4.2.2)</a>
<ul>
<li><a href="#org2583a1e">2.5.1. Logistic regression recap</a></li>
<li><a href="#org892bee5">2.5.2. MLE in logistic regression</a></li>
<li><a href="#org063dce5">2.5.3. Optimization problem</a></li>
<li><a href="#org4d3e2f4">2.5.4. Optimization algorithms</a></li>
<li><a href="#orgff31d19">2.5.5. Newton-Raphson Method</a></li>
<li><a href="#org73b6948">2.5.6. Taylor Expansion consideration</a></li>
<li><a href="#orga2f7a5c">2.5.7. Newton-Raphson Method in Statistics</a></li>
<li><a href="#org0f094bc">2.5.8. MLE of logistic regression</a></li>
<li><a href="#org005b791">2.5.9. Algorithm for MLE in logistic regression</a></li>
</ul>
</li>
<li><a href="#org9f8f89c">2.6. Simplest logistic regression (4.2.3)</a>
<ul>
<li><a href="#org26b38bb">2.6.1. Data in Simplest Logistic Regression</a></li>
<li><a href="#org4d9e4bd">2.6.2. Is X and Y associated?</a></li>
<li><a href="#org41ad07d">2.6.3. The likelihood function</a></li>
<li><a href="#orga8044bf">2.6.4. The MLE estimator of \(\beta_i\) 's</a></li>
<li><a href="#org0c83542">2.6.5. Fisher Information Matrix</a></li>
<li><a href="#org8176bff">2.6.6. Variance of MLE</a></li>
<li><a href="#orgd84b920">2.6.7. Are X and Y associated?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdd90867">3. Week 5: Linear Classification Cont'd (Module 3)</a>
<ul>
<li><a href="#org5064503">3.1. Example: CHD</a>
<ul>
<li><a href="#org94e234c">3.1.1. CI of coefficients in Log Reg</a></li>
<li><a href="#orga7c4573">3.1.2. Simplest logistic regression model</a></li>
<li><a href="#org1188510">3.1.3. Analysis in R</a></li>
<li><a href="#orga3b9124">3.1.4. Analysis by hand</a></li>
<li><a href="#orgacacceb">3.1.5. Testing hypothesis</a></li>
</ul>
</li>
<li><a href="#orgc8270af">3.2. Prediction in Logistic Regression</a>
<ul>
<li><a href="#org845e293">3.2.1. Logistic regression model</a></li>
<li><a href="#org4f3d56b">3.2.2. Point estimation</a></li>
<li><a href="#orgfb0ab1e">3.2.3. Point estimation in R</a></li>
<li><a href="#orgbece7e5">3.2.4. Confidence interval for logistic regression</a></li>
<li><a href="#orgdce0dad">3.2.5. Prediction</a></li>
<li><a href="#orgb28bf65">3.2.6. Prediction rule</a></li>
<li><a href="#org01c5491">3.2.7. Choice of cutoff value \(c*\)</a></li>
<li><a href="#orgd9c474f">3.2.8. Choice of \(c*\) from validation</a></li>
</ul>
</li>
<li><a href="#orge194e19">3.3. Model Selection in Logistic Regression</a>
<ul>
<li><a href="#orga5d3a9a">3.3.1. Logistic Regression review</a></li>
<li><a href="#orga0cbf44">3.3.2. Model selection in Logistic Regression</a></li>
<li><a href="#orga1062d4">3.3.3. Information criterion</a></li>
<li><a href="#org1050e20">3.3.4. Hypothesis testing</a></li>
<li><a href="#org21388d5">3.3.5. Wald's test</a></li>
<li><a href="#org3766820">3.3.6. Comparison of 2 models</a></li>
<li><a href="#orgb8a7b8d">3.3.7. Assess performance</a></li>
<li><a href="#org7bf436c">3.3.8. Specificity vs Sensitivity</a></li>
<li><a href="#org9b368a5">3.3.9. ROC curve</a></li>
</ul>
</li>
<li><a href="#org5632fa3">3.4. Case Study: Golf Putting I</a>
<ul>
<li><a href="#orgade18a4">3.4.1. Dataset</a></li>
<li><a href="#orgbe16cb7">3.4.2. Scatter plot</a></li>
<li><a href="#org812d65a">3.4.3. Question</a></li>
<li><a href="#orga5ef9df">3.4.4. Multiple linear regression models</a></li>
<li><a href="#org250d43e">3.4.5. R code</a></li>
<li><a href="#orgad6f2b5">3.4.6. Larger distance?</a></li>
<li><a href="#org0103f4d">3.4.7. Problems with regression models</a></li>
</ul>
</li>
<li><a href="#org1255f63">3.5. Case Study: Golf Putting II</a>
<ul>
<li><a href="#org04ddda2">3.5.1. Logistic regression model</a></li>
<li><a href="#org9654eaa">3.5.2. R code</a></li>
<li><a href="#org440fa46">3.5.3. Fitted model:</a></li>
<li><a href="#org2563cb0">3.5.4. Advantages and disadvantages of logistic regression</a></li>
</ul>
</li>
<li><a href="#org5431ee6">3.6. Case Study: Golf Putting III</a>
<ul>
<li><a href="#org02ed7a4">3.6.1. New model</a></li>
<li><a href="#org9d140d6">3.6.2. Parameter estimation</a></li>
<li><a href="#org78c6384">3.6.3. Fitted model</a></li>
<li><a href="#org9a4f5f6">3.6.4. Concerns</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgf350d69">4. Week 6: Local Smoothers and Additive Models</a>
<ul>
<li><a href="#org33a9bfd">4.1. Overview</a>
<ul>
<li><a href="#orgd03207f">4.1.1. Regression in general</a></li>
<li><a href="#orgf8b8564">4.1.2. Special case: \(p=1\)</a></li>
<li><a href="#org12f4952">4.1.3. Method of least squares</a></li>
<li><a href="#orgd9e6d14">4.1.4. Interpolation</a></li>
<li><a href="#org6fb7579">4.1.5. Polynomial interpolation</a></li>
<li><a href="#org1a358a9">4.1.6. Weakness of interpolation</a></li>
<li><a href="#org5536834">4.1.7. Transition to local smoothers</a></li>
<li><a href="#org59ac458">4.1.8. Three popular local smoothers</a></li>
</ul>
</li>
<li><a href="#org6c00712">4.2. LOESS</a>
<ul>
<li><a href="#org74d152c">4.2.1. Problem formulation</a></li>
<li><a href="#org654253f">4.2.2. LOESS algorithm</a></li>
<li><a href="#orga82358c">4.2.3. Weights</a></li>
<li><a href="#org2e5a58d">4.2.4. Weighted least squares</a></li>
<li><a href="#org1afe88f">4.2.5. LOESS smoothing example</a></li>
<li><a href="#org5466d1f">4.2.6. Advantages and disadvantages of LOESS</a></li>
</ul>
</li>
<li><a href="#orgf3216ed">4.3. Kernel 1</a>
<ul>
<li><a href="#org12edb5c">4.3.1. Local Smoothers</a></li>
<li><a href="#orga41643d">4.3.2. Convergence</a></li>
<li><a href="#org8b2647a">4.3.3. Statistical properties</a></li>
<li><a href="#org83c5e3e">4.3.4. Bias-variance decomposition</a></li>
<li><a href="#org73bc429">4.3.5. Kernel smoothing</a></li>
<li><a href="#org1a148b5">4.3.6. Definition of kernel</a></li>
<li><a href="#orgff06f35">4.3.7. Rescaled kernel</a></li>
<li><a href="#org965551c">4.3.8. Kernel functions</a></li>
<li><a href="#org899842a">4.3.9. Idea in kernel smoothing</a></li>
</ul>
</li>
<li><a href="#org79af4f8">4.4. Kernels: Smoothing for deterministic design</a>
<ul>
<li><a href="#org7aef241">4.4.1. Deterministic design</a></li>
<li><a href="#org584e24e">4.4.2. Choose a fixed kernel</a></li>
<li><a href="#orgfad60d4">4.4.3. Priestley-Chao Kernel Estimate</a></li>
<li><a href="#orgfba4b0e">4.4.4. Performance evaluation</a></li>
<li><a href="#orgcbaf171">4.4.5. Statistical properties</a></li>
<li><a href="#orgaebcbe2">4.4.6. Main ideas:</a></li>
<li><a href="#orgb350006">4.4.7. Proof: bias of \(\hat{f}\)</a></li>
<li><a href="#org33d1a95">4.4.8. Optimal choice of \(h\)</a></li>
</ul>
</li>
<li><a href="#orgd461fb3">4.5. Kernel Smoothing in Stochastic Design</a>
<ul>
<li><a href="#org3bf4c93">4.5.1. Stochastic design</a></li>
<li><a href="#org06cfb40">4.5.2. Challenges</a></li>
<li><a href="#org90fc469">4.5.3. Nadaraya-Watson Estimator</a></li>
<li><a href="#org9371997">4.5.4. Alternative form</a></li>
<li><a href="#org5a6ffd1">4.5.5. Consistency properties of NW</a></li>
<li><a href="#org917f1a5">4.5.6. Density estimation</a></li>
<li><a href="#org1d03e82">4.5.7. Asymptotic properties</a></li>
<li><a href="#orgd8cfb12">4.5.8. Other key ideas</a></li>
<li><a href="#org2bbb3b3">4.5.9. Asymptotic normality</a></li>
</ul>
</li>
<li><a href="#org61f4d77">4.6. Advanced topics in kernel smoothing</a>
<ul>
<li><a href="#org4875f83">4.6.1. Estimate derivatives</a></li>
<li><a href="#org8c46e7c">4.6.2. Deterministic design</a></li>
<li><a href="#org5bc91ba">4.6.3. Estimate derivatives</a></li>
<li><a href="#orgc892f4d">4.6.4. Parameter selection</a></li>
<li><a href="#org1745e47">4.6.5. Statistical properties</a></li>
<li><a href="#org682afa2">4.6.6. Mean squared error</a></li>
<li><a href="#org0438571">4.6.7. Optimal MSE</a></li>
<li><a href="#orga558f1a">4.6.8. Optimizing over \(K\)</a></li>
<li><a href="#org09a243f">4.6.9. Lagrange Multipliers</a></li>
<li><a href="#orga89e9d3">4.6.10. Optimal kernel</a></li>
<li><a href="#org0010e7e">4.6.11. Remarks</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org537d7cd">5. Week 7: Local Smoothers and Additive Models (Cont'd)</a>
<ul>
<li><a href="#org2173642">5.1. Spline Interpolation</a>
<ul>
<li><a href="#org56b5575">5.1.1. Recap: Local Smoothing</a></li>
<li><a href="#orgad3f326">5.1.2. Interpolating Splines</a></li>
<li><a href="#orgb580dd2">5.1.3. Zero-order spline</a></li>
<li><a href="#org900796f">5.1.4. First-order spline</a></li>
<li><a href="#orgad6ff0d">5.1.5. $d$-order splines</a></li>
<li><a href="#org2d573d3">5.1.6. Cubic spline interpolation</a></li>
<li><a href="#org724776f">5.1.7. Parameters in cubic splines</a></li>
<li><a href="#org22717d3">5.1.8. Constraints in cubic splines</a></li>
<li><a href="#org5287076">5.1.9. Natural cubic splines</a></li>
<li><a href="#orgfc4c05d">5.1.10. Simulation study</a></li>
<li><a href="#org8bcc2de">5.1.11. Spline interpolation in R</a></li>
<li><a href="#org9680bd7">5.1.12. Summary</a></li>
</ul>
</li>
<li><a href="#org48010f5">5.2. Cubic spline smoothing</a>
<ul>
<li><a href="#org4b2c1b8">5.2.1. Local smoothing</a></li>
<li><a href="#org90a0c04">5.2.2. Interpolation overfits</a></li>
<li><a href="#orgc925631">5.2.3. Roughness constraint</a></li>
<li><a href="#org194688c">5.2.4. Cubic spline smoother</a></li>
<li><a href="#org40ab426">5.2.5. Impact of &lambda; parameter</a></li>
<li><a href="#org5f38dd6">5.2.6. Optimal &lambda;</a></li>
</ul>
</li>
<li><a href="#orgb8f43b6">5.3. Optimality and algorithms in splines</a>
<ul>
<li><a href="#orgef2552f">5.3.1. Cubic splines</a></li>
<li><a href="#org6900136">5.3.2. Relation of cubic splines</a></li>
<li><a href="#org646bb04">5.3.3. Optimality properties</a></li>
<li><a href="#org8d814a5">5.3.4. Proof</a></li>
<li><a href="#org664f398">5.3.5. Algorithms for cubic splines</a></li>
<li><a href="#orge6814f3">5.3.6. Two useful facts</a></li>
<li><a href="#org49a902a">5.3.7. Ridge regression approach</a></li>
</ul>
</li>
<li><a href="#org96baccd">5.4. Additive models</a>
<ul>
<li><a href="#org989623c">5.4.1. Additive models detail</a></li>
<li><a href="#orga860829">5.4.2. Fitting additive model</a></li>
<li><a href="#org894cd1b">5.4.3. Estimation issues</a></li>
<li><a href="#org5b61f82">5.4.4. Key idea</a></li>
<li><a href="#org9d8311b">5.4.5. Backfitting algorithm</a></li>
<li><a href="#org2a590e2">5.4.6. Example for backfitting</a></li>
<li><a href="#org42a9ed3">5.4.7. Illustrative example</a></li>
<li><a href="#org3962795">5.4.8. R code for backfitting</a></li>
<li><a href="#orgab53dbc">5.4.9. R output</a></li>
</ul>
</li>
<li><a href="#org0a1062b">5.5. Generalized additive models</a>
<ul>
<li><a href="#orgbe35ccf">5.5.1. Additive models cont'd</a></li>
<li><a href="#orgccd1f8d">5.5.2. Link functions in GAM</a></li>
<li><a href="#org2f5cac6">5.5.3. Additive logistic models</a></li>
<li><a href="#org7b2811a">5.5.4. Recall logistic regression</a></li>
<li><a href="#org55acf30">5.5.5. Review of Newton-Raphson Algorithm</a></li>
<li><a href="#org0b17b81">5.5.6. Estimation in additive logistic</a></li>
<li><a href="#org67c44dd">5.5.7. Local scoring algorithm</a></li>
</ul>
</li>
<li><a href="#org2dfd71e">5.6. Splines applied to Spam dataset</a></li>
</ul>
</li>
<li><a href="#org1cf14e6">6. Week 8: Tree-Based and Ensemble Methods</a>
<ul>
<li><a href="#org1dc71bc">6.1. Introduction to Trees</a>
<ul>
<li><a href="#org0226189">6.1.1. Tree method</a></li>
<li><a href="#org0b12ea4">6.1.2. Motivating example</a></li>
<li><a href="#org36cd149">6.1.3. Basic ideas</a></li>
<li><a href="#org523b448">6.1.4. Illustrative example</a></li>
<li><a href="#org580ae39">6.1.5. Tree-based model</a></li>
<li><a href="#orgb7a97cf">6.1.6. 2 types of trees</a></li>
<li><a href="#org352470c">6.1.7. 3 fundamental issues in tree-based models</a></li>
</ul>
</li>
<li><a href="#orgff72416">6.2. Tree growing and tree pruning</a>
<ul>
<li><a href="#org49fecb3">6.2.1. Tree-based model</a></li>
<li><a href="#orgc5e6a1f">6.2.2. RSS criterion for regression tree</a></li>
<li><a href="#org91eb1c4">6.2.3. Choices of regions R<sub>m</sub></a></li>
<li><a href="#orgaf925ea">6.2.4. Greedy algorithm</a></li>
<li><a href="#org5960b7b">6.2.5. When to stop growing tree?</a></li>
<li><a href="#org2cdb1a6">6.2.6. Tree pruning</a></li>
<li><a href="#org3a0b4de">6.2.7. Cost-complexity criterion for pruning</a></li>
<li><a href="#orgf5c5f0d">6.2.8. Weakest link cutting algorithm</a></li>
<li><a href="#org4893461">6.2.9. How to choose &alpha;</a></li>
</ul>
</li>
<li><a href="#org4bb328d">6.3. Classification trees</a>
<ul>
<li><a href="#orgfe82562">6.3.1. Classification problem</a></li>
<li><a href="#org22bd56b">6.3.2. Majority vote in classification</a></li>
<li><a href="#orgd22f21a">6.3.3. Classification criterion</a></li>
<li><a href="#org5ec4e96">6.3.4. Drawback of natural criterion</a></li>
<li><a href="#org278c928">6.3.5. Alternative 1: Gini index</a></li>
<li><a href="#org3d2c9ce">6.3.6. Alternative 2: Cross-entropy</a></li>
<li><a href="#org55d8fc3">6.3.7. Classification tree</a></li>
</ul>
</li>
<li><a href="#org3e691c4">6.4. Practical issues in tree-based methods</a>
<ul>
<li><a href="#orgfb67157">6.4.1. Prediction problems</a></li>
<li><a href="#orgcffec73">6.4.2. Advantage of tree methods</a></li>
<li><a href="#orgff6fc0c">6.4.3. Missing values</a></li>
<li><a href="#orgfde7bd5">6.4.4. Disadvantage of tree methods</a></li>
<li><a href="#org894b43f">6.4.5. Trees in R and Spam example</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgedff1a6">7. Week 9: Tree-Based and Ensemble Methods (Cont'd)</a>
<ul>
<li><a href="#orgaaa0e0a">7.1. Introduction to Ensemble Methods</a>
<ul>
<li><a href="#org59aea4d">7.1.1. Supervised learning</a></li>
<li><a href="#org8dd9439">7.1.2. ML and data mining methods</a></li>
<li><a href="#org379ecb2">7.1.3. Ensemble methods</a></li>
<li><a href="#org7c19849">7.1.4. Central premise</a></li>
<li><a href="#org0193703">7.1.5. Models in ensemble method</a></li>
<li><a href="#org9773f43">7.1.6. Main techniques</a></li>
</ul>
</li>
<li><a href="#org96645d8">7.2. Bayesian model averaging and stacking</a>
<ul>
<li><a href="#org6686ef4">7.2.1. Bayesian statistics</a></li>
<li><a href="#orgaaaceb8">7.2.2. Bayesian model averaging (BMA)</a></li>
<li><a href="#orgf2f4839">7.2.3. Frequentist viewpoint</a></li>
<li><a href="#orgd72412a">7.2.4. Stacking</a></li>
<li><a href="#org0448f47">7.2.5. Stacking weights</a></li>
<li><a href="#orgba1ffb3">7.2.6. Aspects of stacking</a></li>
</ul>
</li>
<li><a href="#org710df8f">7.3. Bootstrapping algorithm</a>
<ul>
<li><a href="#orgfe4d71c">7.3.1. Motivating example</a></li>
<li><a href="#org44b0850">7.3.2. Idea in Bootstrapping algorithm</a></li>
<li><a href="#orgc22d413">7.3.3. Resample with replacement</a></li>
<li><a href="#org0f3a64a">7.3.4. New use of bootstrapping</a></li>
<li><a href="#org827d403">7.3.5. Bootstrapping in R</a></li>
<li><a href="#orgf1af47f">7.3.6. More efficient implementation</a></li>
</ul>
</li>
<li><a href="#org3f16bc9">7.4. Bagging algorithm</a>
<ul>
<li><a href="#orgb461ad9">7.4.1. Improve tree methods</a></li>
<li><a href="#org8c0d9d4">7.4.2. Variance reduction</a></li>
<li><a href="#org651ac8a">7.4.3. Ideas in bagging</a></li>
<li><a href="#orge5fabd8">7.4.4. Bagging algorithm</a></li>
<li><a href="#org8540b47">7.4.5. Remarks on bagging</a></li>
<li><a href="#org9750f1f">7.4.6. Theoretical issues</a></li>
</ul>
</li>
<li><a href="#org9ae7830">7.5. Random Forest</a>
<ul>
<li><a href="#orgd1132b6">7.5.1. Improve tree methods</a></li>
<li><a href="#orgb28911b">7.5.2. Recall bagging algorithm</a></li>
<li><a href="#orgda598fe">7.5.3. Ideas in random forest</a></li>
<li><a href="#orgb1e7ec1">7.5.4. Random forest</a></li>
<li><a href="#org0c1630c">7.5.5. Random Feature Selection</a></li>
<li><a href="#org721a848">7.5.6. Out-of-bag estimate</a></li>
<li><a href="#org6bb804c">7.5.7. Relative importance</a></li>
</ul>
</li>
<li><a href="#orgdbec756">7.6. Random Forest R Lab</a></li>
</ul>
</li>
<li><a href="#orge37917e">8. Week 10: Tree-Based and Ensemble Methods (Cont'd)</a>
<ul>
<li><a href="#org86903af">8.1. Introduction to Boosting</a>
<ul>
<li><a href="#org388e229">8.1.1. Improving Tree-Based Methods</a></li>
<li><a href="#orgb4264cb">8.1.2. Ensemble Methods</a></li>
<li><a href="#org40ee941">8.1.3. Boosting history</a></li>
<li><a href="#orgf64e407">8.1.4. AdaBoost motivation</a></li>
<li><a href="#org65dff10">8.1.5. Idea in AdaBoost</a></li>
<li><a href="#org4cf1b99">8.1.6. Modern viewpoint of boosting</a></li>
</ul>
</li>
<li><a href="#org19dba01">8.2. AdaBoost for Binary classification</a>
<ul>
<li><a href="#org83b9854">8.2.1. Problem setup</a></li>
<li><a href="#org8381f84">8.2.2. AdaBoost algorithm</a></li>
<li><a href="#org6f4ed85">8.2.3. Example</a></li>
<li><a href="#org3bceba3">8.2.4. 3 weaker learners</a></li>
<li><a href="#org6f89c89">8.2.5. AdaBoost round 1</a></li>
<li><a href="#org27676fa">8.2.6. AdaBoost round 2</a></li>
<li><a href="#orga53209f">8.2.7. AdaBoost round 3</a></li>
<li><a href="#orgc356bc2">8.2.8. Output classifier</a></li>
<li><a href="#org1c9263c">8.2.9. AdaBoost output</a></li>
</ul>
</li>
<li><a href="#orge5e1231">8.3. Statistical view of AdaBoost</a>
<ul>
<li><a href="#org225ac18">8.3.1. Problem setup</a></li>
<li><a href="#org189f8de">8.3.2. AdaBoost algorithm</a></li>
<li><a href="#org75c1f51">8.3.3. Statistical properties of AdaBoost</a></li>
<li><a href="#org00085f6">8.3.4. Exponential loss function</a></li>
<li><a href="#org121bdad">8.3.5. Optimization problem</a></li>
<li><a href="#org47a6bae">8.3.6. Optimization in function</a></li>
<li><a href="#org071cb2b">8.3.7. Optimization in coefficient</a></li>
<li><a href="#org3ea75ca">8.3.8. Implementation update</a></li>
</ul>
</li>
<li><a href="#org94b1740">8.4. General boosting algorithm</a>
<ul>
<li><a href="#orgd0c6a13">8.4.1. Problem setup</a></li>
<li><a href="#org424cc03">8.4.2. Recall regression models</a></li>
<li><a href="#org06d15c2">8.4.3. Extended additive model</a></li>
<li><a href="#org022397b">8.4.4. Forward stagewise additive model</a></li>
<li><a href="#org15d3ac3">8.4.5. Shrinkage parameter</a></li>
<li><a href="#org7cdabcf">8.4.6. General boosting algorithm</a></li>
<li><a href="#org05b832a">8.4.7. Parameter tuning</a></li>
</ul>
</li>
<li><a href="#orgee2c1f4">8.5. Boosting in R</a></li>
</ul>
</li>
<li><a href="#org7fe807d">9. Week 12: Advanced Supervised Learning</a>
<ul>
<li><a href="#org8075801">9.1. Introduction to Support Vector Machines</a>
<ul>
<li><a href="#org2f78a8d">9.1.1. Binary classification problem</a></li>
<li><a href="#orgba6556d">9.1.2. Toy motivating example</a></li>
<li><a href="#org9138eca">9.1.3. Good separate line?</a></li>
<li><a href="#orgf6d0041">9.1.4. Poor separation</a></li>
<li><a href="#org0ad635f">9.1.5. Margin maximization</a></li>
<li><a href="#orgf32f435">9.1.6. Distance in 2 dimensions</a></li>
<li><a href="#orgfe86601">9.1.7. Distance from point to hyperplane</a></li>
<li><a href="#org5b23828">9.1.8. Distance between hyperplanes</a></li>
</ul>
</li>
<li><a href="#orgffc01f8">9.2. Maximum Margin Optimization for SVM</a>
<ul>
<li><a href="#orgf5d5389">9.2.1. Linear classification</a></li>
<li><a href="#org580450e">9.2.2. Linear SVM</a></li>
<li><a href="#orgb283ecb">9.2.3. Margin computation</a></li>
<li><a href="#org1d1066c">9.2.4. Maximum margin optimization</a></li>
<li><a href="#org33950ff">9.2.5. Constraints for SVM</a></li>
<li><a href="#org2913d3e">9.2.6. Mathematical formulation</a></li>
<li><a href="#orgdad2a2a">9.2.7. A simplified formulation</a></li>
<li><a href="#orgac4c522">9.2.8. Constrained optimization</a></li>
</ul>
</li>
<li><a href="#org391cda9">9.3. SVM for Linearly Separable</a>
<ul>
<li><a href="#orgcc540ed">9.3.1. Linear SVM</a></li>
<li><a href="#orgff05d6f">9.3.2. Maximum margin optimization</a></li>
<li><a href="#org1079b02">9.3.3. Unconstrained optimization</a></li>
<li><a href="#org74e60e6">9.3.4. Local minimum</a></li>
<li><a href="#org47d9718">9.3.5. KKT necessary condition</a></li>
<li><a href="#org83f217d">9.3.6. Objective function in dual space</a></li>
<li><a href="#org37ed284">9.3.7. Dual space formulation of SVM</a></li>
<li><a href="#org134274f">9.3.8. The SVM classifier</a></li>
</ul>
</li>
<li><a href="#org6a1d31a">9.4. Slack variables for linearly non-separable</a>
<ul>
<li><a href="#orgc316857">9.4.1. Linear classification</a></li>
<li><a href="#org3f58ad1">9.4.2. Linearly separable scenario</a></li>
<li><a href="#orgaf593a9">9.4.3. Linearly non-separable scenario</a></li>
<li><a href="#org9880898">9.4.4. Slack variable</a></li>
<li><a href="#orgf204993">9.4.5. Implication of slack variables</a></li>
<li><a href="#org42c6a3e">9.4.6. Equality scenario</a></li>
<li><a href="#org4aff3ac">9.4.7. Linearly non-separable scenarios</a></li>
<li><a href="#orgcd683e4">9.4.8. Comparison of two scenarios</a></li>
</ul>
</li>
<li><a href="#org20a3493">9.5. Optimization problem for linearly non-separable</a>
<ul>
<li><a href="#org8a6c437">9.5.1. Three criteria in SVM</a></li>
<li><a href="#org8666130">9.5.2. Possible formulation?</a></li>
<li><a href="#org57ebede">9.5.3. Key idea of new formulation</a></li>
<li><a href="#org5073cde">9.5.4. Constrained optimization</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgd101c46" class="outline-2">
<h2 id="orgd101c46"><span class="section-number-2">1.</span> Week 3: Linear Regression (II)</h2>
<div class="outline-text-2" id="text-1">
<p>
M2T2
</p>
</div>
<div id="outline-container-orgeea1f8f" class="outline-3">
<h3 id="orgeea1f8f"><span class="section-number-3">1.1.</span> James-Stein Estimator</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-orgc8c650c" class="outline-4">
<h4 id="orgc8c650c"><span class="section-number-4">1.1.1.</span> It's a special case of linear regression</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
in LR model with
\[
Y_{n\times 1} = X_{n\times p} \beta_{p\times 1} + \epsilon_{n\times 1}, \text{s.t.} \epsilon \sim N(0, \sigma^2 I_{n\times n})
\]
</p>

<p>
Special case:
</p>
<ul class="org-ul">
<li>\(n=p\)</li>
<li>\(X_{n\times p} = I_{p\times p}\)</li>
</ul>

<p>
OLS yields the estimator of:
</p>

<p>
\[
\hat{\beta_{ols}} = (X^T X)^{-1} X^T Y =
(I^T_{p\times p} I_{p\times p})^{-1}
I^T_{p\times p} Y_{p\times 1} =
\bf{Y_{p\times 1}}
\]
</p>

<p>
When \(\bf{p\ge 3}\), is it possible to do better than OLS?
</p>
</div>
</div>
<div id="outline-container-org8c3f412" class="outline-4">
<h4 id="org8c3f412"><span class="section-number-4">1.1.2.</span> Simultaneous estimation</h4>
<div class="outline-text-4" id="text-1-1-2">
<ul class="org-ul">
<li><p>
Problem of estimating \(p\) # of parameters \(\beta_i\)'s simultaneously from \(p\) observations (\(Y_i\)'s) under model:
</p>

<p>
\[
  \bf{Y_i} \sim N(\beta_i, \sigma^2), \text{for }i = 1, 2, ..., p
  \]
</p></li>
<li><p>
OLS (a.k.a. Maximum Likelihood Estimator, MLE) yields estimator:
</p>

<p>
\[
  \hat{\beta_i} = Y_i \text{ for }i = 1, 2, ..., p
  \]
</p>

<p>
Is it possible to do better here?
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org65dcaf9" class="outline-4">
<h4 id="org65dcaf9"><span class="section-number-4">1.1.3.</span> JS estimator</h4>
<div class="outline-text-4" id="text-1-1-3">
<ul class="org-ul">
<li><p>
Showed MLS/MLE estimator inadmissible for \(p\ge 3\); dominated by JS estimator.
</p>

<p>
\[
  \hat{\beta_i^{MLE}} = Y_i \text{ for }i = 1, 2, ..., p
  \]
</p>

<p>
\[
  \hat{\beta_i^{(JS)}} = w Y_i + (1-w) \bar{Y} \text{ for }i = 1, 2, ..., p;
  \]
</p>

<p>
\[
  w = 1 - \frac{(p-3)\sigma^2}{\sum^p_i(Y_i-\bar{Y})^2}
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org8b6bef3" class="outline-4">
<h4 id="org8b6bef3"><span class="section-number-4">1.1.4.</span> Baseball example</h4>
<div class="outline-text-4" id="text-1-1-4">
<ul class="org-ul">
<li>Observe \(Y_1, Y_2, ... Y_p\) batting averages (where \(Y_i\) is the batting average for p=18 players), 45 AB</li>
<li>"True" values \(\mu_i\) are the averages over remainder of seasons, 370 AB</li>
<li>Qn: how to predict season averages \(\mu_i\) from early statistics \(Y_i\)?</li>
<li>Estimators: MLE and JS</li>
</ul>
</div>
</div>
<div id="outline-container-orgbdc5c28" class="outline-4">
<h4 id="orgbdc5c28"><span class="section-number-4">1.1.5.</span> Comparing MLE and JS</h4>
<div class="outline-text-4" id="text-1-1-5">
<ul class="org-ul">
<li>JS has lower predictive squared error than MLS (by 50%)</li>
<li>JS estimator is <b><b>shrinkage</b></b> estimator.
<ul class="org-ul">
<li>Each MLE value shrunken towards <b><b>grand mean</b></b></li>
<li>Data-based estimator, compromises between:
<ul class="org-ul">
<li>null hypothesis: all means the same</li>
<li>MLE assumption: no relationship between all \(\mu_i\) values</li>
</ul></li>
<li>Difficult to estimate \(p\ge 3\) parameters simultaneously.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8093d88" class="outline-3">
<h3 id="org8093d88"><span class="section-number-3">1.2.</span> Shrinkage Methods</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>Estimation in linear regression: JS works in only specific cases (<a href="#orgc8c650c">1.1.1</a>) when \(p\ge 3\)</li>
<li>How to do better <i>generally</i>?</li>
<li>Shrinkage methods (penalized, regularized)
<ul class="org-ul">
<li>Based on subtracting penalty from log-likelihood</li>
<li>Penalty is a function of <i>decay parameter</i></li>
<li>Sort the variables to be included by size of <i>decay parameter</i></li>
<li>This reduces to a nested case</li>
<li>After estimating <i>decay parameter</i>, variable or model selection is complete!</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgffdc9b1" class="outline-4">
<h4 id="orgffdc9b1"><span class="section-number-4">1.2.1.</span> Setting up shrinkage method</h4>
<div class="outline-text-4" id="text-1-2-1">
<ul class="org-ul">
<li>Needs: \(Y_1, x_{11}, x_{12}, ..., x_{1,p}, \text{such that } i = 1,2,...,n\)</li>
<li>Assume all X &amp; Y are standardized i.e.:
\[
  \sum^n_{i=1}Y_i = 0,
  \sum^n_{i=i}x_{ij} = 0,
  \sum^n_{i=1}x^2_{ij} = 1
  \]</li>
<li><p>
If not standardized, do linear transformations:
\[
  Y^{*}_i = Y_i - \bar{Y}
  \]
</p>

<p>
\[
  x^{*}_{ij} = \frac{x_{ij}-\bar{x_j}}{\sqrt{\text{Var}_j}}
  \]
</p></li>
<li>With this assumption, \(\beta_0 = 0\) in the model, i.e.
\[
  Y_i = \bf{0 } \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{i,p} + \epsilon_i
  \]</li>
<li><p>
<b>The shrinkage method solves this optimization problem</b>
</p>

<p>
\[
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  \]
</p>
<dl class="org-dl">
<dt>penalty function</dt><dd>\(J(|\beta_j)\)</dd>
<dt>decay or tuning parameter</dt><dd>\(\lambda \ge 0\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org07c4a63" class="outline-4">
<h4 id="org07c4a63"><span class="section-number-4">1.2.2.</span> Alternative formulation</h4>
<div class="outline-text-4" id="text-1-2-2">
<ul class="org-ul">
<li>Shrinkage method solves the <b>unconstrained</b> optimization problem
\[
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  \]</li>
<li><p>
Alternative formulation solves a <b>constrained</b> optimization problem
</p>

<p>
\[
  \min_{\beta} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2, \text{ subject to: }
  \sum^p_{j=1} J(|B_j|) \le s
  \]
</p>
<dl class="org-dl">
<dt>tuning parameter</dt><dd>\(s \gt 0\)</dd>
</dl></li>

<li>The alternative formulation may greatly facilitate computation <b>at times</b>, e.g. in LASSO which is piecewise linear in \(s\).</li>
</ul>
</div>
</div>
<div id="outline-container-org4e351ea" class="outline-4">
<h4 id="org4e351ea"><span class="section-number-4">1.2.3.</span> Bayesian interpretation</h4>
<div class="outline-text-4" id="text-1-2-3">
<ul class="org-ul">
<li>For LR model (<a href="#orgc8c650c">1.1.1</a>):
<dl class="org-dl">
<dt>prior on &beta;</dt><dd>\(\pi(\beta)\)</dd>
<dt>independent prior on \(\sigma^2\)</dt><dd>\(\pi(\sigma^2)\)</dd>
<dt>posterior for \((\beta, sigma^2)\)</dt><dd>proportional to
\[
    \pi (\sigma^2)(\sigma^2)^{(n-1)/2}\exp\{-\frac{1}{2\sigma^2} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \log \pi(\beta) \}
    \]</dd>
</dl></li>
<li><p>
<b>Posterior maximization method</b> yields shrinkage estimator
</p>

<p>
\[
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \textbf{Pen}(\beta)
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgd839dcf" class="outline-4">
<h4 id="orgd839dcf"><span class="section-number-4">1.2.4.</span> Choices of priors</h4>
<div class="outline-text-4" id="text-1-2-4">
<p>
i.e. choice of prior \(\pi(\beta)\)
</p>
<dl class="org-dl">
<dt>Normal prior</dt><dd>yields <b>ridge regression</b> etsimator</dd>
<dt>Laplace prior</dt><dd>yields <b>LASSO</b> estimator</dd>
</dl>
</div>
</div>
<div id="outline-container-orgb3942f7" class="outline-4">
<h4 id="orgb3942f7"><span class="section-number-4">1.2.5.</span> Ridge regression</h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
<b>Normal prior</b> assumes \(\beta_1 ... \beta_p\) are i.i.d. \(N(0, \tau^2)\) with prior density
\[
\pi(\beta) = \prod^p_{i=1} \frac{1}{\sqrt{2\pi}\tau} \exp\left
(-\frac{1}{2\tau^2}\beta_i^2\right)
\]
</p>

<p>
Yields <b>ridge regression</b> estimator, which minimizes
</p>

<p>
\[
\parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
\]
</p>
</div>
</div>
<div id="outline-container-org54754bf" class="outline-4">
<h4 id="org54754bf"><span class="section-number-4">1.2.6.</span> LASSO estimator</h4>
<div class="outline-text-4" id="text-1-2-6">
<ul class="org-ul">
<li><b>Laplace Prior</b>, assume  \(\beta_1 ... \beta_p\) are i.i.d. double-exponential (Laplace) \(\sim \text{Lapalce} (,\tau)\) with prior density
\[
  \pi(\beta) = \prod^p_{i=1} \frac{1}{2\tau} \exp \left(- \frac{1}{\tau} |\beta_i| \right)
  \]</li>
<li><p>
Yields <b>LASSO</b> estimator that minimizes
</p>

<p>
\[
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}|\beta_i|
  \]
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga0a1645" class="outline-3">
<h3 id="orga0a1645"><span class="section-number-3">1.3.</span> Ridge Regression</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-orga3b7cc8" class="outline-4">
<h4 id="orga3b7cc8"><span class="section-number-4">1.3.1.</span> Ridge Regression Estimator</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
Assume these are observed: \(Y_i, x_{i1}, ..., x_{ip}\), and all are standardized:
\[
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
\]
</p>

<p>
In linear regression model without intercepts (<a href="#orgc8c650c">1.1.1</a>)
</p>

<p>
The ridge regression estimator is defined as:
</p>

<p>
\[
\hat{\beta^{\text{ridge}}} = \min_{\beta}
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
\]
</p>
</div>
</div>
<div id="outline-container-org3f2c8c0" class="outline-4">
<h4 id="org3f2c8c0"><span class="section-number-4">1.3.2.</span> Mathematical solution</h4>
<div class="outline-text-4" id="text-1-3-2">
<ul class="org-ul">
<li>Explicit expression is thus
\[
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  \]</li>
<li>Ridge regression <b>estimator or prediction</b>:
\[
  \hat{Y}^{\text{ridge}} = X_{n\times p}  \hat{\beta^{\text{ridge}}}
  \]</li>
<li>Requires <b>choosing</b> the tuning parameter \(\lambda\), based on data, usually <b>by cross-validation</b></li>
</ul>
</div>
</div>
<div id="outline-container-orgeb44451" class="outline-4">
<h4 id="orgeb44451"><span class="section-number-4">1.3.3.</span> Properties of Ridge Regression</h4>
<div class="outline-text-4" id="text-1-3-3">
<ul class="org-ul">
<li>Ridge regression <b>most useful</b> when \(X_{n\times p}\) is <b>non-singular</b>, but has <b>high collinearity</b>
<ul class="org-ul">
<li>i.e. \(X^T_{n\times p} X_{n\times p}\) has eigenvalue close to 0</li>
</ul></li>
<li>\(\hat{\beta^{\text{ridge}}}\) is biased, with bias \(\rightarrow\) 0 as \(\lambda \rightarrow 0\)</li>
<li>As \(\lambda\) increases, \(\hat{\beta^{\text{ridge}}}\) \(\rightarrow 0\), though rarely = 0.</li>
<li>Despite the bias, \(\text{Var}(\hat{\beta^{\text{ridge}}})\) will usually be smaller than OLS
<ul class="org-ul">
<li>Therefore better prediction than OLS.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgdbea4a0" class="outline-4">
<h4 id="orgdbea4a0"><span class="section-number-4">1.3.4.</span> Computational issues</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li>How to compute ridge regression efficiently for any \(\lambda\)?
\[
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  \]</li>
<li>It is highly non-trivial to compute the inverse of a large \(p\times p\) matrix.</li>
<li><b>Singular Value Decomposition</b> (SVD) algorithm:
<ul class="org-ul">
<li>Write the matrix \(X_{n\times p}\) in its SVD form
\[
    X_{n\times p} = U_{n\times p} D_{p\times p} V^T_{p\times p}
    \]
where: \(U\) and $V are orthogonal; D = diag(\(d_1, ..., d_p\)) is diagonal.</li>
</ul></li>
<li>Then: ridge regression estimator becomes the matrix product:
\[
   \hat{\beta^{\text{ridge}}} = V_{p\times p} \text{diag} \left(\frac{d_1}{d^2_1 + \lambda}, ..., \frac{d_p}{d^2_p + \lambda} \right) U^T_{p\times n} Y_{n\times 1}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org6affc92" class="outline-4">
<h4 id="org6affc92"><span class="section-number-4">1.3.5.</span> Example of SVD</h4>
<div class="outline-text-4" id="text-1-3-5">
<ul class="org-ul">
<li>Find SVD of matrix
\[
  X_{3 \times 2}
  = \begin{pmatrix}
  1 & 0 \\
  0 & 1 \\
  1 & 1 \\
  \end{pmatrix}
  = U_{n\times p} D_{p\times p} V^T_{p\times p}
  \]</li>
<li>Steps (required: \(p \leq n\)):
<ol class="org-ol">
<li>\(U_{n\times p}\) is the normalized \(p\) (largest) eigenvectors of \(XX^T\)</li>
<li>\(V_{p\times p}\) is the normalized eigenvectors of \(X^T X\)</li>
<li>Matrix \(D = \text{diag}(d_1, ..., d_p)\) with \(d_j\) being the square root of \(p\) (largest) eigenvalues of \(XX^T\) or \(X^T X\).</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org1098e83" class="outline-4">
<h4 id="org1098e83"><span class="section-number-4">1.3.6.</span> SVD Example (I): \(XX^T\)</h4>
<div class="outline-text-4" id="text-1-3-6">
<p>
For matrix <a href="#org6affc92">1.3.5</a>:
we have
\[
XX^T = \begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 1 \\
1 & 1 &2\end{pmatrix}
\]
</p>
<ul class="org-ul">
<li>Characteristic polynomial is
\(-\lambda^3 + 4\lambda^2 - 3\lambda = -\lambda(\lambda-1)(\lambda-3)\)</li>
<li>The eigenvalues of \(XX^T\) are \(\lambda = 3, 1, 0\)</li>
<li>Corresponding eigenvectors are:
\[
  u'_1 = \begin{pmatrix}
  1 \\
  1 \\
  2\end{pmatrix},
  u'_2 = \begin{pmatrix}
  1 \\
  -1 \\
  0\end{pmatrix},
  u'_3 = \begin{pmatrix}
  1 \\
  1 \\
  -1\end{pmatrix},
  \]</li>
<li>Normalizing yields
\[
  U_{3\times 2} = \begin{pmatrix}
  1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
  1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
  2\over{\sqrt{6}} & 0 \end{pmatrix},
  \]</li>
<li>\(d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1\)</li>
</ul>
</div>
</div>

<div id="outline-container-org1dac47d" class="outline-4">
<h4 id="org1dac47d"><span class="section-number-4">1.3.7.</span> SVD Example (II): \(X^T X\)</h4>
<div class="outline-text-4" id="text-1-3-7">
<p>
For matrix <a href="#org6affc92">1.3.5</a>:
\[
X^T X
= \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix}
\]
</p>

<ul class="org-ul">
<li>Characteristic polynomial is
\(\lambda^2 - 4\lambda + 3 = (\lambda -1)(\lambda -3)\)</li>
<li>Eigenvalues of \(XX^T\) are: \(\lambda = 3, 1\).</li>
<li>Corresponding eigenvalues are:
\[
  v'_1 = \begin{pmatrix}
  1 \\
  1\end{pmatrix},
  v'_2 = \begin{pmatrix}
  1 \\
  -1\end{pmatrix}
  \]</li>
<li>Normalizing them yields:
\[
  V_{2\times 2} = (v_1, v_2) =
  \begin{pmatrix}
  1\over\sqrt{2} & 1\over\sqrt{2} \\
  1\over\sqrt{2} & -1\over\sqrt{2}\end{pmatrix},
  \]</li>
<li>\(d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1\)</li>
</ul>
</div>
</div>
<div id="outline-container-org28c4dfa" class="outline-4">
<h4 id="org28c4dfa"><span class="section-number-4">1.3.8.</span> SVD verification</h4>
<div class="outline-text-4" id="text-1-3-8">
<p>
Might need to multiply some eigenvectors by -1.
</p>

\begin{equation}
X_{n\times p}
=
\begin{pmatrix}
1 & 0 \\
0 & 1 \\
1 & 1\end{pmatrix}
=
\begin{pmatrix}
1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
2\over{\sqrt{6}} & 0\end{pmatrix}
\begin{pmatrix}
\sqrt{3} & 0 \\
0 & 1\end{pmatrix}
\begin{pmatrix}
1\over{\sqrt{2}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{2}} & -1\over{\sqrt{2}}\end{pmatrix}

= U_{n\times p} D_{p\times p}V^T_{p\times p}
\end{equation}

\begin{equation}
= \lambda_1 u_1 v^T_1 + \lambda_2 u_2 v^T_2 =
0.5\begin{pmatrix}
1 & 1 \\
1 & 1 \\
2 & 2 \end{pmatrix} +
0.5\begin{pmatrix}
1 & -1 \\
-1 & 1 \\
0 & 0 \end{pmatrix}
\end{equation}
</div>
</div>
</div>
<div id="outline-container-orgfbfb14b" class="outline-3">
<h3 id="orgfbfb14b"><span class="section-number-3">1.4.</span> LASSO (3.2.1)</h3>
<div class="outline-text-3" id="text-1-4">
</div>
<div id="outline-container-org46aadb3" class="outline-4">
<h4 id="org46aadb3"><span class="section-number-4">1.4.1.</span> LASSO estimator</h4>
<div class="outline-text-4" id="text-1-4-1">
<ul class="org-ul">
<li>Assume these are observed: \(Y_i, x_{i1}, ..., x_{ip}\), and all are standardized:</li>
</ul>
<p>
\[
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
\]
</p>

<p>
In linear regression model without intercepts (<a href="#orgc8c650c">1.1.1</a>)
</p>

<dl class="org-dl">
<dt>LASSO</dt><dd>Least Absolute Selection and Shrinkage Operator</dd>

<dt>Definition</dt><dd>\[
  \hat{\beta}^{\text{lasso}} = \min_\beta \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1} |\beta_j|
  \]</dd>
<dt>s.t.</dt><dd>tuning parameter \(\lambda > 0\)</dd>
</dl>
</div>
</div>
<div id="outline-container-orga817a0c" class="outline-4">
<h4 id="orga817a0c"><span class="section-number-4">1.4.2.</span> L2-norm vs L1-norm</h4>
<div class="outline-text-4" id="text-1-4-2">

<div id="org1d46388" class="figure">
<p><img src="./img/l2-l1-norm.png" alt="l2-l1-norm.png" />
</p>
</div>
<ul class="org-ul">
<li>L1-norm: <b>sparse</b>, as boundary points of the L1-norm ball have lower dimensions (are in lower-dimensional space, \(x_1 = 0, or x_2=0\))</li>
</ul>
</div>
</div>
<div id="outline-container-org86c32f7" class="outline-4">
<h4 id="org86c32f7"><span class="section-number-4">1.4.3.</span> Mathematical solution for LASSO estimator</h4>
<div class="outline-text-4" id="text-1-4-3">
<ul class="org-ul">
<li>In the LASSO optimization, there is <b>no explicit</b> mathematical solution to \(\hat{\beta}^{\text{lasso}}\)</li>
<li>Hence, need to use computational algorithms to get solution</li>
<li>Explicit solution only available when \(X^T X = I_{n\times n}\)
<ul class="org-ul">
<li><p>
In this case, LASSO estimator is:
</p>

\begin{equation}
\hat{\beta}^{\text{lasso}}_j =
\begin{cases}
  \hat{\beta}_j^{ols} - \frac{\lambda}{2} & \text{if }\hat{\beta}_j^{ols}> \frac{\lambda}{2}\\
  0 & \text{if }|\hat{\beta}_j^{ols}| \leq \frac{\lambda}{2}\\
  \hat{\beta}_j^{ols} + \frac{\lambda}{2} & \text{if }\hat{\beta}_j^{ols} < -\frac{\lambda}{2}\\
\end{cases}
\end{equation}</li>
</ul></li>
<li><p>
As: LASSO can be simplified to 1-dimensional optimization problem
</p>

<p>
\[
  \min_{-\infty < x < \infty} (x - \hat{\beta}_j^{ols})^2 + \lambda |x|
  \]
</p></li>
</ul>

<p>
since:
</p>

<p>
\[
\parallel Y - X\beta \parallel^2 = \parallel Y-X\hat{\beta}^{ols} \parallel^2 + (\beta - \hat{\beta}^{ols})^T X^T X(\beta - \hat{\beta}^{ols})
\]
</p>
</div>
</div>
<div id="outline-container-orgd4d0d7d" class="outline-4">
<h4 id="orgd4d0d7d"><span class="section-number-4">1.4.4.</span> Properties of LASSO</h4>
<div class="outline-text-4" id="text-1-4-4">
<ul class="org-ul">
<li><b>Good empirical performance when true model is sparse</b>
<ul class="org-ul">
<li>If so, outperforms AIC, BIC, stepwise, ridge</li>
</ul></li>
<li>Nice theoretical properties, i.e. high probability of the following under certain regularity conditions:
<dl class="org-dl">
<dt>parameter recovery</dt><dd>when \(|\hat{\beta}_j^{lasso}-\hat{\beta}^{true}|^2\) is small</dd>
<dt>variable selection</dt><dd>\(\textbf{supp}(\hat{\beta}_j^{lasso}) = \textbf{supp}(\beta^{true})\)</dd>
<dt>prediction error bound</dt><dd>\(|X\hat{\beta}_j^{lasso} - X\beta^{true}|^2\) is small</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org19ea55c" class="outline-4">
<h4 id="org19ea55c"><span class="section-number-4">1.4.5.</span> LASSO weaknesses</h4>
<div class="outline-text-4" id="text-1-4-5">
<ul class="org-ul">
<li>Not always consistent</li>
<li>Tends to select over-parameterized model</li>
<li>Does poorly when
<ol class="org-ol">
<li>True model is <b>not sparse</b></li>
<li>When few X variables are highly correlated (LASSO picks 1 randomly)</li>
<li>When the design \(X\) matrix is too correlated (Ridge outperforms)</li>
<li>When there are outliers in responses</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org1422a25" class="outline-4">
<h4 id="org1422a25"><span class="section-number-4">1.4.6.</span> Computation issues of LASSO</h4>
<div class="outline-text-4" id="text-1-4-6">
<ul class="org-ul">
<li>Computation algorithms include:
<ul class="org-ul">
<li>Coordinate descent</li>
<li>Sub-gradient methods</li>
<li>Proximal gradient methods</li>
</ul></li>
<li>Would be ideal to compute entire solution path all at once, i.e. for all values for \(\lambda\) simultaneously.</li>
</ul>
</div>
</div>
<div id="outline-container-org4164dfb" class="outline-4">
<h4 id="org4164dfb"><span class="section-number-4">1.4.7.</span> LASSO is piecewise linear</h4>
<div class="outline-text-4" id="text-1-4-7">
<ul class="org-ul">
<li>The number of linear pieces in LASSO path is approximately \(p\),</li>
<li>The computational complexity of getting whole LASSO path is \(O(np^2)\)
<ul class="org-ul">
<li>i.e. same cost as computing least-squares fit</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org77d27b3" class="outline-4">
<h4 id="org77d27b3"><span class="section-number-4">1.4.8.</span> Standard error of LASSO</h4>
<div class="outline-text-4" id="text-1-4-8">
<ul class="org-ul">
<li>How to estimate standard error of LASSO estimator i.e.
\[
  \hat{\beta}^{\text{lasso}} = \min_\beta \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1} |\beta_j|
  \]</li>
<li>Answer: bootstrapping:
<ol class="org-ol">
<li>Fix \(\lambda\), generate a set of bootstrap samples</li>
<li>Obtain corresponding \(\hat{\beta}^{lasso}(\lambda)\)</li>
<li>Repeat for \(L\) times and use them to estimate standard error</li>
<li>If not determined/fixed, \(\lambda\) can be estimated by cross-validation (e.g. 5 fold CV).</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orge17186f" class="outline-4">
<h4 id="orge17186f"><span class="section-number-4">1.4.9.</span> Variants of L1-norm</h4>
<div class="outline-text-4" id="text-1-4-9">
<ul class="org-ul">
<li>Elastic net
\[
  \hat{\beta}^{enet} = \min_\beta \parallel Y_{n\times 1} - X_{n \times p} \beta_{p \times 1} \parallel^2 + \lambda_1 \sum^p_{j=1} |\beta_j| + \lambda_2 \sum^p_j (\beta_j)^2
  \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5270205" class="outline-3">
<h3 id="org5270205"><span class="section-number-3">1.5.</span> Principal Components (3.2.2)</h3>
<div class="outline-text-3" id="text-1-5">
<p>
Assume these are observed: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)
</p>

<p>
Classical datasets: mostly small values of \(p\); modern datasets: large \(p\).
</p>

<p>
Essential to conduct <b>dimension reduction</b> to reduce the number of variables.
</p>
</div>
<div id="outline-container-org69b70a8" class="outline-4">
<h4 id="org69b70a8"><span class="section-number-4">1.5.1.</span> Dimension reduction</h4>
<div class="outline-text-4" id="text-1-5-1">
<ul class="org-ul">
<li>2 approaches:
<ol class="org-ol">
<li><b>variable selection</b>, i.e.: AIC, BIC, stepwise algorithm, LASSO, etc.</li>
<li><b>feature extraction</b>, i.e. identify which functions of data are most important. <b>no restricted</b> to using existing features/variables. Options are:
<ul class="org-ul">
<li>Principal component analysis</li>
<li>Partial least squares</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org66f6942" class="outline-4">
<h4 id="org66f6942"><span class="section-number-4">1.5.2.</span> Motivation of PCA</h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
"Obtain more variance by transforming axes"
</p>
<ul class="org-ul">
<li>Find <b>linear combinations</b> of \((x_1, ..., x_p)\) that express as much variability in \(X\) as possible.
<ul class="org-ul">
<li>A linear combination with <b>high</b> variance will likely affect the response the most</li>
<li>If most variation of \(X\) comes from the first few PCs then: enough to build models.</li>
<li>Other linear combination vary so little among different observations &rarr; can be ignored</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org76e89d4" class="outline-4">
<h4 id="org76e89d4"><span class="section-number-4">1.5.3.</span> Find the PC's:  Population version</h4>
<div class="outline-text-4" id="text-1-5-3">
<ul class="org-ul">
<li><b>Optimization problem for PC's</b>: Given a \(p\) -dim random vector
\[
  \textbf{X} = (X_1, ..., X_p)^T, \text{ with covariance } \Sigma = \text{Cov}(X)
  \]</li>
<li>PC1: Find \(U_1 = \alpha_1 X_1 + ... + \alpha_p X_p\) that maximizes
\[
  \textbf{Var}(\alpha_1 X_1 + ... + \alpha_p X_p) = \textbf{Var}(\alpha^T \textbf{X}) = \alpha^T \Sigma \alpha
  \]
subject to:
\[
  \alpha^2_1 + ... \alpha^2_p = 1, \text{i.e. } \alpha^T \alpha = 1, \text{where } \alpha = (\alpha_1, ..., \alpha_p)^T
  \]</li>
<li>PC2: Find \(U_2 = \alpha_1 X_1 + ... + \alpha_p X_p\) that maximizes \text{Var}(&alpha;<sup>T</sup> X) = &alpha;<sup>T</sup> &Sigma;&alpha;$, subject to <b>constraints</b>: - \(\alpha^T \alpha = 1\)
<ul class="org-ul">
<li>\(\text{Cov}(U_1, U_2) = 0\)</li>
</ul></li>
<li>Other (later) PC are defined analogously and uncorrelated with all previous PC's</li>
</ul>
</div>
</div>
<div id="outline-container-org8a81b99" class="outline-4">
<h4 id="org8a81b99"><span class="section-number-4">1.5.4.</span> Eigenvectors lead to PC's</h4>
<div class="outline-text-4" id="text-1-5-4">
<ul class="org-ul">
<li>Theorem: Let covariance matrix \(\bf{\Sigma} = \text{Cov}(\bf{X})\) have eignvectors \(e_1, ..., e_p\) with corresponding eigenvalues \(\lambda_1 \ge ... \ge \lambda_p \ge 0\). For \(j=1,2,...,p\),
<ul class="org-ul">
<li>\(j\) -th PC is
\[
    U_j = e_j^T X = e_{j1}X_1 + ... + e_{jp}X_p
    \]</li>
<li>Variance of \(j\) -th PC is
\[
    \text{Var}(U_j) = \bf{e_j^T\Sigma e_j} = \lambda_j
    \]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga4a3bbc" class="outline-4">
<h4 id="orga4a3bbc"><span class="section-number-4">1.5.5.</span> Proof by Lagrange Multipliers</h4>
<div class="outline-text-4" id="text-1-5-5">
<ul class="org-ul">
<li>The Lagrange multiplier is to maximize
\[
  \phi(\alpha) = \alpha^T \Sigma\alpha - \lambda(\alpha^T \alpha -1)
  \]</li>
<li>Setting derivatives qual = 0 gives:
\[
  \frac{\partial \phi(\alpha)}{\partial\alpha} = 2\Sigma\alpha - 2\lambda\alpha = 0
  \]</li>
<li>Thus: \(\Sigma\alpha = \lambda\alpha\) &rarr;
&lambda; is an eigenvalue of &Sigma; and &alpha; is the corresponding normalized eigenvector.</li>
<li>For \(U = \alpha^T X\), we have \(\text{Var}(U) = \alpha^T \Sigma\alpha = \alpha^T (\lambda\alpha) = \lambda\).</li>
<li>For PC1, we need to find largest eigenvector of &Sigma;.</li>
<li>Proofs of other Pcs are similar.</li>
</ul>
</div>
</div>
<div id="outline-container-orge85a72b" class="outline-4">
<h4 id="orge85a72b"><span class="section-number-4">1.5.6.</span> PCs in Empirical Version</h4>
<div class="outline-text-4" id="text-1-5-6">
<p>
In many real world applications, only given dataset
\(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)
How to find PC's?
</p>
<ul class="org-ul">
<li>Key idea: estimate the unknown &Sigma; by \(\hat{\Sigma}_{p\times p}\) from the data, then find the PC's by the eigenvalues and eigenvectors of \(\hat{\Sigma}_{p \times p}\)</li>
<li><b>Empirical covariance matrix</b> \(\hat{\Sigma}_{p\times p}\) is widely used when \(p<n\)
<ul class="org-ul">
<li>Here, the \((r,s)\) entry of \(\hat{\Sigma}_{p\times p}\) is defined as:
\[
    \hat{\Sigma}_{rs} = \frac{1}{n} \sum^n_i(x_{ir}-\bar{x}_r)(x_{is}-\bar{x}_s)
    \]</li>
<li>Research tbd on how to estimate \(\Sigma\) effectively when \(p>>n\).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf922d42" class="outline-4">
<h4 id="orgf922d42"><span class="section-number-4">1.5.7.</span> Principal component regression</h4>
<div class="outline-text-4" id="text-1-5-7">
<ul class="org-ul">
<li>Original data: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>After we extract all PC's, raw data can be written as new format
\((Y_i, u_{i1}, ..., u_{ip})\) for \(i=1,2,...,n\)</li>
<li>Principal component regression: linear regression by using only first \(k\) PC's:
\[
  Y_i = \beta_0 + \beta_1 u_{i1} + ... + \beta_k u_{ik} + \epsilon_i
  \]</li>
<li>Choosing \(k\): done by <b>cross-validation</b>.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge1c0c62" class="outline-3">
<h3 id="orge1c0c62"><span class="section-number-3">1.6.</span> Partial least squares (3.2.3)</h3>
<div class="outline-text-3" id="text-1-6">
</div>
<div id="outline-container-org1ea3cb8" class="outline-4">
<h4 id="org1ea3cb8"><span class="section-number-4">1.6.1.</span> Dimension reduction</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
There are 2 kinds of dimension reduction algorithms:
</p>
<ol class="org-ol">
<li>Unsupervised dimension reduction, e.g. PCA. Criticisms: it explains \(X\) but no reason to be sure that the result also explains a response \(Y\).</li>
<li>Supervised dimension reduction, i.e. conduct reduction on \(X\) by using the extra information in \(Y\).
<ul class="org-ul">
<li>Reasonable to believe that supervised techniques will <b>do better</b></li>
<li><b>Partial least squares</b> is one such technique</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-orgfe707e3" class="outline-4">
<h4 id="orgfe707e3"><span class="section-number-4">1.6.2.</span> Partial least squares</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
Collection of techniques with 2 common properties:
</p>
<ol class="org-ol">
<li><b>Maximizes correlation between \(Y \& X\)</b>, rather than maximizing variance of \(Y\) only.</li>
<li>Can be interpreted as finding the underlying factors of \(X\) that are also underlying factors of \(Y\).</li>
</ol>
</div>
</div>
<div id="outline-container-org7d45fda" class="outline-4">
<h4 id="org7d45fda"><span class="section-number-4">1.6.3.</span> 2 versions of PLS</h4>
<div class="outline-text-4" id="text-1-6-3">
<ol class="org-ol">
<li>Simple PLS algorithm: variant of PC's but using <b>correlation</b> instead of variance</li>
<li>PLS model: identify common factors of \(X \& Y\)</li>
</ol>
</div>
</div>
<div id="outline-container-orgb8849ab" class="outline-4">
<h4 id="orgb8849ab"><span class="section-number-4">1.6.4.</span> Simple PLS algorithm</h4>
<div class="outline-text-4" id="text-1-6-4">
<ul class="org-ul">
<li>Given \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>Let \(x_i = (x_{i1}, ..., x_{ip})^T\).</li>
<li>First PLS, \(V_1 = \alpha_1 X_1 + ... + \alpha_p X_p\) is defined as finding \(\alpha = (\alpha_1, ..., \alpha_p)^T\) that maximizes <b>covariance</b>
\[
  \hat{\textbf{CoV}}(Y, V_1) = \frac{1}{n} \sum^n_i (Y_i-\bar{Y})(v_i-\bar{v}),
  \]
when
\(v_i = \alpha_1 x_{i1} + ... + \alpha_p x_{ip}\) for \(i=1,2,...n\) subject to \(\alpha^T \alpha = 1\)</li>
<li>Later PLSs are defined analogously to maximize the covariance and are <b>assumed to be uncorrelated</b> with all previous PLSs.</li>
<li><b>Solution</b>: the \(\alpha\)'s are the eigenvectors of the \(p \times p\) matrix \(X^T Y Y^T X\) when the data matrices \(X \& Y\) have column mean zero.</li>
</ul>
</div>
</div>
<div id="outline-container-orged797a5" class="outline-4">
<h4 id="orged797a5"><span class="section-number-4">1.6.5.</span> The PLS model</h4>
<div class="outline-text-4" id="text-1-6-5">
<ul class="org-ul">
<li>Data: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>Assume data have mean 0.</li>
<li>Write data matrix as \((Y_{n \times  q}, X_{n \times  p})\)</li>
<li>Goal: find \(\ell\) linear combinations from \(X \& Y\) to use as new dimensions.</li>
<li><p>
The PLS model: noniterative iterative partial least squares (NIPALS):
</p>

<p>
\[
  X_{n \times  p} = T_{n \times \ell} P_{\ell \times  p} + E,
  Y_{n \times  q} = U_{n \times  \ell} Q_{\ell \times  q} + F
  \]
where:
</p>
<ul class="org-ul">
<li>\(T_{n \times \ell}\) and \(U_{n \times \ell}\) represent \(\ell\) factors</li>
<li>\(P_{\ell \times  p}\) and \(Q_{\ell \times q}\) are loadings.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc6c3db7" class="outline-4">
<h4 id="orgc6c3db7"><span class="section-number-4">1.6.6.</span> Key idea in The PLS Model</h4>
<div class="outline-text-4" id="text-1-6-6">
<ul class="org-ul">
<li>How to estimate the &ell; factors, or the \(T_{n \times \ell}\) and \(U_{n \times \ell}\) matrices?</li>
<li>Answer:
<ul class="org-ul">
<li>Write the first column of  \(T_{n \times \ell}\) and \(U_{n \times \ell}\)  as \(t=\bf{X} r\) and \(u=\bf{Y} s\) for two unit vectors, \(\parallel r \parallel = \parallel s \parallel = 1\)</li>
<li>Find \(r\) and \(s\) that maximizes the <b>covariance-squared</b>: \(\textbf{Cov}^2(Xr, Ys)\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgfb12769" class="outline-4">
<h4 id="orgfb12769"><span class="section-number-4">1.6.7.</span> PLS for linear regression</h4>
<div class="outline-text-4" id="text-1-6-7">
<ul class="org-ul">
<li>Original data: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>After extracting all PLS's, raw data can be written as new formats \((Y_i, v_{i1}, ..., v_{ip})\) for \(i=1,2,...,n\)</li>
<li>Partial least squares regression: linear regression using only <b>first k</b> PLSs:
\[
  Y_i = \beta_0 + \beta_1 v_[i1] + ... + \beta_k v_{ik} + \epsilon_i
  \]</li>
<li>Choosing \(k\): by <b>cross-validation</b></li>
</ul>
</div>
</div>
<div id="outline-container-org401e9ae" class="outline-4">
<h4 id="org401e9ae"><span class="section-number-4">1.6.8.</span> Canonical correlation analysis (CCA)</h4>
<div class="outline-text-4" id="text-1-6-8">
<ul class="org-ul">
<li>CCA: find the unit vectors \((r, s)\) that maximizes the <b>correlation coefficient</b>
\[
  \text{Corr}(Xr, Ys) = \frac{r^T \Sigma_{XY}s}{\sqrt{r^T \Sigma_{XX}r}\sqrt{s^T \Sigma_{YY}s}}
  \]</li>
<li><p>
Solution: in the population(?) version with \((X_1, ..., X_p)\) and \((Y_1, ..., Y_q)\), consider \(r^T X = r_1 X_1 + ... + r_p X_p\) and \(s^T Y = s_1 Y_1 + ... + s_q Y_q\), the optimal \(r, s\) values are the respective eigenvectors of
</p>

<p>
\[
  \Sigma^{-1}_{XX} \Sigma^{-1}_{YY} \Sigma_{YX} \text{ and }
  \Sigma^{-1}_{YY} \Sigma^{-1}_{XX} \Sigma_{XY}
  \]
</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc5b56bc" class="outline-2">
<h2 id="orgc5b56bc"><span class="section-number-2">2.</span> Week 4: Linear Classification</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org928369e" class="outline-3">
<h3 id="org928369e"><span class="section-number-3">2.1.</span> Overview of Linear Discriminant Analysis (4.1.1)</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Supervised learning recap
</p>
<ul class="org-ul">
<li>Data: \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\)</li>
<li>Objective: predict \(Y\) for given new input \(x_{\text{new}} = (x_1, ..., x_p)\)</li>
<li>Types of tasks:
<ol class="org-ol">
<li>Regression: why response \(Y\) is continuous</li>
<li>Regression: When response $Y is binary, or discrete values representing classes</li>
</ol></li>
</ul>
</div>
<div id="outline-container-org6c9aa8f" class="outline-4">
<h4 id="org6c9aa8f"><span class="section-number-4">2.1.1.</span> Classification problem</h4>
<div class="outline-text-4" id="text-2-1-1">
<ul class="org-ul">
<li>For \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\) where \(Y_i \in {1, 2, ...K}\) is the class label</li>
<li>Objective: find a decision function to <b>discriminate</b> among data form the \(K\) different classes.
<ul class="org-ul">
<li>Learn a decision rule \(h(x) \in {1, 2, ...k}\) used to separate the \(K\) classes</li>
<li>Predict the class label for new input \(x_{\text{new}}\)</li>
</ul></li>
<li>Example: classifying patients in hospital ER into classes (low, med, high risk) by age, gender, weight, BP, insurance, etc.
<ul class="org-ul">
<li>Assign treatment priority to high-risk patients?</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5b31ffe" class="outline-4">
<h4 id="org5b31ffe"><span class="section-number-4">2.1.2.</span> Popular classification methods and R packages</h4>
<div class="outline-text-4" id="text-2-1-2">
<dl class="org-dl">
<dt>Discriminant analysis</dt><dd>?</dd>
</dl>
<p>
a- Tree-based classifiers :: <code>rpart</code>
</p>
<dl class="org-dl">
<dt>Boosting</dt><dd><code>gbm</code></dd>
<dt>Random forest</dt><dd><code>randomForest</code></dd>
<dt>Neural networks</dt><dd><code>nnet</code></dd>
<dt>svm</dt><dd><code>e1071</code></dd>
</dl>
</div>
</div>
<div id="outline-container-org22a46c7" class="outline-4">
<h4 id="org22a46c7"><span class="section-number-4">2.1.3.</span> Discriminant analysis</h4>
<div class="outline-text-4" id="text-2-1-3">
<ul class="org-ul">
<li>A multi-classifier is associated with a \(K\) - dim vector
\[
  d(x) = (d_1 (x), ..., d_K (x))
  \]
where \(d_k (x)\) represents the strength of evidence that \(x\) is in class \(k\).</li>
<li><p>
For given new input \(x_{\text{new}}\), predict the class label
</p>

<p>
\[
  \hat{k} = \text{argmax}_{k=1, 2, ..., K} d_k (x_\text{new})
  \]
</p></li>
<li>Discriminant functions \(d_k (x)\) are <b>linear</b> functions of \(x = (x_1, ..., x_p)\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgda33d6a" class="outline-4">
<h4 id="orgda33d6a"><span class="section-number-4">2.1.4.</span> Example for 3-class problem</h4>
<div class="outline-text-4" id="text-2-1-4">
<p>
<img src="./img/3-class.png" alt="3-class.png" />
Predict \(Y \in {1,2,3}\) based on \((X_1, X_2)\):
</p>
<ul class="org-ul">
<li>Class 1 if \(d_1 > \max(d_2, d_3)\)</li>
<li>Class 2 if \(d_2 > \max(d_1, d_3)\)</li>
<li><p>
Class 3 if \(d_3 > \max(d_1, d_2)\)
</p>

<p>
In this example, boundaries \({(x_1, x_2): d_i (x_1, x_2) = d_j(x_1, x_2)}\) are linear.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org4e98900" class="outline-4">
<h4 id="org4e98900"><span class="section-number-4">2.1.5.</span> Discriminant functions</h4>
<div class="outline-text-4" id="text-2-1-5">
<p>
How to construct discriminant functions \(d_k (x)\) 's?
3 approaches are available:
</p>
<ol class="org-ol">
<li>Distance-based discriminant analysis, e.g. \(d_k\) is the distance between the sample mean of the \(k\) -th class and the new input \(x\).</li>
<li>Bayes Rules</li>
<li>Probability-based rule, i.e. Logistic Regression</li>
</ol>
</div>
</div>
<div id="outline-container-orgff608e9" class="outline-4">
<h4 id="orgff608e9"><span class="section-number-4">2.1.6.</span> Multi-class vs. Binary</h4>
<div class="outline-text-4" id="text-2-1-6">
<p>
If the number \(K\) of classes is not too large, can simplify the multi-class problems into <b>series of</b> binary problems via two approaches:
</p>
<dl class="org-dl">
<dt>one vs rest</dt><dd>training binary classifiers with &part;<sub>k</sub>(X) separating class \(k\) from the rest. It's easy to implement but poor performance if no dominating class. Binary problems are unbalanced.</dd>
<dt>pairwise comparison approach</dt><dd>train \(K(K-1)/2\) binary classifiers. The final class prediction is decided by a voting scheme among all classifiers.</dd>
</dl>
</div>
</div>
</div>
<div id="outline-container-orgd66c131" class="outline-3">
<h3 id="orgd66c131"><span class="section-number-3">2.2.</span> Linear discrimination analysis continued (4.1.2)</h3>
<div class="outline-text-3" id="text-2-2">
<p>
See: <a href="#org22a46c7">2.1.3</a>.
</p>

<p>
LDA was first developed as distance-based classification (Fisher, 1936). Alternate interpretation: Bayes.
</p>
</div>
<div id="outline-container-org3064f6b" class="outline-4">
<h4 id="org3064f6b"><span class="section-number-4">2.2.1.</span> Bayes classifier</h4>
<div class="outline-text-4" id="text-2-2-1">
<ul class="org-ul">
<li>Under \(0-1\) loss, Bayes classifier is
\[
  \text{argmax}_k  (P(Y=k | x))
  = \text{argmax}_k (\pi_k f_k (x))
  = \text{argmax}_k (\log(\pi_k) + \log(f_k (x)))
  \]
where:
<dl class="org-dl">
<dt>\(\pi_k\)</dt><dd>a prior</dd>
<dt>\(f_k\)</dt><dd>the density function of the \(k\) -th class</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org676ed39" class="outline-4">
<h4 id="org676ed39"><span class="section-number-4">2.2.2.</span> Normality asumption</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
LDA is the Bayes Rule under assumption that <b>densities</b> \(f_k\) are <b>multivariate normal</b> with the <b>common covariance</b>, i.e. \(N(\mu_k, \Sigma)\).
</p>

<p>
Mathematically, LDA assumes:
\[
f_k(x) = \frac{1}{\sqrt{2\pi}|\text{det}(\Sigma)} \exp\left({-\frac{1}{2} (x-\mu_k)^T \Sigma^{-1}(x-\mu_k)}\right)
\]
for \(k=1,2,...,K\)
</p>
</div>
</div>
<div id="outline-container-orga63d38c" class="outline-4">
<h4 id="orga63d38c"><span class="section-number-4">2.2.3.</span> Bayes classifier for normal distribution</h4>
<div class="outline-text-4" id="text-2-2-3">
<ul class="org-ul">
<li><p>
If the \(f_k\) are normal with common variance, i.e. \(N(\mu_k, \Sigma)\), then the Bayes classification rule is
\[
  \text{argmax}_k \left(
  -\frac{1}{2} \text{logdet}(\Sigma) - \frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) + \log \pi_k
  \right)
  \]
</p>

<p>
\[
  = \text{argmax}_k \left(
  x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu^T_k \Sigma^{-1} \mu_k + \log \pi_k
  \right)
  \]
</p>

<p>
since the common variance term drops out.
</p></li>

<li>Leads to linear discrimination function:
\[
  d_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1} \mu_k + \log \pi_k
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgb6ef57f" class="outline-4">
<h4 id="orgb6ef57f"><span class="section-number-4">2.2.4.</span> LDA in practice</h4>
<div class="outline-text-4" id="text-2-2-4">
<ul class="org-ul">
<li>Data: for \(Y_i, x_{i1}, ..., x_{ip}\) for \(i=1,2,...,n\) where \(Y_i \in {1, 2, ...K}\) is the class label</li>
<li>LDA uses the  discrimination function:
\[
  d_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1} \mu_k + \log \pi_k
  \]</li>
<li>In practice, parameters are estimated from training data as follows:
<dl class="org-dl">
<dt>\(\hat{\pi_k}\)</dt><dd>\(\frac{n_k}{n}\)</dd>
<dt>\(\hat{\mu_k}\)</dt><dd>\(\frac{1}{n_k} \sum_{y_i=k} x_i\) where \(n_k\) is the no. of observations in class \(k\)</dd>
<dt>\(\hat{\Sigma}\)</dt><dd>\(\frac{1}{\sum^K_{k=1}(n_k -1)} \sum^K_{k=1} \sum_{y_i=k}(x_i - \hat{\mu}_k)(x_i-\hat{\mu}_k)^T\) which is the within-class sample variance</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org1081a1f" class="outline-4">
<h4 id="org1081a1f"><span class="section-number-4">2.2.5.</span> LDA for K=2 classes</h4>
<div class="outline-text-4" id="text-2-2-5">
<ul class="org-ul">
<li><p>
Classifies into class 2 if and only if
</p>
\begin{equation}
x_T \hat{\Sigma}^{-1} - \frac{1}{2} \hat{\mu}^T_2 \hat{\Sigma}^{-1} \hat{\mu}_2 + \log \hat{\pi}_2 >
x^T \Sigma^{-1}\hat{\mu}_1 - \frac{1}{2} \hat{\mu}_1^T \hat{\Sigma}^{-1} \hat{\Sigma}_1 + \log \hat{\pi}_1
\end{equation}
<p>
or
</p>
 \begin{equation}
 x^T \Sigma^{-1}(\hat{\mu}_2- \hat{\mu}_1) >
  \frac{1}{2} \hat{\mu}_2^T \hat{\Sigma}^{-1} \hat{\mu}_2 -
  \frac{1}{2}\hat{\mu}_1^T \Sigma^{-1} \hat{\mu}_1 + \log{\frac{n_1}{n}} - \log{\frac{n_2}{n}}
\end{equation}

<ul class="org-ul">
<li>let \(w=\Sigma^{-1} (\hat{\mu}_2 - \hat{\mu}_1)\), the LHS = \(x^T w = w \cdot x = w_1 x_1 + ... + w_p x_p\), is the projection of the \(p\) -th dimensional vector \(x\) to a real-valued number</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org87919a7" class="outline-4">
<h4 id="org87919a7"><span class="section-number-4">2.2.6.</span> Fisher's distance-based approach</h4>
<div class="outline-text-4" id="text-2-2-6">
<ul class="org-ul">
<li>for \(k=2\) classes:
<ul class="org-ul">
<li>projects the p-dimensional vector \(\textbf{x}\) to a real-valued number
\[
    L = \textbf{w} \cdot \textbf{x} = x^T w = w_1x_1 + ... + w_p x_p
    \]</li>
<li>Find the optimal direction \(\textbf{w}\) that best separates two classes on the projection line, using <b>training data</b></li>
<li>Assign new point \(x\) to class 2 if and only if:
\[
    \textbf{w} \cdot \textbf{x} > \textbf{w} \cdot \frac{\hat{\mu}_1 + \hat{\mu}_2}{2}
    \]</li>
<li><p>
Equivalently: assign \(x\) to class 2 if
</p>
\begin{equation}
x^T \Sigma^{-1}(\hat{\mu}_2- \hat{\mu}_1) > \frac{1}{2} \hat{\mu}_2^T \hat{\Sigma}^{-1} \hat{\mu}_2 - \frac{1}{2}\hat{\mu}_1^T \Sigma^{-1} \hat{\mu}_1
\end{equation}

<p>
which is the same as Bayes-based LDA when the classes have equal numbers of observations.
</p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8053979" class="outline-3">
<h3 id="org8053979"><span class="section-number-3">2.3.</span> Quadratic Discrimination Analysis classifier (4.1.3)</h3>
<div class="outline-text-3" id="text-2-3">
<p>
See <a href="#org22a46c7">2.1.3</a>.
How to choose suitable discriminant functions \(d_k(x)\)?
</p>
</div>
<div id="outline-container-org1a1a690" class="outline-4">
<h4 id="org1a1a690"><span class="section-number-4">2.3.1.</span> Bayes classifier</h4>
<div class="outline-text-4" id="text-2-3-1">
<ul class="org-ul">
<li>The discriminant functions \(d_k(x)\) are based on <b>posterior</b> distributions</li>
<li><p>
The Bayes classifier is defined as:
</p>
\begin{equation}
\text{argmax}_k (P(Y=k|x))
= \text{argmax}_k (\pi_k f_k (x))
= \text{argmax}_k (\log \pi_k + \log f_k (x))
\end{equation}
<p>
where:
</p>
<ul class="org-ul">
<li>\(\pi_k\) is a <b>prior</b></li>
<li>\(f_k\) is the density function of the \(k\) th class</li>
</ul>
<p>
Question: how to model the density functions \(f_k\)? <b>Normal distribution!</b>
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org35788bb" class="outline-4">
<h4 id="org35788bb"><span class="section-number-4">2.3.2.</span> Normal distribution</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
Univariate normal distribution \(N(\mu, \sigma^2)\)
</p>
<ul class="org-ul">
<li>Probability density function is given by:
\[
  p(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp \left( -\frac{1}{2\sigma^2} (x-\mu)^2\right)
  \]</li>
<li>parameter estimation of \((\mu, \sigma^2)\) from training data:
<dl class="org-dl">
<dt>sample mean, \(\hat{\mu}\)</dt><dd>\(\bar{x} = \frac{1}{n} \sum^n_{i=1}x_i\)</dd>
<dt>sample variance, \(\hat{\sigma}^2\)</dt><dd>\(\frac{1}{n-1} \sum^n_{i=1} (x_i - \bar{x})^2\)</dd>
</dl></li>
<li>properties: the components of \(X\) are independent iif \(\Sigma\) is diagonal!</li>
</ul>
</div>
</div>
<div id="outline-container-org3a67e24" class="outline-4">
<h4 id="org3a67e24"><span class="section-number-4">2.3.3.</span> Normal model for \(f_k\)</h4>
<div class="outline-text-4" id="text-2-3-3">
<ul class="org-ul">
<li>See <a href="#org22a46c7">2.1.3</a></li>
<li>Bayes classifier:
\(\text{argmax}_k (\log \pi_k + \log f_k (x))\) where \(\pi_k\) is a prior and \(f_k\) is the pdf of the \(k\) th class.</li>
<li>Model: assume pdfs \(f_k(x) = f_k(x_1, ..., x_p) \sim N(\mu_k, \Sigma_k)\)</li>
<li>Question: how to estimate \((\mu_k, \Sigma_k)\)?</li>
</ul>
</div>
</div>
<div id="outline-container-org0140fca" class="outline-4">
<h4 id="org0140fca"><span class="section-number-4">2.3.4.</span> Three approaches to estimate</h4>
<div class="outline-text-4" id="text-2-3-4">
<p>
Different assumptions:
</p>
<dl class="org-dl">
<dt>Linear discriminant analysis</dt><dd>When \(\Sigma_k \equiv \Sigma\) (common variance), estimated by within-sample covariance</dd>
<dt>Quadratic discriminant analysis</dt><dd>when \(\Sigma_k\) is estimated by the sample covariance of the \(k\) -th class</dd>
<dt>Naive Bayes</dt><dd>when each component of \(\textbf{X}\) is independent, i.e. when \(\Sigma_k \equiv \Sigma = \text{diag}(\sigma^2_{k1}, ..., \sigma^2_{kp})\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org0141512" class="outline-4">
<h4 id="org0141512"><span class="section-number-4">2.3.5.</span> QDA classifier</h4>
<div class="outline-text-4" id="text-2-3-5">
<ul class="org-ul">
<li>Assumes that \(f_k\) are normal \(N(\mu_k, \Sigma_k)\)</li>
<li>Assigns data \(x\) to the class:
\[
  \text{argmax}_k \left(
  -\frac{1}{2} \log \text{det}(\Sigma_k) -
  \frac{1}{2}(x-\mu_k)^T \Sigma^{-1}_k (x-\mu_k) + \log \pi_k
  \right)
  \]</li>
<li>In practice, estimated from training data:
<dl class="org-dl">
<dt>\(\hat{\pi}_k\)</dt><dd>\(\frac{n_k}{n}\)</dd>
<dt>\(\hat{\mu}_k\)</dt><dd>\(\frac{1}{n_k} \sum_{y_i = k} x_i\) where n<sub>k</sub> is the no. of observations in class \(k\)</dd>
<dt>\(\hat{\Sigma}_k\)</dt><dd>\(\frac{1}{n_k-1} \sum_{y_i=k} (x_i-\hat{\mu}_k)(x_i - \hat{\mu}_k)^T\) which is the sample covariance of class \(k\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-orge5c1e61" class="outline-4">
<h4 id="orge5c1e61"><span class="section-number-4">2.3.6.</span> Naive Bayes classifier</h4>
<div class="outline-text-4" id="text-2-3-6">
<ul class="org-ul">
<li>Assumes that \(\Sigma_k \equiv \Sigma = \text{diag}(\sigma^2_{k1}, ..., \sigma^2_{kp})\), or equivalently:
\[
  f_k(x) = f_k (x_1, ..., x_p) = \prod^p_{j=1} f_{kj}(x_j),
  f_{kj} \sim N(\mu_{kj}, \sigma^2_{kj})
  \]</li>
<li>Assigns \(x\) to class \(\text{argmax}_k (\pi_k \prod^p_{j=1}) f_{k,j}(x_j)\)</li>
<li>In practice, parameters \((\mu_{kj}, \sigma^2_{kj})\) estimated from the \(j\) -th component of \(X\) variables for \(k\) -th class in training data:
<dl class="org-dl">
<dt>\(\hat{\pi}\)</dt><dd>\(\frac{n_k}{n}\)</dd>
<dt>\(\hat{\mu}_{kj}\)</dt><dd>\(\frac{1}{n_k} \sum_{y_i = k} x_{ij}\)</dd>
<dt>\(\hat{\sigma^2_{kj}}\)</dt><dd>\(\frac{1}{n_k-1} \sum_{y_i = k} (x_{ij}-\bar{x}_j)^2\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-orge131716" class="outline-4">
<h4 id="orge131716"><span class="section-number-4">2.3.7.</span> General Naive Bayes classifier</h4>
<div class="outline-text-4" id="text-2-3-7">
<ul class="org-ul">
<li><b>Ignore</b> any dependence between explanatory variables and assume that \(X_j\)'s are independent.</li>
<li>Thus, corresponding classifier is
\[
  \text{argmax}_k (\pi_k \prod^p_{j=1}f_{k,j}(x_j))
  \]</li>
<li>In practice, marginal densities \(f_{k,j} (\cdot)\) are usually assumed to be parameterized by some parameters, which must be estimated, e.g.:
<ul class="org-ul">
<li>Gaussian Naive Bayes</li>
<li>Bernoulli Naive Bayes</li>
<li>Multinomial Naive Bayes</li>
<li>Poisson Naive Bayes</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4f17f65" class="outline-3">
<h3 id="org4f17f65"><span class="section-number-3">2.4.</span> Logistic Regression: Estimation (4.2.1)</h3>
<div class="outline-text-3" id="text-2-4">
</div>
<div id="outline-container-orge430750" class="outline-4">
<h4 id="orge430750"><span class="section-number-4">2.4.1.</span> Classification methods</h4>
<div class="outline-text-4" id="text-2-4-1">
<ul class="org-ul">
<li>With data \((Y_i, X_i)\) for \(i=1,...,n\), where \(Y_i\) is the class label</li>
<li>Two approaches to develop classifiers:
<ol class="org-ol">
<li>Model conditional densities \(f_k = p(\textbf{X}|Y=k)\) at the given \(k\) -th class
<ul class="org-ul">
<li><b>Normality assumption on X</b>: LDA, QDA, Naive Bayes</li>
</ul></li>
<li>Model the conditional density \(P(Y=k|\textbf{X})\) directly:
<ul class="org-ul">
<li><b>Bernoulli assumption on Y</b>: Logistic Regression</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org5959986" class="outline-4">
<h4 id="org5959986"><span class="section-number-4">2.4.2.</span> Binary logistic regression</h4>
<div class="outline-text-4" id="text-2-4-2">
<ul class="org-ul">
<li>With data: \((Y_i, x_{i1}, ..., x_{i,p-1})\) for \(i=1,...,n\), where \(Y_i \in {0,1}\) is the class label</li>
<li>Logistic regression defined as 2 components:
<ol class="org-ol">
<li>Model the response \(\textbf{Y}\) as Bernoulli distribution:
\[
     P(Y_i=1) = \pi_i \\ P(Y_i = 0) = 1-\pi_i
     \]</li>
<li>Link the model parameters to the independent \(\textbf{X}\) variables:
\[
     \log \frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 x_{i1} + ... + \beta_{p-1} x_{i,p-1}
     \]
with \(p\) = number of \(\beta\) coefficients</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orga802dfd" class="outline-4">
<h4 id="orga802dfd"><span class="section-number-4">2.4.3.</span> Conditional probability</h4>
<div class="outline-text-4" id="text-2-4-3">
<ul class="org-ul">
<li>Under logistic regression, at given \(X=(x_1, ..., x_{p-1})\), the conditional probabilities are
\[
  P(Y=1|X) = \pi = \frac{e^{\beta_0 + \beta_1 x_1 + ... + \beta_{p-1} x_{p-1}}}{1+ e^{\beta_0 + \beta_1 x_1 + ... + \beta_{p-1} x_{p-1}}} \\
  P(Y=0|X) = 1-\pi = \frac{1}{1+ e^{\beta_0 + \beta_1 x_1 + ... + \beta_{p-1} x_{p-1}}}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org3515b81" class="outline-4">
<h4 id="org3515b81"><span class="section-number-4">2.4.4.</span> Statistical inference</h4>
<div class="outline-text-4" id="text-2-4-4">
<p>
Statistical questions in logistic regression:
</p>
<ul class="org-ul">
<li>How to estimate the \(\beta\) parameters in the logistic regression model from training data?</li>
<li>How to conduct hypothesis testing or get confidence interval?</li>
<li>How to use logistic regression model for prediction?</li>
</ul>
</div>
</div>
<div id="outline-container-org2b22e03" class="outline-4">
<h4 id="org2b22e03"><span class="section-number-4">2.4.5.</span> Maximum likelihood estimation</h4>
<div class="outline-text-4" id="text-2-4-5">
<ul class="org-ul">
<li>The likelihood function of the logistic regression model is
\[
  L(\beta) = \prod^n_{i=1} \pi_i^{y_i} (1-\pi_i)^{1-y_i} \\
  = ... \\
  = \prod^n_{i=1} \frac{e^{y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i,p-1})}}{1+e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i,p-1}}}
  \]</li>
<li>MLE \(\hat{\beta}\) of the \(\beta_i\) 's can be found by maximizing \(L(\beta)\).</li>
</ul>
</div>
</div>
<div id="outline-container-org8c4af41" class="outline-4">
<h4 id="org8c4af41"><span class="section-number-4">2.4.6.</span> Asymptotic properties of MLE</h4>
<div class="outline-text-4" id="text-2-4-6">
<ul class="org-ul">
<li><p>
MLE \(\hat{\beta}\) has nice asymptotic properties:
\[
  \hat{\beta} \sim N(\beta, I^{-1}_{p\times p})
  \]
where \(I_{p\times p}\) is the observed Fisher Information Matrix defined by the negative values of the <b>2nd order derivatives</b> of the log-likelihood function (\(\log L(\beta)\)) at \(\hat{\beta}\), i.e.
</p>

<p>
\[
  I_{p\times p} = (-\frac{\partial^2\log L}{\partial \beta_i \partial \beta_j})|_{\hat{\beta}}
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org921ed4b" class="outline-4">
<h4 id="org921ed4b"><span class="section-number-4">2.4.7.</span> Other link function</h4>
<div class="outline-text-4" id="text-2-4-7">
<ul class="org-ul">
<li>With data: \((Y_i, x_{i1}, ..., x_{i,p-1})\) for \(i=1,...,n\), where \(Y_i \in {0,1}\) is the class label</li>
<li>Generalized linear model: two steps
<ol class="org-ol">
<li>\[
     P(Y_i = 1) = \pi_i, P(Y_i = 0) = 1-\pi_i
     \]</li>
<li>\[
     g(\pi_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i, p-1}
     \]
where \(g(\cdot): (0,1) \rightarrow (-\infty, infty)\) is called a <b>link function</b></li>
</ol></li>
<li>Other link functions available:
<ol class="org-ol">
<li>Normal/probit link: \(g=\phi^{-1}\) where \(\phi(t) = P(N(0,1) \le t)\) is the c.d.f. of the normal distribution</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org788f8a0" class="outline-4">
<h4 id="org788f8a0"><span class="section-number-4">2.4.8.</span> Logistic or LDA?</h4>
<div class="outline-text-4" id="text-2-4-8">
<p>
Comparison:
</p>
<ul class="org-ul">
<li>Similar: both have discriminant functions that are linear combinations of independent \(X\) variables</li>
<li>Difference: how to estimate linear coefficients
<ul class="org-ul">
<li>LDA: assume \(X|Y = k\) is <b>Gaussian</b></li>
<li>Logistic regression: <b>ignore</b> \(\textbf{P(X)}\)</li>
</ul></li>
<li>Generally, <b>logistic regression</b> is thought to be <b>safer &amp; more robust</b> than LDA
<ul class="org-ul">
<li>Often, same results</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgbbff3dd" class="outline-3">
<h3 id="orgbbff3dd"><span class="section-number-3">2.5.</span> Optimizations in Logistic Regression (4.2.2)</h3>
<div class="outline-text-3" id="text-2-5">
</div>
<div id="outline-container-org2583a1e" class="outline-4">
<h4 id="org2583a1e"><span class="section-number-4">2.5.1.</span> Logistic regression recap</h4>
<div class="outline-text-4" id="text-2-5-1">
<ul class="org-ul">
<li>With data: \((Y_i, x_{i1}, ..., x_{i,p-1})\) for \(i=1,...,n\), where \(Y_i \in {0,1}\) is the class label</li>
<li>Logistic regression model defined as 2 components
\[
  P(Y_i=1) = \pi_i \\
  P(Y_i = 0) = 1-\pi_i \\
  \log \frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 x_{i1} + ... + \beta_{p-1} x_{i,p-1}
  \]</li>
<li><b>Question</b>: how to estimate the \(\beta_i\) in the model?</li>
</ul>
</div>
</div>
<div id="outline-container-org892bee5" class="outline-4">
<h4 id="org892bee5"><span class="section-number-4">2.5.2.</span> MLE in logistic regression</h4>
<div class="outline-text-4" id="text-2-5-2">
<p>
As seen previously, MLE can be found by maximizing likelihood function:
\[
L(\beta) =
\prod^n_{i=1} \frac{e^{y_i(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i,p-1})}}{1+e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i,p-1}}}
\]
</p>
<ul class="org-ul">
<li>Challenges:
<ul class="org-ul">
<li><b>No explicit</b> solutions to MLE \(\hat{\beta}\).</li>
<li>Need to rely on numerical solutions to apply efficient optimization algorithm to find MLE \(\hat{\beta}\).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org063dce5" class="outline-4">
<h4 id="org063dce5"><span class="section-number-4">2.5.3.</span> Optimization problem</h4>
<div class="outline-text-4" id="text-2-5-3">
<p>
In statistics and machine learning, we often face optimization problem, e.g.:
\[
\hat{\theta} =
\text{argmin}_{\theta \in \Theta \subset \Re^p}
\]
Often \(h(\theta)\) is smooth and we want to solve the first derivative \(h'(\theta) = 0\).
</p>
<ul class="org-ul">
<li><b>Iterative method</b>: one widely used optimization algorithm:
<ol class="org-ol">
<li>Find sequence of \(\theta^{(i)}\) values until convergence to \(\hat{\theta}\).</li>
<li>When converged, we have \(h'(\hat{\theta}) = 0\).</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org4d3e2f4" class="outline-4">
<h4 id="org4d3e2f4"><span class="section-number-4">2.5.4.</span> Optimization algorithms</h4>
<div class="outline-text-4" id="text-2-5-4">
<p>
Two basic iterative algorithms to solve \(\hat{\theta} = \text{argmin}_\theta h(\theta)\):
</p>
<ol class="org-ol">
<li><b>Gradient descent</b>, widely used in machine learning
\[
   \theta_{\text{new}} = \theta_{\text{old}} - \lambda h'(\theta_{\text{old}})
   \]
where \(\lambda\) is the learning rate</li>
<li><b>Newton-Raphson Method</b>, very popular in statistics
\[
   \theta_{\text{new}} = \theta_{\text{old}} - [h''(\theta_{\text{old}})]^{-1} h'(\theta_{\text{old}})
   \]</li>
</ol>
</div>
</div>
<div id="outline-container-orgff31d19" class="outline-4">
<h4 id="orgff31d19"><span class="section-number-4">2.5.5.</span> Newton-Raphson Method</h4>
<div class="outline-text-4" id="text-2-5-5">
<p>
<img src="./img/n-r-method.png" alt="n-r-method.png" />
Mathematically:
</p>
<ol class="org-ol">
<li>When starting at \(\theta_{\text{old}}\) and want to update to \(\theta_{\text{new}} = \theta_{\text{old}} + \epsilon\), <b>how to choose \(\epsilon\)</b> so that \(h'(\theta_{\text{new}})=0\)?</li>
<li>Answer: Taylor series expansion of \(h(\theta)\) on \(\theta_{\text{old}}\)!</li>
</ol>
</div>
</div>
<div id="outline-container-org73b6948" class="outline-4">
<h4 id="org73b6948"><span class="section-number-4">2.5.6.</span> Taylor Expansion consideration</h4>
<div class="outline-text-4" id="text-2-5-6">
<ul class="org-ul">
<li>When \(\theta\) is 1-dim, by <b>Taylor series expansion</b>,
\[
  h'(\theta_{\text{old}} + \epsilon) \approx h'(\theta_{\text{old}}) + \epsilon h''(\theta_{\text{old}})
  \]</li>
<li>Setting to 0 yields:
\[
  \epsilon \approx - \frac{h'(\theta_{\text{old}})}{h''(\theta_{\text{old}})}
  \]</li>
<li>The idea is identical for high-dim &theta;. This leads to Newton-Raphson method:
\[
  \theta_{\text{new}} = \theta_{\text{old}} - [h''(\theta_{\text{old}})]^{-1} h'(\theta_{\text{old}})
  \]
where updates are done till \(\theta\) converges.</li>
</ul>
</div>
</div>
<div id="outline-container-orga2f7a5c" class="outline-4">
<h4 id="orga2f7a5c"><span class="section-number-4">2.5.7.</span> Newton-Raphson Method in Statistics</h4>
<div class="outline-text-4" id="text-2-5-7">
<p>
Why is this method popular in statistics?
</p>
<ul class="org-ul">
<li>When converged, Newton-Raphson Method provides 2 values:
\[
  \hat{\theta} \text{ and} \\
  \hat{V} = -[h''(\hat{\theta})]^{-1}
  \]</li>
<li>When \(h(\theta)\) is the log-likelihood function in statistics, the maximum likelihood estimator is \(\hat{\theta}\)
<ul class="org-ul">
<li>By the asymptotic theory of MLE, \(\frac{\hat{\theta}-\theta}{\sqrt{\hat{V}}} \sim N(0,1)\) and thus the 95% CI on \(\theta\) is \(\hat{\theta} \pm 1.96 \sqrt{\hat{V}}\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org0f094bc" class="outline-4">
<h4 id="org0f094bc"><span class="section-number-4">2.5.8.</span> MLE of logistic regression</h4>
<div class="outline-text-4" id="text-2-5-8">
<ul class="org-ul">
<li>In logistic regression, the log-likelihood function (in vector notation) is:
\[
  \log L(\beta) = \sum^n_{i=1} [y_i \beta^T x_i - \log(1+e^{\beta^T x_i})]
  \]</li>
<li>First order derivative
\[
  \frac{\partial \log L(\beta)}{\partial\beta} = \sum^n_{i=1} [y_i x_i - \frac{e^{\beta^T x_i}}{1+e^{\beta^T x_i}}x_i] \\
  = \sum^n_{i=1} (y_i - \pi_i)x_i
  \]</li>
<li>Second order derivatives
\[
  \frac{\partial^2 \log L(\beta)}{\partial \beta \partial\beta^T} \\
  = -\sum^n_{i=1} x_i \frac{e^{\beta^T x_i}(1+e^{\beta^T x_i})-e^{\beta^T x_i}\cdot e^{\beta^T x_i}}{(1-e^{\beta^T x_i})^2}x^T_i \\
  = -\sum^n_{i=1} x_i \pi_i (1-\pi_i)x_i^T
  \]</li>
<li><p>
Let:
</p>
\begin{equation}
\pi =
\begin{pmatrix}
\pi_1 \\
... \\
\pi_n
\end{pmatrix}
;
W =
\begin{pmatrix}
\pi_1 (1-\pi_1) & & \\
 & ... & \\
 & & \pi_n (1-\pi_n)
 \end{pmatrix}
 \end{equation}</li>
<li>We have \(h'(\beta) = X^T (Y-\pi)\) and \(h''(\beta) = -X^T WX\)</li>
<li>Applying Newton-Raphson Method to the MLE of logistic regression:
\[
  \beta_{\text{new}} = \beta_{\text{old}} - [h''(\beta_{\text{old}})]^{-1} h'\beta_{\text{old}} \\
  = \beta_{\text{old}} + (X^T WX)^{-1} X^T(Y-\pi) \\
  = (X^T WX)^{-1} X^T W [X\beta_{\text{old}} + W^{-1}(Y-\pi)] \\
  = (X^T WX)^{-1} X^T WZ
  \]
where \(Z = X\beta_{\text{old}} + W^{-1}(Y-\pi)\)</li>
<li>This is <b>weighted least squares</b>.</li>
</ul>
</div>
</div>
<div id="outline-container-org005b791" class="outline-4">
<h4 id="org005b791"><span class="section-number-4">2.5.9.</span> Algorithm for MLE in logistic regression</h4>
<div class="outline-text-4" id="text-2-5-9">
<ul class="org-ul">
<li>Initialize \(\beta_{\text{init}}=0\)</li>
<li>Given \(\beta_{\text{old}}\),
<ol class="org-ol">
<li>Compute 3 new variables:
\[
     \hat{\pi_i} = \frac{e^{\beta^T_{\text{old}}}x_i}{1+e^{\beta^T_{\text{old}}}x_i} \\
     w_i = \hat{\pi}_i (1-\hat{\pi}_i) \\
     Z_i = \beta^T_{\text{old}} x_i + \frac{Y_i - \hat{\pi}_i}{\hat{\pi}_i (1-\hat{\pi}_i)}
     \]</li>
<li>Conduct weighted least squares:
\[
     \beta_{\text{new}} \leftarrow \text{argmin}_{\beta} [(Z_X\beta)^T W(Z-X\beta)]
     \]</li>
</ol></li>
<li>Repeat the 2nd step until convergence.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9f8f89c" class="outline-3">
<h3 id="org9f8f89c"><span class="section-number-3">2.6.</span> Simplest logistic regression (4.2.3)</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>With data: \((Y_i, x_{i1}, ..., x_{i,p-1})\) for \(i=1,...,n\), where \(Y_i \in {0,1}\) is the class label</li>
<li>Logistic regression is defined as two components:
\[
  P(Y_i=1) = \pi_i \\
  P(Y_i = 0) = 1-\pi_i \\
  \log \frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 x_{i1} + ... + \beta_{p-1} x_{i,p-1}
  \]</li>
<li>Simplest logistic regression model is when \(p = 2\) and \(x_{i1}\in{0,1}\)
<ul class="org-ul">
<li>Explicit solution of MLE exists</li>
<li>Helps to better understand general theory</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org26b38bb" class="outline-4">
<h4 id="org26b38bb"><span class="section-number-4">2.6.1.</span> Data in Simplest Logistic Regression</h4>
<div class="outline-text-4" id="text-2-6-1">
<p>
<img src="./img/data-slr.png" alt="data-slr.png" />
Can also be summarised as 2x2 table, typical in biostatistics, i.e.:
</p>

<p>
\(n=a+b+c+d\)
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Y<sub>i</sub> = 1</th>
<th scope="col" class="org-left">Y<sub>i</sub>=0</th>
<th scope="col" class="org-left">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">x<sub>i</sub> = 1</td>
<td class="org-left">a</td>
<td class="org-left">b</td>
<td class="org-left">a+b</td>
</tr>

<tr>
<td class="org-left">x<sub>i</sub> = 0</td>
<td class="org-left">c</td>
<td class="org-left">d</td>
<td class="org-left">c+d</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org4d9e4bd" class="outline-4">
<h4 id="org4d9e4bd"><span class="section-number-4">2.6.2.</span> Is X and Y associated?</h4>
<div class="outline-text-4" id="text-2-6-2">
<ul class="org-ul">
<li>Example context:
<ul class="org-ul">
<li>X refers to exposure, e.g. does subject smoke?</li>
<li>Y refers to disease, e.g. does subject have lung cancer</li>
</ul></li>
<li>Based on the table, is X associated with Y?</li>
<li>Equivalent to hypothesis test:
\[
  H_0: \beta_1 = 0 \text{ vs. } H_1: \beta_1 \neq 0
  \]</li>
<li>If we can find 95% CI of \(\beta_1\), e.g.
\[
  \hat{\beta_1} \pm 1.96 \sqrt{\hat{\text{Var}}(\hat{\beta_1})}
  \]</li>
<li>Then we reject \(H_0\) if and only if 0 is <b>outside</b> the CI.</li>
</ul>
</div>
</div>
<div id="outline-container-org41ad07d" class="outline-4">
<h4 id="org41ad07d"><span class="section-number-4">2.6.3.</span> The likelihood function</h4>
<div class="outline-text-4" id="text-2-6-3">
<p>
is:
\[
L(\beta) = \prod^n_{i=1} \frac{e^{Y_i}(\beta_0+\beta_1 x_i)}{1+e^{\beta_0 + \beta_1 x_i}} \\
= \frac{e^{\beta_0(a+c) + \beta_1 a}}{(1+e^{\beta_0+\beta_1})^{a+b}(1+e^{\beta_0})^{c+d}}
\]
</p>
</div>
</div>
<div id="outline-container-orga8044bf" class="outline-4">
<h4 id="orga8044bf"><span class="section-number-4">2.6.4.</span> The MLE estimator of \(\beta_i\) 's</h4>
<div class="outline-text-4" id="text-2-6-4">
<ul class="org-ul">
<li>The log-likelihood function
\[
  \log L = (a+c) \beta_0 + \alpha \beta_1 - (c+d)\log(1+e^{\beta_0}) - (a+b) \log(1+e^{\beta_0 + \beta_1})
  \]</li>
<li>Setting derivatives to 0 yields:
\[
  \frac{\partial \log L}{\partial \beta_0}
  = (a+c) - (c+d) \frac{e^{\beta_0}}{1+e^{\beta_0}} - (a+b) \frac{e^{\beta_0 + \beta_1}}{1+ e^{\beta_0 + \beta_1}} = 0 \\
  \frac{\partial \log L}{\partial \beta_1} = a - (a+b) \frac{e^{\beta_0 + \beta_1}}{1+ e^{\beta_0+\beta_1}} = 0
  \]</li>
<li>Solving these equations simultaneously yields MLE:
\[
  \hat{\beta_0} = \log(\frac{c}{d})
  \text{ and }
  \hat{\beta_1} = \log(\frac{ad}{bc})
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org0c83542" class="outline-4">
<h4 id="org0c83542"><span class="section-number-4">2.6.5.</span> Fisher Information Matrix</h4>
<div class="outline-text-4" id="text-2-6-5">
<ul class="org-ul">
<li><p>
For \(\theta\) = (&beta;<sub>0</sub>, &beta;<sub>1</sub>)<sup>T</sup>, the 2nd order derivatives yield the Fisher Information Matrix:
</p>
\begin{equation}
\frac{\partial^2 \log L(\theta)}{\partial\theta\partial\theta^T} =
\begin{pmatrix}
-\frac{(a+b)e^{\beta_0+\beta_1}}{(1+e^{\beta_0+\beta_1})^2}-\frac{(c+d)e^{\beta_0}}{(1+e^{\beta_0})^2} &
-\frac{(a+b)e^{\beta_0+\beta_1}}{(1+e^{\beta_0+\beta_1})^2} \\
-\frac{(a+b)e^{\beta_0+\beta_1}}{(1+e^{\beta_0+\beta_1})^2} & -\frac{(a+b)e^{\beta_0+\beta_1}}{(1+e^{\beta_0+\beta_1})^2}
\end{pmatrix}
\end{equation}
\begin{equation}
I(\hat{\theta}) = -\frac{\partial^2\log L(\theta)}{\partial\theta\partial\theta^T} |_{\hat{\theta}} =

\begin{pmatrix}
\frac{ab}{a+b}+\frac{cd}{c+d} & \frac{ab}{a+b} \\
\frac{ab}{a+b} & \frac{ab}{a+b}
\end{pmatrix}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-org8176bff" class="outline-4">
<h4 id="org8176bff"><span class="section-number-4">2.6.6.</span> Variance of MLE</h4>
<div class="outline-text-4" id="text-2-6-6">
<ul class="org-ul">
<li><p>
By general theory of MLE,
</p>
\begin{equation}
\text{Cov}(\hat{\theta}) = I(\hat{\theta})^{-1} =
\begin{pmatrix}
\frac{1}{c} + \frac{1}{d} & -(\frac{1}{c} + \frac{1}{d}) \\
-(\frac{1}{c} + \frac{1}{d}) & 1/a + 1/b + 1/c + 1/d
\end{pmatrix}
\end{equation}</li>
<li><p>
Use the fact of the inverse 2&times; 2 matrix:
</p>
\begin{equation}
\begin{pmatrix}
\alpha & \gamma \\
\beta & \delta
\end{pmatrix} ^{-1}
= \frac{1}{\alpha\delta - \beta\gamma}
\begin{pmatrix}
\delta & -\gamma \\
-\beta & \alpha
\end{pmatrix}
\end{equation}</li>
</ul>
</div>
</div>
<div id="outline-container-orgd84b920" class="outline-4">
<h4 id="orgd84b920"><span class="section-number-4">2.6.7.</span> Are X and Y associated?</h4>
<div class="outline-text-4" id="text-2-6-7">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Y=1</th>
<th scope="col" class="org-left">Y=0</th>
<th scope="col" class="org-left">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">X=1</td>
<td class="org-left">a</td>
<td class="org-left">b</td>
<td class="org-left">a+b</td>
</tr>

<tr>
<td class="org-left">X=0</td>
<td class="org-left">c</td>
<td class="org-left">d</td>
<td class="org-left">c+d</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li>Thus \(\beta_1 = \log \frac{ad}{bc}\) , \(\hat{\text{Var}}(\hat{\beta_1}) = 1/a+1/b+1/c+1/d\)</li>
<li>A 95% approximate CI for \(\beta_1\) is
\[
  \log \frac{ad}{bc} \pm 1.96 \sqrt{1/a+1/b+1/c+1/d}
  \]</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org6ccbb4f"></a>MLEs<br />
<div class="outline-text-5" id="text-2-6-7-1">
<ul class="org-ul">
<li>MLE of \(\beta_0 = \ln(\frac{c}{d})\)</li>
<li>MLE of \(\beta_1 = \ln(\frac{ad}{bc})\)</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgdd90867" class="outline-2">
<h2 id="orgdd90867"><span class="section-number-2">3.</span> Week 5: Linear Classification Cont'd (Module 3)</h2>
<div class="outline-text-2" id="text-3">
<p>
Logistic Regression II
</p>
</div>
<div id="outline-container-org5064503" class="outline-3">
<h3 id="org5064503"><span class="section-number-3">3.1.</span> Example: CHD</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Example dataset: Coronary Heart Disease (CHD)
</p>
<ul class="org-ul">
<li>Response: 0 or 1 (Disease)</li>
<li>Independent: X (Age)</li>
<li>Observations: 100, 43 = 1, 57 = 0</li>
</ul>
<p>
Question: Is age associated with CHD? Can predict CHD from age?
</p>
</div>
<ol class="org-ol">
<li><a id="org074cb8f"></a>Logistic regression model<br />
<div class="outline-text-5" id="text-3-1-0-1">
<div class="org-src-container">
<pre class="src src-r">glm1 &lt;- glm(CHD ~ Age, family=binomial(link="logit"),
data=data0)
</pre>
</div>

<p>
Coefficients:
</p>
<ul class="org-ul">
<li>Intercept: -5.31</li>
<li>Age: 0.11</li>
</ul>
</div>
</li>
<li><a id="orgdeb8645"></a>Log reg equation<br />
<div class="outline-text-5" id="text-3-1-0-2">
<p>
Hence, equation is
</p>

<p>
\[
\log \frac{P(Y=1)}{1-P(Y=1)} = -5.31 + 0.11 \text{Age} \\
\text{logit}(\pi_i) = -5.31 + 0.11 \text{Age}_i
\]
</p>

<div class="org-src-container">
<pre class="src src-<language>">plot(Age, CHD)
lines(Age, fitted.values(glm1), col="red")
</pre>
</div>

<p>
The above produces a sigmoid curve:
<img src="./img/sigmoid1.png" alt="sigmoid1.png" />
</p>
</div>
</li>
</ol>
<div id="outline-container-org94e234c" class="outline-4">
<h4 id="org94e234c"><span class="section-number-4">3.1.1.</span> CI of coefficients in Log Reg</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
How to find the \(1-\alpha\) CI on \(\beta_0, \beta_1\) in R?
</p>
<pre class="example">
confint(glm1, level=0.95)
</pre>

<p>
produces 2.5% and 97.5% values of Intercept and Age coefficients.
</p>
</div>
</div>
<div id="outline-container-orga7c4573" class="outline-4">
<h4 id="orga7c4573"><span class="section-number-4">3.1.2.</span> Simplest logistic regression model</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
Define a new variable \(\text{Flag} = I(Age\ge 50)\)
</p>
<ul class="org-ul">
<li>Objective: will I(Age&ge; 50) be risk factor of CHD?</li>
<li>Model: \(\log\frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 \text{Flag}_i\)</li>
<li>Statistical question: hypothesis test:
<ol class="org-ol">
<li>\(H_0: \beta_1 = 0\)</li>
<li>\(H_1: \beta_1 \ne 0\)</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org1188510" class="outline-4">
<h4 id="org1188510"><span class="section-number-4">3.1.3.</span> Analysis in R</h4>
<div class="outline-text-4" id="text-3-1-3">
<pre class="example">
flag &lt;- I(Age&gt;=50)
glm2 &lt;- glm(CHD ~ flag, family = binomial(link="logit"), data=data0)
</pre>
<p>
Coefficients:
</p>
<ul class="org-ul">
<li>Intercept: -1.04</li>
<li>flagTRUE: 2.10</li>
</ul>

<p>
Std error:
</p>
<ul class="org-ul">
<li>Intercept: 0.28</li>
<li>flagTRUE: 0.48</li>
</ul>

<p>
hence:
\[
\log\frac{P(Y=1)}{1-P(Y=1)} = -1.04 + 2.10 I(\text{Age}\ge 50)
\]
</p>
</div>
</div>
<div id="outline-container-orga3b9124" class="outline-4">
<h4 id="orga3b9124"><span class="section-number-4">3.1.4.</span> Analysis by hand</h4>
<div class="outline-text-4" id="text-3-1-4">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">CHD+ Y=1</td>
<td class="org-left">CHD- Y=0</td>
<td class="org-right">Sum</td>
</tr>

<tr>
<td class="org-left">Age &ge; 50 X=1</td>
<td class="org-left">a=26</td>
<td class="org-left">b=9</td>
<td class="org-right">35</td>
</tr>

<tr>
<td class="org-left">Age &lt; 50 X=0</td>
<td class="org-left">c=17</td>
<td class="org-left">d=48</td>
<td class="org-right">65</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">43</td>
<td class="org-left">57</td>
<td class="org-right">100</td>
</tr>
</tbody>
</table>

<p>
When fitting \(\log \frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 \text{Flag}_i\) we have:
</p>

<dl class="org-dl">
<dt>\(\hat{\beta_0}\)</dt><dd>\(\log c/d = ln (17/48) = -1.04\)</dd>
<dt>\(\hat{\beta_1}\)</dt><dd>\(\log ad/bc = ln(26*48)/(9*17) = 2.10\)</dd>
<dt>std error \(\hat{\beta_0}\)</dt><dd>\(\sqrt{1/17 + 1/48}=0.28\)</dd>
<dt>std error \(\hat{\beta_1}\)</dt><dd>\(\sqrt{1/26 + 1/9 + 1/17 + 1/48} = 0.48\)</dd>
</dl>
</div>
</div>
<div id="outline-container-orgacacceb" class="outline-4">
<h4 id="orgacacceb"><span class="section-number-4">3.1.5.</span> Testing hypothesis</h4>
<div class="outline-text-4" id="text-3-1-5">
<p>
Statistical question: hypothesis test:
</p>
<ol class="org-ol">
<li>\(H_0: \beta_1 = 0\)</li>
<li>\(H_1: \beta_1 \ne 0\)</li>
</ol>


<ul class="org-ul">
<li>Testing statistic is given by:
\[
   Z_\text{obs} = \frac{\hat{\beta_1}-0}{se(\hat{\beta_1})} = \frac{2.09-0}{0.48} = 4.39
   \]</li>
<li>p-value is:
\[
   P(|Z| > |Z_\text{obs}|) \\
   = 2P(N(0,1) > 4.38) = 1.17e-05
   \]</li>
<li>in R:
<code>2*(1-pnorm(4.38))</code></li>
<li>We conclude that Age &ge; 50 is a risk factor of CHD</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc8270af" class="outline-3">
<h3 id="orgc8270af"><span class="section-number-3">3.2.</span> Prediction in Logistic Regression</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<div id="outline-container-org845e293" class="outline-4">
<h4 id="org845e293"><span class="section-number-4">3.2.1.</span> Logistic regression model</h4>
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i1}, ..., x_{i, p-1}),  i = 1, ..., n; Y_i \in \{0,1\}\)</li>
<li>Model:
\[
  P(Y_i = 1) = \pi_i; P(Y_i=0) = 1-\pi_i \\
  \log \frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i, p-1}
  \]</li>
<li>Computing MLE and its asymptotic \(p \times p\) covariance matrix
\[
  \hat{\beta} = (\hat{\beta_0}, ..., \hat{\beta_{p-1}}) \\
  \hat{V} = \text{Var}(\hat{\beta})
  \]</li>
<li>Question: how to predict future response for new variable <b><b>X</b></b>?
\[
  x_{\text{new}} = (1, x_1, x_2, ..., x_{p-1})^T
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org4f3d56b" class="outline-4">
<h4 id="org4f3d56b"><span class="section-number-4">3.2.2.</span> Point estimation</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
When \(x_{\text{new}} = (1, x_1, x_2, ..., x_{p-1})^T\), for the MLE \(\hat{\beta} = (\hat{\beta_0}, ..., \hat{\beta_{p-1}}) \\\),
</p>
<ul class="org-ul">
<li>the point estimate is \(\hat{\beta}^T x\)</li>
<li>since \(\log \frac{\pi}{1-\pi} = \hat{\beta}^T x\), the point estimate of the probability \(P(Y=1|x) = \pi(x)\) is
\[
  \hat{\pi}(x) = \frac{\exp(\hat{\beta}^T x)}{1+\exp(\hat{\beta}^T x)}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgfb0ab1e" class="outline-4">
<h4 id="orgfb0ab1e"><span class="section-number-4">3.2.3.</span> Point estimation in R</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
Given logistic regression object "glm.model", 2 kinds of R commands to get <b><b>different</b></b> point estimates:
</p>
<dl class="org-dl">
<dt><code>predict(glm.model, newdata)</code></dt><dd>provides the value \(\hat{\beta}^T x\)</dd>
<dt><code>predict(glm.model, newdata, type=response)</code></dt><dd>provides the estimated probability \(\hat{\pi}(x) = \frac{\exp(\hat{\beta}^T x)}{1+\exp(\hat{\beta}^T x)}\)</dd>
</dl>
</div>
</div>
<div id="outline-container-orgbece7e5" class="outline-4">
<h4 id="orgbece7e5"><span class="section-number-4">3.2.4.</span> Confidence interval for logistic regression</h4>
<div class="outline-text-4" id="text-3-2-4">
<p>
How to find CI for \(\pi(x) = P(Y=1|x)?\)
</p>

<p>
Steps:
</p>
<ol class="org-ol">
<li>Find a CI for \(\beta^T x\).
\[
   \text{Var}(\hat{\beta}^T x) = x^T \hat{V} x \\
   Z = \frac{\hat{\beta}^T x - \beta^T x}{\sqrt{x^T \hat{V} x}} \sim N(0,1)
   \]
With 95% CI, -1.96 &le; Z &le; 1.96, hence:
\[
   L = \hat{\beta}^T x - 1.96\sqrt{x^T \hat{V} x} \le \beta^T x \le \hat{\beta}^T x + 1.96\sqrt{x^T \hat{V}x} = U
   \]</li>
<li><p>
Transform CI of \(\beta^T x\) to \(\pi(x)\) by using \(\log \frac{\pi(x)}{1-\pi(x)} = \beta^T x\)
</p>

<p>
With 95% CI,
\[
   L \le \beta^T x = \log \frac{\pi(x)}{1-\pi(x)} \le U
   \]
</p>

<p>
Solve \(\pi(x)\) to yield:
</p>

<p>
\[
   \frac{e^L}{1+e^L} \le \pi(x) \le \frac{e^U}{1+e^U}
   \]
</p>

<p>
which is the desired 95% CI for the true probability \(\pi(x) = P(Y=1|x)\)
</p></li>
</ol>
</div>
</div>
<div id="outline-container-orgdce0dad" class="outline-4">
<h4 id="orgdce0dad"><span class="section-number-4">3.2.5.</span> Prediction</h4>
<div class="outline-text-4" id="text-3-2-5">
<p>
When \(x_\text{new}=(1, x_1, ..., x_{p-1})^T=\) after computing the estimated probability
\[
\hat{\pi}(x) = P(Y=1|X) = \frac{\exp(\hat{\beta}^T x)}{1+\exp(\hat{\beta}^T x)}
\]
and its confidence interval, need to ask how to predict future response Y=1, Y=0
</p>
<ul class="org-ul">
<li>Intuitively, predict \(\hat{Y}=1\) if \(\hat{\pi}(x) \ge 0.5\) and \(\hat{Y}=0\) if \(\hat{\pi}(x) \lt 0.5\)
<ul class="org-ul">
<li>Might be reasonable in some applications with balanced training data, but not for others, e.g. when predict rare events or diseases</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb28bf65" class="outline-4">
<h4 id="orgb28bf65"><span class="section-number-4">3.2.6.</span> Prediction rule</h4>
<div class="outline-text-4" id="text-3-2-6">
<ul class="org-ul">
<li>General prediction rule is to predict:
\[
  \hat{Y} = 1 \text{ if } \hat{\pi} \ge c* \\
  \hat{Y} = 0 \text{ if } \hat{\pi} \lt c*
  \]</li>
<li>Question: how to choose the threshold \(c*\)?</li>
</ul>
</div>
</div>
<div id="outline-container-org01c5491" class="outline-4">
<h4 id="org01c5491"><span class="section-number-4">3.2.7.</span> Choice of cutoff value \(c*\)</h4>
<div class="outline-text-4" id="text-3-2-7">
<p>
Possible approaches:
</p>
<ul class="org-ul">
<li>\(c* = 0.5\)</li>
<li>\(c* = \% \text{ of Y=0 in training data}\)</li>
<li>Choose based on validation data or cross-validation</li>
</ul>
</div>
</div>
<div id="outline-container-orgd9c474f" class="outline-4">
<h4 id="orgd9c474f"><span class="section-number-4">3.2.8.</span> Choice of \(c*\) from validation</h4>
<div class="outline-text-4" id="text-3-2-8">
<p>
If there's new validation data:
</p>
<ul class="org-ul">
<li>\(c*\) == optimal value that minimizes classification rate \(P(\hat{Y} \ne Y)\), widely used in machine learning</li>
<li>\(c*\) == optimal value that minimizes
\[
  w_0 P(\hat{Y} \ne Y|Y=0) + w_1 P(\hat{Y} \ne Y | Y = 1)
  \]
More useful in biostatistics when effect of misclassifying false positives is more severe</li>
<li>Both can be combined with cross-validation.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge194e19" class="outline-3">
<h3 id="orge194e19"><span class="section-number-3">3.3.</span> Model Selection in Logistic Regression</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<div id="outline-container-orga5d3a9a" class="outline-4">
<h4 id="orga5d3a9a"><span class="section-number-4">3.3.1.</span> Logistic Regression review</h4>
<div class="outline-text-4" id="text-3-3-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i1}, ..., x_{i, p-1}),  i = 1, ..., n; Y_i \in \{0,1\}\)</li>
<li>Model:
\[
  P(Y_i = 1) = \pi_i; P(Y_i=0) = 1-\pi_i \\
  \log \frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_{p-1} x_{i, p-1}
  \]</li>
<li>Question: will all \(X\) variables be useful in predicting outcome \(Y\)?
<ul class="org-ul">
<li><b><b>Model selection &amp; variable selection</b></b></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga0cbf44" class="outline-4">
<h4 id="orga0cbf44"><span class="section-number-4">3.3.2.</span> Model selection in Logistic Regression</h4>
<div class="outline-text-4" id="text-3-3-2">
<ul class="org-ul">
<li>Approaches:
<ul class="org-ul">
<li>Information criteria, e.g. AIC, BIC+ greedy search</li>
<li>Hypothesis testing, to test a specific hypothesis given a subset of coefficients are 0</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga1062d4" class="outline-4">
<h4 id="orga1062d4"><span class="section-number-4">3.3.3.</span> Information criterion</h4>
<div class="outline-text-4" id="text-3-3-3">
<ul class="org-ul">
<li>Definition of IC of a model with \(k \beta\) as:
<dl class="org-dl">
<dt>AIC</dt><dd>\(-2 \times \log(\text{likelihood}) + 2k\)</dd>
<dt>BIC</dt><dd>\(-2 \times \log(\text{likelihood}) + \log(n) k\)</dd>
</dl></li>
<li>Among all possible \(2^{p-1}\) models find a model that <b>minimizes</b> information criteria e.g. AIC</li>
<li>In R, use: <code>step()</code></li>
</ul>
</div>
</div>
<div id="outline-container-org1050e20" class="outline-4">
<h4 id="org1050e20"><span class="section-number-4">3.3.4.</span> Hypothesis testing</h4>
<div class="outline-text-4" id="text-3-3-4">
<p>
When testing \(H_0\) given some \(\beta_j\) 's, say \(k\) of them are 0
</p>
<ul class="org-ul">
<li>Log-likelihood (or deviance) test:
<ol class="org-ol">
<li>Fit model 1: Reduced model that omits the \(k\) independent variables</li>
<li>Fit model 2: Full model that includes all variables</li>
<li>Log-likelihood statistic is
\[
     LR_\text{obs} = -2 \log(\hat{L_\text{reduced}})- (-2\log(\hat{L}_\text{full}))
     \]</li>
<li>p-value is \(P(\chi^2_k \ge LR_\text{obs})\) where \(\chi_k^2\) has df = \(k\).</li>
<li>Accept \(H_0\) if p-value is large, e.g. &ge; 5%.</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org21388d5" class="outline-4">
<h4 id="org21388d5"><span class="section-number-4">3.3.5.</span> Wald's test</h4>
<div class="outline-text-4" id="text-3-3-5">
<p>
Besides the above, other methods such as Wald's test also exist.
</p>
<ul class="org-ul">
<li>Wald statistic is:
\[
  w_\text{obs} = \hat{\beta}^T_k \hat{V}_k^{-1} \hat{\beta}_k
  \]
where \(\hat{V} k\) is the covariance matrix of the \(k \beta_j\) 's</li>
<li>p-value is thus
\[
  P(\chi^2_k \ge W_\text{obs})
  \]</li>
<li>Accept \(H_0\) if p-value is large, e.g. &ge; 5%.</li>
</ul>
</div>
</div>
<div id="outline-container-org3766820" class="outline-4">
<h4 id="org3766820"><span class="section-number-4">3.3.6.</span> Comparison of 2 models</h4>
<div class="outline-text-4" id="text-3-3-6">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i1}, ..., x_{i, p-1}),  i = 1, ..., n; Y_i \in \{0,1\}\)</li>
<li>Suppose we fit 2 logistic regression models with different \(X\) variables
<ol class="org-ol">
<li>Model 1:
\[
     \log \frac{P(Y_i=1)}{1-P(Y_i=1)} = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i5} + ... + \beta_m x_{i,m}
     \]</li>
<li>Model 2:
\[
     \log \frac{P(Y_i=1)}{1-P(Y_i=1)} = \beta_0' + \beta_1' x_{i2} + \beta_2' x_{i3} + ... + \beta_k' x_{i,k}
     \]</li>
</ol></li>
<li>Question: how to compare these 2 models on validation/training data? Since model performance will depend on the choice of cutoff value \(c*\) for \(\pi(x)\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgb8a7b8d" class="outline-4">
<h4 id="orgb8a7b8d"><span class="section-number-4">3.3.7.</span> Assess performance</h4>
<div class="outline-text-4" id="text-3-3-7">
<p>
How to evaluate performance of logistic regression model without worrying about choice of cutoff value \(c*\) ?
</p>
<ul class="org-ul">
<li>Let \(c*\) vary from 0 to 1 and consider its performance against validation/training data:
<dl class="org-dl">
<dt>Specificity: true negative rate</dt><dd>1 - P(Type I error)</dd>
<dt>Sensitivity: true positive rate</dt><dd>1 - P(Type II error)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org7bf436c" class="outline-4">
<h4 id="org7bf436c"><span class="section-number-4">3.3.8.</span> Specificity vs Sensitivity</h4>
<div class="outline-text-4" id="text-3-3-8">
<p>
For a given cutoff value \(c*\), we compute:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">Truth Y=0</td>
<td class="org-left">Truth Y=1</td>
</tr>

<tr>
<td class="org-left">Pred \(\hat{Y}_i=0\)</td>
<td class="org-left">a</td>
<td class="org-left">b</td>
</tr>

<tr>
<td class="org-left">Pred \(\hat{Y}_i=1\)</td>
<td class="org-left">c</td>
<td class="org-left">d</td>
</tr>
</tbody>
</table>

<dl class="org-dl">
<dt>Specificity</dt><dd>\(\frac{a}{a+b}\)</dd>
<dt>1-Specificity</dt><dd>\(\frac{b}{a+b}\)</dd>
<dt>Sensitivity</dt><dd>\(\frac{d}{c+d}\)</dd>
</dl>

<blockquote>
<p>
where 285 of these testing observations are from class 0 and the remaining 344 testing observations are from class 1.
</p>
</blockquote>

<p>
means:
</p>
<ul class="org-ul">
<li>285 are predicted class 0</li>
<li>344 are predicted class 1</li>
</ul>
</div>
</div>

<div id="outline-container-org9b368a5" class="outline-4">
<h4 id="org9b368a5"><span class="section-number-4">3.3.9.</span> ROC curve</h4>
<div class="outline-text-4" id="text-3-3-9">
<dl class="org-dl">
<dt>Receiver operating characteristic curve</dt><dd>curve of a given classification model or algorithm when cutoff value varies from 0 to 1</dd>
<dt>X-axis</dt><dd>1-Specificity (False Positive Rate)</dd>
<dt>Y-axis</dt><dd>Sensitivity (True positive rate)</dd>
</dl>


<div id="org312c8cf" class="figure">
<p><img src="./img/auc-roc.png" alt="auc-roc.png" />
</p>
</div>

<p>
Model with <b><b>larger</b></b> Area Under Curve of ROC is better!
</p>
</div>
</div>
</div>

<div id="outline-container-org5632fa3" class="outline-3">
<h3 id="org5632fa3"><span class="section-number-3">3.4.</span> Case Study: Golf Putting I</h3>
<div class="outline-text-3" id="text-3-4">
<ul class="org-ul">
<li>5.2.1</li>
<li>Gelman and Nolan, 07</li>
<li>Putting: less than 60% successful on 5-ft putts.</li>
</ul>
</div>

<div id="outline-container-orgade18a4" class="outline-4">
<h4 id="orgade18a4"><span class="section-number-4">3.4.1.</span> Dataset</h4>
<div class="outline-text-4" id="text-3-4-1">
<ol class="org-ol">
<li>Distance</li>
<li>Tried</li>
<li>Success</li>
<li>% Made</li>
</ol>
</div>
</div>

<div id="outline-container-orgbe16cb7" class="outline-4">
<h4 id="orgbe16cb7"><span class="section-number-4">3.4.2.</span> Scatter plot</h4>
<div class="outline-text-4" id="text-3-4-2">
<ul class="org-ul">
<li>Rate (100 &times; Made) vs distance</li>
</ul>
</div>
</div>

<div id="outline-container-org812d65a" class="outline-4">
<h4 id="org812d65a"><span class="section-number-4">3.4.3.</span> Question</h4>
<div class="outline-text-4" id="text-3-4-3">
<p>
Based on training data, what's the predicted success rate from given distance \(x\)?
</p>
</div>
</div>

<div id="outline-container-orga5ef9df" class="outline-4">
<h4 id="orga5ef9df"><span class="section-number-4">3.4.4.</span> Multiple linear regression models</h4>
<div class="outline-text-4" id="text-3-4-4">
<ul class="org-ul">
<li>Linear regression model:
\[
  \text{Rate } = \beta_0 + \beta_1 \text{Distance} + \epsilon
  \]</li>

<li>Quadratic regression model:
\[
  \text{Rate } = \beta_0 + \beta_1 \text{Distance} + \beta_2 (\text{Distance})^2 + \epsilon
  \]</li>
</ul>
</div>
</div>

<div id="outline-container-org250d43e" class="outline-4">
<h4 id="org250d43e"><span class="section-number-4">3.4.5.</span> R code</h4>
<div class="outline-text-4" id="text-3-4-5">
<ul class="org-ul">
<li>Linear regression
<code>lm1 &lt;- lm(Rate ~ Distance)</code></li>
<li>Quadratic regression
<code>lm2 &lt;- lm(Rate ~ Distance + l(Distance^2))</code></li>
<li>Plotting
<code>plot(Distance, Rate)</code>
<code>abline(lm1)</code>
<code>lines(Distance, fitted(lm2), col="red")</code></li>
</ul>
</div>
</div>

<div id="outline-container-orgad6f2b5" class="outline-4">
<h4 id="orgad6f2b5"><span class="section-number-4">3.4.6.</span> Larger distance?</h4>
<div class="outline-text-4" id="text-3-4-6">
<ul class="org-ul">
<li>Fit is good for small distance</li>
<li>But bad for large distance</li>
</ul>
</div>
</div>

<div id="outline-container-org0103f4d" class="outline-4">
<h4 id="org0103f4d"><span class="section-number-4">3.4.7.</span> Problems with regression models</h4>
<div class="outline-text-4" id="text-3-4-7">
<ul class="org-ul">
<li>Linear regression:
<ul class="org-ul">
<li>2 to 3 ft change is different from 19 to 20 ft</li>
<li>Probability \(\notin(0, 1)\)</li>
</ul></li>
<li>Quadratic regression:
<ul class="org-ul">
<li>Might have larger R<sup>2</sup>, yet an increasing probability at larger distance is not likely</li>
<li>Probability \(\notin(0,1)\)</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org1255f63" class="outline-3">
<h3 id="org1255f63"><span class="section-number-3">3.5.</span> Case Study: Golf Putting II</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Previously: Linear regression and quadratic regression.
However, they yield undesired predictions.
How about logistic regression?
</p>
</div>
<div id="outline-container-org04ddda2" class="outline-4">
<h4 id="org04ddda2"><span class="section-number-4">3.5.1.</span> Logistic regression model</h4>
<div class="outline-text-4" id="text-3-5-1">
<ul class="org-ul">
<li>Data: \((\text{Success } Y_i, \text{Tried } n_i, \text{Distance } x_i = i \text{ for } i=2, 3, ..., 20)\)</li>
<li>Logistic regression model:
\[
  Y_i \sim \text{Binomial}(n_i, \pi_i) \\
  P(Y_i = y) = {n_i \choose y} (\pi_i)^y (1- \pi_i)^{n_i - y} \\
  \log \frac{\pi}{1-\pi_i} = \beta_0 + \beta_1 x_1
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org9654eaa" class="outline-4">
<h4 id="org9654eaa"><span class="section-number-4">3.5.2.</span> R code</h4>
<div class="outline-text-4" id="text-3-5-2">
<pre class="example">
Rate = 100 * Success / Tried
# first way
glm1 &lt;- glm(cbind(Success, Tried-Success)~ Distance, family=binomial)
summary(glm1)

# second way
glm1b &lt;- glm(Success/Tried ~ Distance, weights=Tried, family=binomial)
summary(glm1b)
</pre>
<p>
Coefficients:
</p>
<ul class="org-ul">
<li>Intercept = 2.23</li>
<li>Distance = -0.26</li>
</ul>
</div>
</div>
<div id="outline-container-org440fa46" class="outline-4">
<h4 id="org440fa46"><span class="section-number-4">3.5.3.</span> Fitted model:</h4>
<div class="outline-text-4" id="text-3-5-3">
<p>
\[
\log \frac{P(\text{success})}{1-P(\text{success})}
= 2.23 - 0.26 \times \text{Distance}
\]
<img src="./img/lr.png" alt="lr.png" />
</p>

<pre class="example">
pred3 &lt;- 100 * predict(glm1, xnew, type="response")
</pre>
</div>
</div>
<div id="outline-container-org2563cb0" class="outline-4">
<h4 id="org2563cb0"><span class="section-number-4">3.5.4.</span> Advantages and disadvantages of logistic regression</h4>
<div class="outline-text-4" id="text-3-5-4">
<ul class="org-ul">
<li>Advantage:
<ul class="org-ul">
<li>Fitted probability \(\in(0,1)\)</li>
<li>Fitted probability &rarr; 0 as distance &rarr; &infin;</li>
</ul></li>
<li>Disadvantage
<ul class="org-ul">
<li>As distance &rarr; 0, fitted success probability &rarr; \(\frac{e^{\hat{\beta_0}}}{1+e^{\hat{\beta_0}}} = 0.903\) , seems low</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5431ee6" class="outline-3">
<h3 id="org5431ee6"><span class="section-number-3">3.6.</span> Case Study: Golf Putting III</h3>
<div class="outline-text-3" id="text-3-6">
<p>
Domain knowledge-based regression model.
In logistic regression model, \(\pi(x) = \frac{e^{\hat{\beta_0}}}{1+e^{\hat{\beta_0}}}\).
</p>
<ul class="org-ul">
<li>What's the scientific reason for this form of \(\pi(x)\)</li>
<li>Any more meaningful form of \(\pi(x)\)? -&gt; Domain knowledge!</li>
</ul>
</div>
<div id="outline-container-org02ed7a4" class="outline-4">
<h4 id="org02ed7a4"><span class="section-number-4">3.6.1.</span> New model</h4>
<div class="outline-text-4" id="text-3-6-1">
<p>
<img src="./img/putting.png" alt="putting.png" />
Let:
</p>
<ul class="org-ul">
<li>&theta; = hitting angle</li>
<li>Ball enters only if \(|\theta| \le \theta_0\)</li>
<li>\(\theta_0 = \arcsin [(R-r)/x]\)</li>
<li>Model hitting angle \(\theta \sim N(0, \sigma^2)\)</li>
<li>Probability of successful put is:
\[
  \pi(x) = P(-\theta_0 \le \theta \le \theta_0) \\
  = P(-\frac{\theta_0}{\sigma} \le Z \le \frac{\theta_0}{\sigma}) \\
  = 2\Phi (\frac{1}{\sigma}\theta_0)-1
  \]</li>
<li>Threshold angle as given above, thus, substituting,
\[
  P(\text{success}) = \pi(x)
  = 2\Phi(\frac{1}{\sigma} \frac{4.25-1.68}{24x}) -1
  \]
<ul class="org-ul">
<li>where &sigma; is tuning parameter</li>
<li>How to estimate &sigma;?</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org9d140d6" class="outline-4">
<h4 id="org9d140d6"><span class="section-number-4">3.6.2.</span> Parameter estimation</h4>
<div class="outline-text-4" id="text-3-6-2">
<ul class="org-ul">
<li>Use Method of Least Squares: find the &sigma; that minimizes residual sum of squares
\[
  RSS = \sum^N_{i=1}(\text{Rate}_i - [2\Phi(\frac{1}{\sigma}\arcsin \frac{4.25-1.68}{24x} -1 )])^2
  \]</li>
<li>This is nonlinear least squares in statistics.</li>
<li><p>
Finding optimal &sigma; in R:
</p>
<pre class="example">
nls1 &lt;- nls(Rate ~ 100 *(2*pnorma((1/sigma0)*asin((4.25-1.68)/(24*Distance)/)*))-1, start=list(sigma0=5))
pred4 &lt;- predict(nls1, xnew)
</pre></li>
</ul>
</div>
</div>
<div id="outline-container-org78c6384" class="outline-4">
<h4 id="org78c6384"><span class="section-number-4">3.6.3.</span> Fitted model</h4>
<div class="outline-text-4" id="text-3-6-3">
<ul class="org-ul">
<li>By NLS, \(\hat{\sigma} = 0.025\) i.e. 1.5 degress, thus new fitted model is
\[
  P(\text{success}) = \pi(x)
  = 2\Phi(\frac{1}{0.025}\arcsin \frac{4.25-1.68}{24x}) -1
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org9a4f5f6" class="outline-4">
<h4 id="org9a4f5f6"><span class="section-number-4">3.6.4.</span> Concerns</h4>
<div class="outline-text-4" id="text-3-6-4">
<ul class="org-ul">
<li>Model only accounts for angle at which ball is hit</li>
<li>Does not account for short puts, balls that cover the hole and go in</li>
<li>Does not account for terrain, playing conditions, ability</li>
<li>Need more complex model?</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgf350d69" class="outline-2">
<h2 id="orgf350d69"><span class="section-number-2">4.</span> Week 6: Local Smoothers and Additive Models</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org33a9bfd" class="outline-3">
<h3 id="org33a9bfd"><span class="section-number-3">4.1.</span> Overview</h3>
<div class="outline-text-3" id="text-4-1">
<p>
6.1.1
</p>
</div>
<div id="outline-container-orgd03207f" class="outline-4">
<h4 id="orgd03207f"><span class="section-number-4">4.1.1.</span> Regression in general</h4>
<div class="outline-text-4" id="text-4-1-1">
<ul class="org-ul">
<li>Data: \((Y_i, X_{i1}, ..., X_{ip})\) for \(i=1, 2,..., n\)</li>
<li><p>
OLS:
\[
  Y = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p + \epsilon
  \]
</p>

<p>
(which might be too restrictive)
</p></li>
<li>A more general model is:
\[
  Y = f(X_1, X_2, ..., X_p) + \epsilon
  \]
for a broad class of "smooth" function \(f\).</li>
</ul>
</div>
</div>
<div id="outline-container-orgf8b8564" class="outline-4">
<h4 id="orgf8b8564"><span class="section-number-4">4.1.2.</span> Special case: \(p=1\)</h4>
<div class="outline-text-4" id="text-4-1-2">
<ul class="org-ul">
<li>Objective, predict Y at given X=x.</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i\) for some nonlinear function \(f(\cdot)\).</li>
<li>How to estimate \(f\)?</li>
</ul>
</div>
</div>
<div id="outline-container-org12f4952" class="outline-4">
<h4 id="org12f4952"><span class="section-number-4">4.1.3.</span> Method of least squares</h4>
<div class="outline-text-4" id="text-4-1-3">
<p>
By this method, we want to find function \(f\) to minimize the residual sum of squares
\[
\text{RSS } = \sum^n_{i=1} \epsilon_i^2
= \sum^n_{i=1}(Y_i-f(x_i))^2
\]
</p>
<ul class="org-ul">
<li>Find a function \(f\) that passes through all the points?</li>
<li><b><b>Interpolation</b></b></li>
</ul>
</div>
</div>
<div id="outline-container-orgd9e6d14" class="outline-4">
<h4 id="orgd9e6d14"><span class="section-number-4">4.1.4.</span> Interpolation</h4>
<div class="outline-text-4" id="text-4-1-4">
<p>
Three popular methods:
</p>
<ul class="org-ul">
<li>Piecewise constant (discontinuous)</li>
<li>Piecewise linear interpolation</li>
<li>Polynomial interpolation (very smooth)</li>
</ul>
</div>
</div>
<div id="outline-container-org6fb7579" class="outline-4">
<h4 id="org6fb7579"><span class="section-number-4">4.1.5.</span> Polynomial interpolation</h4>
<div class="outline-text-4" id="text-4-1-5">
<ul class="org-ul">
<li>Theorem: there exists a unique polynomial of degree at most \(n-1\) that interpolate the data \((Y_i, x_i), i = 1, 2, ..., n\).</li>
<li>Solution:
\[
  \hat{f}(x) = \sum^n_{i=1} Y_i h_i (x)
  \text{ where }
  h_i (x) = \prod_{k \ne i} \frac{x-x_k}{x_i-x_k}
  \]</li>
</ul>

<div id="org9a3f75f" class="figure">
<p><img src="./img/poly-inter.png" alt="poly-inter.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org1a358a9" class="outline-4">
<h4 id="org1a358a9"><span class="section-number-4">4.1.6.</span> Weakness of interpolation</h4>
<div class="outline-text-4" id="text-4-1-6">
<ul class="org-ul">
<li>Interpolation curves are easy to understand, provide good picture,</li>
<li>But generally inadequate for predict or estimation, as it's difficult to control model complexity</li>
<li>Local vs global approximation
<ul class="org-ul">
<li>Piecewise constant or linear interpolation is too local; only use information from one or two data points</li>
<li>Polynomial interpolation uses all information from all data points, but unfortunately does not work well when sample size \(n\) is large</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5536834" class="outline-4">
<h4 id="org5536834"><span class="section-number-4">4.1.7.</span> Transition to local smoothers</h4>
<div class="outline-text-4" id="text-4-1-7">
<ul class="org-ul">
<li>Data: \((Y_i, x_i), i = 1, 2, ..., n\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i\) for some nonlinear function \(f(\cdot)\)</li>
<li>Interpolation: assume that all error terms \(\epsilon_i \equiv 0\)</li>
<li>Local smoother:
<ul class="org-ul">
<li>Allows that error term \(\epsilon_i\) is non-zero (but ideally, small)</li>
<li>Assumes that \(f(\cdot)\) is locally smooth</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org59ac458" class="outline-4">
<h4 id="org59ac458"><span class="section-number-4">4.1.8.</span> Three popular local smoothers</h4>
<div class="outline-text-4" id="text-4-1-8">
<dl class="org-dl">
<dt>LOESS/LOWESS</dt><dd>locally estimated/weighted scatter plot smoothing: combine weighted least squares with k-nearest neighbourhood</dd>
<dt>Kernel smoothing</dt><dd></dd>

<dt>Spline smoothing</dt><dd></dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-org6c00712" class="outline-3">
<h3 id="org6c00712"><span class="section-number-3">4.2.</span> LOESS</h3>
<div class="outline-text-3" id="text-4-2">
<p>
6.1.2
</p>
</div>
<div id="outline-container-org74d152c" class="outline-4">
<h4 id="org74d152c"><span class="section-number-4">4.2.1.</span> Problem formulation</h4>
<div class="outline-text-4" id="text-4-2-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i), i = 1, 2, ..., n\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i\) for some nonlinear function \(f(\cdot)\)</li>
<li>Question: can we find an estimate of \(f(x)\) at given \(x\)?</li>
</ul>
</div>
</div>
<div id="outline-container-org654253f" class="outline-4">
<h4 id="org654253f"><span class="section-number-4">4.2.2.</span> LOESS algorithm</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
Locally estimated/weighted scatterplot smoothing (LOESS) algorithm:
</p>
<ul class="org-ul">
<li>k-nearest neighbour: for given \(x\), find the \(k\) training data points that are closest to \(x\)</li>
<li>weighted linear regression: use these \(k\) data points to fit a low-degree polynomial, often of degree 1 to 3, by locally weighted least squares.
<ul class="org-ul">
<li>Closer an \(x_i\) is to \(x\), the more weight it gets</li>
</ul></li>
<li>\(\hat{Y} = \hat{f}(x)\) is the predicted value of this local polynomial at \(x\).</li>
</ul>
</div>
</div>
<div id="outline-container-orga82358c" class="outline-4">
<h4 id="orga82358c"><span class="section-number-4">4.2.3.</span> Weights</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
In the LOESS algorithm, at a given \(x\), it defines the tri-cube weight
</p>

\begin{equation}
w_i =
\begin{cases}
(1-|\frac{x_i - x}{d_k}|^3)^3 & \text{if }|x_i - x| \le d_k \\
0 & \text{otherwise}
\end{cases}
\end{equation}

<p>
for each training data \(i=1, ..., n\) where \(d_k\) = the k-th smallest distance from any \(x_i\) to \(x\).
</p>
</div>
</div>
<div id="outline-container-org2e5a58d" class="outline-4">
<h4 id="org2e5a58d"><span class="section-number-4">4.2.4.</span> Weighted least squares</h4>
<div class="outline-text-4" id="text-4-2-4">
<p>
In LOESS:
</p>
<ul class="org-ul">
<li>Fits the weighted least squares algorithm
\[
  \hat{\beta} = \text{argmin}. \sum^n_{i=1} w_i (Y_i - (\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3))^2
  \]
if \(x_i\) not among \(k\) closest points to \(x\), then \(w_i=0\)</li>
<li>Predicts:
\[
  \hat{f}(x) = (\hat{\beta_0} + \hat{\beta_1} x_i + \hat{\beta_2} x_i^2 + \hat{\beta_3} x_i^3
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org1afe88f" class="outline-4">
<h4 id="org1afe88f"><span class="section-number-4">4.2.5.</span> LOESS smoothing example</h4>
<div class="outline-text-4" id="text-4-2-5">
<p>
Wang, Mei, Holte (2008)
<img src="./img/loess.png" alt="loess.png" />
</p>
</div>
</div>
<div id="outline-container-org5466d1f" class="outline-4">
<h4 id="org5466d1f"><span class="section-number-4">4.2.6.</span> Advantages and disadvantages of LOESS</h4>
<div class="outline-text-4" id="text-4-2-6">
<ul class="org-ul">
<li>Advantages
<ul class="org-ul">
<li>Non parametric, does not fit a closed-form function</li>
<li>Works best on large, densely sampled, low-dimensional data set</li>
</ul></li>
<li>Disadvantages
<ul class="org-ul">
<li>Computationally expensive</li>
<li>Needs a lot of data to get good fit</li>
<li>Does not provide convenient closed form function
<ul class="org-ul">
<li>Difficult to communicate results</li>
</ul></li>
<li>Not intended for high-dimensional data</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgf3216ed" class="outline-3">
<h3 id="orgf3216ed"><span class="section-number-3">4.3.</span> Kernel 1</h3>
<div class="outline-text-3" id="text-4-3">
<p>
6.1.3
</p>
</div>
<div id="outline-container-org12edb5c" class="outline-4">
<h4 id="org12edb5c"><span class="section-number-4">4.3.1.</span> Local Smoothers</h4>
<div class="outline-text-4" id="text-4-3-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\) for \(i=1, 2,..., n\), assume \(x \in [a, b]\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i; \textbf{E} (\epsilon_i) = 0\) for function \(f\)</li>
<li>Goal: recover the function \(f(x) = \textbf{E}(Y|X=x); a \le x \le b\)</li>
<li>Statistical issue: how to assess the quality of \(\hat{f}\) (or sometimes denoted \(\hat{f_n}\) to emphasize the sample size \(n\))</li>
</ul>
</div>
</div>
<div id="outline-container-orga41643d" class="outline-4">
<h4 id="orga41643d"><span class="section-number-4">4.3.2.</span> Convergence</h4>
<div class="outline-text-4" id="text-4-3-2">
<dl class="org-dl">
<dt>Pointwise convergence</dt><dd>sequence \(f_n\) converges pointwise to \(f\) if
\(\lim_{n \rightarrow \infty} f_n(x) = f(x)\) for each \(a \le x \le b\)</dd>
<dt>Uniform convergence</dt><dd>the sequence \(f_n\) converges uniformly to \(f\) if
\(\lim_{n \rightarrow \infty} \text{sup}_{a \le x \le b} |f_n (x) - f(x)| = 0\)</dd>
<dt>Theorem</dt><dd><p>
Let \(f_n\) be a sequence of continuous functions defined on closed interval \([a,b]\). If \(f_n\) converges uniformly to \(f\) on \([a,b]\) then
\(\lim_{n \rightarrow \infty} \int^b_a f_n (x) dx = \int^b_a f(x) dx\)
</p>

<p>
By dominated convergence theorem or monotone convergence theorem, <b><b>pointwise</b></b> convergence almost gives convergence of integrals.
</p></dd>
</dl>
</div>
</div>
<div id="outline-container-org8b2647a" class="outline-4">
<h4 id="org8b2647a"><span class="section-number-4">4.3.3.</span> Statistical properties</h4>
<div class="outline-text-4" id="text-4-3-3">
<p>
Suppose we want to approximate or estimate the function \(E(Y)= f(x)\) based on training data \((Y_i, x_i)\) for \(i=1, 2, ..., n\)
</p>
<ul class="org-ul">
<li>Inspired by pointwise convergence, first fix a value \(x\) and investigate the statistical properties \(\hat{f} (x)\)</li>
<li>In regression, for an estimate function \(\hat{f} = \hat{f} (x) = \hat{f} (x, \text{training data})\) then mean squared data <b><b>MSE</b></b> of \(\hat{f}\) at a given \(x\) is
\[
  \text{MSE}[\hat{f}(x)] = \textbf{E} (\hat{f}-f)^2 = \textbf{E}(\hat{f}(x) - f(x))^2
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org83c5e3e" class="outline-4">
<h4 id="org83c5e3e"><span class="section-number-4">4.3.4.</span> Bias-variance decomposition</h4>
<div class="outline-text-4" id="text-4-3-4">
<p>
\[
\textbf{E}(W-c)^2 = (\textbf{E}(W)-c)^2 + \text{Var}(W) \text{ for random variable } W \text{ and constant } c,\\
\text{where } \text{Var}(W) = \textbf{E}(W-\textbf{E}(W))^2
\]
</p>
<dl class="org-dl">
<dt>Bias of \(\hat{f}\) at \(x\)</dt><dd>\[
  \text{Bias}(\hat{f}) = \textbf{E}(\hat{f}(x)) - f(x)
  \]</dd>
<dt>Variance of \(\hat{f}\)</dt><dd>\[
  \text{Var}(\hat{f}) = \textbf{E}(\hat{f} (x)- \textbf{E}(\hat{f}(x)))^2
  \]</dd>
<dt>MSE of \(\hat{f}\) at \(x\)</dt><dd>can be decomposed as \[
  \text{MSE}[\hat{f}(x)] = [\text{Bias}(\hat{f})]^2 + \text{Var}(\hat{f})
  \]</dd>
</dl>
</div>
</div>
<div id="outline-container-org73bc429" class="outline-4">
<h4 id="org73bc429"><span class="section-number-4">4.3.5.</span> Kernel smoothing</h4>
<div class="outline-text-4" id="text-4-3-5">
<ul class="org-ul">
<li>Question: how to estimate the unknown function \(f\) in the model \(Y= f(x) + \epsilon\) and \(\textbf{E}(\epsilon)=0\) based on training data?</li>
<li>There are many methods or algorithms available and one of them is <b><b>kernel smoothing</b></b>.
<ul class="org-ul">
<li>Enjoys nice statistical properties (bias, variance, MSE)</li>
</ul></li>
<li>Central quantity in kernel smoothing is the <b><b>kernel</b></b></li>
</ul>
</div>
</div>
<div id="outline-container-org1a148b5" class="outline-4">
<h4 id="org1a148b5"><span class="section-number-4">4.3.6.</span> Definition of kernel</h4>
<div class="outline-text-4" id="text-4-3-6">
<dl class="org-dl">
<dt>Definition of kernel (1 dimension)</dt><dd>a kernel \(K\) is bounded, continuous \((-\infty, \infty)\) satisfying (1) \(K(u) \ge 0\) (2) \(\int^{\infty}_{-\infty} K(u) du =1\)</dd>
</dl>

<p>
In many applications, the kernel \(K\) is chosen to satisfy the additional conditions so as to simplify the theoretical analysis and computational complexity:
</p>

<p>
(3) \(\int^{\infty}_{-\infty} u K(u) du = 0\) or \((K(-u) = K(u))\)
</p>

<p>
(4) \(\int^{\infty}_{-\infty} u^2 K(u) du = \mu_2 (K) < \infty\)
</p>
</div>
</div>
<div id="outline-container-orgff06f35" class="outline-4">
<h4 id="orgff06f35"><span class="section-number-4">4.3.7.</span> Rescaled kernel</h4>
<div class="outline-text-4" id="text-4-3-7">
<p>
\[
K_h (u) = \frac{1}{h} K (\frac{u}{h})
\]
where \(h>0\) is called the bandwidth or smoothing parameter
</p>
<ul class="org-ul">
<li>If the support of K is \(\text{supp}(K) = [-1,1]\), then \(\text{supp}(K_h) = [-h, h]\)</li>
<li>It is clear that \(\int^{\infty}_{-\infty} K_h (u) du =1\) for each \(h\)</li>
<li>Example: \(K= \frac{1}{\sqrt{2\pi}} \exp (-\frac{u^2}{2})\) = pdf of \(N(0,1)\) and \(K_h\) = pdf of \(N(0, h^2)\)</li>
</ul>
</div>
</div>
<div id="outline-container-org965551c" class="outline-4">
<h4 id="org965551c"><span class="section-number-4">4.3.8.</span> Kernel functions</h4>
<div class="outline-text-4" id="text-4-3-8">
<dl class="org-dl">
<dt>Triangle</dt><dd>\begin{equation}
K(u) =
\begin{cases}(1-|u|) & |u|<1 \\
0 & \text{otherwise}
\end{cases}
\end{equation}</dd>
<dt>Epanechnikov</dt><dd>\begin{equation}
K(u) =
\begin{cases} \frac{3}{4}(1-u^2) & |u|<1 \\
0 & \text{otherwise}
\end{cases}
\end{equation}</dd>

<dt>Biweight</dt><dd>\begin{equation}
K(u) =
\begin{cases} \frac{15}{16} (1-u^2)^2 & |u|<1 \\
0 & \text{otherwise}
\end{cases}
\end{equation}</dd>
</dl>


<dl class="org-dl">
<dt>Gaussian</dt><dd>\(K(u) = \phi(u) = \frac{1}{\sqrt{2 \pi}} \exp (-\frac{u^2}{2})\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org899842a" class="outline-4">
<h4 id="org899842a"><span class="section-number-4">4.3.9.</span> Idea in kernel smoothing</h4>
<div class="outline-text-4" id="text-4-3-9">
<ul class="org-ul">
<li>Data: \((Y_i, x_i), i=1, ..., n\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i, i=1, ...,n\)</li>
<li>High level, kernel smoothing is <b><b>linear</b></b> in the sense that
\[
  \hat{f}(x) = \sum^n{i=1} W_i (x) Y_i
  \]
where weights \(W_i(x)\) are constructed from kernel \(K_h (u)\) to describe the distance between \(x_i\) and \(x\)
<ul class="org-ul">
<li>Shape of weights come from shape of kernel \(K(u)\)</li>
<li>Size of weights determined by bandwidth \(h\)</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org79af4f8" class="outline-3">
<h3 id="org79af4f8"><span class="section-number-3">4.4.</span> Kernels: Smoothing for deterministic design</h3>
<div class="outline-text-3" id="text-4-4">
<p>
6.2.1
</p>
</div>
<div id="outline-container-org7aef241" class="outline-4">
<h4 id="org7aef241"><span class="section-number-4">4.4.1.</span> Deterministic design</h4>
<div class="outline-text-4" id="text-4-4-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i), i=1, 2, ..., n\). Assume design points are equidistant in \([0,1]\) i.e. \(x_i = \frac{i-1}{n-1}\) for \(i=1,2,...,n\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i, i=1, 2, ..., n\) where \(\epsilon_i \sim N(0, \sigma^2)\)</li>
<li>Question: estimate the underlying function \(f: [0,1] \rightarrow R\)</li>
</ul>
</div>
</div>
<div id="outline-container-org584e24e" class="outline-4">
<h4 id="org584e24e"><span class="section-number-4">4.4.2.</span> Choose a fixed kernel</h4>
<div class="outline-text-4" id="text-4-4-2">
<ul class="org-ul">
<li>Definition of a kernel: a kernel \(K\) is a bounded, continuous function on \((-\infty, \infty)\) satisfying \(K(u) \ge 0\) and \(\int^{\infty}{-\infty} K(u) du = 1\)</li>
<li>Choose a fixed kernel that is also symmetric about 0, i.e. \(K(-u) = K(u)\) and \(\int^{\infty}{-\infty} u^2 K(u) du = \mu^2(K) < \infty\)</li>
<li>Rescaled kernel: \(K_h(u) = \frac{1}{h} K (\frac{u}{h})\) where \(h>0\) is the bandwidth or smoothing parameter</li>
</ul>
</div>
</div>
<div id="outline-container-orgfad60d4" class="outline-4">
<h4 id="orgfad60d4"><span class="section-number-4">4.4.3.</span> Priestley-Chao Kernel Estimate</h4>
<div class="outline-text-4" id="text-4-4-3">
<ul class="org-ul">
<li>Data:
\[
  (Y_i, x_i = \frac{i-1}{n-1}), i=1,2,...,n
  \]</li>
<li>Model:
\[
  Y_i = f(x_i) + \epsilon_i \\
  i=1, 2, ..., n \text{ where }\epsilon_i \sim N(0, \sigma^2)
  \]</li>
<li>Priestley-Chao kernel estimate of \(f\) for a deterministic design:
\[
  \hat{f}(x) = \frac{1}{n} \sum^n_{i=1} K_h (x-x_i) Y_i = \frac{1}{nh} \sum^n{i=1} K (\frac{x-x_i}{h})Y_i
  \]
Often, \(h=h_n\), depends on the sample size \(n\)
<ul class="org-ul">
<li><span style='background-color: #FFFF00;'>General form for an interval</span> \([a, b]\): \[
    \frac{b-a}{nh} \sum^n_i K(\frac{x-x_i}{h}) Y_i
    \]
where: \(a, b, x, h, n\) are provided and \(K(\cdot)\) is the kernel function. Note: \((Y_i, x_i)\) where \(Y\) is provided first.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgfba4b0e" class="outline-4">
<h4 id="orgfba4b0e"><span class="section-number-4">4.4.4.</span> Performance evaluation</h4>
<div class="outline-text-4" id="text-4-4-4">
<ul class="org-ul">
<li>Will Priestley-Chao kernel estimate \(\hat{f}(x)\) be good approximation of the true function \(f(x)\)?</li>
<li>Statistical performance:
<dl class="org-dl">
<dt>Mean square error (MSE): bias variance decomposition</dt><dd>\[
    \text{MSE}(\hat{f}) = \textbf{E}(\hat{f} - f)^2 = [\text{Bias}(\hat{f})]^2 + \text{Var}(\hat{f}); \text{ Bias } = \textbf{E}(\hat{f})-f
    \]</dd>
<dt>Asymptotic mean square error, AMSE(\hat{f})</dt><dd>\(a^2_n \textbf{E}(U^2)\) if \(\frac{\hat{f}-f}{a_n}\) converges to finite random variable \(U\) as sample size \(n \rightarrow \infty\). Often (but not always) true that AMSE is the limit of \(\text{MSE}(\hat{f}_n)\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-orgcbaf171" class="outline-4">
<h4 id="orgcbaf171"><span class="section-number-4">4.4.5.</span> Statistical properties</h4>
<div class="outline-text-4" id="text-4-4-5">
<ul class="org-ul">
<li>Theorem: suppose kernel K has compact support and is twice continuously differentiable. For Priestley-Chao (PC) kernel estimate
\[
  \hat{f}(x) = \frac{1}{n} \sum^n_{i=1} K_h (x-x_i) Y_i = \frac{1}{nh} \sum^n_{i=1} K (\frac{x-x_i}{h})Y_i
  \]
the AMSE at \(x \in [0,1]\) is
\[
  \text{AMSE} (\hat{f}) = \textbf{E}(\hat{f}-f)^2 = \frac{(\mu_2(K) f'' (x))^2}{4} h^4 + \frac{1}{nh} \sigma^2 S(K)
  \]
where \(\mu_2(K) = \int u^2 K(u) du, S(K) = \int K^2 (u) du\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgaebcbe2" class="outline-4">
<h4 id="orgaebcbe2"><span class="section-number-4">4.4.6.</span> Main ideas:</h4>
<div class="outline-text-4" id="text-4-4-6">
<ol class="org-ol">
<li>Bias-variance decomposition for MSE:
\[
    \text{MSE}(\hat{f}) = \textbf{E}(\hat{f} - f)^2 = [\text{Bias}(\hat{f})]^2 + \text{Var}(\hat{f}); \text{ Bias } = \textbf{E}(\hat{f})-f
   \]</li>
<li>Relationship between integral and summation. For discrete design \(x_i = \frac{i-1}{n-1}\) with \(i=1, 2, ..., n\) and any continuous function \(g\), we have
\[
   \int^1_0 = g(t) dt = \frac{1}{n} \sum^n_{i=1} g(x_i) + o(\frac{1}{n})
   \]</li>
<li>Taylor series expansions:
\[
   g(x-hu) \approx g(x) - g'(x) hu + \frac{1}{2} g''(x) h^2 u^2
   \]</li>
</ol>
</div>
</div>
<div id="outline-container-orgb350006" class="outline-4">
<h4 id="orgb350006"><span class="section-number-4">4.4.7.</span> Proof: bias of \(\hat{f}\)</h4>
<div class="outline-text-4" id="text-4-4-7">
<p>
\[
\hat{f} = \frac{1}{nh} \sum^n_{i=1} K (\frac{x-x_i}{h})Y_i \text{ with } \textbf{E}(Y_i) = f(x_i), \text{Var}(Y_i) = \sigma^2
\]
</p>

<p>
Bias is :
</p>

<p>
\[
\text{Bias}(\hat{f}) = \textbf{E}(\hat{f}) -f =
\frac{1}{nh} \sum^n_{i=1} K (\frac{x-x_u}{h})f(x_i)- f(x) \\
= \frac{1}{h} \int^1_0 K (\frac{x-t}{h}) f(t) dt + O(\frac{1}{n}) - f(x) \\
(\text{let }g(t) = K(\frac{x-t}{h})f(t)) \\
= \int^{x/h}_{(x-1)/h} K(u) f(x-hu) du - f(x) + O(\frac{1}{n}) \\
(\text{let }u=\frac{x-t}{h})
\]
</p>

<p>
Next, by Taylor's expansion to \(f(x-hu)\) at \(x\):
\[
f(x-hu) = f(x) - huf'(x) + \frac{1}{2} h^2 u^2 f'' (x) + o(h^2)
\]
</p>

<p>
and the compact support of K eg \([-1,1]\), we have:
\[
\text{Bias}(\hat{f}) = \int^{x/h}{(x-h)/h} K(u) f(x-hu)du - f(x) + O(1/n) \\
\approx f(x) \int^1_{-1} K(u) du - f'(x) h \int^1_{-1} uK(u) du + \frac{1}{2} h^2 f'' (x) \int^{1}_{-1} u^2 K(u) du - f(x)
= \frac{1}{2} h^2 f'' (x) \mu_2 (K)
\]
</p>

<p>
Proof of variance is similar:
\[
\text{Var}(\hat{f}) = \frac{1}{n^2 h^2} \sum^n_{i=1} K^2 (\frac{x-x_u}{h}) \text{Var} (Y_i) \\
= \frac{\sigma^2}{n^2 h^2} \sum^n_{i=1}K^2 (\frac{x-x_u}{h}) \\
\approx \frac{\sigma^2}{nh} \int K^2 (u) du = \frac{\sigma^2}{nh} S(K)
\]
</p>

<p>
Theorem follows from bias-variance decomposition:
\[
\text{MSE} (\hat{f}) = \textbf{E}(\hat{f} - f)^2 = [\text{Bias}(\hat{f})]^2 + \text{Var}(\hat{f})
\]
</p>
</div>
</div>
<div id="outline-container-org33d1a95" class="outline-4">
<h4 id="org33d1a95"><span class="section-number-4">4.4.8.</span> Optimal choice of \(h\)</h4>
<div class="outline-text-4" id="text-4-4-8">
<p>
What is the optimal choice of bandwidth or smoothing parameter \(h\)?
</p>
<ul class="org-ul">
<li>Recall: MSE(\hat{f}) is of the form Ah<sup>4</sup> + \frac{1}{nh} B</li>
<li>Taking derivative wrt h and setting to 0:
\[
  0 = \frac{\partial \text{MSE}(\hat{f})}{\partial h} = 4h^3 A - \frac{B}{nh^2}
  \]</li>
<li>Solving h:
\[
  h^5 = \frac{B}{4An}
  \]</li>
<li>Thus:
\(h_{\text{opt}} = O(n^{-1/5})\)
and
\(\text{MSE}_{\text{opt}} \sim O(n^{-4/5})\)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgd461fb3" class="outline-3">
<h3 id="orgd461fb3"><span class="section-number-3">4.5.</span> Kernel Smoothing in Stochastic Design</h3>
<div class="outline-text-3" id="text-4-5">
<p>
6.2.2
</p>
</div>
<div id="outline-container-org3bf4c93" class="outline-4">
<h4 id="org3bf4c93"><span class="section-number-4">4.5.1.</span> Stochastic design</h4>
<div class="outline-text-4" id="text-4-5-1">
<ul class="org-ul">
<li>Data: \((Y_i, X_i) \text{ for } i=1, 2, ..., n\)</li>
<li>Model: \(Y_i = f(X_i) + \epsilon_i \text{  for} i=1, 2, ..., n\) where \(X_1, ..., X_n\) have common <b><b>density</b></b> \(p(x)\) and \(\epsilon_1, ..., \epsilon_n\) are iid \(N(0, \sigma^2)\) independent of \(X_1, ..., X_n\)</li>
<li>Goal: Estimate the regression function \(f(x) = \textbf{E}(Y|X=x)\) from the data</li>
</ul>
</div>
</div>
<div id="outline-container-org06cfb40" class="outline-4">
<h4 id="org06cfb40"><span class="section-number-4">4.5.2.</span> Challenges</h4>
<div class="outline-text-4" id="text-4-5-2">
<p>
In stochastic design, the \(X_i\) values are not fixed; intuitively:
</p>
<ul class="org-ul">
<li>The estimate \(\hat{f}\) must be responsive to whatever value of \(X\) that occurs</li>
<li>Weight assigned to specific \(x\) must be random</li>
</ul>
</div>
</div>
<div id="outline-container-org90fc469" class="outline-4">
<h4 id="org90fc469"><span class="section-number-4">4.5.3.</span> Nadaraya-Watson Estimator</h4>
<div class="outline-text-4" id="text-4-5-3">
<p>
The NW kernel estimate of \(f(x)\) is given by:
\[
\hat{f}(x) = \frac{\sum^n_{i=1}K_h (x-X_i)Y_i}{\sum^n_{i=1}K_h(x-X_i)}
\]
</p>
<ul class="org-ul">
<li>Motivation: since:
\[
  f(x) = \textbf{E}(Y|X=x) =
  \frac{\int y p_{X,Y}(x,y) dy}{p_X (x)}
  \]
then estimate the numerator and denominator respectively</li>
</ul>
</div>
</div>
<div id="outline-container-org9371997" class="outline-4">
<h4 id="org9371997"><span class="section-number-4">4.5.4.</span> Alternative form</h4>
<div class="outline-text-4" id="text-4-5-4">
<p>
NW kernel estimate can <b><b>also</b></b> be written as:
\[
\hat{f}(x) = \frac{\hat{q}(x)}{\hat{p}(x)}
\]
where:
\[
\hat{q}(x) = \frac{1}{n} \sum^n_{i=1} K_h(x-X_i)Y_i \\
\hat{p}(x) = \frac{1}{n} \sum^n_{i=1} K_h(x-X_i)
\]
</p>
</div>
</div>
<div id="outline-container-org5a6ffd1" class="outline-4">
<h4 id="org5a6ffd1"><span class="section-number-4">4.5.5.</span> Consistency properties of NW</h4>
<div class="outline-text-4" id="text-4-5-5">
<p>
Theorem 1: let K be kernel satisfying \(\lim_{|u| \rightarrow \infty} uK(u) = 0\). For any \(x\) at which \(p(x)\) and \(f(x)\) are continuous, if \(h \rightarrow 0\) and \(nh \rightarrow \infty\), then:
\[
\hat{p}(x) = \frac{1}{n} K_h (x-X_i) \rightarrow^P p(x)
\]
where \(p(x)\) is the density function of \(X\) and
\[
\hat{q}(x) = \frac{1}{n} \sum^n_{i=1} K_h (x-X_i) Y_i \rightarrow^P p(x)f(x)
\]
</p>

<p>
Thus:
\[
\hat{f}(x) = \hat{q} (x) / \hat{p} (x) \rightarrow f(x) \text{ whenever }p(x) \ne 0
\]
</p>
</div>
</div>
<div id="outline-container-org917f1a5" class="outline-4">
<h4 id="org917f1a5"><span class="section-number-4">4.5.6.</span> Density estimation</h4>
<div class="outline-text-4" id="text-4-5-6">
<ul class="org-ul">
<li>Key step in stochastic design is to investigate density estimate of the \(X\) variable:
\[
  \hat{p}(x) = \frac{1}{n} \sum^n_{i=1} K_h (x-X_i) = \frac{1}{n} \sum^n_{i=1} Z_i
  \]</li>
<li>If the \(X_i\) are iid with \(p(x)\) then the \(Z_i\) are iid with mean as follows:
\[
  E(Z_i) = E(K_h (x-X_i)) \\
  = \int K_h (x-t) p(t) dt = \int K(u) P(x-hu) du \\
  = p(x) + \frac{1}{2} h^2 \mu_2 (K) p'' (x) + ...
  \]
which converges to \(p(x)\) if \(h=h_n \rightarrow 0\)</li>
</ul>
</div>
</div>
<div id="outline-container-org1d03e82" class="outline-4">
<h4 id="org1d03e82"><span class="section-number-4">4.5.7.</span> Asymptotic properties</h4>
<div class="outline-text-4" id="text-4-5-7">
<p>
As h &rarr; 0 and nh &rarr; &infin;, for the density estimate
 \[
 \hat{p}(x) = \frac{1}{n} \sum^n_{i=1} \frac{1}{h} K_h (\frac{x-X_i}{h}) = \frac{1}{n} \sum^n_{i=1} Z_i
 \]
</p>
<ul class="org-ul">
<li><b><b>Consistency</b></b> by the law of large numbers: \(\hat{p}(x) \rightarrow E(Z_i) \approx p(x)\)</li>
<li><b><b>Asymptotic normality</b></b> by Central Limit Theorem:
\[
  \sqrt{nh} (\hat{p}(x) - p(x)) \rightarrow^d N(\frac{1}{2} \mu_2 (K)p'' (x), S(K)p(x))
  \]
where \(S(K) = \int K^2 (u) du\) and \(\mu_2(K) = \int u^2 K(u) du\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgd8cfb12" class="outline-4">
<h4 id="orgd8cfb12"><span class="section-number-4">4.5.8.</span> Other key ideas</h4>
<div class="outline-text-4" id="text-4-5-8">
<p>
Two other ideas in deriving statistical properties of \(\hat{f}(x) = \frac{\hat{q}(x)}{\hat{p}(x)}\):
</p>
<ol class="org-ol">
<li>The estimate
\[
   \hat{q}(x) = \frac{1}{n} \sum^n_{i=1} K_h (x-X_i) Y_i = \frac{1}{n} \sum^n_{i=1} V_i
   \]
where the \(V_i\)'s are iid with mean \(E(V_i) \approx q(x) + \frac{1}{2} \mu_2 (K)q'' (x)h^2\).
Here \(q(x) = f(x)p(x)\) and \(q''(x) = f''(x)p(x) + 2f'(x)p'(x) + f(x) p''(x)\)</li>
<li>For \(\hat{f}(x) = \hat{q}(x) / \hat{p}(x)\):
\[
   \hat{f}(x) - f(x) \approx
   \frac{\hat{p}(x)}{p(x)}
   (\hat{f}(x) - f(x)) \\
   = \frac{1}{p(x)} \hat{q}(x) - \frac{f(x)}{p(x)} \hat{p}(x)
   \]</li>
</ol>
</div>
</div>
<div id="outline-container-org2bbb3b3" class="outline-4">
<h4 id="org2bbb3b3"><span class="section-number-4">4.5.9.</span> Asymptotic normality</h4>
<div class="outline-text-4" id="text-4-5-9">
<p>
Theorem 2: For the NW estimator
\[
\hat{f}(x) = \frac{\sum^n_{i=1}K_h (x-X_i)Y_i}{\sum^n_{i=1}K_h(x-X_i)}
\]
Under reasonable conditions, set \(h=O(n^{-1/5})\); as \(n \rightarrow \infty\) we have
\[
\sqrt{nh}(\hat{f}(x) - f(x)) \rightarrow^d N(B(x), V(x))
\]
where
\[
B(x) = \frac{1}{2} \mu_2 (K) (f''(x) + 2f'(x) \frac{p'(x)}{p(x)}) \text{ and } V(x) = \frac{\sigma^2 S(K)}{p(x)}
\]
</p>
</div>
</div>
</div>
<div id="outline-container-org61f4d77" class="outline-3">
<h3 id="org61f4d77"><span class="section-number-3">4.6.</span> Advanced topics in kernel smoothing</h3>
<div class="outline-text-3" id="text-4-6">
<p>
6.2.3
</p>
</div>
<div id="outline-container-org4875f83" class="outline-4">
<h4 id="org4875f83"><span class="section-number-4">4.6.1.</span> Estimate derivatives</h4>
<div class="outline-text-4" id="text-4-6-1">
<ul class="org-ul">
<li>Data = \((Y_i, x_i), i=1,2,...,n\)</li>
<li>Model: \(Y_i = f(x_i)+\epsilon_i, i=1,2...,n\) where \(\epsilon_i \sim N(0, \sigma^2)\). Assume \(f\) is \(d\) times continuously differentiable</li>
<li>Question: Estimate the \(j\) -th derivative \(f^{(j)}(x)\) for \(j=1,2,...,d\)</li>
</ul>
</div>
</div>
<div id="outline-container-org8c46e7c" class="outline-4">
<h4 id="org8c46e7c"><span class="section-number-4">4.6.2.</span> Deterministic design</h4>
<div class="outline-text-4" id="text-4-6-2">
<p>
Consider the deterministic design, the Priestley-Chao kernel estimator of \(f(x)\) is
\[
\hat{f}(x) = \frac{1}{n} \sum^n_{i=1} K_h (x-x_i) Y_i = \frac{1}{nh} \sum^n_{i=1} K (\frac{x-x_i}{h}) Y_i
\]
</p>

<p>
Taking derivative w.r.t. \(x\):
\[
\hat{f}'(x) = \frac{1}{nh^2} K' (\frac{x-x_i}{h}) Y_i
\]
</p>
</div>
</div>
<div id="outline-container-org5bc91ba" class="outline-4">
<h4 id="org5bc91ba"><span class="section-number-4">4.6.3.</span> Estimate derivatives</h4>
<div class="outline-text-4" id="text-4-6-3">
<p>
In general, the \(j\) -th derivatives of the Priestley-Chao kernel estimator is
\[
\hat{f}^{(j)}(x) = \frac{1}{nh^{(j+1)}} \sum^n_{i=1} K^{(j)} (\frac{x-x_i}{h}) Y_i
\]
which provides a natural estimate of the \(j\) -th derivative \(f^{(j)} (x)\).
</p>

<p>
This idea can be extended to stochastic design.
</p>

<p>
We can also investigate the bias, variance and MSE of \(\hat{f}^{(j)}(x)\)
</p>
</div>
</div>
<div id="outline-container-orgc892f4d" class="outline-4">
<h4 id="orgc892f4d"><span class="section-number-4">4.6.4.</span> Parameter selection</h4>
<div class="outline-text-4" id="text-4-6-4">
<p>
Kernel smoothing estimators often involve scaled-kernel \(K_h(u) = \frac{1}{h} K(\frac{u}{h})\)
</p>

<p>
Question: if we want kernel smoothing estimator to have the smallest MSE (or smallest prediction error), how to choose:
</p>
<ul class="org-ul">
<li>Kernel \(K(u)\) and</li>
<li>Bandwidth or smoothing parameter \(h\)?</li>
</ul>

<p>
We have shown that \(h_\text{opt} = O(n^{-1/5})\). How to choose \(K(u)\) optimally then?
</p>
</div>
</div>
<div id="outline-container-org1745e47" class="outline-4">
<h4 id="org1745e47"><span class="section-number-4">4.6.5.</span> Statistical properties</h4>
<div class="outline-text-4" id="text-4-6-5">
<p>
For the kernel estimator \(\hat{f}\) we have shown that
</p>
<dl class="org-dl">
<dt>Bias</dt><dd>\[
  \text{Bias}(\hat{f}) = \textbf{E}(\hat{f}) - f = B(K)h^2
  \]
where B(K) is proportional to \(\mu_2(K) = \int u^2 K(u)du\)</dd>
<dt>Variance</dt><dd>\[
  \text{Var}(\hat{f}) = \frac{1}{nh} V(K)
  \]
where V(K) is proportional to S(K) = \(\int K^2 (u)du\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org682afa2" class="outline-4">
<h4 id="org682afa2"><span class="section-number-4">4.6.6.</span> Mean squared error</h4>
<div class="outline-text-4" id="text-4-6-6">
<ul class="org-ul">
<li>The MSE is:
\[
  \text{MSE}(\hat{f}) = [\text{Bias}(\hat{f})]^2 + \text{Var}(\hat{f})
  = [B(K)]^2 h^4 + \frac{1}{nh} V(K)
  \]</li>
<li>Taking derivative with respect to \(h\) and setting to 0:
\[
  0 = \frac{\partial \text{MSE}(\hat{f})}{\partial h} = 4[B(K)]^2 h^3 - \frac{V(K)}{nh^2}
  \]</li>
<li>Thus:
\[
  h^5 = \frac{V(K)}{4[B(K)]^2 n}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org0438571" class="outline-4">
<h4 id="org0438571"><span class="section-number-4">4.6.7.</span> Optimal MSE</h4>
<div class="outline-text-4" id="text-4-6-7">
<ul class="org-ul">
<li>Hence,
\[
  h_{\text{opt}} = \frac{[V(K)]^{1/5}}{4^{1/5}[B(K)]^{2/5}} n^{-1/5}
  \]</li>
<li>and this yields the smallest MSE value of
\[
  \text{MSE}(\hat{f}) = [B(K)]^2 h^4 + \frac{1}{nh} V(K) \sim C n^{-4\over5}[B(K)(V(K))^2]^{2\over5}
  \]
where \(C\) is a constant that does not depend on the kernel \(K\)</li>
<li>Note that
\[
  B(K)(V(K))^2 \sim \mu^2 (K)[S(K)]^2 = [\int u^2 K(u) du][\int K^2 (u)du]^2
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orga558f1a" class="outline-4">
<h4 id="orga558f1a"><span class="section-number-4">4.6.8.</span> Optimizing over \(K\)</h4>
<div class="outline-text-4" id="text-4-6-8">
<ul class="org-ul">
<li>In order to minimize the MSE, it is necessary to choose a kernel \(K(u)\) that minimizes
\[
  [\int u^2 K(u)du][\int K^2 (u) du]^2
  \]</li>
<li>Under canonical kernels, the constrained optimization problem is:
\[
  \min \int K^2(u) du
  \]
subject to
<ol class="org-ol">
<li>\(\int K(u) du = 1\)</li>
<li>\(\int u^2 K(u) du = 1\)</li>
<li>\(K(u) = K(-u)\)</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org09a243f" class="outline-4">
<h4 id="org09a243f"><span class="section-number-4">4.6.9.</span> Lagrange Multipliers</h4>
<div class="outline-text-4" id="text-4-6-9">
<ul class="org-ul">
<li>By Lagrange multipliers on the constraints, it's sufficient to minimize:
\[
  L(K) = \int K^2 (u) du - \lambda_1 [\int K(u) du-1] - \lambda_2 [\int u^2 K(u) du -1]
  \]</li>
<li>Let \(\Delta = \Delta (u)\) denote a small variation from the minimizer of interest. Then, \(L(K+\Delta) \ge L(K)\) if K is the minimizer.</li>
<li>This implies:
<ul class="org-ul">
<li>\[
    2\int K(u) du \Delta(u) du + \int \Delta^2 (u) du - \lambda_1[\int\Delta(u)du]-\lambda_2[\int u^2 \Delta(u) du] \ge 0
    \]</li>
</ul></li>
</ul>
<p>
or:
</p>
<ul class="org-ul">
<li>\(\int[2K(u)-\lambda_1 - \lambda_2 u^2] \Delta(u) du + \int \Delta^2 (u) du \ge 0\) for any function \(\Delta\)</li>
</ul>
</div>
</div>
<div id="outline-container-orga89e9d3" class="outline-4">
<h4 id="orga89e9d3"><span class="section-number-4">4.6.10.</span> Optimal kernel</h4>
<div class="outline-text-4" id="text-4-6-10">
<ul class="org-ul">
<li>This leads to:
\[
  2K(u) - \lambda_1 - \lambda_2 u^2 = 0
  \]</li>
<li>It can be verified that the <b><b>Epanechnikov</b></b> kernel, defined by
\[
  K(u) = \frac{3}{4} (1-u^2) \text{ for } -1 \le u \le 1
  \]
satisfies the condition and constraints above, and it thus optimal</li>
<li>But the choice of kernel has little practical impact on
\([\int u^2 K(u) du][\int K^2 (u) du]^2\)</li>
<li>Optimum: \(\frac{9}{125} \approx 0.072\)  versus Gaussian kernel \(\frac{1}{4 \pi} = 0.0796\)</li>
</ul>
</div>
</div>
<div id="outline-container-org0010e7e" class="outline-4">
<h4 id="org0010e7e"><span class="section-number-4">4.6.11.</span> Remarks</h4>
<div class="outline-text-4" id="text-4-6-11">
<ul class="org-ul">
<li>In most applications, the bandwidth \(h\) contributes much more error than the choice of the kernel</li>
<li>While we have shown that \(h_{\text{opt}} = O(n^{-1/5})\), unfortunately the exact value of \(h_\text{opt}\) needs information of the unknown function \(f\) and is thus impractical in real usage</li>
<li>The bandwidth \(h\) is often chosen by <b><b>cross-validation</b></b> in practice.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-org537d7cd" class="outline-2">
<h2 id="org537d7cd"><span class="section-number-2">5.</span> Week 7: Local Smoothers and Additive Models (Cont'd)</h2>
<div class="outline-text-2" id="text-5">
<p>
Spline Smoothing and Additive Models
</p>
</div>
<div id="outline-container-org2173642" class="outline-3">
<h3 id="org2173642"><span class="section-number-3">5.1.</span> Spline Interpolation</h3>
<div class="outline-text-3" id="text-5-1">
</div>
<div id="outline-container-org56b5575" class="outline-4">
<h4 id="org56b5575"><span class="section-number-4">5.1.1.</span> Recap: Local Smoothing</h4>
<div class="outline-text-4" id="text-5-1-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\), i = 1, 2, &#x2026;, n, where \(a=x_1 < x_2 < ... < x_n = b\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i\)</li>
<li>How to estimate function \(f(x)\) at given \(x \in [a,b]\)?</li>
<li>Methods:
<ul class="org-ul">
<li>LOESS/LOWESS</li>
<li>Kernel smoothing</li>
<li>Splines: Interpolating and Smoothing</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgad3f326" class="outline-4">
<h4 id="orgad3f326"><span class="section-number-4">5.1.2.</span> Interpolating Splines</h4>
<div class="outline-text-4" id="text-5-1-2">
<ul class="org-ul">
<li>Definition:
<ul class="org-ul">
<li>A spline is a <span style='background-color: #FFFF00;'>piecewise polynomial</span> function</li>
<li>Interpolating spline is a spline \(s(x)\) satisfying \(s(x_i) = Y_i, i=1, ..., n\)</li>
</ul></li>
<li>Formally:
<ul class="org-ul">
<li><p>
Let \(a=x_1 < x_2 < ... < x_n = b\) be ordered design points, a spline of order \(d\) is of the form and where each \(s_i(x)\) is a polynomial of degree \(d\).
</p>
\begin{equation}
s(x) =
\begin{cases}
s_1(x) & \text{if }x_1 \le x \le x_2 \\
s_2(x) & \text{if }x_2 \le x \le x_3 \\
... & \\
s_{n-1}(x) & \text{if }x_{n-1} \le x \le x_n
\end{cases}
\end{equation}</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb580dd2" class="outline-4">
<h4 id="orgb580dd2"><span class="section-number-4">5.1.3.</span> Zero-order spline</h4>
<div class="outline-text-4" id="text-5-1-3">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\), \(i=1,2,...,n\) where \(x_1 < x_2< ... < x_n\)</li>
<li>Simplest interpolating spline is of zero order (d=0) and corresponds to a piecewise constant function defined by
\[
  s_i(x) = Y_i \text{  if } x_i \le x \le x_{i+1}
  \]
for \(i=1, 2, ..., n\) where \(x_{n+1} = \infty\)</li>
<li>Remark: zero-order splines are step functions and not continuous</li>
</ul>
</div>
</div>
<div id="outline-container-org900796f" class="outline-4">
<h4 id="org900796f"><span class="section-number-4">5.1.4.</span> First-order spline</h4>
<div class="outline-text-4" id="text-5-1-4">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\), \(i=1,2,...,n\) where \(x_1 < x_2< ... < x_n\)</li>
<li>First order spline (\(d=1\)) is a piecewise linear function defined by:
\[
  s_i(x) = Y_i + \frac{Y_{i+1} - Y_i}{x_{i+1}-x_i}(x-x_i)  \text{  if }x_i \le x \lt x_{i+1}
  \]</li>
<li>i.e., data points are connected by straight lines</li>
<li>Remarks:
<ul class="org-ul">
<li>Since \(s_i(x_i) = Y\) and  d  \(s_{i-1}(x_i) = Y_{i-1} + \frac{Y_i - Y_{i-1}}{x_i-x_{i-1}}(x_i - x_{i-1}) = Y_i\), we have \(s_{i-1}(x_i) = s_i(x_i)\), this <span style='background-color: #FFFF00;'>first-order splines are continuous</span></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgad6ff0d" class="outline-4">
<h4 id="orgad6ff0d"><span class="section-number-4">5.1.5.</span> $d$-order splines</h4>
<div class="outline-text-4" id="text-5-1-5">
<ul class="org-ul">
<li>Linear splines (d=1) can be quite good, but the derivative of linear splines is <b><b>not</b></b> continuous</li>
<li>If we want spline to be differentiable, should look at the next level, which is quadratic (d=2).</li>
<li>In practice, cubic splines (d=3) are widely used due to nice statistical properties for function estimation:
<ul class="org-ul">
<li>Cubic spline s(x) is twice continuously differentiable!</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2d573d3" class="outline-4">
<h4 id="org2d573d3"><span class="section-number-4">5.1.6.</span> Cubic spline interpolation</h4>
<div class="outline-text-4" id="text-5-1-6">
<p>
Satisfies the following properties:
</p>
<ul class="org-ul">
<li>\(s\) matches the training data i.e. \(s(x_i) = Y_i\) for \(i=1,...,n\)</li>
<li>\(s\) is continuous i.e. \(s_{i-1}(x_i) = s_i (x_i)\) for \(i=2,...,n-1\)</li>
<li>\(s\) is twice continuously differentiable. In particular:
<ul class="org-ul">
<li>First derivative continuity: \(s'_{i-1}(x_i) = s'_i (x_i)\) for \(i=2, ..., n-1\)</li>
<li>Second derivative continuity: \(s''_{i-1}(x_i) = s''_i (x_i)\) for \(i=2,...,n-1\)</li>
</ul></li>
<li>Each piece \(s_i(x)\) is a cubic polynomial in each interval \([x_i, x_{i+1})\) i.e.
\[
  s_i(x) = \beta_{0i} + \beta_{1i} (x-x_i) + \beta_{2i}(x-x_i)^2 + \beta_{3i}(x-x_i)^3 \text{  for }x \in [x_i, x_{i+1})
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org724776f" class="outline-4">
<h4 id="org724776f"><span class="section-number-4">5.1.7.</span> Parameters in cubic splines</h4>
<div class="outline-text-4" id="text-5-1-7">
<ul class="org-ul">
<li>Dimension of spaces of interpolating cubic splines:</li>
<li>Data: \((Y_i, x_i)\), \(i=1,2,...,n\) where \(x_1 < x_2< ... < x_n\)</li>
<li>There are \(4(n-1)\beta\) parameters in the cubic spline:
<ul class="org-ul">
<li>Each piece \(s_i(x)\) involves 4 &beta; parameters to characterise a polynomial of degree 3</li>
<li>There are \(n-1\) sub intervals</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org22717d3" class="outline-4">
<h4 id="org22717d3"><span class="section-number-4">5.1.8.</span> Constraints in cubic splines</h4>
<div class="outline-text-4" id="text-5-1-8">
<ul class="org-ul">
<li>For interpolating cubic splines, there are
\[
  n+3\times (n-2) = 4n-6 = 4(n-1) - 2
  \]
constraint equations due to
<ul class="org-ul">
<li>interpolation conditions i.e. \(s(x_i) = Y_i\) for \(i=1,...,n\)</li>
<li>The continuity of the 0th, 1st, 2nd derivatives at the \(x_i\) 's for \(i=2, 3, ..., n-1\)</li>
<li>Recall that there are \(4(n-1)\) parameters in cubic splines</li>
<li>Thus <span style='background-color: #FFFF00;'>need 2 more constraints to uniqlely define</span> an interpolating cubic spline</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5287076" class="outline-4">
<h4 id="org5287076"><span class="section-number-4">5.1.9.</span> Natural cubic splines</h4>
<div class="outline-text-4" id="text-5-1-9">
<ul class="org-ul">
<li>Since the behaviour of the spline function at its endpoints is relatively free, pinning down the behaviour of \(s''\) at \(x_i\) and \(x_n\) finishes the specification of cubic splines.</li>
<li>Two more constraints for natural cubic splines at the endpoints:
\(s''(x_i) = s''(x_n) = 0\)</li>
<li>The natural cubic spline is uniquely defined</li>
</ul>
</div>
</div>
<div id="outline-container-orgfc4c05d" class="outline-4">
<h4 id="orgfc4c05d"><span class="section-number-4">5.1.10.</span> Simulation study</h4>
<div class="outline-text-4" id="text-5-1-10">
<ul class="org-ul">
<li>Consider \(Y= f(x) + \epsilon\) where
\[
  f(x) = \frac{\sin(x)}{x}, x\in [-10, 10], \epsilon \sim N(0, 0.2^2)
  \]</li>
<li>Assume \(x_i\) are from fixed design with equally spaced design points (\(n=201\))</li>
<li>How is interpolating cubic spline?</li>
</ul>
</div>
</div>
<div id="outline-container-org8bcc2de" class="outline-4">
<h4 id="org8bcc2de"><span class="section-number-4">5.1.11.</span> Spline interpolation in R</h4>
<div class="outline-text-4" id="text-5-1-11">
<pre class="example">
x &lt;- seq(-10,10,0.1);
ytrue &lt;- sin(x)/x; ytrue[101]=1; set.seed(123);
y &lt;- ytrue+ 0.2*rnorm(length(x)); plot(x, y, ylim=c(min(y), max(y))) lines(x, ytrue, lwd=3);
## Interpolating Cubic Spline
lines(spline(x, y, method = "natural"), col = "red")
</pre>
</div>
</div>
<div id="outline-container-org9680bd7" class="outline-4">
<h4 id="org9680bd7"><span class="section-number-4">5.1.12.</span> Summary</h4>
<div class="outline-text-4" id="text-5-1-12">
<ul class="org-ul">
<li>d-order splines:
<dl class="org-dl">
<dt>d=0</dt><dd>piecewise constant</dd>
<dt>d=1</dt><dd>piecewise linear</dd>
<dt>d=2</dt><dd>cubic spline</dd>
</dl></li>
<li>2 kinds of cubic splines:
<ol class="org-ol">
<li>Interpolating</li>
<li>Smoothing</li>
</ol></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org48010f5" class="outline-3">
<h3 id="org48010f5"><span class="section-number-3">5.2.</span> Cubic spline smoothing</h3>
<div class="outline-text-3" id="text-5-2">
<p>
7.1.2
</p>
</div>
<div id="outline-container-org4b2c1b8" class="outline-4">
<h4 id="org4b2c1b8"><span class="section-number-4">5.2.1.</span> Local smoothing</h4>
<div class="outline-text-4" id="text-5-2-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\), \(i=1,2,...,n\) where \(x_1 < x_2< ... < x_n = b\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i\)</li>
<li>Question: how to estimate the function \(f(x)\) at a given \(x \in [a,b]\)</li>
<li>Cubic spline:
<ul class="org-ul">
<li>Interpolating splines i.e. \(Y_i = \hat{f} (x_i)\) or \(\epsilon_i = 0\)</li>
<li>Smoothing splines: extension to the local smoothing context</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org90a0c04" class="outline-4">
<h4 id="org90a0c04"><span class="section-number-4">5.2.2.</span> Interpolation overfits</h4>
<div class="outline-text-4" id="text-5-2-2">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\), \(i=1,2,...,n\) where \(x_1 < x_2< ... < x_n = b\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i\)</li>
<li>Interpolating cubic splines yield unacceptably high variability and overfitting</li>
<li>Approaches to overcome overfitting:
<ul class="org-ul">
<li>Restrict the class of functions in parametric models</li>
<li>Impose smoothness constraints in non-parametric models</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc925631" class="outline-4">
<h4 id="orgc925631"><span class="section-number-4">5.2.3.</span> Roughness constraint</h4>
<div class="outline-text-4" id="text-5-2-3">
<p>
Criterion to evaluate the smoothness/roughness of a function:
</p>
<ul class="org-ul">
<li>Suppose a function is twice continuously differentiable on the interval \([a,b]\).</li>
<li>Denote by \(H^2[a,b]\) the space of all such functions</li>
<li>A roughness criterion of function \(f\) can be defined by the total curvature penalty of the function \(f\), given by:
\[
  J(f) = \int^b_a (f''(x))^2 dx
  \]</li>
<li>Many other roughness criteria are also available</li>
</ul>
</div>
</div>
<div id="outline-container-org194688c" class="outline-4">
<h4 id="org194688c"><span class="section-number-4">5.2.4.</span> Cubic spline smoother</h4>
<div class="outline-text-4" id="text-5-2-4">
<ul class="org-ul">
<li>Implicitly defined as
\[
  \hat{f} = \argmin_{f\in H^2[a,b]} \left[ 1/n \sum^n_{i=1}(Y_i - f(X_i))^2 + \lambda \int^b_a (f''(t))^2 dt \right]
  \]</li>
<li>For a given smoothing parameter \(\lambda>0\), the optimizer is a cubic spline with coefficients that best fit the data</li>
<li>This is a special case of regularized or penalized risk minimization, which also includes ridge regression, LASSO, SVN, etc.</li>
</ul>
</div>
</div>
<div id="outline-container-org40ab426" class="outline-4">
<h4 id="org40ab426"><span class="section-number-4">5.2.5.</span> Impact of &lambda; parameter</h4>
<div class="outline-text-4" id="text-5-2-5">
<ul class="org-ul">
<li>The choice of \(\lambda\) is crucial in optimization problem above.</li>
<li>For smaller \(\lambda\):
<ul class="org-ul">
<li>Less smoothing</li>
<li>Fitter \(\hat{f}\) is rougher</li>
</ul></li>
<li>Too much smoothing (very large \(\lambda\)) will make the fitted curve \(\hat{f}\) too close to a straight line</li>
<li>Cross-validation can find the optimal \(\lambda\)</li>
</ul>
</div>
</div>
<div id="outline-container-org5f38dd6" class="outline-4">
<h4 id="org5f38dd6"><span class="section-number-4">5.2.6.</span> Optimal &lambda;</h4>
<div class="outline-text-4" id="text-5-2-6">
<ul class="org-ul">
<li>Given \(\lambda\), define the residual sum squares of leave-one-out CV:
\[
  CV(\lambda) = 1/n \sum^n_i (Y_i - \hat{f}_{\lambda}^{(-1)}(X_i))^2
  \]</li>
<li>Where \(\hat{f}_{\lambda}^{(-1)}(X_i)\) is the spline smoothing estimate at \(X_i\) for the given \(\lambda\) without using observation \(i\)</li>
<li>It turns out that the cubic spline smoother can be written as the form \(\hat{Y} = \hat{f}_{\lambda} = W^{(\lambda)}X\) and we define the generalized CV as
\[
  GCV(\lambda) = \left[ \frac{n}{trace(I-W^{\lambda})} \right]^2 \left[ 1/n \sum^n_i (Y-i-\hat{f}_{\lambda}(X_i))^2 \right]
  \]</li>
<li>In practice, we choose \(\lambda\) to minimize \(GCV(\lambda)\)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgb8f43b6" class="outline-3">
<h3 id="orgb8f43b6"><span class="section-number-3">5.3.</span> Optimality and algorithms in splines</h3>
<div class="outline-text-3" id="text-5-3">
<p>
7.1.3
</p>
</div>
<div id="outline-container-orgef2552f" class="outline-4">
<h4 id="orgef2552f"><span class="section-number-4">5.3.1.</span> Cubic splines</h4>
<div class="outline-text-4" id="text-5-3-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\), \(i=1,2,...,n\) where \(x_1 < x_2< ... < x_n = b\)</li>
<li>Model: \(Y_i = f(x_i) + \epsilon_i\)</li>
<li>Question: how to estimate the function \(f(x)\) at given \(x \in [a,b]\)?</li>
<li>Cubic splines:
<ul class="org-ul">
<li>Interpolating, i.e. \(Y_i = f(X_i)\) for \(i=1,...,n\)</li>
<li>Smoothing, see <a href="#org194688c">5.2.4</a></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org6900136" class="outline-4">
<h4 id="org6900136"><span class="section-number-4">5.3.2.</span> Relation of cubic splines</h4>
<div class="outline-text-4" id="text-5-3-2">
<p>
Theorem: the cubic spline smoother in <a href="#org194688c">5.2.4</a> is a cubic spline interpolation for \((X_i, Y*_i)\) for some well-defined \(Y*_i\) for \(i=1, 2, ..., n\). Here,
\[
Y*_{n\times 1} = (I_{n\times n} + \lambda K_{n \times n})^{-1} Y_{n \times 1}
\] for some symmetric matrix \(K\)
</p>
</div>
</div>
<div id="outline-container-org646bb04" class="outline-4">
<h4 id="org646bb04"><span class="section-number-4">5.3.3.</span> Optimality properties</h4>
<div class="outline-text-4" id="text-5-3-3">
<ul class="org-ul">
<li>Natural cubic spline in the interpolating context:
<ul class="org-ul">
<li>Exists, and can be uniquely characterised</li>
<li>Optimal in the sense of minimizing a roughness criterion among all interpolating, twice-differentiable functions</li>
</ul></li>
<li>Theorem: suppose \(n \ge 2, a = x_1 < x_2 < ... < x_n = b\). For any set of response \((Y_1, Y_2,..., Y_n)\) if \(s(x)\) is an interpolating natural cubic spline, then
\[
  \int^b_a (s''(t))^2 dt \le \int^b_a (z'' (t))^2 dt
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org8d814a5" class="outline-4">
<h4 id="org8d814a5"><span class="section-number-4">5.3.4.</span> Proof</h4>
<div class="outline-text-4" id="text-5-3-4">
<p>
See 7.1.3<sub>Optimality.Algorithm.Splines</sub><sub>FWv2.pdf</sub>
</p>
</div>
</div>
<div id="outline-container-org664f398" class="outline-4">
<h4 id="org664f398"><span class="section-number-4">5.3.5.</span> Algorithms for cubic splines</h4>
<div class="outline-text-4" id="text-5-3-5">
<ul class="org-ul">
<li>Develop efficient algorithms to solve penalized optimization problem in <a href="#org194688c">5.2.4</a></li>
<li>Main idea: B-spline basis decomposition:
\[
  \hat{f} (x) = \sum^{n-d}_{i=1} \beta_i \beta_{i,d}(x)
  \]
where \({B_{i,d}, i = 1, ..., n-d}\) is a B-spline basis of order \(d\) that can be defined recursively as
\(B_{i,0} (x) = I(x_i \le x < x_{i+1})\) and
\[
  B_{i,d}(x) = \frac{x-x_i}{x_{i+d} - x_i} B_{i, d-1} (x) + \frac{x_{i+d+1}-x}{x_{i+d+1}-x_{i+1}} B_{i+1, d-1}(x)
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orge6814f3" class="outline-4">
<h4 id="orge6814f3"><span class="section-number-4">5.3.6.</span> Two useful facts</h4>
<div class="outline-text-4" id="text-5-3-6">
<p>
Basis decomposition: \(\hat{f}(x) = \sum^{n-d}{i=1} \beta_i B_i(x)\) and \(\beta = (\beta_1, ..., \beta_{n-d})\)
</p>
<ol class="org-ol">
<li>Define the matrix \(U\) with the elements \(u_{ji} = B_i(x_j)\) then
\[
  \hat{f} = (\hat{f}(x_1), ..., \hat{f}(x_n))^T = U \beta
  \]</li>
<li>Define the matrix \(V\) with the elements \(V_{ij} = \int^b_a B''_i(x) B''_j (x) dx\), we have
\[
   \int^b_a(f''(t))^2 dt = \int^b_a (f''(x))^2 dx = \int^b_a(\sum^{n-d}_{i=1} \beta_i B''_i (x))(\sum^{n-d}{j=1}\beta_j B''_j(x))dx
   \\ = \sum_i \sum_j \beta_i V_{ij} \beta_j = \beta^T V \beta
   \]</li>
</ol>
</div>
</div>
<div id="outline-container-org49a902a" class="outline-4">
<h4 id="org49a902a"><span class="section-number-4">5.3.7.</span> Ridge regression approach</h4>
<div class="outline-text-4" id="text-5-3-7">
<ul class="org-ul">
<li>When \(\hat{f} (x) = \sum^{n-d}_{i=1} \beta_i B_i(x)\) the original optimization <a href="#org194688c">5.2.4</a>
can be reformulated as a ridge regression problem:
\[
  \hat{\beta} = \text{argmin}_{\beta} [|Y-U\beta|^2 + \lambda* \beta^T V \beta] \text{  with }\lambda* = n\lambda
  \]</li>
<li>Taking derivative wrt &beta; and setting to 0 yields:
\(-2U^T(Y-U\beta) + 2\lambda* V \beta = 0\)</li>
<li>Thus, the optimal solution is
\[
  \hat{\beta} = (U^T U + \lambda* V)^{-1} U^T Y
  \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org96baccd" class="outline-3">
<h3 id="org96baccd"><span class="section-number-3">5.4.</span> Additive models</h3>
<div class="outline-text-3" id="text-5-4">
<p>
7.2.1
</p>
</div>
<div id="outline-container-org989623c" class="outline-4">
<h4 id="org989623c"><span class="section-number-4">5.4.1.</span> Additive models detail</h4>
<div class="outline-text-4" id="text-5-4-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\)</li>
<li>Recall for linear regression model, we assume
\[
  Y_i = \beta_0 + \beta_1 x_{i,1} + ... + \beta_p x_{i,p} + \epsilon_i, i=1,2,...,n
  \]</li>
<li>In additive models, we assume
\[
  Y_i = \beta_0 + f_1 (x_{i,1}) + ... + f_p (x_{i,p}) + \epsilon_i, i = 1, 2, ..., n
  \]
where f<sub>i</sub> can be arbitrary functions of interest</li>
<li>How to estimate the functions \(f_1, ..., f_p\) ?</li>
</ul>
</div>
</div>
<div id="outline-container-orga860829" class="outline-4">
<h4 id="orga860829"><span class="section-number-4">5.4.2.</span> Fitting additive model</h4>
<div class="outline-text-4" id="text-5-4-2">
<ul class="org-ul">
<li>One approach is to find the functions \((f_1, ..., f_p)\) that minimizes the penalized residual sum of squares (PRSS):
\[
  PRSS = \sum^n_{i=1} (Y_i - \beta_0 - \sum^p_{j=1}f_j(x_{i,j}))^2 + \sum^p_{j=1}\lambda_j \int (f''_j(t))^2 dt
  \]</li>
<li>The minimizer is an additive cubic spline model in the sense that each \(\hat{f}_j\) is a cubic spline on the j-th independent variable</li>
</ul>
</div>
</div>
<div id="outline-container-org894cd1b" class="outline-4">
<h4 id="org894cd1b"><span class="section-number-4">5.4.3.</span> Estimation issues</h4>
<div class="outline-text-4" id="text-5-4-3">
<ul class="org-ul">
<li>In the additive model (<a href="#org989623c">5.4.1</a>)</li>
<li>The constant \(\beta_0\) is not identifiable, since we can add or subtract any constant to any function \(f_j\). The standard convention is to assume:
\[
  \sum^n_{i=1} f_j (x_{i,j}) = 0, \hat{\beta}_0. = 1/n \sum^n_{i=1}Y_i
  \]</li>
<li>When \(p=1\), this is local smoothing, e.g. cubic spline, kernel smoothing</li>
<li>How to extend from 1-dimensional to p-dimensional to estimate \(f_1, ..., f_p\) simultaneously?</li>
</ul>
</div>
</div>
<div id="outline-container-org5b61f82" class="outline-4">
<h4 id="org5b61f82"><span class="section-number-4">5.4.4.</span> Key idea</h4>
<div class="outline-text-4" id="text-5-4-4">
<ul class="org-ul">
<li>For the \(p\) dimensional additive model, we can write it as a 1-dimensional model for local smoothing:
\[
  Y_i - \beta_0 - \sum_{k \ne j} f_k (x_{i,k}) = f_j (x_{i,j}) + \epsilon_i
  \]</li>
<li>If we knew \(f_k\) for all \(k \ne j\) then we can use local smoothing methods to estimate \(f_j\) e.g. LOESS, Kernel, Spline</li>
<li>If we don't know \(f_k\), we can recursively estimate them one at a time &rarr; backfitting algorithm</li>
</ul>
</div>
</div>
<div id="outline-container-org9d8311b" class="outline-4">
<h4 id="org9d8311b"><span class="section-number-4">5.4.5.</span> Backfitting algorithm</h4>
<div class="outline-text-4" id="text-5-4-5">
<ol class="org-ol">
<li>Initialize \(\hat{\beta}_0 = 1/n \sum^n_{i=1} Y_i\) and \(\hat{f}_j = 0\) for all \(j=1, 2, ..., p\)</li>
<li>Cycle for $j=1, 2, &#x2026;, p, 1, 2, &#x2026;p, &#x2026;$
\[
   \hat{f}_j \leftarrow L_j (x_{i,j}, Y_i - \hat{\beta_0}-\sum_{k \ne j}\hat{f}_k (x_{i,k})) \\
   \hat{f}_j \leftarrow \hat{f}_j - 1/n \sum^n_{i=1} \hat{f}_j (x_{ij})
   \]
where \(L_j\) denotes a local smoother (e.g. kernel, spline).
Iterate until the functions \(\hat{f}_j\) change less than a pre-specified threshold.</li>
</ol>
</div>
</div>
<div id="outline-container-org2a590e2" class="outline-4">
<h4 id="org2a590e2"><span class="section-number-4">5.4.6.</span> Example for backfitting</h4>
<div class="outline-text-4" id="text-5-4-6">
<p>
Consider the following:
\[
2x_1 + x_2 = 1 \\
x_1 + 2x_2 = 2
\]
</p>
<ul class="org-ul">
<li>Linear algebra solution: x<sub>1</sub> = 0, x<sub>2</sub> = 1</li>
<li>How to solve via backfitting?
\[
  x_1 = \frac{1-x_2}{2}, x_2 = \frac{2-x_1}{2}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org42a9ed3" class="outline-4">
<h4 id="org42a9ed3"><span class="section-number-4">5.4.7.</span> Illustrative example</h4>
<div class="outline-text-4" id="text-5-4-7">
<p>
Applying backfitting algorithm
</p>
</div>
<ol class="org-ol">
<li><a id="org6a26bed"></a>Initialization<br />
<div class="outline-text-5" id="text-5-4-7-1">
<p>
\(x_1 = 0, x_2=0\)
</p>
</div>
</li>
<li><a id="orgc915b7a"></a>Round 1<br />
<div class="outline-text-5" id="text-5-4-7-2">
<p>
\[
x_1 = \frac{1-0}{2} = 0.5, x_2 = \frac{2-0}{2}=1=
\]
</p>
</div>
</li>
<li><a id="orged020da"></a>Round 2<br />
<div class="outline-text-5" id="text-5-4-7-3">
<p>
\[
x_1 = \frac{1-1}{2} = 0, x_2 = \frac{2-0.5}{2} = 0.75
\]
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org3962795" class="outline-4">
<h4 id="org3962795"><span class="section-number-4">5.4.8.</span> R code for backfitting</h4>
</div>
<div id="outline-container-orgab53dbc" class="outline-4">
<h4 id="orgab53dbc"><span class="section-number-4">5.4.9.</span> R output</h4>
</div>
</div>
<div id="outline-container-org0a1062b" class="outline-3">
<h3 id="org0a1062b"><span class="section-number-3">5.5.</span> Generalized additive models</h3>
<div class="outline-text-3" id="text-5-5">
<p>
7.2.2
</p>
</div>
<div id="outline-container-orgbe35ccf" class="outline-4">
<h4 id="orgbe35ccf"><span class="section-number-4">5.5.1.</span> Additive models cont'd</h4>
<div class="outline-text-4" id="text-5-5-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\)</li>
<li>In additive models, we assume
\[
  Y_i = \beta_0 + f_1 (x_{i,1}) + ... + f_p (x_{i,p}) + \epsilon_i, i = 1, 2, ..., n
  \]</li>
<li>What if the response \(Y_i\) is class label, or count data?</li>
<li>Use generalized additive models:</li>
<li>Two stages:
<ol class="org-ol">
<li>Specify a distribution for response \(Y_i\), say, with mean \(\mu_i\)</li>
<li>Specify a link function between \(\mu_i\) and predictor variables
\[
     g(\mu_i) = \beta_0 + f_1(x_{i,1}) + ... + f_p (x_{i,p}) \text{ for } i=1, 2, ..., n
     \]</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgccd1f8d" class="outline-4">
<h4 id="orgccd1f8d"><span class="section-number-4">5.5.2.</span> Link functions in GAM</h4>
<div class="outline-text-4" id="text-5-5-2">
<ul class="org-ul">
<li>Example of link functions:
<ul class="org-ul">
<li>\(\mu_i = E(Y_i | X_i)\)</li>
<li>\(g(\mu_i) = \beta_0 + f_1(x_{i,1}) + ... + f_p (x_{i,p}) \text{ for } i=1, 2, ..., n\)</li>
</ul></li>
</ul>
<p>
Link functions:
</p>
<dl class="org-dl">
<dt>identity link</dt><dd>\(g(\mu) = \mu\) for normal distribution</dd>
<dt>logit link</dt><dd>\(g(\mu) = \log(\frac{\mu}{1-\mu})\) for Bernoulli distribution &rarr; additive logistic model</dd>
<dt>log link</dt><dd>\(g(\mu) = \log(\mu)\) for poission distribution &rarr; additive poission model.</dd>
</dl>
</div>
</div>
<div id="outline-container-org2f5cac6" class="outline-4">
<h4 id="org2f5cac6"><span class="section-number-4">5.5.3.</span> Additive logistic models</h4>
<div class="outline-text-4" id="text-5-5-3">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) where \(Y_i \in {0,1}\)</li>
<li>Additive logistic model: 2 stages:
<ol class="org-ol">
<li>The distribution of \(Y_i\) is Bernoulli(&pi;<sub>i</sub>), i.e.
\(P(Y_i = 1) = \pi_i\) and \(P(Y_i=0)=1-\pi_i\)</li>
<li>Link function:
\[
     \log\frac{\pi_i}{1-\pi_i} = \beta_0 + f_1(x_{i,1}) + ... + f_p(x_{i,p}) \text{ for } i=1,..., n
     \]</li>
</ol></li>
<li>How to estimate functions?</li>
</ul>
</div>
</div>
<div id="outline-container-org7b2811a" class="outline-4">
<h4 id="org7b2811a"><span class="section-number-4">5.5.4.</span> Recall logistic regression</h4>
<div class="outline-text-4" id="text-5-5-4">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) where \(Y_i \in {0,1}\) is the class label</li>
<li>Logistic regression model:
<ul class="org-ul">
<li>\(P(Y_i=1) = \pi_i\) and \(P(Y_i=0) = 1-\pi_i\)</li>
<li>\(\log \frac{\pi_i}{1-\pi_i} = \beta_0 + \beta_1 x_{i1} + .. + \beta_{p-1}x_{i,p-1}\)</li>
</ul></li>
<li>Newton-Raphson method to estimate MLE of \(\beta_i\) in the model
\(\beta_{\text{new}} = \beta_{\text{old}} - [\frac{\partial^2}{\partial^2 \beta} \log L (\beta_{\text{old}})]^{-1} \frac{\partial}{\partial \beta} \log L (\beta_{\text{old}})\)</li>
</ul>
</div>
</div>
<div id="outline-container-org55acf30" class="outline-4">
<h4 id="org55acf30"><span class="section-number-4">5.5.5.</span> Review of Newton-Raphson Algorithm</h4>
<div class="outline-text-4" id="text-5-5-5">
<p>
See 7.2.2<sub>Generalized</sub><sub>Addtive</sub><sub>Models</sub><sub>FWv2.pdf</sub>
</p>
</div>
</div>
<div id="outline-container-org0b17b81" class="outline-4">
<h4 id="org0b17b81"><span class="section-number-4">5.5.6.</span> Estimation in additive logistic</h4>
<div class="outline-text-4" id="text-5-5-6">
<ul class="org-ul">
<li>Additive logistic model:
\(P(Y_i =1) = \pi_i\) and \(P(Y_i=0) = 1-\pi_i\)
\(\log \frac{\pi_i}{1-\pi_i} = \beta_0 + f_1 (x_{i,1})+ ... + f_p(x_{i,p})\) for \(i=1,2,...,n\)</li>
<li>Hastie &amp; Tibshirani proposed local scoring algorithm to estimate \(f_j\) by combining
<ul class="org-ul">
<li>Newton-Raphson Method in the logistic regression model</li>
<li>Backfitting method in the additive model</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org67c44dd" class="outline-4">
<h4 id="org67c44dd"><span class="section-number-4">5.5.7.</span> Local scoring algorithm</h4>
<div class="outline-text-4" id="text-5-5-7">
<ul class="org-ul">
<li>The local scoring method to estiamte \(f_j\) in the additive logistic model:
\(\log \frac{\pi_i}{1-\pi_i} = \beta_0 + f_1 (x_{i,1})+ ... + f_p(x_{i,p})\) for \(i=1,2,...,n\)
<ul class="org-ul">
<li>In each iteration, by Newton-Raphson method in logistic regression,
\[
    \hat{\eta_i} = \hat{\beta_0} + \hat{f_1} (x_{i,1}) + ... + \hat{f_p}(x_{i,p}) \\
    \hat{\pi_i} = \frac{e^{\hat{\eta_i}}}{1+e^{\hat{\eta_i}}} \\
    w_i = \hat{\pi_i}(1-\hat{\pi_i}) \\
    Z_i = \hat{\eta_i} + \frac{Y_i - \pi_i}{\hat{\pi_i}(1-\hat{\pi_i})}
    \]</li>
<li>Update the function \(f_j\) by the backfitting method in additive model
\[
    Z_i \sim \beta_0 + f_1 (x_{i,1}) + ... + f_p(x_{i,p})
    \]
with weights \(w_i = \hat{\pi_i}(x-\hat{\pi_i})\). Repeat until convergence.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2dfd71e" class="outline-3">
<h3 id="org2dfd71e"><span class="section-number-3">5.6.</span> Splines applied to Spam dataset</h3>
<div class="outline-text-3" id="text-5-6">
<p>
See 7.2.3<sub>Spam</sub><sub>data</sub><sub>example</sub><sub>FWv2</sub>
</p>
</div>
</div>
</div>
<div id="outline-container-org1cf14e6" class="outline-2">
<h2 id="org1cf14e6"><span class="section-number-2">6.</span> Week 8: Tree-Based and Ensemble Methods</h2>
<div class="outline-text-2" id="text-6">
<p>
Tree-Based Methods
</p>
</div>
<div id="outline-container-org1dc71bc" class="outline-3">
<h3 id="org1dc71bc"><span class="section-number-3">6.1.</span> Introduction to Trees</h3>
<div class="outline-text-3" id="text-6-1">
<p>
8.1.1
</p>
</div>
<div id="outline-container-org0226189" class="outline-4">
<h4 id="org0226189"><span class="section-number-4">6.1.1.</span> Tree method</h4>
<div class="outline-text-4" id="text-6-1-1">

<div id="org2f06686" class="figure">
<p><img src="./img/tree.png" alt="tree.png" />
</p>
</div>
<ol class="org-ol">
<li>Splits data into 2 or more subsets based on the value of variables</li>
<li>Continuously split each subset into finer subsets</li>
<li>Stop growing trees, or start pruning trees</li>
</ol>
</div>
</div>
<div id="outline-container-org0b12ea4" class="outline-4">
<h4 id="org0b12ea4"><span class="section-number-4">6.1.2.</span> Motivating example</h4>
<div class="outline-text-4" id="text-6-1-2">
<p>
Deciding to play sports based on weather variables.
<img src="./img/tree-weather-table.png" alt="tree-weather-table.png" />
Example tree
<img src="./img/tree-weather-tree.png" alt="tree-weather-tree.png" />
</p>
</div>
</div>
<div id="outline-container-org36cd149" class="outline-4">
<h4 id="org36cd149"><span class="section-number-4">6.1.3.</span> Basic ideas</h4>
<div class="outline-text-4" id="text-6-1-3">
<ul class="org-ul">
<li>Partition the feature space into set of triangles (i.e. recursive binary partition)</li>
<li>Fit a simple model (e.g. constant) in each region</li>
</ul>
</div>
</div>
<div id="outline-container-org523b448" class="outline-4">
<h4 id="org523b448"><span class="section-number-4">6.1.4.</span> Illustrative example</h4>
<div class="outline-text-4" id="text-6-1-4">

<div id="orgfeade18" class="figure">
<p><img src="./img/tree-illustrative.png" alt="tree-illustrative.png" />
</p>
</div>
<ul class="org-ul">
<li>Here, we split the R<sup>2</sup> feature space into 5 disjoint regions,</li>
<li>Then, model the response as a constant \(c_m\) in each region \(R_m\) for \(m=1,...,5\)</li>
</ul>
</div>
</div>
<div id="outline-container-org580ae39" class="outline-4">
<h4 id="org580ae39"><span class="section-number-4">6.1.5.</span> Tree-based model</h4>
<div class="outline-text-4" id="text-6-1-5">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\) with \(x_i = (x_{i1}, ..., x_{ip})\) for \(i=1,..,n\)</li>
<li>Tree-based model:
<ul class="org-ul">
<li>Suppose we have a partition of feature space into M regions</li>
<li>We model the response Y as a constant \(c_m\) in each region \(R_m\)</li>
<li>The tree-based model can then be thought of as a special case of the <b><b>additive</b></b> model
\[
    \hat{f}(x) = \sum^M_{m=1} c_m I(x \in R_m)
    \]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb7a97cf" class="outline-4">
<h4 id="orgb7a97cf"><span class="section-number-4">6.1.6.</span> 2 types of trees</h4>
<div class="outline-text-4" id="text-6-1-6">
<dl class="org-dl">
<dt>Regression tree</dt><dd>response \(Y\) is continuous</dd>
<dt>Classification tree</dt><dd>response \(Y\) is class label</dd>
</dl>
</div>
</div>
<div id="outline-container-org352470c" class="outline-4">
<h4 id="org352470c"><span class="section-number-4">6.1.7.</span> 3 fundamental issues in tree-based models</h4>
<div class="outline-text-4" id="text-6-1-7">
<ol class="org-ol">
<li>How to estimate the response \(c_m\) in each region \(R_m\)
<ul class="org-ul">
<li>Easy: if we know the region \(R_m\) we can simply use the average of training data in other regions to estimate \(c_m\)</li>
</ul></li>
<li>How to best partition the feature space into disjoint regions \(R_m\)</li>
<li>How to suitably choose the number \(M\) of disjoint regions?</li>
</ol>
<p>
For 2. and 3. use algorithm approach to answer the questions with training data
</p>
</div>
</div>
</div>
<div id="outline-container-orgff72416" class="outline-3">
<h3 id="orgff72416"><span class="section-number-3">6.2.</span> Tree growing and tree pruning</h3>
<div class="outline-text-3" id="text-6-2">
<p>
8.1.2
</p>
</div>
<div id="outline-container-org49fecb3" class="outline-4">
<h4 id="org49fecb3"><span class="section-number-4">6.2.1.</span> Tree-based model</h4>
<div class="outline-text-4" id="text-6-2-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\) with \(x_i = (x_{i1}, ..., x_{ip})\) for \(i=1,..,n\)</li>
<li>For regression tree and classification tree:
\[
  \hat{f}(x) = \sum^M_{m=1} c_m I(x \in R_m)
  \]</li>
<li>Questions: what are good choices for the following when \(m=1,...,M\)
<ol class="org-ol">
<li>\(M\)</li>
<li>\((R_m, c_m)\)</li>
</ol></li>
<li>Related to tree pruning</li>
</ul>
</div>
</div>
<div id="outline-container-orgc5e6a1f" class="outline-4">
<h4 id="orgc5e6a1f"><span class="section-number-4">6.2.2.</span> RSS criterion for regression tree</h4>
<div class="outline-text-4" id="text-6-2-2">
<ul class="org-ul">
<li>Regression tree:
\[
  \hat{f}(x) = \sum^M_{m=1} c_m I(x \in R_m)
  \]</li>
<li>Criterion: minimize the residual sum of squares over training data
\(RSS = \sum^n_{i=1} (Y_i - f(x_i))^2\)</li>
<li>Under RSS criterion, for given \(M\) and given regions \(R_m\), the  optimal
\[
  \hat{c}_m = \text{average} (Y_i | x_i \in R_m)
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org91eb1c4" class="outline-4">
<h4 id="org91eb1c4"><span class="section-number-4">6.2.3.</span> Choices of regions R<sub>m</sub></h4>
<div class="outline-text-4" id="text-6-2-3">
<ul class="org-ul">
<li>It's computationally infeasible to search all possible disjoint regions over feature space</li>
<li>A greedy algorithm can be used instead with 3 steps:
<ol class="org-ol">
<li>Each time we split the region into 2 sub-regions depending on whether \(x_j \le s\) or \(x_j \gt s\). There are at most \(np\) possibilities in the search of \((j,s)\)</li>
<li>Find the optimal \((j,s)\) with maximum homogeneity (i.e. minimum RSS) within each subregion. In other words, we find a certain split \((j,s)\) so that you will filter the training dataset well</li>
<li>Finally, repeat the process on each subregion until we have a larger (or large enough) split grown in the tree</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgaf925ea" class="outline-4">
<h4 id="orgaf925ea"><span class="section-number-4">6.2.4.</span> Greedy algorithm</h4>
<div class="outline-text-4" id="text-6-2-4">
<ul class="org-ul">
<li>Start with <b><b>all</b></b> training data. Consider a splitting independent variable \(X_j\) and split value \(s\) and define 2 subregions:
<ol class="org-ol">
<li>\(R_1 (j,s) = {x_i : x_{ij} \le s}\) and</li>
<li>\(R_2 (j,s) = {x_i: x_{ij} \gt s}\)</li>
</ol></li>
<li>Then seek the pair \((j,s)\) that minimizes RSS e.g.
\[
  \min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_i (j,s)} (Y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2 (j,s)} (Y_i - c_2)^2\right]
  \]</li>
<li>Partition the data into resulting 2 subregions</li>
<li>Repeat the above process on each of the 2 subregions</li>
</ul>
</div>
</div>
<div id="outline-container-org5960b7b" class="outline-4">
<h4 id="org5960b7b"><span class="section-number-4">6.2.5.</span> When to stop growing tree?</h4>
<div class="outline-text-4" id="text-6-2-5">
<ul class="org-ul">
<li>Key issue is when to stop growing tree? or equivalently, how large should we grow the tree, or how large the number \(M\) of subregions should be
<ul class="org-ul">
<li>Large tree can overfit leading to poor predictive performance. Largest tree can theoretically be large enough s.t. each end node has only 1 data point, equal to Kn algorithm with K=1</li>
<li>Small tree can be biased and thus not capture important structure, e.g. splits only once or no split at all.</li>
</ul></li>
<li>Simple stopping rule:
<ul class="org-ul">
<li>Use a threshold and declare a node terminal if decrease in RSS due to split is smaller than threshold</li>
<li>But this tends to underfit as it does not look far enough forward</li>
</ul></li>
<li>Better approach is to grow a large tree \(T_0\) and then prune it back</li>
</ul>
</div>
</div>
<div id="outline-container-org2cdb1a6" class="outline-4">
<h4 id="org2cdb1a6"><span class="section-number-4">6.2.6.</span> Tree pruning</h4>
<div class="outline-text-4" id="text-6-2-6">
<ul class="org-ul">
<li>Subtree: given a large tree \(T_0\), define a subtree \(T\) to be any tree that can be obtained by pruning \(T_0\) i.e. collapsing any number of its internal (non-terminal) nodes</li>
<li>Key idea:
<ul class="org-ul">
<li>Theory: want to find a subtree that minimizes certain pruning criteria (training error vs tree complexity)</li>
<li>Algorithm: create a nested sequence of subtrees of an initial large tree \(T_0\) by the weakest link cut algorithm</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org3a0b4de" class="outline-4">
<h4 id="org3a0b4de"><span class="section-number-4">6.2.7.</span> Cost-complexity criterion for pruning</h4>
<div class="outline-text-4" id="text-6-2-7">
<ul class="org-ul">
<li>The cost-complexity measure of a subtree \(T\) is defined as \(C_{\alpha} (T) = RSS_T + \alpha |T|\)
<ul class="org-ul">
<li>\(|T|\): the number of terminal nodes of a tree \(T\)</li>
<li>\(\alpha\): relative importance of fit vs model complexity
<ul class="org-ul">
<li>When \(\alpha \rightarrow \infty\) then optimal tree would be single-node tree</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf5c5f0d" class="outline-4">
<h4 id="orgf5c5f0d"><span class="section-number-4">6.2.8.</span> Weakest link cutting algorithm</h4>
<div class="outline-text-4" id="text-6-2-8">
<ul class="org-ul">
<li>Main theory: given larger tree \(T_0\) the optimal subtrees are nested
<ul class="org-ul">
<li>If \(\alpha_1 > \alpha_2\) then the optimal tree \(T_{\alpha_1}\)  is a subtree of the optimal tree \(T_{\alpha_2}\)</li>
</ul></li>
<li>Algorithm is to:
<ul class="org-ul">
<li>Successively collapse the internal node that produces the smallest per-node increase in RSS</li>
<li>Continue until we produce the single node (root) tree</li>
</ul></li>
<li>This yields an infinite sequence of subtrees! But one of them will be optimal
\[
    T_{\alpha} = \text{argmin}_{T \subset T_0} C_\alpha (T)
    \]</li>
</ul>
</div>
</div>
<div id="outline-container-org4893461" class="outline-4">
<h4 id="org4893461"><span class="section-number-4">6.2.9.</span> How to choose &alpha;</h4>
<div class="outline-text-4" id="text-6-2-9">
<ul class="org-ul">
<li>For any \(\alpha \in [0,\infty)\) the optimal subtree \(T_{\alpha} = \text{argmin}_{T \subset T_0} C_\alpha (T)\) is among the finite sequence of subtrees</li>
<li>How to choose \(\alpha\) or optimal subtree? <b><b>Cross-validation</b></b>
<ul class="org-ul">
<li>We choose the value \(\alpha\) and corresponding optimal subtree that minimizes the cross-validated residual sum of squares (RSS)</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4bb328d" class="outline-3">
<h3 id="org4bb328d"><span class="section-number-3">6.3.</span> Classification trees</h3>
<div class="outline-text-3" id="text-6-3">
<p>
8.1.3
</p>
</div>
<div id="outline-container-orgfe82562" class="outline-4">
<h4 id="orgfe82562"><span class="section-number-4">6.3.1.</span> Classification problem</h4>
<div class="outline-text-4" id="text-6-3-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\) with \(x_i = (x_{i1}, ..., x_{ip})\) for \(i=1,..,n\). Assume that the response are class labels \(Y_i \in {1,2,...,K}\)</li>
<li>Tree-based models: how to adapt from the regression context to classification?
\[
  \hat{f}(x) = \sum^M_{m=1} c_m I(x \in R_m)
  \]</li>
<li>In classification, the constant \(c_m\) identifies the class</li>
<li>How to estimate \(c_m\)?</li>
</ul>
</div>
</div>
<div id="outline-container-org22bd56b" class="outline-4">
<h4 id="org22bd56b"><span class="section-number-4">6.3.2.</span> Majority vote in classification</h4>
<div class="outline-text-4" id="text-6-3-2">
<ul class="org-ul">
<li>In the classification tree, we estimate \(c_m\) by the majority vote of training data at each given region \(R_m\). At each given \(m=1,...,M\):
<ol class="org-ol">
<li>We compute the percentage of training data for each class
\[
     N_m = \sum_{x_i \in R_m} 1 \\
     \hat{p}_{m,k} = \frac{1}{N_m} \sum_{x_i \in R_m} I(Y_i = k), \text{for }k=1,...,K
     \]</li>
<li>Then we estimate \(c_m\) or classify this region as the most common class
\[
     \hat{k}_m = \text{argmax}_{k\in{1,2,...,K}} \hat{p}_{m,k}
     \]</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgd22f21a" class="outline-4">
<h4 id="orgd22f21a"><span class="section-number-4">6.3.3.</span> Classification criterion</h4>
<div class="outline-text-4" id="text-6-3-3">
<ul class="org-ul">
<li>In regression, we assess performance under RSS criterion</li>
<li>In classification, what is a suitable criterion?
<ul class="org-ul">
<li>Naturally, use misclassification error rate, i.e. proportion of the training observations that do not belong to the most common class
\[
    \frac{1}{N_m} \sum_{x_i \in R_m} I(Y_i \ne \hat{k}_m) = 1- \hat{p}_{m, \hat{k}_m}
    \]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org5ec4e96" class="outline-4">
<h4 id="org5ec4e96"><span class="section-number-4">6.3.4.</span> Drawback of natural criterion</h4>
<div class="outline-text-4" id="text-6-3-4">
<ul class="org-ul">
<li>It is not sensitive to tree growing:
<ul class="org-ul">
<li>In 2 class problem of 400 obs per class, consider splits:
<ol class="org-ol">
<li>A=300, B=100 vs A=100, B=300</li>
<li>A=200, B=0 vs A=200, B=400</li>
</ol></li>
<li>Remarks: these 2 splits have same misclassification rate, but 2 is probably better as it's a pure node</li>
<li>In practice, 2 alternative criteria are more popular</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org278c928" class="outline-4">
<h4 id="org278c928"><span class="section-number-4">6.3.5.</span> Alternative 1: Gini index</h4>
<div class="outline-text-4" id="text-6-3-5">
<p>
\[
G=\sum_{k \neq k^{\prime}} \hat{p}_{m, k} \, \hat{p}_{m, k^{\prime}}=\sum_{k=1}^{K} \hat{p}_{m, k} \big( 1-\hat{p}_{m, k} \big)=1-\sum_{k=1}^{K} \hat{p}_{m, k}^{2}
\]
</p>
<ul class="org-ul">
<li>Interpretation: the Gini index is:
<ol class="org-ol">
<li>The training error of a random rule that classifies an observation to class \(k\) with probability \(\hat{p}_{m,k}\) and</li>
<li>a measure of total variances of all \(K\) classes</li>
<li>small if one of \(\hat{p}_{m,k}\) is close to 1, whereas all other  \(\hat{p}_{m,k}\) are close to 0.</li>
</ol></li>
<li>Often called impurity measure for classification tree</li>
</ul>
</div>
</div>
<div id="outline-container-org3d2c9ce" class="outline-4">
<h4 id="org3d2c9ce"><span class="section-number-4">6.3.6.</span> Alternative 2: Cross-entropy</h4>
<div class="outline-text-4" id="text-6-3-6">
<ul class="org-ul">
<li>Cross-entropy or deviance is another measure of impurity
\[
  D=-\sum_{k=1}^{K} \hat{p}_{m, k} \operatorname{log} \hat{p}_{m, k}
  \]</li>
<li>Interpretation: Cross-entropy is from information theory in engineering:
<ol class="org-ol">
<li>Gives an interpretation in terms of information gain</li>
<li>Often gives numerical results similar to Gini index</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org55d8fc3" class="outline-4">
<h4 id="org55d8fc3"><span class="section-number-4">6.3.7.</span> Classification tree</h4>
<div class="outline-text-4" id="text-6-3-7">
<p>
It can be constructed in the same way as regression tree, with RSS being replaced by a classification criterion:
</p>
<ol class="org-ol">
<li>Growing tree: a greedy algorithm to minimize an impurity measure (either Gini or cross-entropy)</li>
<li>Pruning tree: the weakest link cut algorithm to minimize the cross-complexity criterion, which can be misclassifcation rate (typically), Gini index, or cross-entropy/deviance</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org3e691c4" class="outline-3">
<h3 id="org3e691c4"><span class="section-number-3">6.4.</span> Practical issues in tree-based methods</h3>
<div class="outline-text-3" id="text-6-4">
<p>
8.2.1
</p>
</div>
<div id="outline-container-orgfb67157" class="outline-4">
<h4 id="orgfb67157"><span class="section-number-4">6.4.1.</span> Prediction problems</h4>
<div class="outline-text-4" id="text-6-4-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_i)\) with \(x_i = (x_{i1}, ..., x_{ip})\) for \(i=1,..,n\)</li>
<li>Tree-based model: with new input \(x\), predict Y by:
\[
  \hat{f}(x) = \sum^M_{m=1} c_m I(x \in R_m)
  \]</li>
<li>Tree-based methods: use tree growing and tree pruning techniques to estimate the (1+2M) parameters \({M, (R_m, c_m)^M_{m=1}}\)
<ul class="org-ul">
<li>Regression tree</li>
<li>Classification tree</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgcffec73" class="outline-4">
<h4 id="orgcffec73"><span class="section-number-4">6.4.2.</span> Advantage of tree methods</h4>
<div class="outline-text-4" id="text-6-4-2">
<ul class="org-ul">
<li>Simple to interpret and easy to represent virtually</li>
<li>Can be used for both numerical or categorical data</li>
<li>Does not require variable transformation for a single predictor, but may need to create new predictors e.g. \(x_1 x_2, x^3_1 x_3 x_4\)</li>
<li>Can handle missing data</li>
</ul>
</div>
</div>
<div id="outline-container-orgff6fc0c" class="outline-4">
<h4 id="orgff6fc0c"><span class="section-number-4">6.4.3.</span> Missing values</h4>
<div class="outline-text-4" id="text-6-4-3">
<p>
4 ways to handle missing predictor values.
</p>

<p>
First 3 are applicable to any other method:
</p>
<ol class="org-ol">
<li>Delete or discard rows with missing values</li>
<li>Fill in or impute missing values with the mean of the predictor over valid observations</li>
<li>Create a new category for missing if <b><b>predictor is categorical</b></b></li>
<li>Surrogate split based on the predictors that are not missing:
<ul class="org-ul">
<li>If the best splitting predictor is missing, then use the 2nd best to split points,</li>
<li>If best and 2nd best are missing, use 3rd best and so on</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-orgfde7bd5" class="outline-4">
<h4 id="orgfde7bd5"><span class="section-number-4">6.4.4.</span> Disadvantage of tree methods</h4>
<div class="outline-text-4" id="text-6-4-4">
<ul class="org-ul">
<li>Lack of smoothness as the regions are disjoint and thus predicted values are not smooth
<ul class="org-ul">
<li>Might be ok for classification with 0/1 loss, but</li>
<li>Degrades performance in regression where we generally expect underlying function to be smooth</li>
</ul></li>
<li>Instability of trees:
<ul class="org-ul">
<li>A small change in the training data can result in very different series of splits</li>
<li>Tree-based methods often have <span style='background-color: #FFFF00;'>high variance</span> which leads to poor predictive performance</li>
<li>Price to pay for estimating a simple tree-based structure from training data</li>
<li>Main idea to improve trees is to reduce variance, say by combining many trees to make global decision.</li>
<li>Ensemble methods: bagging, random forest, boosting, etc.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org894b43f" class="outline-4">
<h4 id="org894b43f"><span class="section-number-4">6.4.5.</span> Trees in R and Spam example</h4>
<div class="outline-text-4" id="text-6-4-5">
<p>
See:
</p>
<ul class="org-ul">
<li><code>8.2.2_Tree.in.R_FW.pdf</code></li>
<li><code>8.2.3_R_example.Tree_FW.pdf</code></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgedff1a6" class="outline-2">
<h2 id="orgedff1a6"><span class="section-number-2">7.</span> Week 9: Tree-Based and Ensemble Methods (Cont'd)</h2>
<div class="outline-text-2" id="text-7">
<p>
Ensemble Methods
</p>
</div>
<div id="outline-container-orgaaa0e0a" class="outline-3">
<h3 id="orgaaa0e0a"><span class="section-number-3">7.1.</span> Introduction to Ensemble Methods</h3>
<div class="outline-text-3" id="text-7-1">
<p>
9.1.1
</p>
</div>
<div id="outline-container-org59aea4d" class="outline-4">
<h4 id="org59aea4d"><span class="section-number-4">7.1.1.</span> Supervised learning</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
Supervised learning framework:
</p>
<ul class="org-ul">
<li>Observed data (i.e. training dataset): \((Y_i, x_{i1}, ..., x_{ip})\) for \(i=1,2,...,n\)</li>
<li>Objective: find a function \(h(x_{\text{new}})=h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ... x_p)\) in the testing dataset</li>
<li>Focus on predictive performance without worrying about explaining or interpreting models</li>
</ul>
</div>
</div>
<div id="outline-container-org8dd9439" class="outline-4">
<h4 id="org8dd9439"><span class="section-number-4">7.1.2.</span> ML and data mining methods</h4>
<div class="outline-text-4" id="text-7-1-2">
<ul class="org-ul">
<li>Different kinds of ML or data mining methods / algorithms for prediction e.g.:
<ul class="org-ul">
<li>KNN</li>
<li>linear regression (variable selection, ridge, lasso, PCA, PLS)</li>
<li>linear methods for classification (LDA, QDA, Naive Bayes, Logistic)</li>
<li>Local smoothing (LOESS, Kernel, Spline)</li>
<li>Generalized additive models (Backfitting)</li>
<li>Tree, etc</li>
</ul></li>
</ul>
<p>
Often we want to choose a specific method that works best for given application or dataset.
</p>
</div>
</div>
<div id="outline-container-org379ecb2" class="outline-4">
<h4 id="org379ecb2"><span class="section-number-4">7.1.3.</span> Ensemble methods</h4>
<div class="outline-text-4" id="text-7-1-3">
<ul class="org-ul">
<li>Ensemble method is to:
<ul class="org-ul">
<li>Back off from choosing specific method and</li>
<li>Be content with averaging prediction from several or many models</li>
</ul></li>
<li>Ensemble method can improve predictive performance</li>
</ul>
</div>
</div>
<div id="outline-container-org7c19849" class="outline-4">
<h4 id="org7c19849"><span class="section-number-4">7.1.4.</span> Central premise</h4>
<div class="outline-text-4" id="text-7-1-4">
<ul class="org-ul">
<li>Pooling methods represent a richer model class than choosing any one of them. The weighted sum of predictions from a collection of models may give better performance</li>
<li>Evaluation of performance is predictive rather than model-based</li>
</ul>
</div>
</div>
<div id="outline-container-org0193703" class="outline-4">
<h4 id="org0193703"><span class="section-number-4">7.1.5.</span> Models in ensemble method</h4>
<div class="outline-text-4" id="text-7-1-5">
<ul class="org-ul">
<li>An ensemble method combines predictions from several methods to obtain one overall prediction.
<ul class="org-ul">
<li>Not that same as combing several methods to get one model that is interpretable</li>
</ul></li>
<li>Ensemble methods can improve overall predictive performance when the models in the ensemble give different predictions</li>
<li>Some ensemble methods combine same objects (e.g. tree with other trees &rarr; Random Forest, or neural net with other neural nets</li>
<li>Other ensemble methods, e.g. stacking, combine very different models</li>
</ul>
</div>
</div>
<div id="outline-container-org9773f43" class="outline-4">
<h4 id="org9773f43"><span class="section-number-4">7.1.6.</span> Main techniques</h4>
<div class="outline-text-4" id="text-7-1-6">
<ul class="org-ul">
<li>Bayesian model averaging</li>
<li>Stacking</li>
<li>Bagging</li>
<li>Random Forest</li>
<li>Boosting</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org96645d8" class="outline-3">
<h3 id="org96645d8"><span class="section-number-3">7.2.</span> Bayesian model averaging and stacking</h3>
<div class="outline-text-3" id="text-7-2">
<p>
9.1.2
</p>
</div>
<div id="outline-container-org6686ef4" class="outline-4">
<h4 id="org6686ef4"><span class="section-number-4">7.2.1.</span> Bayesian statistics</h4>
<div class="outline-text-4" id="text-7-2-1">
<ul class="org-ul">
<li>Key feature of Bayesian statistics is the treatment of the estimand as a random variable with some distributions (prior and posterior distributions)</li>
<li>Bayesian statistics has been successfully applied to ML and data mining:
<ul class="org-ul">
<li>Bayesian non-parametrics: Dirichlet Process Priors, Mixture Models, Bayesian Neural Networks, Gaussian Process</li>
<li>Bayesian ensemble method: Bayesian Model Averaging</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgaaaceb8" class="outline-4">
<h4 id="orgaaaceb8"><span class="section-number-4">7.2.2.</span> Bayesian model averaging (BMA)</h4>
<div class="outline-text-4" id="text-7-2-2">
<ul class="org-ul">
<li>BMA is a Bayesian ensemble method that
<ul class="org-ul">
<li>Assigns a discrete prior on models in the ensemble</li>
<li>Assigns priors on many parameters within each model</li>
<li>Predicts by posterior mean over all models</li>
</ul></li>
<li>BMA is Bayes risk optimal over the squared error loss</li>
<li>BMA inspires one to develop frequentist approaches on putting different weights on different models in the ensemble</li>
</ul>
</div>
</div>
<div id="outline-container-orgf2f4839" class="outline-4">
<h4 id="orgf2f4839"><span class="section-number-4">7.2.3.</span> Frequentist viewpoint</h4>
<div class="outline-text-4" id="text-7-2-3">
<ul class="org-ul">
<li>BMA from a frequentist viewpoint:
<ul class="org-ul">
<li>Given predictions \({\hat{h}_1 (x), ..., \hat{h}_M (x)}\) under the squared error loss in the regression context,
\[
    h_\text{BMA} (x) = \sum^M_{m=1} \hat{h}_m (x)
    \]
where the weights \(w_m\) are related to posterior model probabilities</li>
<li>Can we estimate the weights \(w_m\) directly from training data?</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd72412a" class="outline-4">
<h4 id="orgd72412a"><span class="section-number-4">7.2.4.</span> Stacking</h4>
<div class="outline-text-4" id="text-7-2-4">
<p>
Stacking (or stacked generalization) is a way to estimate the weights \(\hat{w}_m\) directly from the training data, then form the stacking prediction
\[
h_\text{stack} (x) = \sum^M_{m=1} \hat{h}_m (x)
\]
</p>
<ul class="org-ul">
<li>Invented by Wolpert (1992) and studied by Breimen (1996)</li>
<li>Weights \(\hat{w}_m\) are estimated from cross-validation, and the final prediction mimics Bayesian Model Averaging</li>
</ul>
</div>
</div>
<div id="outline-container-org0448f47" class="outline-4">
<h4 id="org0448f47"><span class="section-number-4">7.2.5.</span> Stacking weights</h4>
<div class="outline-text-4" id="text-7-2-5">
<p>
The stacking weights \(\hat{w}_m\) are obtained as follows:
</p>
<ol class="org-ol">
<li>Leave one out cross validation (LOOCV): let \(\hat{h}^{(-i)}_m (x)\) be the prediction at \(x\) using the \(m\) th model in the ensemble, as estimated from the training data with the \(i\) th observation removed.</li>
<li>Linear regression: the estimated weights solve a linear regression problem
\[
   ( \widehat{w}_{1}, \cdots, \widehat{w}_{M} )=\underset{( w_{1}, \cdots, w_{M} )} {\mathrm{a r g m i n}} \sum_{i=1}^{n} \left[ Y_{i}-\sum_{m=1}^{M} w_{m} \widehat{h}_{m}^{(-i )} ( x_{i} ) \right]^{2}
   \]</li>
</ol>
<p>
Remark: this puts <span style='background-color: #FFFF00;'>low weights</span> on models that have poor LOOCV accuracy in the training data
</p>
</div>
</div>
<div id="outline-container-orgba1ffb3" class="outline-4">
<h4 id="orgba1ffb3"><span class="section-number-4">7.2.6.</span> Aspects of stacking</h4>
<div class="outline-text-4" id="text-7-2-6">
<p>
Discussion of stacking algorithm: it's flexible and <span style='background-color: #FFFF00;'>easy to apply</span>
</p>
<ul class="org-ul">
<li>Optimization over \((w_1, ..., w_M)\) is essentially the methods of least squares in linear regression, and we can impose additional constraints that the weights are positive and sum to 1</li>
<li>LOOCV can be replaced by other cross-validation methods such as 5-fold, 10-fold CV, or Monte Carlo CV</li>
<li>Stacking is closely related to BMA, and can be thought of as an approximation of BMA without any priors</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org710df8f" class="outline-3">
<h3 id="org710df8f"><span class="section-number-3">7.3.</span> Bootstrapping algorithm</h3>
<div class="outline-text-3" id="text-7-3">
<p>
9.1.3
</p>
</div>
<div id="outline-container-orgfe4d71c" class="outline-4">
<h4 id="orgfe4d71c"><span class="section-number-4">7.3.1.</span> Motivating example</h4>
<div class="outline-text-4" id="text-7-3-1">
<ul class="org-ul">
<li>Data: \(z = (z_1, ..., z_n)\) where \(z_i = (Y_i, x_{i1}, ..., x_{ip}), i=1,...,n\)</li>
<li>Parameter estimation: we derived real-valued summary statistics
\(S(z) = S(z_1, ..., z_n)\) which is en estimator of population parameter \(\theta\) (e.g. mean, median, correlation coefficient, regression coefficient, etc.)</li>
<li>Objective: derive a <b><b>robust</b></b> estimator of the confidence interval of \(\theta\) or the standard error of \(S(z)\)</li>
<li>Challenges: we don't know the distribution of data, and are unable to obtain additional training datasets</li>
</ul>
</div>
</div>
<div id="outline-container-org44b0850" class="outline-4">
<h4 id="org44b0850"><span class="section-number-4">7.3.2.</span> Idea in Bootstrapping algorithm</h4>
<div class="outline-text-4" id="text-7-3-2">
<ul class="org-ul">
<li>Intuitive idea:
<ul class="org-ul">
<li>Estimate the standard error of \(S(z) = S(z_1, ..., z_n)\) based on the sample standard deviation if we have many values of \(S(z)\) or many independent copies of training data</li>
<li><span style='background-color: #FFFF00;'>Resample with replacement</span> from the original training dataset (\(z= z_1, z_2, ..., z_n\)) to generate many copies of "new" training dataset. This allows us to compute many values of \(S(z)\)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc22d413" class="outline-4">
<h4 id="orgc22d413"><span class="section-number-4">7.3.3.</span> Resample with replacement</h4>
<div class="outline-text-4" id="text-7-3-3">

<div id="org5e0495e" class="figure">
<p><img src="./img/bootstrap-resample.png" alt="bootstrap-resample.png" />
</p>
</div>
<ul class="org-ul">
<li>High level:
<ul class="org-ul">
<li>Input Data: \(z = (z_1, ..., z_n)\) where \(z_i = (Y_i, x_{i1}, ..., x_{ip}), i=1,...,n\) and estimator of \(S(z)\) for \(\theta\)</li>
<li>For \(b=1,...,B\)
<ul class="org-ul">
<li><b><b>Sample with replacement</b></b> to get bootstrap sample \(z^{*b} = (z_1^{*b}, ..., z_n^{*b})\)</li>
<li>Compute the value \(S(z^{*b}) = S(z_1^{*b}, ..., z_n^{*b})\) for this bootstrap sample</li>
</ul></li>
<li>Once the \(B\) values of \(S(z^{*b})\) have been computed,
<ul class="org-ul">
<li>The quantiles of \(S(z^{*b})\)'s provide an empirical distribution of \(S(z)\)</li>
<li>They can be used to provide confidence intervals of \(\theta\)</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org0f3a64a" class="outline-4">
<h4 id="org0f3a64a"><span class="section-number-4">7.3.4.</span> New use of bootstrapping</h4>
<div class="outline-text-4" id="text-7-3-4">
<ul class="org-ul">
<li>Bootstrapping was originally developed by Efron (1979) to estimate the variance in the classical statistics context</li>
<li>Breiman (1996) found a surprising used of bootstrapping to improve the predictive performance of tree-based method and model:
<ul class="org-ul">
<li>Bagging (bootstrap aggregating)</li>
<li>Random forest</li>
</ul></li>
<li>Since then, bootstrapping algorithm has been used to combine other algorithms in machine learning and data mining</li>
</ul>
</div>
</div>
<div id="outline-container-org827d403" class="outline-4">
<h4 id="org827d403"><span class="section-number-4">7.3.5.</span> Bootstrapping in R</h4>
<div class="outline-text-4" id="text-7-3-5">
<ul class="org-ul">
<li>Given vector \(Z\), how to write your own functions to generate the bootstrap sample in R?</li>
<li>Naively:
<ol class="org-ol">
<li>Extract sample size of <code>Z</code> with <code>length()</code> and store it in <code>n</code></li>
<li>Copy <code>Z</code> to <code>sample</code></li>
<li>Draw randomly an integer between 1 and n with <code>ceiling(runif())</code></li>
<li>Corresponding value of <code>Z</code> is extracted and stored in <code>sample</code> which is finally returned</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-orgf1af47f" class="outline-4">
<h4 id="orgf1af47f"><span class="section-number-4">7.3.6.</span> More efficient implementation</h4>
<div class="outline-text-4" id="text-7-3-6">
<ul class="org-ul">
<li>Avoid loops to generate data index simultaneously</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3f16bc9" class="outline-3">
<h3 id="org3f16bc9"><span class="section-number-3">7.4.</span> Bagging algorithm</h3>
<div class="outline-text-3" id="text-7-4">
<p>
9.2.1
</p>
</div>
<div id="outline-container-orgb461ad9" class="outline-4">
<h4 id="orgb461ad9"><span class="section-number-4">7.4.1.</span> Improve tree methods</h4>
<div class="outline-text-4" id="text-7-4-1">
<ul class="org-ul">
<li>Observed (training) dataset: \((Y_i, x_{i1}, ..., x_{ip})\) for \(i=1,...,n\)</li>
<li>Objective: find an function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset</li>
<li>Tree-based method is easy to interpret but might have poor predictive performance due to <span style='background-color: #FFFF00;'>high variance</span></li>
<li>Can we improve the tree-based model and methods?</li>
</ul>
</div>
</div>
<div id="outline-container-org8c0d9d4" class="outline-4">
<h4 id="org8c0d9d4"><span class="section-number-4">7.4.2.</span> Variance reduction</h4>
<div class="outline-text-4" id="text-7-4-2">
<ul class="org-ul">
<li>In statistics, sample mean can reduce the variance:
if \(U_1, ..., U_m\) are i.i.d. with variance \(\sigma^2\) then the sample mean \(\bar{U} = \frac{U_1+...+U_m}{m}\) has the variance \(\frac{\sigma^2}{m}\)</li>
<li>Thus we can reduce the variance of the tree-based method if we can use the average of predictions from several independent trees</li>
<li>This would require us to have <b><b>several independent</b></b> datasets, but in most applications, we only have 1 training dataset</li>
</ul>
</div>
</div>
<div id="outline-container-org651ac8a" class="outline-4">
<h4 id="org651ac8a"><span class="section-number-4">7.4.3.</span> Ideas in bagging</h4>
<div class="outline-text-4" id="text-7-4-3">
<p>
Bagging (bootstrap aggregating) was proposed by Breiman (1994), and is a popular ensemble method.
</p>

<p>
Key ideas:
</p>
<ul class="org-ul">
<li>Use the training dataset to generate many bootstrapping samples</li>
<li>Use each bootstrap sample to build a distinct tree</li>
<li>Our final prediction is the <span style='background-color: #FFFF00;'>average</span> of predictions from these bootstrapping trees, which will have smaller variance and thus lead to better predictive performance</li>
</ul>
</div>
</div>
<div id="outline-container-orge5fabd8" class="outline-4">
<h4 id="orge5fabd8"><span class="section-number-4">7.4.4.</span> Bagging algorithm</h4>
<div class="outline-text-4" id="text-7-4-4">
<ul class="org-ul">
<li>Given training dataset, fit a base model (e.g. Tree) that predicts the response as \(\hat{h}(x)\) at the new datapoint \(x\).</li>
<li>For \(b=1,...,B\):
<ol class="org-ol">
<li>Draw a bootstrap sample from the training dataset</li>
<li>Fit a model \(\hat{h}^{*b} (x)\) to each bootstrap sample</li>
<li>Then the bagging prediction at the new datapoint \(x\) for regression:
\[
     \widehat{\boldsymbol{h}}_{bag} ( \boldsymbol{x} )=\frac{1} {B} \sum_{b=1}^{B} \widehat{\boldsymbol{h}}^{* b} ( \boldsymbol{x} )
     \]
<ul class="org-ul">
<li>for classification: majority vote</li>
</ul></li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org8540b47" class="outline-4">
<h4 id="org8540b47"><span class="section-number-4">7.4.5.</span> Remarks on bagging</h4>
<div class="outline-text-4" id="text-7-4-5">
<ul class="org-ul">
<li>Bagging can improve a good, but unstable procedure (e.g. Tree, neural networks, best subset selections in linear regression, etc.)</li>
<li>Bagging generally reduces higher-order variation: if one uses a decomposition into linear and higher-order terms, bagging affects the variability of higher-order terms</li>
<li>Bagging <span style='background-color: #FFFF00;'>re-used the data</span> to get more information in them but at the same time introduces a new variability, that of the model. Strangely, more variability can be an improvement at times.</li>
</ul>
</div>
</div>
<div id="outline-container-org9750f1f" class="outline-4">
<h4 id="org9750f1f"><span class="section-number-4">7.4.6.</span> Theoretical issues</h4>
<div class="outline-text-4" id="text-7-4-6">
<ul class="org-ul">
<li>Given a training dataset of \(n\) distinct observations, and suppose we generate a bootstrap sample,
<ul class="org-ul">
<li>What is the chance of a given observation not being selected in the bootstrap sample?</li>
</ul></li>
</ul>
<p>
\[
p_{1}=\frac{( n-1 )^{n}} {n^{n}}=\left( 1-\frac{1} {n} \right)^{n} \ \ \to\ \frac{1} {e}=3 6. 8 9 / \ \ a s \, n \to\infty
\]
</p>

<ul class="org-ul">
<li><p>
Given a training dataset of \(n\) distinct observations, suppose there are \(B=100\) bootstrap samples
</p>
<ul class="org-ul">
<li>Let \(W\) be the number of bootstrap samples that do not include the given observation. What is the distribution of \(W\)?</li>
</ul>
<p>
\[
  \text{Binomial}(B=100, p=p1 \approx \frac{1}{e})
  \]
</p></li>
<li>On average, there are \(E(W) = B \times p_1 \approx 36.8\) bootstrap samples that do not include this given observation.
<ul class="org-ul">
<li>In other words, roughly 1/3 of the bootstrap samples will provide the cross-validation or out-of-bag estimate of predictive performance.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9ae7830" class="outline-3">
<h3 id="org9ae7830"><span class="section-number-3">7.5.</span> Random Forest</h3>
<div class="outline-text-3" id="text-7-5">
<p>
9.2.2
</p>
</div>
<div id="outline-container-orgd1132b6" class="outline-4">
<h4 id="orgd1132b6"><span class="section-number-4">7.5.1.</span> Improve tree methods</h4>
<div class="outline-text-4" id="text-7-5-1">
<ul class="org-ul">
<li>Observed data (training dataset): \((Y_i, X_{i1}, ..., X_{ip})\) for \(i=1,...,n\)</li>
<li>Objective: find a function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset.</li>
<li>Tree-based method is easy to interpret but might have poor predictive performance due to <span style='background-color: #FFFF00;'>high variance</span></li>
<li>Ensemble methods to improve tree: bagging, random forest.</li>
</ul>
</div>
</div>
<div id="outline-container-orgb28911b" class="outline-4">
<h4 id="orgb28911b"><span class="section-number-4">7.5.2.</span> Recall bagging algorithm</h4>
<div class="outline-text-4" id="text-7-5-2">
<p>
Given a training dataset and fit a base model (e.g. Tree) that can predict the response as \(\hat{h}(x)\) at the new data point \(x\)
</p>
<ul class="org-ul">
<li>Bagging algorithm: For \(b=1,...,B\)
<ul class="org-ul">
<li>Draw a bootstrap sample from the training dataset</li>
<li>Fit a model \(\hat{h}^{*b}(x)\) to each bootstrap sample</li>
</ul></li>
<li>Then the bagging prediction at the new data point \(x\) is</li>
</ul>

<p>
\[
\widehat{\boldsymbol{h}}_{bag} ( \boldsymbol{x} )=\frac{1} {B} \sum_{b=1}^{B} \widehat{\boldsymbol{h}}^{* b} ( \boldsymbol{x} )
\]
</p>
</div>
</div>
<div id="outline-container-orgda598fe" class="outline-4">
<h4 id="orgda598fe"><span class="section-number-4">7.5.3.</span> Ideas in random forest</h4>
<div class="outline-text-4" id="text-7-5-3">
<ul class="org-ul">
<li>Random forest was invented by Breiman (2000) and later developed by Breiman and Cutler (2004), mainly to improve <b><b>classification tree</b></b></li>
<li>Main idea is to combine <b><b>bagging algorithm</b></b> with another clever trick of <b><b>random feature selection</b></b>:
<ul class="org-ul">
<li>The bootstrapping trees in <b><b>bagging</b></b> might be similar, since we exam all the same \(p\) variables in all bootstrapping samples to determine the best split when growing bootstrapping trees</li>
<li>Random forest usually pick \(\sqrt{p}\) out of the \(p\) variables <b><b>at random</b></b> for <b><b>each bootstrap sample</b></b> and considers the best splits among them</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb1e7ec1" class="outline-4">
<h4 id="orgb1e7ec1"><span class="section-number-4">7.5.4.</span> Random forest</h4>
<div class="outline-text-4" id="text-7-5-4">
<p>
Random forest algorithm:
</p>
<ul class="org-ul">
<li>For \(b=1,...,B\)
<ul class="org-ul">
<li>Draw a bootstrap sample from the training dataset</li>
<li>For each bootstrap sample, randomly pack \(\sqrt{p}\) out of the \(p\) input variables and take the best splits among them when <b><b>growing</b></b> a tree. <b><b>No pruning</b></b> is needed in random forest</li>
</ul></li>
<li>For regression: use the average of all \(B\) trees for prediction</li>
<li>For classification: use all \(B\) trees on a majority vote</li>
</ul>
</div>
</div>
<div id="outline-container-org0c1630c" class="outline-4">
<h4 id="org0c1630c"><span class="section-number-4">7.5.5.</span> Random Feature Selection</h4>
<div class="outline-text-4" id="text-7-5-5">
<p>
What are the advantages of randomly picking \(\sqrt{p}\) out of the \(p\) input variables at each bootstrap sample?
</p>
<ul class="org-ul">
<li>It makes the different trees in the random forest <b><b>less similar</b></b></li>
<li>It allows <b><b>highly correlated</b></b> variables to play <b><b>nearly equivalent</b></b> roles (otherwise those that are slightly more predictive will always be chosen)</li>
<li>It <b><b>decorrelates</b></b> the trees and lowers predictive error</li>
</ul>
</div>
</div>
<div id="outline-container-org721a848" class="outline-4">
<h4 id="org721a848"><span class="section-number-4">7.5.6.</span> Out-of-bag estimate</h4>
<div class="outline-text-4" id="text-7-5-6">
<ul class="org-ul">
<li>Recall that for each observation, approximately 1/3 (i.e. \(1/e\)) of the bootstrap samples will not contain it</li>
<li>Thus, approximately 1/3 of the trees in the random forest can be used to provide out-of-bag error estimate of the given observation</li>
<li>In other words, random forest automatically provide cross-validation-like "out of bag" estimate of misclassification rate</li>
</ul>
</div>
</div>
<div id="outline-container-org6bb804c" class="outline-4">
<h4 id="org6bb804c"><span class="section-number-4">7.5.7.</span> Relative importance</h4>
<div class="outline-text-4" id="text-7-5-7">
<p>
Random forest can be used to estimate the <b><b>relative importance</b></b> of the \(j\) th explanatory variable
</p>
<ul class="org-ul">
<li>Run each observation through all the trees for which the observation is out-of-bag and count the number of votes for the correct class each observation gets</li>
<li>This vote can be compared to the corresponding vote count after randomly permuting the values of variable \(j\) among the observations</li>
<li>The difference in the number of correct votes under the two procedures is the <b><b>raw importance</b></b> for variable \(j\)</li>
<li>Usually this is standardized to <b><b>relative importance</b></b> when the scores sum to 1</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgdbec756" class="outline-3">
<h3 id="orgdbec756"><span class="section-number-3">7.6.</span> Random Forest R Lab</h3>
<div class="outline-text-3" id="text-7-6">
<p>
See <code>9.2.3_R.Lab.RandomForest_FW.pdf</code>
</p>
</div>
</div>
</div>

<div id="outline-container-orge37917e" class="outline-2">
<h2 id="orge37917e"><span class="section-number-2">8.</span> Week 10: Tree-Based and Ensemble Methods (Cont'd)</h2>
<div class="outline-text-2" id="text-8">
<p>
Ensemble Methods (Cont'd)
</p>
</div>
<div id="outline-container-org86903af" class="outline-3">
<h3 id="org86903af"><span class="section-number-3">8.1.</span> Introduction to Boosting</h3>
<div class="outline-text-3" id="text-8-1">
<p>
10.1.1
</p>
</div>
<div id="outline-container-org388e229" class="outline-4">
<h4 id="org388e229"><span class="section-number-4">8.1.1.</span> Improving Tree-Based Methods</h4>
<div class="outline-text-4" id="text-8-1-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\)</li>
<li>Objective: find a function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset.</li>
<li>Tree-based method is easy to interpret but might have poor predictive performance</li>
<li>How to improve tree?</li>
</ul>
</div>
</div>
<div id="outline-container-orgb4264cb" class="outline-4">
<h4 id="orgb4264cb"><span class="section-number-4">8.1.2.</span> Ensemble Methods</h4>
<div class="outline-text-4" id="text-8-1-2">
<p>
2 types of ensemble methods can improve trees:
</p>
<dl class="org-dl">
<dt>Re-sampling</dt><dd>bagging, random forest
<ul class="org-ul">
<li>Create many trees <span style='background-color: #FFFF00;'>in parallel</span></li>
<li>Each tree built in bootstrap dataset, independent of other trees</li>
<li><b><b>average</b></b> of trees reduces variance</li>
</ul></dd>
<dt>Re-weighting</dt><dd>boosting
<ul class="org-ul">
<li>Create trees <span style='background-color: #FFFF00;'>sequentially</span></li>
<li>Each tree built to correct misclassification of previous trees</li>
<li><b><b>Weighted average</b></b> of trees improves training error</li>
</ul></dd>
</dl>
</div>
</div>
<div id="outline-container-org40ee941" class="outline-4">
<h4 id="org40ee941"><span class="section-number-4">8.1.3.</span> Boosting history</h4>
<div class="outline-text-4" id="text-8-1-3">
<dl class="org-dl">
<dt>Adaboost for binary classification</dt><dd>Freund and Schapire, 1997</dd>
<dt>Statistical view of boosting</dt><dd>Friendman, Hastie and Tibshirani, 2000</dd>
<dt>gbm (gradient boosting machine) algorithm</dt><dd>Friedman, 2001</dd>
<dt>XGBoost algorithm</dt><dd>Chen and Guestrin, 2016</dd>
</dl>
</div>
</div>
<div id="outline-container-orgf64e407" class="outline-4">
<h4 id="orgf64e407"><span class="section-number-4">8.1.4.</span> AdaBoost motivation</h4>
<div class="outline-text-4" id="text-8-1-4">
<ul class="org-ul">
<li>Developed for classification, when responses \(Y_i \in {-1,1}\)</li>
<li>Original motivation is to combine a set of "weak learners" to create a single "strong learner"
<dl class="org-dl">
<dt>Weaker learner</dt><dd>classifier that is only slightly better than <b><b>random guess</b></b></dd>
<dt>Strong learner</dt><dd>a classifier that has very good classification performance</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org65dff10" class="outline-4">
<h4 id="org65dff10"><span class="section-number-4">8.1.5.</span> Idea in AdaBoost</h4>
<div class="outline-text-4" id="text-8-1-5">
<p>
<span style='background-color: #FFFF00;'>Re-weighting</span>:
</p>
<ul class="org-ul">
<li>Weights of training data is <b><b>readjusted</b></b> after a weaker learner is added, i.e. put more weights to misclassified data</li>
<li>Future weak learners are applied to the same training data with different weights
<ul class="org-ul">
<li>i.e., focus more on the data that the previous weak learners misclassified</li>
</ul></li>
<li><p>
Final classifier is based on <b><b>weighted average</b></b> of all weaker learners i.e.
</p>

<p>
\[
  \text{sign}(\sum^M_{m=1} w_m f_m (x))
  \]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org4cf1b99" class="outline-4">
<h4 id="org4cf1b99"><span class="section-number-4">8.1.6.</span> Modern viewpoint of boosting</h4>
<div class="outline-text-4" id="text-8-1-6">
<ul class="org-ul">
<li>Researchers have spent significant time and effort to understand why it works, and how to extend</li>
<li>Modern viewpoint of boosting is to <b><b>sequentially</b></b> fit the <span style='background-color: #FFFF00;'>Forward Stagewise Additive Regression Model</span> of the form
\[
  h(x) = \sum^m_{m=1} w_m f(x, \gamma_m)
  \]</li>
<li>The choice of \(f(x, \gamma_m)\) is based on <span style='background-color: #FFFF00;'>gradient descent</span></li>
<li>Prefer to choose <span style='background-color: #FFFF00;'>small coefficients</span> of \(w_m\) with large \(M\)
<ul class="org-ul">
<li>i.e., prefer to <b><b>learn slowly</b></b></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org19dba01" class="outline-3">
<h3 id="org19dba01"><span class="section-number-3">8.2.</span> AdaBoost for Binary classification</h3>
<div class="outline-text-3" id="text-8-2">
</div>
<div id="outline-container-org83b9854" class="outline-4">
<h4 id="org83b9854"><span class="section-number-4">8.2.1.</span> Problem setup</h4>
<div class="outline-text-4" id="text-8-2-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1E}\)</li>
<li>Objective: find a function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset.</li>
<li>Weak learners:
<ul class="org-ul">
<li>we have many tree classifiers \({h_m(x)}\) available to predict \(Y\) but each learner is only slightly better than random guessing</li>
</ul></li>
<li>How to use weak learners to construct a better classifier?</li>
</ul>
</div>
</div>
<div id="outline-container-org8381f84" class="outline-4">
<h4 id="org8381f84"><span class="section-number-4">8.2.2.</span> AdaBoost algorithm</h4>
<div class="outline-text-4" id="text-8-2-2">
<ol class="org-ol">
<li>Start with equal weights for all training data i.e.
\[
   w_i = 1/n, i=1,...,n
   \]</li>
<li>Repeat for \(m=1,2,...,M\):
<ol class="org-ol">
<li>Fit the best classifier \(h_m (x)\) to the training data with weights \(w_i\)</li>
<li>Compute constants:
\[
      \text{err}_m = E_w I(Y \neq h_m (x)) \\
      c_m = \log(\frac{1-\text{err}_m}{\text{err}_m})
      \]</li>
<li>Set weights \(w_i \leftarrow  w_i \exp(c_m I(Y_i \neq h_m (x_i)))\) and re-normalize \(\sum_i w_i = 1\)</li>
</ol></li>
<li>Output the final classifier \(h(x) = \text{sign}(\sum^M_{m=1} c_m h_m (x))\)</li>
</ol>
</div>
</div>
<div id="outline-container-org6f4ed85" class="outline-4">
<h4 id="org6f4ed85"><span class="section-number-4">8.2.3.</span> Example</h4>
<div class="outline-text-4" id="text-8-2-3">

<div id="org93c88c4" class="figure">
<p><img src="./img/ada1.png" alt="ada1.png" />
</p>
</div>
<ul class="org-ul">
<li>Training data: \(n=3, p=1\)
\[
  (x_1, Y_1) = (-1,1) \\
  (x_2, Y_2) = (0,-1) \\
  (x_3, Y_3) = (1,1)
  \]</li>
<li>Weaker learners: only consider tree stump (i.e., two-node tree) of the form \(x<v\) or \(x \ge v\)</li>
</ul>
</div>
</div>
<div id="outline-container-org3bceba3" class="outline-4">
<h4 id="org3bceba3"><span class="section-number-4">8.2.4.</span> 3 weaker learners</h4>
<div class="outline-text-4" id="text-8-2-4">
<ol class="org-ol">
<li>Learner 1 misclassifies \((x_1, Y_1) = (-1,1)\) and has loss \(w_1\)
\[
   h_{1} ( x )=\left\{\begin{matrix} {{1 ~ ~ ~ ~ i f ~ ~ x \geq0. 5}} \\ {{-1 ~ ~ i f ~ ~ x < 0. 5}} \\ \end{matrix} \right.
   \]</li>
<li>Learner 2 misclassifies \((x_2, Y_2) = (0,-1)\) and has loss \(w_2\)
\[
   h_2 (x) = 1 \text{for all }x
   \]</li>
<li>Learner 3 misclassifies \((x_3, Y_3) = (1,1)\) and has loss \(w_3\)
\[
	 h_{3} ( x )=\left\{\begin{matrix} {{-1 ~ ~ ~ ~ i f ~ ~ x \geq-0. 5}} \\ {{1 ~ ~ i f ~ ~ x <- 0. 5}} \\ \end{matrix} \right.
	 \]</li>
</ol>
</div>
</div>
<div id="outline-container-org6f89c89" class="outline-4">
<h4 id="org6f89c89"><span class="section-number-4">8.2.5.</span> AdaBoost round 1</h4>
<div class="outline-text-4" id="text-8-2-5">
<p>
\(m=1\)
</p>
<ul class="org-ul">
<li>Original weights
\[
  w_1 = \frac{1}{3}, w_2 = \frac{1}{3}, w_3 = \frac{1}{3}
  \]</li>
<li>Fit a best classifier. Without loss of generality, consider \(h_2 (x) =1\) for all x</li>
<li>Compute constants \(\text{err}\) and \(c=\log((1-\text{err})/\text{err})\)
\[
  \text{err} = w_2 = \frac{1}{3} \\
  c = \log \frac{1-1/3}{1/3} = \log 2
  \]</li>
<li>Output: \(h(x) = \text{sign}(\sum^M_{m=1}c_m h_m (x)) = \text{sign}(\log(2) h_2 (x)) = 1\) for all \(x\)</li>
</ul>
</div>
</div>
<div id="outline-container-org27676fa" class="outline-4">
<h4 id="org27676fa"><span class="section-number-4">8.2.6.</span> AdaBoost round 2</h4>
<div class="outline-text-4" id="text-8-2-6">
<p>
\(m=2\)
</p>
<ul class="org-ul">
<li>Update weights
\[
  w_1 \leftarrow \frac{1}{3},
  w_2 \leftarrow \frac{1}{3} e^{\log(2)} = \frac{2}{3},
  w_3 \leftarrow \frac{1}{3}
  \]
Renormalizing we have \(w_1 = \frac{1}{4}, w_2 = \frac{1}{2}, w_3 = \frac{1}{4}\)</li>
<li>Fit a best classifier: WLOG, consider
\[
	h_{1} ( x )=\left\{\begin{matrix} {{1 ~ ~ ~ ~ i f ~ ~ x \geq0. 5}} \\ {{-1 ~ ~ i f ~ ~ x < 0. 5}} \\ \end{matrix} \right.
	\]</li>
<li>Compute constants: \(\text{err}\) and \(c=\log((1-\text{err})/\text{err})\)
\[
  \text{err} = w_1 = \frac{1}{4}, \\
  c = \log(\frac{1-1/4}{1/4}) = \log(3)
  \]</li>
<li>Output:
\[
  h(x) = \text{sign}(\log(2)h_2 (x) + \log(3) h_1 (x)) = h_1 (x)
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orga53209f" class="outline-4">
<h4 id="orga53209f"><span class="section-number-4">8.2.7.</span> AdaBoost round 3</h4>
<div class="outline-text-4" id="text-8-2-7">
<p>
\(m=3\)
</p>
<ul class="org-ul">
<li>Update weights:
\[
  w_1 \leftarrow \frac{1}{4} e^{\log(3)} = \frac{3}{4},
  w_2 \leftarrow \frac{1}{2},
  w_3 \leftarrow \frac{1}{4}
  \]
Renormalizing we have \(w_1 = \frac{1}{2}, w_2 = \frac{1}{3}, w_3 = \frac{1}{6}\)</li>
<li>Fit a best classifier
\[
	h_{3} ( x )=\left\{\begin{matrix} {{-1 ~ ~ ~ ~ i f ~ ~ x \geq-0. 5}} \\ {{1 ~ ~ i f ~ ~ x <- 0. 5}} \\ \end{matrix} \right.
	\]</li>
<li>Compute constants: \(\text{err}\) and \(c=\log((1-\text{err})/\text{err})\)
\[
  \text{err} = w_3 = \frac{1}{6} \\
  c = \log(\frac{1-1/6}{1/6}) = \log(5)
  \]</li>
<li>Output: \(h(x) = \text{sign}(\log(2) h_2 (x) + \log(3) h_1 (x) + \log(5) h_3 (x))\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgc356bc2" class="outline-4">
<h4 id="orgc356bc2"><span class="section-number-4">8.2.8.</span> Output classifier</h4>
<div class="outline-text-4" id="text-8-2-8">
<p>
At end of round 3,
</p>

\begin{equation}
h(x) =
\text{sign of}
\begin{cases}
        \log(2) - \log(3) + \log(5) & \text{if } x < -0.5 \\
        \log(2) - \log(3) - \log(5) & \text{if } -0.5 \leq x < 0.5 \\
        \log(2) + \log(3) + \log(5) & \text{if } x \geq 0.5 \\
\end{cases}
\end{equation}

<p>
\[
\left\{\begin{matrix} {{{1}}} & {{{i f x <-0. 5}}} \\ {{{-1}}} & {{{i f-0. 5 \leq x < 0. 5}}} \\ {{{1}}} & {{{i f x \geq0. 5}}} \\ \end{matrix} \right.
\]
</p>
</div>
</div>
<div id="outline-container-org1c9263c" class="outline-4">
<h4 id="org1c9263c"><span class="section-number-4">8.2.9.</span> AdaBoost output</h4>
<div class="outline-text-4" id="text-8-2-9">
<p>
<img src="./img/ada3.png" alt="ada3.png" />
The <b><b>strong learner</b></b> at \(M=3\):
\[
h(x) = \text{sign}(\sum^M_{m=1} c_m h_m (x)) =
\]
\[
\left\{\begin{matrix} {{{1}}} & {{{i f x <-0. 5}}} \\ {{{-1}}} & {{{i f-0. 5 \leq x < 0. 5}}} \\ {{{1}}} & {{{i f x \geq0. 5}}} \\ \end{matrix} \right.
\]
correctly classifies all 3 training data points.
</p>
</div>
</div>
</div>
<div id="outline-container-orge5e1231" class="outline-3">
<h3 id="orge5e1231"><span class="section-number-3">8.3.</span> Statistical view of AdaBoost</h3>
<div class="outline-text-3" id="text-8-3">
<p>
10.1.3
</p>
</div>
<div id="outline-container-org225ac18" class="outline-4">
<h4 id="org225ac18"><span class="section-number-4">8.3.1.</span> Problem setup</h4>
<div class="outline-text-4" id="text-8-3-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1}\)</li>
<li>Objective: find a function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset.</li>
<li>Trees: we have many tree classifiers \({h_m (x)}\) available to predict the response \(Y\) but each tree is only slightly better than random guessing</li>
<li>AdaBoost uses those weak learners to construct a very good classifier</li>
</ul>
</div>
</div>
<div id="outline-container-org189f8de" class="outline-4">
<h4 id="org189f8de"><span class="section-number-4">8.3.2.</span> AdaBoost algorithm</h4>
<div class="outline-text-4" id="text-8-3-2">
<ul class="org-ul">
<li>Final classifier: \(h(x) = \text{sign}(H_M (x))\) where</li>
</ul>

<p>
\[
H_{M} ( x )=\sum_{m=1}^{M} c_{m} h_{m} ( x )=H_{M-1} ( x )+c_{m} h_{m} ( x )
\]
</p>
<ul class="org-ul">
<li>At the m-th round, the function \(h_m(x)\) is chosen to minimize the error rate \(\text{err} = E_w I(Y\ne h(x))\)
<ul class="org-ul">
<li>And the coefficient \(c_m\) is chosen as \(c_m = \log(\frac{1- \text{err}_m}{\text{err}_m})\)</li>
</ul></li>
<li>What is the statistical interpretation of the choice of \((h_m (x), c_m)\)?</li>
</ul>
</div>
</div>
<div id="outline-container-org75c1f51" class="outline-4">
<h4 id="org75c1f51"><span class="section-number-4">8.3.3.</span> Statistical properties of AdaBoost</h4>
<div class="outline-text-4" id="text-8-3-3">
<ul class="org-ul">
<li>Theorem: AdaBoost builds an additive logistic regression model via gradient descent when minimizing the exponential loss function
<dl class="org-dl">
<dt>Additive logistic regression model</dt><dd>\[
		\log\frac{P ( Y=1 )} {1-P ( Y=1 )}=H ( \boldsymbol{x} )=\sum_{m=1}^{M} c_{m} h_{m} ( \boldsymbol{x} )
		\]</dd>
<dt>Exponential loss function</dt><dd>\[
		L ( Y, H )=e^{-Y H ( x )}
		\]</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org00085f6" class="outline-4">
<h4 id="org00085f6"><span class="section-number-4">8.3.4.</span> Exponential loss function</h4>
<div class="outline-text-4" id="text-8-3-4">
<ul class="org-ul">
<li><p>
When predicting \(Y\in{-1,1}\) by the \(\hat{Y} = \text{sign}(H(x))\) the standard 0-1 loss for classification is
</p>
\begin{equation}
L^{*}(Y,H) = I(Y\neq\hat{Y})
= \begin{cases}
0 & \text{if }YH(x) \ge 0 \\
1 & \text{if }YH(x) \lt 0
\end{cases}
\end{equation}</li>
<li>In AdaBoost, we consider the exponential loss, which is a surrogate function
\[
  L ( Y, H )=e^{-Y H ( x )}
  \]</li>
</ul>

<div id="org99b1841" class="figure">
<p><img src="./img/ada-loss.png" alt="ada-loss.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org121bdad" class="outline-4">
<h4 id="org121bdad"><span class="section-number-4">8.3.5.</span> Optimization problem</h4>
<div class="outline-text-4" id="text-8-3-5">
<p>
Given the current \(H(x)\) we want to add \(ch(x)\) that minimizes the exponential loss function
\[
E \left( e^{-Y \left( H ( x )+c h ( x ) \right)} \right)
\]
</p>
<dl class="org-dl">
<dt>What is the optimal function \(h(x)\)</dt><dd>gradient descent</dd>
<dt>Given \(H(x) \text{and } h(x)\), what is the optimal value for \(c\)?</dt><dd>Direct optimization over the re-weighted data</dd>
</dl>
</div>
</div>
<div id="outline-container-org47a6bae" class="outline-4">
<h4 id="org47a6bae"><span class="section-number-4">8.3.6.</span> Optimization in function</h4>
<div class="outline-text-4" id="text-8-3-6">
<p>
Given the current \(H(x)\), by Taylor Series expansion, when adding \(ch(x)\),
</p>
\begin{aligned} {{{\mathbf{E} \left( e^{-Y \left( H ( x )+c h ( x ) \right)} \right)}}} & {{} {{} {\approx \mathbf{E} \left( e^{-Y H ( x )} \left( 1-c Y h ( x )+{\frac{c^{2} Y^{2} h ( x )^{2}} {2}} \right) \right)}}} \\ {{{}}} & {{} {{} {{}=\mathbf{E} \left( e^{-Y H ( x )} \left( 1-c Y h ( x )+{\frac{c^{2}} {2}} \right) \right) \quad\mathbf{s i n c e} \ Y h ( x )=\pm1}}} \\ {{{}}} & {{} {{} {{}=\mathbf{E}_{\mathbf{w}} \left( 1-c Y h ( x )+{\frac{c^{2}} {2}} \right)}}} \\ \end{aligned}
<ul class="org-ul">
<li>Gradient descent finds \(h(x) \in \pm 1\) that minimizes
\[
  \frac{1} {2} \mathbf{E}_{\mathbf{w}} \big( 1-Y h ( x ) \big)=\mathbf{P}_{\mathbf{w}} ( Y h ( x )=-1 )=\mathbf{P}_{\mathbf{w}} \big( Y \neq h ( x ) \big)= \text{err}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org071cb2b" class="outline-4">
<h4 id="org071cb2b"><span class="section-number-4">8.3.7.</span> Optimization in coefficient</h4>
<div class="outline-text-4" id="text-8-3-7">
<ul class="org-ul">
<li><p>
Given the \(H(x) \text{ and } h(x)\), we can directly find the coefficient \(c\) that minimizes the risk
</p>
\begin{array} {c} {{{\mathbf{E} \bigl( e^{-Y \bigl( H ( \boldsymbol{x} )+c h ( \boldsymbol{x} ) \bigr)} \bigr)=\mathbf{E}_{\mathbf{w}} ( e^{-c Y h ( \boldsymbol{x} )} )}}} \\ {{{=e^{c} \, \mathbf{P}_{\mathbf{w}} ( Y h ( \boldsymbol{x} )=-1 )+e^{-c} \mathbf{P}_{\mathbf{w}} ( Y h ( \boldsymbol{x} )=1 )}}} \\ {{{=e^{c} \, * \, \text{err}+e^{-c} \, * \, ( 1-\text{err} )}}} \\ \end{array}</li>
<li>Taking derivative with respect to \(c\) and setting to \(0\), the risk is minimized at
\[
  c = \frac{1}{2}\log\frac{1-\text{err}}{\text{err}}
  \]</li>
<li>This value is similar to \(c_m\) in AdaBoost, with the exception of adding factor \(\frac{1}{2}\)</li>
</ul>
</div>
</div>
<div id="outline-container-org3ea75ca" class="outline-4">
<h4 id="org3ea75ca"><span class="section-number-4">8.3.8.</span> Implementation update</h4>
<div class="outline-text-4" id="text-8-3-8">
<ul class="org-ul">
<li>By gradient descent, the optimal update should be
\[
	H ( x ) \gets H ( x )+{\frac{1} {2}} \text{log} {\frac{1-\text{err}} {\text{err}}} \, h ( x ) \,, \qquad\quad w \gets w e^{-c Y h ( x )}
	\]</li>
<li>A simpler implementation:
<ul class="org-ul">
<li>Since \(-Yh(x) = 2I(Y\ne h(x))-1\), the update is equivalent to
\[
    w \leftarrow w \exp[\log\frac{1-\text{err}}{\text{err}} I(Y\ne h(x))]
    \]</li>
<li>The sign function is the same if
\(H(x) \leftarrow H(x) + \log \frac{1-\text{err}}{\text{err}} h(x)\)</li>
</ul></li>
<li>These are of an identical form to those in AdaBoost.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org94b1740" class="outline-3">
<h3 id="org94b1740"><span class="section-number-3">8.4.</span> General boosting algorithm</h3>
<div class="outline-text-3" id="text-8-4">
<p>
10.2.1
</p>
</div>
<div id="outline-container-orgd0c6a13" class="outline-4">
<h4 id="orgd0c6a13"><span class="section-number-4">8.4.1.</span> Problem setup</h4>
<div class="outline-text-4" id="text-8-4-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\)</li>
<li>Objective: find a function \(h(x_{\text{new}}) = h(x_1, ..., x_p)\) that can predict \(Y\) well for any given input \(x_{\text{new}} = (x_1, ..., x_p)\) in the testing dataset.</li>
<li>Base learners: a set of predictions, e.g. trees
\[
  {h_m(x) = b(x, \gamma_m)}
  \]</li>
<li>AdaBoost: combines the base learners to construct a better procedure for <span style='background-color: #FFFF00;'>binary classification</span>.</li>
<li>Can we extend the idea to other contexts?</li>
</ul>
</div>
</div>
<div id="outline-container-org424cc03" class="outline-4">
<h4 id="org424cc03"><span class="section-number-4">8.4.2.</span> Recall regression models</h4>
<div class="outline-text-4" id="text-8-4-2">
<p>
The models for regression and classification are closely related:
</p>
<ol class="org-ol">
<li>Linear regression model e.g. variable selection
\[
	Y=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{p} x_{p}+\epsilon
	\]</li>
<li>Generalized linear model e.g. logistic regression
\[
   \mathrm{g} \big( \mathrm{E} ( Y | \boldsymbol{x} ) \big)=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{p} x_{p}
   \]</li>
<li>Additive model: local smoothing and backfitting algorithm
\[
	 Y=\beta_{0}+\beta_{1} f_{1} ( x_{1} )+\cdots+\beta_{p} f_{p} ( x_{p} )+\epsilon
	 \]</li>
</ol>
<p>
Can we extend the bases of \(x_i\) of \(f_i(x_i)\) to a broad family, e.g. \(b(x, \gamma_m)\)?
</p>
</div>
</div>
<div id="outline-container-org06d15c2" class="outline-4">
<h4 id="org06d15c2"><span class="section-number-4">8.4.3.</span> Extended additive model</h4>
<div class="outline-text-4" id="text-8-4-3">
<p>
Suppose we have a family of base learners $  {h<sub>m</sub>(x) = b(x, &gamma;<sub>m</sub>)}$ e.g., the base learner \(b(x, \gamma_m)\) can be a tree with vector parameter \(\gamma_m\)
</p>
<ul class="org-ul">
<li>Treat the base learners as new independent variables or features</li>
<li>Extended additive model is based on the new variables:
<dl class="org-dl">
<dt>Regression</dt><dd>\(\mathbf{Y}=\beta_{0}+\beta_{1} b ( x, \gamma_{1} )+\cdots+\beta_{M} b ( x, \gamma_{M} )+\epsilon\)</dd>
<dt>Classification</dt><dd>\(\mathbf{g} \big( \mathbf{E} ( \mathbf{Y} | \mathbf{x} ) \big)=\boldsymbol{\beta}_{0}+\boldsymbol{\beta}_{1} b ( x, \gamma_{1} )+\cdots+\boldsymbol{\beta}_{M} b ( x, \gamma_{M} )\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-org022397b" class="outline-4">
<h4 id="org022397b"><span class="section-number-4">8.4.4.</span> Forward stagewise additive model</h4>
<div class="outline-text-4" id="text-8-4-4">
<p>
The forward stagewise additive model is to sequentially fit the extended additive model
\[
\widehat{\mathrm{Y}}=H_{M} ( \boldsymbol{x} )=\beta_{0}+\beta_{1} b ( \boldsymbol{x}, \gamma_{1} )+\cdots+\beta_{M} b ( \boldsymbol{x}, \gamma_{M} )
\]
</p>
<ol class="org-ol">
<li>Initialize \(H_0 (x) = 0\) for all \(x\)</li>
<li>For \(m=1\) to \(M\):
<ol class="org-ol">
<li>Optimize the empirical training risk
		  \[
			( \beta_{m}, \gamma_{m} )=\text{argmin}_{\beta, \gamma} \sum_{i=1}^{n} L \big( Y_{i}, H_{m-1} ( x_{i} )+\beta b ( x_{i}, \gamma) \big).
			\]
Here, \(L \big( Y, h ( x ) \big)=\exp \big(-Y h ( x ) \big) ; \ \big( Y-h ( x ) \big)^{2}\)</li>
<li>Update the output
\(H_{m} ( \boldsymbol{x} )=H_{m-1} ( \boldsymbol{x} )+\beta_{m} b ( \boldsymbol{x}, \gamma_{m} )\)</li>
</ol></li>
</ol>
</div>
</div>
<div id="outline-container-org15d3ac3" class="outline-4">
<h4 id="org15d3ac3"><span class="section-number-4">8.4.5.</span> Shrinkage parameter</h4>
<div class="outline-text-4" id="text-8-4-5">
<ul class="org-ul">
<li>With the optimized \((\beta_m, \gamma_m)\) at each round, is it near-sighted to update the output \(H_m(x) = H_{m-1}(x) + \beta_m b(x, \gamma_m)\) ?</li>
<li>One idea is to update the output function <span style='background-color: #FFFF00;'>slowly</span> by adding a shrinkage parameter \(\lambda \in (0,1]\):
\[
  H_m(x) = H_{m-1} (x) + \lambda \beta_m b(x, \gamma_m)
  \]</li>
<li>The shrinkage parameter \(\lambda\) controls the rate at which the model learns</li>
</ul>
</div>
</div>
<div id="outline-container-org7cdabcf" class="outline-4">
<h4 id="org7cdabcf"><span class="section-number-4">8.4.6.</span> General boosting algorithm</h4>
<div class="outline-text-4" id="text-8-4-6">
<ul class="org-ul">
<li>In the regression context, the General Boosting Algorithm is to sequentially fir the Extended Additive Model through base learners such as trees:
\[
  \hat{Y} = H_M(x) = \beta_0 + \beta_1 b(x, \gamma_1) + ... + \beta_M b(x, \gamma_M)
  \]
<ol class="org-ol">
<li>Initialize $H<sub>0</sub>(x) =0 $ for all \(x\) and setn \(r_i = Y_i\) for all $i in the training data</li>
<li>For \(m=1\) to \(M\):
<ol class="org-ol">
<li>Fit a tree \(b(x, \gamma_m)\) to the training data \((x_i, r_i)^n_{i=1}\)</li>
<li>Update the prediction by adding a shunken version of the tree
\(\hat{H} \leftarrow \hat{H} + \lambda b (x, \gamma_m)\)</li>
<li>Update the residuals \(r_i \leftarrow r_i - \lambda b(x, \gamma_m)\)</li>
</ol></li>
<li>Output the final boosting model \(\hat{H} = \sum^M_{m=1} \lambda b(x, \gamma_m)\)</li>
</ol></li>
</ul>
</div>
</div>
<div id="outline-container-org05b832a" class="outline-4">
<h4 id="org05b832a"><span class="section-number-4">8.4.7.</span> Parameter tuning</h4>
<div class="outline-text-4" id="text-8-4-7">
<p>
How to tune the parameters in the general boosting algorithm:
</p>
<dl class="org-dl">
<dt>M</dt><dd>the number of iterations or the number of base trees</dd>
<dt>\(b(x, \gamma)\)</dt><dd>the form of trees, e.g. interaction variables for splitting</dd>
<dt>\(\lambda\)</dt><dd>the shrinkage parameter</dd>
</dl>

<p>
Cross-validation!
</p>
</div>
</div>
</div>
<div id="outline-container-orgee2c1f4" class="outline-3">
<h3 id="orgee2c1f4"><span class="section-number-3">8.5.</span> Boosting in R</h3>
<div class="outline-text-3" id="text-8-5">
<p>
See:
</p>
<ul class="org-ul">
<li><code>10.2.2_Boosting.R_FW.pdf</code></li>
<li><code>10.2.3_R_example.Boosting_FW.pdf</code></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org7fe807d" class="outline-2">
<h2 id="org7fe807d"><span class="section-number-2">9.</span> Week 12: Advanced Supervised Learning</h2>
<div class="outline-text-2" id="text-9">
<p>
Support Vector Machine in Linear Scenario.
(Week 11 is Spring Break)
</p>
</div>
<div id="outline-container-org8075801" class="outline-3">
<h3 id="org8075801"><span class="section-number-3">9.1.</span> Introduction to Support Vector Machines</h3>
<div class="outline-text-3" id="text-9-1">
</div>
<div id="outline-container-org2f78a8d" class="outline-4">
<h4 id="org2f78a8d"><span class="section-number-4">9.1.1.</span> Binary classification problem</h4>
<div class="outline-text-4" id="text-9-1-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) where \(Y_i\) is <b><b>binary</b></b></li>
<li>Linear classifier: classify binary response \(Y\) depending on whether \(h(x) = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p = \beta_0 + \beta^T x\) is large or not</li>
<li>Question: how to estimate \((\beta_0, \beta_1, ..., \beta_p)\) from the training data?
<dl class="org-dl">
<dt>Linear discriminant analysis</dt><dd><b><b>testing hypothesis</b></b> after modelling \(x_{i1}, x_{i2}, ..., x_{ip}\) as multivariate normal distribution \(\text{MVN}(\mu_i, \Sigma)\)</dd>
<dt>Logistic regression</dt><dd><b><b>maximum likelihood estimator</b></b> after modelling the relationship between \(P(Y_i=1)\) and the function \(h(x)\)</dd>
<dt>Support vector machine</dt><dd><b><b>optimization-based</b></b>, with \(Y_i \in {-1,1}\)</dd>
</dl></li>
</ul>
</div>
</div>
<div id="outline-container-orgba6556d" class="outline-4">
<h4 id="orgba6556d"><span class="section-number-4">9.1.2.</span> Toy motivating example</h4>
<div class="outline-text-4" id="text-9-1-2">
<ul class="org-ul">
<li>Data: \((Y_i, X_{i1}, X_{i2})\) for \(i=1, ..., n=60\). Here, \(Y_i \in {-1,1}\) are class labels</li>
<li><p>
True generative model:
</p>
\begin{equation}
X_{i2} =
\begin{cases}
2X_{i1} + 1 + \epsilon_i & \text{ if } Y_i=1 \\
2X_{i1} - 1 + \epsilon_i & \text{ if } Y_i=-1 \\
\end{cases}
\text{where }\epsilon_i \text{ are iid N}(0, 0.3^2)
\end{equation}</li>
<li>Objective: Suppose we don't know the true generative model, how do we use the training data to predict the class?</li>
</ul>
</div>
</div>
<div id="outline-container-org9138eca" class="outline-4">
<h4 id="org9138eca"><span class="section-number-4">9.1.3.</span> Good separate line?</h4>
<div class="outline-text-4" id="text-9-1-3">

<div id="org98b00fd" class="figure">
<p><img src="./img/good-sep.png" alt="good-sep.png" />
</p>
</div>
<ul class="org-ul">
<li>If we want to separate two classes along the line \(X_2 + 2X_1 + c\), this yields the classifier of form \(\text{sign}(X_2 - 2X_1 - c)\)
<ul class="org-ul">
<li>The two boundary lines are:
\[
    X_2 = 2X_1 + 0.607 \\
    X_2 = 2X_1 - 0.590
    \]</li>
<li>The distance between the two boundary lines is
\[
    (0.607 + 0.590) / \sqrt{2^2+1} = 0.535
    \]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf6d0041" class="outline-4">
<h4 id="orgf6d0041"><span class="section-number-4">9.1.4.</span> Poor separation</h4>
<div class="outline-text-4" id="text-9-1-4">
<p>
What if we change the line direction; and instead, consider classifier of the form:
\[
\text{sign}(X_2 - X_1 -c)
\]
</p>
<ul class="org-ul">
<li>The two boundary lines are thus:
\[
  X_2 = X_1 + 0.725 \\
  X_2 = X_1 + 0.198
  \]</li>
<li>The <b><b>distance</b></b> between these 2 boundary lines is thus
\[
  (0.725-0.198) / \sqrt {1^2+1} = 0.373
  \]</li>
<li>The separation is thus <b><b>smaller</b></b> and likely leads to <b><b>larger misclassification error</b></b> on testing data</li>
</ul>
</div>
</div>
<div id="outline-container-org0ad635f" class="outline-4">
<h4 id="org0ad635f"><span class="section-number-4">9.1.5.</span> Margin maximization</h4>
<div class="outline-text-4" id="text-9-1-5">
<ul class="org-ul">
<li>Support vector machine: main idea is to find a line direction that <b><b>maximizes the margin</b></b> of the training data</li>
<li>Margin: the distance between the two boundary lines of two classes</li>
</ul>
</div>
</div>
<div id="outline-container-orgf32f435" class="outline-4">
<h4 id="orgf32f435"><span class="section-number-4">9.1.6.</span> Distance in 2 dimensions</h4>
<div class="outline-text-4" id="text-9-1-6">

<div id="orgcaf454b" class="figure">
<p><img src="./img/distance-in-2d.png" alt="distance-in-2d.png" />
</p>
</div>
<ul class="org-ul">
<li>In the 2-dimensional plane, the distance from a point \((x_{1, \text{new}}, x_{2,\text{new}})\) is:
\[
  d = \frac{|\beta_0 - \beta_0^{*}|}{\sqrt{\beta_1^2 + \beta^2_2}}
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgfe86601" class="outline-4">
<h4 id="orgfe86601"><span class="section-number-4">9.1.7.</span> Distance from point to hyperplane</h4>
<div class="outline-text-4" id="text-9-1-7">
<p>
In the \(R^{\mathbb{p}}\) space, the <b><b>distance</b></b> from a point
\[
x_\text{new} = (x_{1, \text{new}}, x_{2, \text{new}}, ..., x_{\text{p, new}})^T
\]
to thy hyperplane
\[
\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = 0 ~ ~ ~ \text{i.e.: }\beta_0 + \beta^T x = 0
\]
is:
\[
d = \frac{|\beta_0 + \beta_1 x_{1, \text{new}} + \beta_2 x_{2,\text{new}} + ... + \beta_p x_{p, \text{new}}|}{\sqrt{\beta^2_1 + \beta^2_2 + ... + \beta^2_p}} \\
= \frac{|\beta_0 + \beta^T x_{\text{new}}}{|\beta|}
\]
</p>
</div>
</div>
<div id="outline-container-org5b23828" class="outline-4">
<h4 id="org5b23828"><span class="section-number-4">9.1.8.</span> Distance between hyperplanes</h4>
<div class="outline-text-4" id="text-9-1-8">
<ul class="org-ul">
<li>In the \(R^{\mathbb{p}}\) space, the distance between two hyperplanes
\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = c_1 ~ ~ ~ \text{i.e.: }\beta_0 - c_1 + \beta^T x = 0 \\
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = c_2 ~ ~ ~ \text{i.e.: }\beta_0 - c_2 + \beta^T x = 0
  \]
is
\[
  d = \frac{|c_1 - c_2|}{\sqrt{\beta^2_1 + \beta^2_2 + ... + \beta^2_p}} = \frac{|c_1 - c_2|}{|\beta|}
  \]</li>
<li>In the special case of \(c_1 = -1\) and \(c_2 = 1\), the margin or distance of the two hyperplanes is
\[
  d = \frac{|-1-1}{\sqrt{\beta^2_1 + \beta^2_2 + ... + \beta^2_p}} = \frac{2}{|\beta|}
  \]</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgffc01f8" class="outline-3">
<h3 id="orgffc01f8"><span class="section-number-3">9.2.</span> Maximum Margin Optimization for SVM</h3>
<div class="outline-text-3" id="text-9-2">
<p>
11.1.2
</p>
</div>
<div id="outline-container-orgf5d5389" class="outline-4">
<h4 id="orgf5d5389"><span class="section-number-4">9.2.1.</span> Linear classification</h4>
<div class="outline-text-4" id="text-9-2-1">
<ul class="org-ul">
<li>Training dataset: - Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1}\)</li>
<li><p>
Linear classification: find the linear function
\[
  h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta_0 + \beta^T x
  \]
and predict the binary response of any new data by:
</p>
\begin{equation}
      \text{sign} \big( h ( x ) \big) = \text{sign} \big( \beta_{0}+\beta^{T} x \big)=
\begin{cases} -1 & \text{ if } \beta_0+\beta^{T} x \leq 0 \\
1 & \text{ if } \beta_0+\beta^{T} x > 0
\end{cases}
      \end{equation}</li>
<li>Different methods to estimate \(\beta_0\) and \(\beta\) from training data:
<ul class="org-ul">
<li>Linear discriminant analysis</li>
<li>Logistic regression</li>
<li>Support vector machine (when <span style='background-color: #FFFF00;'>linearly separable</span>)</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org580450e" class="outline-4">
<h4 id="org580450e"><span class="section-number-4">9.2.2.</span> Linear SVM</h4>
<div class="outline-text-4" id="text-9-2-2">

<div id="orgbf21758" class="figure">
<p><img src="./img/svm-2-lines.png" alt="svm-2-lines.png" />
</p>
</div>
<ul class="org-ul">
<li>Linear SVM in the <span style='background-color: #FFFF00;'>linearly separable scenario</span></li>
<li>Given the direction \(\beta = (\beta_1, \beta_2, ..., \beta_p)\), there are 3 lines / hyperplanes of the form
\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = 0
  \]
Not unique, e.g. multiplying 3 both sides</li>
<li>For uniqueness, we assume <b><b>two boundary lines</b></b> of two classes are defined by
\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = - 1 \\
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = 1
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgb283ecb" class="outline-4">
<h4 id="orgb283ecb"><span class="section-number-4">9.2.3.</span> Margin computation</h4>
<div class="outline-text-4" id="text-9-2-3">
<p>
<img src="./img/svm-margin-compute.png" alt="svm-margin-compute.png" />
Margin =
\[
\frac{2}{\parallel\beta\parallel} =
\frac{2}{\sqrt{\beta^2_1 + ... + \beta^2_p}}
\]
</p>
</div>
</div>
<div id="outline-container-org1d1066c" class="outline-4">
<h4 id="org1d1066c"><span class="section-number-4">9.2.4.</span> Maximum margin optimization</h4>
<div class="outline-text-4" id="text-9-2-4">
<p>
Consider the scenario when the training data are linearly separable.
</p>
<ul class="org-ul">
<li>Linear SVM classifier:
\[
  \hat{Y}   = \text{sign}(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p)
  \]</li>
<li>Objective: maximize the margin
\[
  \frac{2}{\parallel\beta\parallel} =
  \frac{2}{\sqrt{\beta^2_1 + ... + \beta^2_p}}
  \]</li>
<li>This translates to <span style='background-color: #FFFF00;'>low prediction error</span> on new data in test data</li>
</ul>
</div>
</div>
<div id="outline-container-org33950ff" class="outline-4">
<h4 id="org33950ff"><span class="section-number-4">9.2.5.</span> Constraints for SVM</h4>
<div class="outline-text-4" id="text-9-2-5">
<ul class="org-ul">
<li>Linear SVM classifier:
<ul class="org-ul">
<li>Constraints: classify all training data correctly in the scenario when the training data is linearly separable:
<ul class="org-ul">
<li>When \(Y_i = -1 \rightarrow \beta_0 + \beta^T x_i \le -1\)</li>
<li>When \(Y_I = 1 \rightarrow \beta_0 + \beta^T x_i \ge 1\)</li>
</ul></li>
<li>The constraints can be written simply as:
\[
    Y_i(\beta_0 + \beta^T x_i) \ge 1 \text{ for all }i=1,2,...,n
    \]</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2913d3e" class="outline-4">
<h4 id="org2913d3e"><span class="section-number-4">9.2.6.</span> Mathematical formulation</h4>
<div class="outline-text-4" id="text-9-2-6">
<p>
Formula of SVM in the scenario when training data are linearly separable:
</p>
<ul class="org-ul">
<li>Problem 0: find the linear function, or hyperplane below
\[
  h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta_0 + \beta^T x
  \]
that <span style='background-color: #FFFF00;'>maximizes</span> margin:
\[
  \max_{\beta, \beta_0} \frac{2}{\parallel \beta \parallel}
  \]
subject to the constraint on classifying training data well:
\[
  Y_i(\beta_0 + \beta^T x_i) \ge 1 \text{ for all} i=1,2,...,n
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgdad2a2a" class="outline-4">
<h4 id="orgdad2a2a"><span class="section-number-4">9.2.7.</span> A simplified formulation</h4>
<div class="outline-text-4" id="text-9-2-7">
<p>
In the scenario when training data are linearly separable:
</p>
<ul class="org-ul">
<li>Problem 1 (Primal form): Find the linear function, or hyperplane, that <span style='background-color: #FFFF00;'>minimizes the quadratic objective function</span>
\[
  \min_{\beta, \beta_0} (\frac{1}{2} \parallel \beta \parallel^2)
  \]
subject to the linear constraints
\[
  Y_i(\beta_0 + \beta^T x_i) \ge 1 \text{ for all} i=1,2,...,n
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgac4c522" class="outline-4">
<h4 id="orgac4c522"><span class="section-number-4">9.2.8.</span> Constrained optimization</h4>
<div class="outline-text-4" id="text-9-2-8">
<p>
By <span style='background-color: #FFFF00;'>Lagrange multipliers</span>, we reformulate the constrained primal problem into (unconstrained) dual space.
</p>
<ul class="org-ul">
<li>Problem 2: find \((\beta_0, \beta)\) that minimizes the objective function
\[
  g = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
where \(\lambda_i \ge 0\)  are the lagrange multipliers</li>
<li>This is a quadratic optimizaton problem which can be solved conveniently. This gives the SVM in the linearly separable scenario.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org391cda9" class="outline-3">
<h3 id="org391cda9"><span class="section-number-3">9.3.</span> SVM for Linearly Separable</h3>
<div class="outline-text-3" id="text-9-3">
<p>
11.1.3
</p>
</div>
<div id="outline-container-orgcc540ed" class="outline-4">
<h4 id="orgcc540ed"><span class="section-number-4">9.3.1.</span> Linear SVM</h4>
<div class="outline-text-4" id="text-9-3-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1}\)</li>
<li>Support vector machine: predict \(\hat{Y}
  = \text{sign}(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p)\)</li>
<li>In the linearly separable scenario, we assume two boundary hyperplanes are
\[
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = - 1 \\
  \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = 1
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-orgff05d6f" class="outline-4">
<h4 id="orgff05d6f"><span class="section-number-4">9.3.2.</span> Maximum margin optimization</h4>
<div class="outline-text-4" id="text-9-3-2">
<p>
Maximum margin optimization formulation of SVM when the training data are linearly separable.
</p>
<ul class="org-ul">
<li>Problem 1 (Primal form): Find the linear function (or hyperplane) that achieves
\[
  \max{{\beta_0, \beta}} \frac{2}{\parallel \beta \parallel}
  \]
i.e.
\[
  \min_{\beta_0, \beta} (\frac{1}{2} \parallel \beta \parallel^2)
  \]
subject to the constraints on classifying the training data well:
\[
  Y_i (\beta_0 + \beta^T x_i) \ge 1 \text{ for all }i=1,2,...,n
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org1079b02" class="outline-4">
<h4 id="org1079b02"><span class="section-number-4">9.3.3.</span> Unconstrained optimization</h4>
<div class="outline-text-4" id="text-9-3-3">
<p>
By Lagrange multipliers, we reformulate the constrained primal problem into unconstrained dual space.
</p>
<ul class="org-ul">
<li>Problem 2: find \(\beta_0, \beta\) that minimizes the objective function
\[
  g = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
where \(\alpha_i \ge 0\) are the Lagrange multipliers</li>
<li>Let us present some technical details on this unconstrained quadratic optimization problem</li>
</ul>
</div>
</div>
<div id="outline-container-org74e60e6" class="outline-4">
<h4 id="org74e60e6"><span class="section-number-4">9.3.4.</span> Local minimum</h4>
<div class="outline-text-4" id="text-9-3-4">
<ul class="org-ul">
<li>Taking derivatives of
\[
  g = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
yields
\[
  \frac{\partial g}{\partial \beta_0} = -\sum^n_{i=1} \alpha_i Y_i \\
  \frac{\partial g}{\partial \beta} = \beta - \sum^n_{i=1} \alpha_i Y_i x_i
  \]</li>
<li>Thus a local minimum must satisfy
\[
  \sum^n_{i=1} \alpha_i Y_i = 0 \text{ and } \beta = \sum^n_{i=1} \alpha_i Y_i x_i
  \]</li>
</ul>
</div>
</div>
<div id="outline-container-org47d9718" class="outline-4">
<h4 id="org47d9718"><span class="section-number-4">9.3.5.</span> KKT necessary condition</h4>
<div class="outline-text-4" id="text-9-3-5">
<ul class="org-ul">
<li>Karush-Kuhn-Tucker (or KKT) necessary condition for local minima: tat the point of the solutions the product between dual variables and constraints has to vanish</li>
<li>For our objective function
\[
  g = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
by KKT, there exists \((\alpha_1^* ..., \alpha_n^*)\), called KKT multiplier, such that:
<ul class="org-ul">
<li>\(\alpha_i^{*} = 0\) whenever \(Y_i(\beta_0 + \beta^T x_i) \gt 1\)</li>
<li>\(\alpha_i^{*} \gt 0\) only if \(Y_i (\beta_0 + \beta^T x_i) = 1\) i.e. \(\beta_0 + \beta^T x_i = Y_i \in \pm 1\)</li>
<li>The vectors \(x_i\) for which \(\alpha_i^{*} > 0\) are called support vectors as they belong to the boundary hyperplanes</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org83f217d" class="outline-4">
<h4 id="org83f217d"><span class="section-number-4">9.3.6.</span> Objective function in dual space</h4>
<div class="outline-text-4" id="text-9-3-6">
<ul class="org-ul">
<li><p>
Plugging \(\sum^n{i=1} \alpha_i Y_i = 0\) and \(\beta = \sum^n_{i=1} \alpha_i Y_i x_i\) into the objective function
\[
  g_{\text{dual}} = \frac{1}{2} \parallel \beta \parallel^2 - \sum^n_{i=1} \lambda_i [Y_i (\beta_0 + \beta^T x_i) -1]
  \]
</p>

<p>
\[
	= {\frac{1} {2}} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}-\beta_{0} \sum_{i=1}^{n} \alpha_{i} Y_{i}-\sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}+\sum_{i=1}^{n} \alpha_{i}
	\]
</p>

<p>
\[
	= \sum_{i=1}^{n} \alpha_{i}-\frac{1} {2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}
	\]
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org37ed284" class="outline-4">
<h4 id="org37ed284"><span class="section-number-4">9.3.7.</span> Dual space formulation of SVM</h4>
<div class="outline-text-4" id="text-9-3-7">
<ul class="org-ul">
<li>Problem 3: Find the \(\alpha_i\) that minimizes
      \[
	g=\frac{1} {2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} Y_{i} Y_{j} x_{i}^{T} x_{j}-\sum_{i=1}^{n} \alpha_{i}
	\]
subject to these 2 families of constraints
      \[
	\sum_{i=1}^{n} \alpha_{i} Y_{i}=0 \quad \text{and} \quad\alpha_{i} \geq0, \forall i=1, \cdots, n
	\]
<ul class="org-ul">
<li>Standard quadratic programming algorithms can solve the \(\alpha_i\) 's</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org134274f" class="outline-4">
<h4 id="org134274f"><span class="section-number-4">9.3.8.</span> The SVM classifier</h4>
<div class="outline-text-4" id="text-9-3-8">
<p>
Once we determine the \(\alpha_i\), we have
</p>
<ul class="org-ul">
<li>\(\widehat{\beta} = \sum^n_{i=1} \alpha_i Y_i x_i\) which is a p-dimensional vector</li>
<li>By KKT, for \(\alpha_i \ne 0\), we have \(Y_i (\beta_0 + \beta^T x_i)=1\). Multiplying both sides by \(Y_i \in {-1,1}\) we have \(\widehat{\beta_0} = Y_i - \beta^T x_i\)</li>
<li><p>
Since most \(\alpha_i\) are 0, the SVM classifier can be rewritten as
</p>

<p>
\[
	h(x)=\text{sign}\big( \widehat{\beta}_{0}+\widehat{\beta}^{T} \boldsymbol{x} \big)=\text{sign} \left( \widehat{\beta}_{0}+\sum_{i=1}^{n} \alpha_{i} Y_{i} x_{i}^{T} \boldsymbol{x} \right)
	\]
</p>

<p>
\[
	\widehat{\beta}_{0}+\sum_{j=1}^{| s |} \alpha_{s_{j}} Y_{s_{j}} x_{s_{j}}^{T} x
	\]
</p>

<p>
if we eliminate the non-zero terms
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6a1d31a" class="outline-3">
<h3 id="org6a1d31a"><span class="section-number-3">9.4.</span> Slack variables for linearly non-separable</h3>
<div class="outline-text-3" id="text-9-4">
<p>
11.2.1
</p>
</div>
<div id="outline-container-orgc316857" class="outline-4">
<h4 id="orgc316857"><span class="section-number-4">9.4.1.</span> Linear classification</h4>
<div class="outline-text-4" id="text-9-4-1">
<ul class="org-ul">
<li>Data: \((Y_i, x_{i,1}, x_{i,2}, ..., x_{i,p}), i=1,2,...,n\) with \(Y_i \in {-1,1}\)</li>
<li><p>
Linear classification: find the linear function
\[
  h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta_0 + \beta^T x
  \]
where we classify new data based on
</p>
\begin{equation}
      \text{sign} \big( h ( x ) \big) = \text{sign} \big( \beta_{0}+\beta^{T} x \big)=
\begin{cases} -1 & \text{ if } \beta_0+\beta^{T} x \leq 0 \\
1 & \text{ if } \beta_0+\beta^{T} x > 0
\end{cases}
      \end{equation}</li>
<li>It's desirable to find a classifier that <span style='background-color: #FFFF00;'>maximizes margin</span>, which translates to a <span style='background-color: #FFFF00;'>lower prediction error</span> on new test data</li>
</ul>
</div>
</div>
<div id="outline-container-org3f58ad1" class="outline-4">
<h4 id="org3f58ad1"><span class="section-number-4">9.4.2.</span> Linearly separable scenario</h4>
<div class="outline-text-4" id="text-9-4-2">
<p>
See <a href="#orgb283ecb">9.2.3</a>
</p>
</div>
</div>
<div id="outline-container-orgaf593a9" class="outline-4">
<h4 id="orgaf593a9"><span class="section-number-4">9.4.3.</span> Linearly non-separable scenario</h4>
<div class="outline-text-4" id="text-9-4-3">
<p>
<img src="./img/non-sep.png" alt="non-sep.png" />
What if we cannot perfectly classify training data?
</p>
</div>
</div>
<div id="outline-container-org9880898" class="outline-4">
<h4 id="org9880898"><span class="section-number-4">9.4.4.</span> Slack variable</h4>
<div class="outline-text-4" id="text-9-4-4">
<ul class="org-ul">
<li>When the data are linearly non-separable, we can <b><b>soften</b></b> the margins by introducing <span style='background-color: #FFFF00;'>slack</span> variables in constraints</li>
<li>To be more specific, when the hard constraints
\[
  Y_i (\beta_0 + \beta^T x_i) \ge 1
  \]
cannot be satisfied for all \(i\), we replace them by the soft constraint
\[
  Y_i (\beta_0 + \beta^T x_i) - + \xi_i \ge 0, ~ ~ \xi_i \ge 0
  \]
where the new \(\xi_i\) are the <span style='background-color: #FFFF00;'>slack variables</span></li>
</ul>
</div>
</div>
<div id="outline-container-orgf204993" class="outline-4">
<h4 id="orgf204993"><span class="section-number-4">9.4.5.</span> Implication of slack variables</h4>
<div class="outline-text-4" id="text-9-4-5">
<p>
Let the slack variable \(\xi_i \ge 0\)  be the <span style='background-color: #FFFF00;'>smallest value</span> satisfying the constraints
\[
Y_i (\beta_0 + \beta^T x_i) - + \xi_i \ge 0
\]
There are two scenarios:
</p>
<dl class="org-dl">
<dt>Inequality scenario</dt><dd>\(Y_i (\beta_0 + \beta^T x_i) -1 + \xi_i \gt 0\) for <span style='background-color: #FFFF00;'>all</span> \(\xi_i \ge 0\). The smallest slack variable \(\xi_i =0\) and \(Y_i(\beta_0 + \beta^T x_i)>1\)</dd>
<dt>Equality scenario</dt><dd>\(Y_i(\beta_0 + \beta^T x_i) -1 + \xi_i = 0\) for <span style='background-color: #FFFF00;'>some</span> \(\xi_i \ge 0\). Then, \(Y_i(\beta_0+\beta^T x_i)=1-\xi_i\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org42c6a3e" class="outline-4">
<h4 id="org42c6a3e"><span class="section-number-4">9.4.6.</span> Equality scenario</h4>
<div class="outline-text-4" id="text-9-4-6">
<p>
There are 3 subcases for equality scenario
\[
Y_i(\beta_0 + \beta^T x_i) -1 + \xi_i = 0
\]
depending on the values of slack variables \(\xi_i \ge 0\):
</p>
<dl class="org-dl">
<dt>Margin support vectors (\(\xi_i=0\))</dt><dd>we have \(Y_i(\beta_0+\beta^T x_i) =1 ~ \text{i.e. } \beta_0+\beta^Tx_i=Y_i\in\pm1\)</dd>
<dt>Non-margin support vectors (\(0<\xi_i\le1\))</dt><dd>we have \(Y_i(\beta_0+\beta^Tx_i)=1-\xi_i\ge0 ~ \text{i.e. }Y_i=\text{sign }(\beta_0+\beta^Tx_i)\)</dd>
<dt>Misclassification errors (\(\xi_i\gt1\))</dt><dd>since
\(Y_i(\beta_0+\beta^T x_i)=1-\xi_i <0 ~ \text{i.e. } Y_i\ne \text{sign}(\beta_0+\beta^Tx_i)\)</dd>
</dl>
</div>
</div>
<div id="outline-container-org4aff3ac" class="outline-4">
<h4 id="org4aff3ac"><span class="section-number-4">9.4.7.</span> Linearly non-separable scenarios</h4>
<div class="outline-text-4" id="text-9-4-7">
<p>
<img src="./img/non-sep-types.png" alt="non-sep-types.png" />
\(Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0\)
</p>
</div>
</div>
<div id="outline-container-orgcd683e4" class="outline-4">
<h4 id="orgcd683e4"><span class="section-number-4">9.4.8.</span> Comparison of two scenarios</h4>
<div class="outline-text-4" id="text-9-4-8">
<p>
Comparison of SVM classifier between linearly separable and linearly non-separable scenarios:
</p>
<ul class="org-ul">
<li>Hard constraint \(Y_i(\beta_0+\beta^Tx_i)\ge 1\) is replaced by soft constraint \(Y(\beta_0+\beta^Tx_i)-1+\xi_i\ge 0\) and \(\xi_i\ge 0\)</li>
<li>Perfect classification for training data is replaced by <span style='background-color: #FFFF00;'>misclassification error</span> for training data \(\sum^n_{i=1}(\xi_i > 1)\)</li>
<li>Mathematical formulation for SVM classifier in the <span style='background-color: #FFFF00;'>linearly non-separable</span> scenario needs to adjust the training error</li>
<li>Optimization algorithms are similar.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org20a3493" class="outline-3">
<h3 id="org20a3493"><span class="section-number-3">9.5.</span> Optimization problem for linearly non-separable</h3>
<div class="outline-text-3" id="text-9-5">
<p>
11.2.2
</p>
</div>
<div id="outline-container-org8a6c437" class="outline-4">
<h4 id="org8a6c437"><span class="section-number-4">9.5.1.</span> Three criteria in SVM</h4>
<div class="outline-text-4" id="text-9-5-1">
<p>
There are three criteria when evaluating linear SVM based on the linear function/hyperplane \(h(x)=\beta_0+\beta_1x_1+...+\beta_px_p = \beta_0+\beta^Tx\)
</p>
<dl class="org-dl">
<dt>Margin maximization</dt><dd>\[
  \max_{\beta,\beta_0}\frac{2}{\parallel\beta\parallel} ~ \text{ i.e. }\min_{\beta,\beta_0}(\frac{1}{2}\parallel\beta\parallel^2)
  \]</dd>
<dt>Soft constraints on training data</dt><dd>\[
  Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0, ~ , \xi_i \ge 0, ~ \forall i=1,2,...,n
  \]</dd>
<dt>Misclassification error on training data</dt><dd>\[
  \sum^n_{i=1}I(\xi_i>1)
  \]</dd>
</dl>
</div>
</div>
<div id="outline-container-org8666130" class="outline-4">
<h4 id="org8666130"><span class="section-number-4">9.5.2.</span> Possible formulation?</h4>
<div class="outline-text-4" id="text-9-5-2">
<p>
(P1): find the linear function or hyperplane
\[
h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \beta_0 + \beta^T x
\]
that minimizes an objective function to account for training error
\[
\frac{1}{2} \parallel\beta\parallel^2 + C\sum^n_{i=1}I(\xi_i\gt 1)
\]
for some \(C>0\) subject to the soft constraints on the training data
\[
Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0, ~ , \xi_i \ge 0, ~ \forall i=1,2,...,n
\]
</p>
<ul class="org-ul">
<li>Unfortunately, this is difficult to optimize as it is <span style='background-color: #FFFF00;'>non-convex</span></li>
<li>We can motivate another useful formulation</li>
</ul>
</div>
</div>
<div id="outline-container-org57ebede" class="outline-4">
<h4 id="org57ebede"><span class="section-number-4">9.5.3.</span> Key idea of new formulation</h4>
<div class="outline-text-4" id="text-9-5-3">
<p>
New formulation of SVM for linearly non-separable:
</p>
<ul class="org-ul">
<li>(P2): find the linear function \(h(x)=\beta_0+\beta^Tx\) and \((\xi_1,...,\xi_n)\) that minimizes the objective function
\[
  \frac{1}{2} \parallel\beta\parallel^2 + C\sum^n_{i=1}\xi_i
  \]
for some \(C>0\), subject to the soft constraints on training data
\[
  Y_i(\beta_0+\beta^Tx_i)-1+\xi_i\ge0, ~ , \xi_i \ge 0, ~ \forall i=1,2,...,n
  \]
This is a constrained quadratic optimization data that is solvable</li>
</ul>
</div>
</div>
<div id="outline-container-org5073cde" class="outline-4">
<h4 id="org5073cde"><span class="section-number-4">9.5.4.</span> Constrained optimization</h4>
<div class="outline-text-4" id="text-9-5-4">
<ul class="org-ul">
<li>By <span style='background-color: #FFFF00;'>Lagrange multipliers</span>, we reformulate the constrained primal problem into the unconstrained dual space with the objective function
      \[
	g=\frac{1} {2} \| \beta\|^{2}+C \sum_{i=1}^{n} \xi_{i}-\sum_{i=1}^{n} \alpha_{i} \big[ Y_{i} \big( \beta_{0}+\beta^{T} x_{i} \big)-1+\xi_{i} \big]-\sum_{i=1}^{n} \delta_{i} \xi_{i}
	\]
where \(\alpha_i \ge 0, \delta_i \ge 0\) are the <b><b>Lagrange multipliers</b></b></li>
<li>The optimal solution gives the SVM classifier in the linearly non-separable scenario</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: W</p>
<p class="date">Created: 2024-04-04 Thu 21:16</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
