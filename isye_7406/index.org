#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: ISYE 7406: Data Mining and Statistical Learning
* Week 3: Linear Regression (II)
M2T2
** James-Stein Estimator
*** It's a special case of linear regression
in LR model with
$$
Y_{n\times 1} = X_{n\times p} \beta_{p\times 1} + \epsilon_{n\times 1}, \text{s.t.} \epsilon \sim N(0, \sigma^2 I_{n\times n})
$$

Special case:
- $n=p$
- $X_{n\times p} = I_{p\times p}$

OLS yields the estimator of:

$$
\hat{\beta_{ols}} = (X^T X)^{-1} X^T Y =
(I^T_{p\times p} I_{p\times p})^{-1}
I^T_{p\times p} Y_{p\times 1} =
\bf{Y_{p\times 1}}
$$

When $\bf{p\ge 3}$, is it possible to do better than OLS?
*** Simultaneous estimation
- Problem of estimating $p$ # of parameters $\beta_i$'s simultaneously from $p$ observations ($Y_i$'s) under model:

  $$
  \bf{Y_i} \sim N(\beta_i, \sigma^2), \text{for }i = 1, 2, ..., p
  $$
- OLS (a.k.a. Maximum Likelihood Estimator, MLE) yields estimator:

  $$
  \hat{\beta_i} = Y_i \text{ for }i = 1, 2, ..., p
  $$

  Is it possible to do better here?
*** JS estimator
- Showed MLS/MLE estimator inadmissible for $p\ge 3$; dominated by JS estimator.

  $$
  \hat{\beta_i^{MLE}} = Y_i \text{ for }i = 1, 2, ..., p
  $$

  $$
  \hat{\beta_i^{(JS)}} = w Y_i + (1-w) \bar{Y} \text{ for }i = 1, 2, ..., p;
  $$

  $$
  w = 1 - \frac{(p-3)\sigma^2}{\sum^p_i(Y_i-\bar{Y})^2}
  $$
*** Baseball example
- Observe $Y_1, Y_2, ... Y_p$ batting averages (where $Y_i$ is the batting average for p=18 players), 45 AB
- "True" values $\mu_i$ are the averages over remainder of seasons, 370 AB
- Qn: how to predict season averages $\mu_i$ from early statistics $Y_i$?
- Estimators: MLE and JS
*** Comparing MLE and JS
- JS has lower predictive squared error than MLS (by 50%)
- JS estimator is **shrinkage** estimator.
  - Each MLE value shrunken towards **grand mean**
  - Data-based estimator, compromises between:
    - null hypothesis: all means the same
    - MLE assumption: no relationship between all $\mu_i$ values
  - Difficult to estimate $p\ge 3$ parameters simultaneously.
** Shrinkage Methods
- Estimation in linear regression: JS works in only specific cases ([[It's a special case of linear regression]]) when $p\ge 3$
- How to do better /generally/?
- Shrinkage methods (penalized, regularized)
  - Based on subtracting penalty from log-likelihood
  - Penalty is a function of /decay parameter/
  - Sort the variables to be included by size of /decay parameter/
  - This reduces to a nested case
  - After estimating /decay parameter/, variable or model selection is complete!
*** Setting up shrinkage method
- Needs: $Y_1, x_{11}, x_{12}, ..., x_{1,p}, \text{such that } i = 1,2,...,n$
- Assume all X & Y are standardized i.e.:
  $$
  \sum^n_{i=1}Y_i = 0,
  \sum^n_{i=i}x_{ij} = 0,
  \sum^n_{i=1}x^2_{ij} = 1
  $$
- If not standardized, do linear transformations:
  $$
  Y^{*}_i = Y_i - \bar{Y}
  $$

  $$
  x^{*}_{ij} = \frac{x_{ij}-\bar{x_j}}{\sqrt{\text{Var}_j}}
  $$
- With this assumption, $\beta_0 = 0$ in the model, i.e.
  $$
  Y_i = \bf{0 } \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{i,p} + \epsilon_i
  $$
- *The shrinkage method solves this optimization problem*

  $$
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  $$
  - penalty function :: $J(|\beta_j)$
  - decay or tuning parameter :: $\lambda \ge 0$
*** Alternative formulation
- Shrinkage method solves the *unconstrained* optimization problem
  $$
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  $$
- Alternative formulation solves a *constrained* optimization problem

  $$
  \min_{\beta} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2, \text{ subject to: }
  \sum^p_{j=1} J(|B_j|) \le s
  $$
  - tuning parameter :: $s \gt 0$

- The alternative formulation may greatly facilitate computation *at times*, e.g. in LASSO which is piecewise linear in $s$.
*** Bayesian interpretation
- For LR model ([[It's a special case of linear regression]]):
  - prior on \beta :: $\pi(\beta)$
  - independent prior on $\sigma^2$ :: $\pi(\sigma^2)$
  - posterior for $(\beta, sigma^2)$ :: proportional to
    $$
    \pi (\sigma^2)(\sigma^2)^{(n-1)/2}\exp\{-\frac{1}{2\sigma^2} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \log \pi(\beta) \}
    $$
- *Posterior maximization method* yields shrinkage estimator

  $$
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \textbf{Pen}(\beta)
  $$
*** Choices of priors
i.e. choice of prior $\pi(\beta)$
- Normal prior :: yields *ridge regression* etsimator
- Laplace prior :: yields *LASSO* estimator
*** Ridge regression
*Normal prior* assumes $\beta_1 ... \beta_p$ are i.i.d. $N(0, \tau^2)$ with prior density
$$
\pi(\beta) = \prod^p_{i=1} \frac{1}{\sqrt{2\pi}\tau} \exp\left
(-\frac{1}{2\tau^2}\beta_i^2\right)
$$

Yields *ridge regression* estimator, which minimizes

$$
\parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
$$
*** LASSO estimator
- *Laplace Prior*, assume  $\beta_1 ... \beta_p$ are i.i.d. double-exponential (Laplace) $\sim \text{Lapalce} (,\tau)$ with prior density
  $$
  \pi(\beta) = \prod^p_{i=1} \frac{1}{2\tau} \exp \left(- \frac{1}{\tau} |\beta_i| \right)
  $$
- Yields *LASSO* estimator that minimizes

  $$
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}|\beta_i|
  $$
** Ridge Regression
*** Ridge Regression Estimator
Assume these are observed: $Y_i, x_{i1}, ..., x_{ip}$, and all are standardized:
$$
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
$$

In linear regression model without intercepts ([[It's a special case of linear regression]])

The ridge regression estimator is defined as:

$$
\hat{\beta^{\text{ridge}}} = \min_{\beta}
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
$$
*** Mathematical solution
- Explicit expression is thus
  $$
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  $$
- Ridge regression *estimator or prediction*:
  $$
  \hat{Y}^{\text{ridge}} = X_{n\times p}  \hat{\beta^{\text{ridge}}}
  $$
- Requires *choosing* the tuning parameter $\lambda$, based on data, usually *by cross-validation*
*** Properties of Ridge Regression
- Ridge regression *most useful* when $X_{n\times p}$ is *non-singular*, but has *high collinearity*
  - i.e. $X^T_{n\times p} X_{n\times p}$ has eigenvalue close to 0
- $\hat{\beta^{\text{ridge}}}$ is biased, with bias $\rightarrow$ 0 as $\lambda \rightarrow 0$
- As $\lambda$ increases, $\hat{\beta^{\text{ridge}}}$ $\rightarrow 0$, though rarely = 0.
- Despite the bias, $\text{Var}(\hat{\beta^{\text{ridge}}})$ will usually be smaller than OLS
  - Therefore better prediction than OLS.
*** Computational issues
- How to compute ridge regression efficiently for any $\lambda$?
  $$
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  $$
- It is highly non-trivial to compute the inverse of a large $p\times p$ matrix.
- *Singular Value Decomposition* (SVD) algorithm:
  - Write the matrix $X_{n\times p}$ in its SVD form
    $$
    X_{n\times p} = U_{n\times p} D_{p\times p} V^T_{p\times p}
    $$
    where: $U$ and $V are orthogonal; D = diag($d_1, ..., d_p$) is diagonal.
- Then: ridge regression estimator becomes the matrix product:
  $$
   \hat{\beta^{\text{ridge}}} = V_{p\times p} \text{diag} \left(\frac{d_1}{d^2_1 + \lambda}, ..., \frac{d_p}{d^2_p + \lambda} \right) U^T_{p\times n} Y_{n\times 1}
  $$
*** Example of SVD
- Find SVD of matrix
  $$
  X_{3 \times 2}
  = \begin{pmatrix}
  1 & 0 \\
  0 & 1 \\
  1 & 1 \\
  \end{pmatrix}
  = U_{n\times p} D_{p\times p} V^T_{p\times p}
  $$
- Steps (required: $p \leq n$):
  1. $U_{n\times p}$ is the normalized $p$ (largest) eigenvectors of $XX^T$
  2. $V_{p\times p}$ is the normalized eigenvectors of $X^T X$
  3. Matrix $D = \text{diag}(d_1, ..., d_p)$ with $d_j$ being the square root of $p$ (largest) eigenvalues of $XX^T$ or $X^T X$.
*** SVD Example (I): $XX^T$
For matrix [[Example of SVD]]:
we have
$$
XX^T = \begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 1 \\
1 & 1 &2\end{pmatrix}
$$
- Characteristic polynomial is
  $-\lambda^3 + 4\lambda^2 - 3\lambda = -\lambda(\lambda-1)(\lambda-3)$
- The eigenvalues of $XX^T$ are $\lambda = 3, 1, 0$
- Corresponding eigenvectors are:
  $$
  u'_1 = \begin{pmatrix}
  1 \\
  1 \\
  2\end{pmatrix},
  u'_2 = \begin{pmatrix}
  1 \\
  -1 \\
  0\end{pmatrix},
  u'_3 = \begin{pmatrix}
  1 \\
  1 \\
  -1\end{pmatrix},
  $$
- Normalizing yields
  $$
  U_{3\times 2} = \begin{pmatrix}
  1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
  1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
  2\over{\sqrt{6}} & 0 \end{pmatrix},
  $$
- $d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1$

*** SVD Example (II): $X^T X$
For matrix [[Example of SVD]]:
$$
X^T X
= \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix}
$$

- Characteristic polynomial is
  $\lambda^2 - 4\lambda + 3 = (\lambda -1)(\lambda -3)$
- Eigenvalues of $XX^T$ are: $\lambda = 3, 1$.
- Corresponding eigenvalues are:
  $$
  v'_1 = \begin{pmatrix}
  1 \\
  1\end{pmatrix},
  v'_2 = \begin{pmatrix}
  1 \\
  -1\end{pmatrix}
  $$
- Normalizing them yields:
  $$
  V_{2\times 2} = (v_1, v_2) =
  \begin{pmatrix}
  1\over\sqrt{2} & 1\over\sqrt{2} \\
  1\over\sqrt{2} & -1\over\sqrt{2}\end{pmatrix},
  $$
- $d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1$
*** SVD verification
Might need to multiply some eigenvectors by -1.

\begin{equation}
X_{n\times p}
=
\begin{pmatrix}
1 & 0 \\
0 & 1 \\
1 & 1\end{pmatrix}
=
\begin{pmatrix}
1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
2\over{\sqrt{6}} & 0\end{pmatrix}
\begin{pmatrix}
\sqrt{3} & 0 \\
0 & 1\end{pmatrix}
\begin{pmatrix}
1\over{\sqrt{2}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{2}} & -1\over{\sqrt{2}}\end{pmatrix}

= U_{n\times p} D_{p\times p}V^T_{p\times p}
\end{equation}

\begin{equation}
= \lambda_1 u_1 v^T_1 + \lambda_2 u_2 v^T_2 =
0.5\begin{pmatrix}
1 & 1 \\
1 & 1 \\
2 & 2 \end{pmatrix} +
0.5\begin{pmatrix}
1 & -1 \\
-1 & 1 \\
0 & 0 \end{pmatrix}
\end{equation}
** LASSO (3.2.1)
*** LASSO estimator
- Assume these are observed: $Y_i, x_{i1}, ..., x_{ip}$, and all are standardized:
$$
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
$$

In linear regression model without intercepts ([[It's a special case of linear regression]])

- LASSO :: Least Absolute Selection and Shrinkage Operator

- Definition ::

  $$
  \hat{\beta}^{\text{lasso}} = \min_\beta \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1} |\beta_j|
  $$
- s.t. :: tuning parameter $\lambda > 0$
*** L2-norm vs L1-norm
[[./img/l2-l1-norm.png]]
- L1-norm: *sparse*, as boundary points of the L1-norm ball have lower dimensions (are in lower-dimensional space, $x_1 = 0, or x_2=0$)
*** Mathematical solution for LASSO estimator
- In the LASSO optimization, there is *no explicit* mathematical solution to $\hat{\beta}^{\text{lasso}}$
- Hence, need to use computational algorithms to get solution
- Explicit solution only available when $X^T X = I_{n\times n}$
  - In this case, LASSO estimator is:

    \begin{equation}
    \hat{\beta}^{\text{lasso}}_j =
    \begin{cases}
      \hat{\beta}_j^{ols} - \frac{\lambda}{2} & \text{if }\hat{\beta}_j^{ols}> \frac{\lambda}{2}\\
      0 & \text{if }|\hat{\beta}_j^{ols}| \leq \frac{\lambda}{2}\\
      \hat{\beta}_j^{ols} + \frac{\lambda}{2} & \text{if }\hat{\beta}_j^{ols} < -\frac{\lambda}{2}\\
    \end{cases}
    \end{equation}
- As: LASSO can be simplified to 1-dimensional optimization problem

  $$
  \min_{-\infty < x < \infty} (x - \hat{\beta}_j^{ols})^2 + \lambda |x|
  $$

since:

$$
\parallel Y - X\beta \parallel^2 = \parallel Y-X\hat{\beta}^{ols} \parallel^2 + (\beta - \hat{\beta}^{ols})^T X^T X(\beta - \hat{\beta}^{ols})
$$
*** Properties of LASSO
- *Good empirical performance when true model is sparse*
  - If so, outperforms AIC, BIC, stepwise, ridge
- Nice theoretical properties, i.e. high probability of the following under certain regularity conditions:
  - parameter recovery :: when $|\hat{\beta}_j^{lasso}-\hat{\beta}^{true}|^2$ is small
  - variable selection :: $\textbf{supp}(\hat{\beta}_j^{lasso}) = \textbf{supp}(\beta^{true})$
  - prediction error bound ::  $|X\hat{\beta}_j^{lasso} - X\beta^{true}|^2$ is small
*** LASSO weaknesses
- Not always consistent
- Tends to select over-parameterized model
- Does poorly when
  1. True model is *not sparse*
  2. When few X variables are highly correlated (LASSO picks 1 randomly)
  3. When the design $X$ matrix is too correlated (Ridge outperforms)
  4. When there are outliers in responses
*** Computation issues of LASSO
- Computation algorithms include:
  - Coordinate descent
  - Sub-gradient methods
  - Proximal gradient methods
- Would be ideal to compute entire solution path all at once, i.e. for all values for $\lambda$ simultaneously.
*** LASSO is piecewise linear
- The number of linear pieces in LASSO path is approximately $p$,
- The computational complexity of getting whole LASSO path is $O(np^2)$
  - i.e. same cost as computing least-squares fit
*** Standard error of LASSO
- How to estimate standard error of LASSO estimator i.e.
  $$
  \hat{\beta}^{\text{lasso}} = \min_\beta \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1} |\beta_j|
  $$
- Answer: bootstrapping:
  1. Fix $\lambda$, generate a set of bootstrap samples
  2. Obtain corresponding $\hat{\beta}^{lasso}(\lambda)$
  3. Repeat for $L$ times and use them to estimate standard error
  4. If not determined/fixed, $\lambda$ can be estimated by cross-validation (e.g. 5 fold CV).
*** Variants of L1-norm
- Elastic net
  $$
  \hat{\beta}^{enet} = \min_\beta \parallel Y_{n\times 1} - X_{n \times p} \beta_{p \times 1} \parallel^2 + \lambda_1 \sum^p_{j=1} |\beta_j| + \lambda_2 \sum^p_j (\beta_j)^2
  $$
