#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: ISYE 7406: Data Mining and Statistical Learning
* Week 3: Linear Regression (II)
M2T2
** James-Stein Estimator
*** It's a special case of linear regression
in LR model with
$$
Y_{n\times 1} = X_{n\times p} \beta_{p\times 1} + \epsilon_{n\times 1}, \text{s.t.} \epsilon \sim N(0, \sigma^2 I_{n\times n})
$$

Special case:
- $n=p$
- $X_{n\times p} = I_{p\times p}$

OLS yields the estimator of:

$$
\hat{\beta_{ols}} = (X^T X)^{-1} X^T Y =
(I^T_{p\times p} I_{p\times p})^{-1}
I^T_{p\times p} Y_{p\times 1} =
\bf{Y_{p\times 1}}
$$

When $\bf{p\ge 3}$, is it possible to do better than OLS?
*** Simultaneous estimation
- Problem of estimating $p$ # of parameters $\beta_i$'s simultaneously from $p$ observations ($Y_i$'s) under model:

  $$
  \bf{Y_i} \sim N(\beta_i, \sigma^2), \text{for }i = 1, 2, ..., p
  $$
- OLS (a.k.a. Maximum Likelihood Estimator, MLE) yields estimator:

  $$
  \hat{\beta_i} = Y_i \text{ for }i = 1, 2, ..., p
  $$

  Is it possible to do better here?
*** JS estimator
- Showed MLS/MLE estimator inadmissible for $p\ge 3$; dominated by JS estimator.

  $$
  \hat{\beta_i^{MLE}} = Y_i \text{ for }i = 1, 2, ..., p
  $$

  $$
  \hat{\beta_i^{(JS)}} = w Y_i + (1-w) \bar{Y} \text{ for }i = 1, 2, ..., p;
  $$

  $$
  w = 1 - \frac{(p-3)\sigma^2}{\sum^p_i(Y_i-\bar{Y})^2}
  $$
*** Baseball example
- Observe $Y_1, Y_2, ... Y_p$ batting averages (where $Y_i$ is the batting average for p=18 players), 45 AB
- "True" values $\mu_i$ are the averages over remainder of seasons, 370 AB
- Qn: how to predict season averages $\mu_i$ from early statistics $Y_i$?
- Estimators: MLE and JS
*** Comparing MLE and JS
- JS has lower predictive squared error than MLS (by 50%)
- JS estimator is **shrinkage** estimator.
  - Each MLE value shrunken towards **grand mean**
  - Data-based estimator, compromises between:
    - null hypothesis: all means the same
    - MLE assumption: no relationship between all $\mu_i$ values
  - Difficult to estimate $p\ge 3$ parameters simultaneously.
** Shrinkage Methods
- Estimation in linear regression: JS works in only specific cases ([[It's a special case of linear regression]]) when $p\ge 3$
- How to do better /generally/?
- Shrinkage methods (penalized, regularized)
  - Based on subtracting penalty from log-likelihood
  - Penalty is a function of /decay parameter/
  - Sort the variables to be included by size of /decay parameter/
  - This reduces to a nested case
  - After estimating /decay parameter/, variable or model selection is complete!
*** Setting up shrinkage method
- Needs: $Y_1, x_{11}, x_{12}, ..., x_{1,p}, \text{such that } i = 1,2,...,n$
- Assume all X & Y are standardized i.e.:
  $$
  \sum^n_{i=1}Y_i = 0,
  \sum^n_{i=i}x_{ij} = 0,
  \sum^n_{i=1}x^2_{ij} = 1
  $$
- If not standardized, do linear transformations:
  $$
  Y^{*}_i = Y_i - \bar{Y}
  $$

  $$
  x^{*}_{ij} = \frac{x_{ij}-\bar{x_j}}{\sqrt{\text{Var}_j}}
  $$
- With this assumption, $\beta_0 = 0$ in the model, i.e.
  $$
  Y_i = \bf{0 } \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{i,p} + \epsilon_i
  $$
- *The shrinkage method solves this optimization problem*

  $$
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  $$
  - penalty function :: $J(|\beta_j)$
  - decay or tuning parameter :: $\lambda \ge 0$
*** Alternative formulation
- Shrinkage method solves the *unconstrained* optimization problem
  $$
  \parallel
  Y_{n\times 1}-X_{n\times p}\beta_{p\times 1}
  \parallel^2 + \lambda\sum^p_{j=1}J(|\beta_j|)
  $$
- Alternative formulation solves a *constrained* optimization problem

  $$
  \min_{\beta} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2, \text{ subject to: }
  \sum^p_{j=1} J(|B_j|) \le s
  $$
  - tuning parameter :: $s \gt 0$

- The alternative formulation may greatly facilitate computation *at times*, e.g. in LASSO which is piecewise linear in $s$.
*** Bayesian interpretation
- For LR model ([[It's a special case of linear regression]]):
  - prior on \beta :: $\pi(\beta)$
  - independent prior on $\sigma^2$ :: $\pi(\sigma^2)$
  - posterior for $(\beta, sigma^2)$ :: proportional to
    $$
    \pi (\sigma^2)(\sigma^2)^{(n-1)/2}\exp\{-\frac{1}{2\sigma^2} \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \log \pi(\beta) \}
    $$
- *Posterior maximization method* yields shrinkage estimator

  $$
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \textbf{Pen}(\beta)
  $$
*** Choices of priors
i.e. choice of prior $\pi(\beta)$
- Normal prior :: yields *ridge regression* etsimator
- Laplace prior :: yields *LASSO* estimator
*** Ridge regression
*Normal prior* assumes $\beta_1 ... \beta_p$ are i.i.d. $N(0, \tau^2)$ with prior density
$$
\pi(\beta) = \prod^p_{i=1} \frac{1}{\sqrt{2\pi}\tau} \exp\left
(-\frac{1}{2\tau^2}\beta_i^2\right)
$$

Yields *ridge regression* estimator, which minimizes

$$
\parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
$$
*** LASSO estimator
- *Laplace Prior*, assume  $\beta_1 ... \beta_p$ are i.i.d. double-exponential (Laplace) $\sim \text{Lapalce} (,\tau)$ with prior density
  $$
  \pi(\beta) = \prod^p_{i=1} \frac{1}{2\tau} \exp \left(- \frac{1}{\tau} |\beta_i| \right)
  $$
- Yields *LASSO* estimator that minimizes

  $$
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}|\beta_i|
  $$
** Ridge Regression
*** Ridge Regression Estimator
Assume these are observed: $Y_i, x_{i1}, ..., x_{ip}$, and all are standardized:
$$
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
$$

In linear regression model without intercepts ([[It's a special case of linear regression]])

The ridge regression estimator is defined as:

$$
\hat{\beta^{\text{ridge}}} = \min_{\beta}
  \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1}(\beta_i)^2
$$
*** Mathematical solution
- Explicit expression is thus
  $$
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  $$
- Ridge regression *estimator or prediction*:
  $$
  \hat{Y}^{\text{ridge}} = X_{n\times p}  \hat{\beta^{\text{ridge}}}
  $$
- Requires *choosing* the tuning parameter $\lambda$, based on data, usually *by cross-validation*
*** Properties of Ridge Regression
- Ridge regression *most useful* when $X_{n\times p}$ is *non-singular*, but has *high collinearity*
  - i.e. $X^T_{n\times p} X_{n\times p}$ has eigenvalue close to 0
- $\hat{\beta^{\text{ridge}}}$ is biased, with bias $\rightarrow$ 0 as $\lambda \rightarrow 0$
- As $\lambda$ increases, $\hat{\beta^{\text{ridge}}}$ $\rightarrow 0$, though rarely = 0.
- Despite the bias, $\text{Var}(\hat{\beta^{\text{ridge}}})$ will usually be smaller than OLS
  - Therefore better prediction than OLS.
*** Computational issues
- How to compute ridge regression efficiently for any $\lambda$?
  $$
  \hat{\beta^{\text{ridge}}} = (X^T_{n\times p}X_{n\times p}+\lambda I_{p\times p})^{-1} X^T_{n\times p}Y_{n\times 1}
  $$
- It is highly non-trivial to compute the inverse of a large $p\times p$ matrix.
- *Singular Value Decomposition* (SVD) algorithm:
  - Write the matrix $X_{n\times p}$ in its SVD form
    $$
    X_{n\times p} = U_{n\times p} D_{p\times p} V^T_{p\times p}
    $$
    where: $U$ and $V are orthogonal; D = diag($d_1, ..., d_p$) is diagonal.
- Then: ridge regression estimator becomes the matrix product:
  $$
   \hat{\beta^{\text{ridge}}} = V_{p\times p} \text{diag} \left(\frac{d_1}{d^2_1 + \lambda}, ..., \frac{d_p}{d^2_p + \lambda} \right) U^T_{p\times n} Y_{n\times 1}
  $$
*** Example of SVD
- Find SVD of matrix
  $$
  X_{3 \times 2}
  = \begin{pmatrix}
  1 & 0 \\
  0 & 1 \\
  1 & 1 \\
  \end{pmatrix}
  = U_{n\times p} D_{p\times p} V^T_{p\times p}
  $$
- Steps (required: $p \leq n$):
  1. $U_{n\times p}$ is the normalized $p$ (largest) eigenvectors of $XX^T$
  2. $V_{p\times p}$ is the normalized eigenvectors of $X^T X$
  3. Matrix $D = \text{diag}(d_1, ..., d_p)$ with $d_j$ being the square root of $p$ (largest) eigenvalues of $XX^T$ or $X^T X$.
*** SVD Example (I): $XX^T$
For matrix [[Example of SVD]]:
we have
$$
XX^T = \begin{pmatrix}
1 & 0 & 1\\
0 & 1 & 1 \\
1 & 1 &2\end{pmatrix}
$$
- Characteristic polynomial is
  $-\lambda^3 + 4\lambda^2 - 3\lambda = -\lambda(\lambda-1)(\lambda-3)$
- The eigenvalues of $XX^T$ are $\lambda = 3, 1, 0$
- Corresponding eigenvectors are:
  $$
  u'_1 = \begin{pmatrix}
  1 \\
  1 \\
  2\end{pmatrix},
  u'_2 = \begin{pmatrix}
  1 \\
  -1 \\
  0\end{pmatrix},
  u'_3 = \begin{pmatrix}
  1 \\
  1 \\
  -1\end{pmatrix},
  $$
- Normalizing yields
  $$
  U_{3\times 2} = \begin{pmatrix}
  1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
  1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
  2\over{\sqrt{6}} & 0 \end{pmatrix},
  $$
- $d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1$

*** SVD Example (II): $X^T X$
For matrix [[Example of SVD]]:
$$
X^T X
= \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix}
$$

- Characteristic polynomial is
  $\lambda^2 - 4\lambda + 3 = (\lambda -1)(\lambda -3)$
- Eigenvalues of $XX^T$ are: $\lambda = 3, 1$.
- Corresponding eigenvalues are:
  $$
  v'_1 = \begin{pmatrix}
  1 \\
  1\end{pmatrix},
  v'_2 = \begin{pmatrix}
  1 \\
  -1\end{pmatrix}
  $$
- Normalizing them yields:
  $$
  V_{2\times 2} = (v_1, v_2) =
  \begin{pmatrix}
  1\over\sqrt{2} & 1\over\sqrt{2} \\
  1\over\sqrt{2} & -1\over\sqrt{2}\end{pmatrix},
  $$
- $d_1 = \sqrt{3}, d_2 = \sqrt{1} = 1$
*** SVD verification
Might need to multiply some eigenvectors by -1.

\begin{equation}
X_{n\times p}
=
\begin{pmatrix}
1 & 0 \\
0 & 1 \\
1 & 1\end{pmatrix}
=
\begin{pmatrix}
1\over{\sqrt{6}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{6}} & -1\over{\sqrt{2}} \\
2\over{\sqrt{6}} & 0\end{pmatrix}
\begin{pmatrix}
\sqrt{3} & 0 \\
0 & 1\end{pmatrix}
\begin{pmatrix}
1\over{\sqrt{2}} & 1\over{\sqrt{2}} \\
1\over{\sqrt{2}} & -1\over{\sqrt{2}}\end{pmatrix}

= U_{n\times p} D_{p\times p}V^T_{p\times p}
\end{equation}

\begin{equation}
= \lambda_1 u_1 v^T_1 + \lambda_2 u_2 v^T_2 =
0.5\begin{pmatrix}
1 & 1 \\
1 & 1 \\
2 & 2 \end{pmatrix} +
0.5\begin{pmatrix}
1 & -1 \\
-1 & 1 \\
0 & 0 \end{pmatrix}
\end{equation}
** LASSO (3.2.1)
*** LASSO estimator
- Assume these are observed: $Y_i, x_{i1}, ..., x_{ip}$, and all are standardized:
$$
\sum^n_{i=1} Y_i = 0,
\sum^n_{i=1} x_{ij} = 0,
\sum^n_{i=1} x^2_{ij} = 1
$$

In linear regression model without intercepts ([[It's a special case of linear regression]])

- LASSO :: Least Absolute Selection and Shrinkage Operator

- Definition ::

  $$
  \hat{\beta}^{\text{lasso}} = \min_\beta \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1} |\beta_j|
  $$
- s.t. :: tuning parameter $\lambda > 0$
*** L2-norm vs L1-norm
[[./img/l2-l1-norm.png]]
- L1-norm: *sparse*, as boundary points of the L1-norm ball have lower dimensions (are in lower-dimensional space, $x_1 = 0, or x_2=0$)
*** Mathematical solution for LASSO estimator
- In the LASSO optimization, there is *no explicit* mathematical solution to $\hat{\beta}^{\text{lasso}}$
- Hence, need to use computational algorithms to get solution
- Explicit solution only available when $X^T X = I_{n\times n}$
  - In this case, LASSO estimator is:

    \begin{equation}
    \hat{\beta}^{\text{lasso}}_j =
    \begin{cases}
      \hat{\beta}_j^{ols} - \frac{\lambda}{2} & \text{if }\hat{\beta}_j^{ols}> \frac{\lambda}{2}\\
      0 & \text{if }|\hat{\beta}_j^{ols}| \leq \frac{\lambda}{2}\\
      \hat{\beta}_j^{ols} + \frac{\lambda}{2} & \text{if }\hat{\beta}_j^{ols} < -\frac{\lambda}{2}\\
    \end{cases}
    \end{equation}
- As: LASSO can be simplified to 1-dimensional optimization problem

  $$
  \min_{-\infty < x < \infty} (x - \hat{\beta}_j^{ols})^2 + \lambda |x|
  $$

since:

$$
\parallel Y - X\beta \parallel^2 = \parallel Y-X\hat{\beta}^{ols} \parallel^2 + (\beta - \hat{\beta}^{ols})^T X^T X(\beta - \hat{\beta}^{ols})
$$
*** Properties of LASSO
- *Good empirical performance when true model is sparse*
  - If so, outperforms AIC, BIC, stepwise, ridge
- Nice theoretical properties, i.e. high probability of the following under certain regularity conditions:
  - parameter recovery :: when $|\hat{\beta}_j^{lasso}-\hat{\beta}^{true}|^2$ is small
  - variable selection :: $\textbf{supp}(\hat{\beta}_j^{lasso}) = \textbf{supp}(\beta^{true})$
  - prediction error bound ::  $|X\hat{\beta}_j^{lasso} - X\beta^{true}|^2$ is small
*** LASSO weaknesses
- Not always consistent
- Tends to select over-parameterized model
- Does poorly when
  1. True model is *not sparse*
  2. When few X variables are highly correlated (LASSO picks 1 randomly)
  3. When the design $X$ matrix is too correlated (Ridge outperforms)
  4. When there are outliers in responses
*** Computation issues of LASSO
- Computation algorithms include:
  - Coordinate descent
  - Sub-gradient methods
  - Proximal gradient methods
- Would be ideal to compute entire solution path all at once, i.e. for all values for $\lambda$ simultaneously.
*** LASSO is piecewise linear
- The number of linear pieces in LASSO path is approximately $p$,
- The computational complexity of getting whole LASSO path is $O(np^2)$
  - i.e. same cost as computing least-squares fit
*** Standard error of LASSO
- How to estimate standard error of LASSO estimator i.e.
  $$
  \hat{\beta}^{\text{lasso}} = \min_\beta \parallel Y_{n\times 1} - X_{n\times p} \beta_{p\times 1} \parallel^2 + \lambda \sum^p_{j=1} |\beta_j|
  $$
- Answer: bootstrapping:
  1. Fix $\lambda$, generate a set of bootstrap samples
  2. Obtain corresponding $\hat{\beta}^{lasso}(\lambda)$
  3. Repeat for $L$ times and use them to estimate standard error
  4. If not determined/fixed, $\lambda$ can be estimated by cross-validation (e.g. 5 fold CV).
*** Variants of L1-norm
- Elastic net
  $$
  \hat{\beta}^{enet} = \min_\beta \parallel Y_{n\times 1} - X_{n \times p} \beta_{p \times 1} \parallel^2 + \lambda_1 \sum^p_{j=1} |\beta_j| + \lambda_2 \sum^p_j (\beta_j)^2
  $$
** Principal Components (3.2.2)
Assume these are observed: $Y_i, x_{i1}, ..., x_{ip}$ for $i=1,2,...,n$

Classical datasets: mostly small values of $p$; modern datasets: large $p$.

Essential to conduct *dimension reduction* to reduce the number of variables.
*** Dimension reduction
- 2 approaches:
  1. *variable selection*, i.e.: AIC, BIC, stepwise algorithm, LASSO, etc.
  2. *feature extraction*, i.e. identify which functions of data are most important. *no restricted* to using existing features/variables. Options are:
     - Principal component analysis
     - Partial least squares
*** Motivation of PCA
"Obtain more variance by transforming axes"
- Find *linear combinations* of $(x_1, ..., x_p)$ that express as much variability in $X$ as possible.
  - A linear combination with *high* variance will likely affect the response the most
  - If most variation of $X$ comes from the first few PCs then: enough to build models.
  - Other linear combination vary so little among different observations \rightarrow can be ignored
*** Find the PC's:  Population version
- *Optimization problem for PC's*: Given a $p$ -dim random vector
  $$
  \textbf{X} = (X_1, ..., X_p)^T, \text{ with covariance } \Sigma = \text{Cov}(X)
  $$
- PC1: Find $U_1 = \alpha_1 X_1 + ... + \alpha_p X_p$ that maximizes
  $$
  \textbf{Var}(\alpha_1 X_1 + ... + \alpha_p X_p) = \textbf{Var}(\alpha^T \textbf{X}) = \alpha^T \Sigma \alpha
  $$
  subject to:
  $$
  \alpha^2_1 + ... \alpha^2_p = 1, \text{i.e. } \alpha^T \alpha = 1, \text{where } \alpha = (\alpha_1, ..., \alpha_p)^T
  $$
- PC2: Find $U_2 = \alpha_1 X_1 + ... + \alpha_p X_p$ that maximizes \text{Var}(\alpha^T X) = \alpha^T \Sigma\alpha$, subject to *constraints*: - $\alpha^T \alpha = 1$
  - $\text{Cov}(U_1, U_2) = 0$
- Other (later) PC are defined analogously and uncorrelated with all previous PC's
*** Eigenvectors lead to PC's
- Theorem: Let covariance matrix $\bf{\Sigma} = \text{Cov}(\bf{X})$ have eignvectors $e_1, ..., e_p$ with corresponding eigenvalues $\lambda_1 \ge ... \ge \lambda_p \ge 0$. For $j=1,2,...,p$,
  - $j$ -th PC is
    $$
    U_j = e_j^T X = e_{j1}X_1 + ... + e_{jp}X_p
    $$
  - Variance of $j$ -th PC is
    $$
    \text{Var}(U_j) = \bf{e_j^T\Sigma e_j} = \lambda_j
    $$
*** Proof by Lagrange Multipliers
- The Lagrange multiplier is to maximize
  $$
  \phi(\alpha) = \alpha^T \Sigma\alpha - \lambda(\alpha^T \alpha -1)
  $$
- Setting derivatives qual = 0 gives:
  $$
  \frac{\partial \phi(\alpha)}{\partial\alpha} = 2\Sigma\alpha - 2\lambda\alpha = 0
  $$
- Thus: $\Sigma\alpha = \lambda\alpha$ \rightarrow
  \lambda is an eigenvalue of \Sigma and \alpha is the corresponding normalized eigenvector.
- For $U = \alpha^T X$, we have $\text{Var}(U) = \alpha^T \Sigma\alpha = \alpha^T (\lambda\alpha) = \lambda$.
- For PC1, we need to find largest eigenvector of \Sigma.
- Proofs of other Pcs are similar.
*** PCs in Empirical Version
In many real world applications, only given dataset
$Y_i, x_{i1}, ..., x_{ip}$ for $i=1,2,...,n$
How to find PC's?
- Key idea: estimate the unknown \Sigma by $\hat{\Sigma}_{p\times p}$ from the data, then find the PC's by the eigenvalues and eigenvectors of $\hat{\Sigma}_{p \times p}$
- *Empirical covariance matrix* $\hat{\Sigma}_{p\times p}$ is widely used when $p<n$
  - Here, the $(r,s)$ entry of $\hat{\Sigma}_{p\times p}$ is defined as:
    $$
    \hat{\Sigma}_{rs} = \frac{1}{n} \sum^n_i(x_{ir}-\bar{x}_r)(x_{is}-\bar{x}_s)
    $$
  - Research tbd on how to estimate $\Sigma$ effectively when $p>>n$.
*** Principal component regression
- Original data: $Y_i, x_{i1}, ..., x_{ip}$ for $i=1,2,...,n$
- After we extract all PC's, raw data can be written as new format
  $(Y_i, u_{i1}, ..., u_{ip})$ for $i=1,2,...,n$
- Principal component regression: linear regression by using only first $k$ PC's:
  $$
  Y_i = \beta_0 + \beta_1 u_{i1} + ... + \beta_k u_{ik} + \epsilon_i
  $$
- Choosing $k$: done by *cross-validation*.
** Partial least squares (3.2.3)
*** Dimension reduction
There are 2 kinds of dimension reduction algorithms:
1. Unsupervised dimension reduction, e.g. PCA. Criticisms: it explains $X$ but no reason to be sure that the result also explains a response $Y$.
2. Supervised dimension reduction, i.e. conduct reduction on $X$ by using the extra information in $Y$.
   - Reasonable to believe that supervised techniques will *do better*
   - *Partial least squares* is one such technique
*** Partial least squares
Collection of techniques with 2 common properties:
1. *Maximizes correlation between $Y \& X$*, rather than maximizing variance of $Y$ only.
2. Can be interpreted as finding the underlying factors of $X$ that are also underlying factors of $Y$.
*** 2 versions of PLS
1. Simple PLS algorithm: variant of PC's but using *correlation* instead of variance
2. PLS model: identify common factors of $X \& Y$
*** Simple PLS algorithm
- Given $Y_i, x_{i1}, ..., x_{ip}$ for $i=1,2,...,n$
- Let $x_i = (x_{i1}, ..., x_{ip})^T$.
- First PLS, $V_1 = \alpha_1 X_1 + ... + \alpha_p X_p$ is defined as finding $\alpha = (\alpha_1, ..., \alpha_p)^T$ that maximizes *covariance*
  $$
  \hat{\textbf{CoV}}(Y, V_1) = \frac{1}{n} \sum^n_i (Y_i-\bar{Y})(v_i-\bar{v}),
  $$
  when
  $v_i = \alpha_1 x_{i1} + ... + \alpha_p x_{ip}$ for $i=1,2,...n$ subject to $\alpha^T \alpha = 1$
- Later PLSs are defined analogously to maximize the covariance and are *assumed to be uncorrelated* with all previous PLSs.
- *Solution*: the $\alpha$'s are the eigenvectors of the $p \times p$ matrix $X^T Y Y^T X$ when the data matrices $X \& Y$ have column mean zero.
*** The PLS model
- Data: $Y_i, x_{i1}, ..., x_{ip}$ for $i=1,2,...,n$
- Assume data have mean 0.
- Write data matrix as $(Y_{n \times  q}, X_{n \times  p})$
- Goal: find $\ell$ linear combinations from $X \& Y$ to use as new dimensions.
- The PLS model: noniterative iterative partial least squares (NIPALS):

  $$
  X_{n \times  p} = T_{n \times \ell} P_{\ell \times  p} + E,
  Y_{n \times  q} = U_{n \times  \ell} Q_{\ell \times  q} + F
  $$
  where:
  - $T_{n \times \ell}$ and $U_{n \times \ell}$ represent $\ell$ factors
  - $P_{\ell \times  p}$ and $Q_{\ell \times q}$ are loadings.
*** Key idea in The PLS Model
- How to estimate the \ell factors, or the $T_{n \times \ell}$ and $U_{n \times \ell}$ matrices?
- Answer:
  - Write the first column of  $T_{n \times \ell}$ and $U_{n \times \ell}$  as $t=\bf{X} r$ and $u=\bf{Y} s$ for two unit vectors, $\parallel r \parallel = \parallel s \parallel = 1$
  - Find $r$ and $s$ that maximizes the *covariance-squared*: $\textbf{Cov}^2(Xr, Ys)$
*** PLS for linear regression
- Original data: $Y_i, x_{i1}, ..., x_{ip}$ for $i=1,2,...,n$
- After extracting all PLS's, raw data can be written as new formats $(Y_i, v_{i1}, ..., v_{ip})$ for $i=1,2,...,n$
- Partial least squares regression: linear regression using only *first k* PLSs:
  $$
  Y_i = \beta_0 + \beta_1 v_[i1] + ... + \beta_k v_{ik} + \epsilon_i
  $$
- Choosing $k$: by *cross-validation*
*** Canonical correlation analysis (CCA)
- CCA: find the unit vectors $(r, s)$ that maximizes the *correlation coefficient*
  $$
  \text{Corr}(Xr, Ys) = \frac{r^T \Sigma_{XY}s}{\sqrt{r^T \Sigma_{XX}r}\sqrt{s^T \Sigma_{YY}s}}
  $$
- Solution: in the population(?) version with $(X_1, ..., X_p)$ and $(Y_1, ..., Y_q)$, consider $r^T X = r_1 X_1 + ... + r_p X_p$ and $s^T Y = s_1 Y_1 + ... + s_q Y_q$, the optimal $r, s$ values are the respective eigenvectors of

  $$
  \Sigma^{-1}_{XX} \Sigma^{-1}_{YY} \Sigma_{YX} \text{ and }
  \Sigma^{-1}_{YY} \Sigma^{-1}_{XX} \Sigma_{XY}
  $$
