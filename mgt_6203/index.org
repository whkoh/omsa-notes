#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: MGT 6203: Data Analytics in Business
* Module 01: Linear Regression
** Steps in Regression Analysis
1. State problem
2. Use regression for:
   - Diagnostic, or
   - Predictive, or
   - Prescriptive analytics?
3. Select relevant response and explanatory variables
4. Collect data
   - Internal
   - External
   - Purchased
   - Experiment
5. Choose fitting method
   - Ordinary least squares (OLS)
   - Generalized least squares
   - Maximum likelihood
6. Fit model
7. Validate model with diagnostics
8. Refine and iterate from step 3
9. Use model
*** Business examples (Dependent vs independent variables)
- Used car price :: odometer reading, age of car, condition
- Sales :: ad spend
- Time taken to repair product :: experience of technician
- Product added to shopping card :: ratings/price
- Starting salary of new employee :: work and education experience
- Sale price of house :: square feet, bedrooms, location
- Customer default? :: Credit score, income, age
- Customer churn? :: Length of contract, age of customer
*** M1L1 lecture quiz
1. Variable eg price *can* be either dependent or independent variable, depends on purpose of model
2. Binary value variable *can* be dependent variable, eg logistic regression
** Real estate pricing example
1. Example: sell house, predict listing price.
2. Can ask realtors, but is there a more analytical approach?
   - They normally use "comparables" to suggest a listing price
3. If actual data with price, size, bedroom and bathroom number, etc, are available, regression can be used to help get a better price estimate
*** Housing dataset ~ecdat~
- Can be used for this problem
- Data from Canadian city Windsor from 1987
- Or, collect or scrape prices from the web
- Do EDA, e.g.:
  - histogram
  - correlation matrix
  - scatter plot with linear regression line
*** M1L2 lecture quiz
1. For right-skewed distribution, mean is *larger* than median
2. Correlation coefficient only captures *linear* relationships
** Notation in regression analysis
- Explanatory :: independent variables
- Response :: dependent variables
- $i = 1, 2, ..., n$ :: $i$ th observation or record in dataset, typically a sample
- $\{x_{11}, ..., x_{p1}\}, \{...\}, \{x_{1n}, ..., x_{pn}\}$ :: $n$ observations of $p$ explanatory variables
- $y_1, ..., y_n$ :: $n$ observations of the dependent variable
- $\bar{y}$ :: mean value of dependent variable, $y$
- $\bar{x}_k$ :: mean value of $x_k$ th explanatory variable
- $\beta_0, ..., \beta_p$ :: parameters of regression line for population
- $b_0, ..., b_p$ :: estimates of $\beta$ parameters obtained by fitting the *sample* data
- $\epsilon_i$ :: error term for $i$ th observation in population
- $e_i$ :: error term for $i$ th observation int he sample
- $\widehat{y}_i$ :: estimated value of $y$ for the $i$ th observation in sample. Obtain by evaluating regression function at $x_i$
*** Steps with example
1. Observe data in ~ecdat~, which is a sample
2. Build model for population (valid relation)
   $$
   Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
   $$
3. $\epsilon_i$ are independently and identically distributed (iid) random variables, normally distributed with mean 0 and s.d. $\sigma$
4. We don't know $\beta_0$, $\beta_1$, $\sigma$ so need to estimate with *sample* data
5. Using sample, we build model:
   $$
   Y_i = b_0 + b_1 X_i + e_i
   $$
6. Population model is *homoskedastic* i.e., has constant $\sigma$, and has mean 0.
7. Split data into say 100 samples and model these. Of 100, 95 will have the *population* slope.
**** Use OLS to fit the line
Interpretation:
1. Find mean $\bar{y}$
2. Regression line: $\widehat{y} = b_0 + b_1 x$
3. Intercept at $x=0$, $y=b_0$
4. For any point $x_i$:
   - Predicted value is $\widehat{y}_i$.
   - Actual value is $y_i$.
   - Need to minimize total deviation as defined by sum of squared error.
   - Total deviation is against $\bar{y}$:
     - Explained deviation is $\widehat{y} - \bar{y}$
     - Total deviation is $y_i - \bar{y}$
     - *Residual (error term) is unexplained deviation*, i.e. $y_i - \widehat{y}_i$
*** M1L3 lecture quiz
1. The total deviation at $(x_i, y_i)$ is $y_i - \bar{y}$: true
2. In OLS, the intercepts of slope and intercept do depend on the sample being used.
** $R^2$ and Adjusted $R^2$
*** Deviations
- Sum of deviations :: SST = SSE + SSR
- SST :: Total sum of squares = $\sum_{i}(y_i-\bar{y})^2$
- SSE :: Sum of squared errors = $\sum_{i}(y_i-\widehat{y}_i)^2$
- SSR :: Sum of squares regression = $\sum_{i}(\bar{y} - \widehat{y}_i)^2$
*** Regression output $R^2$ and Adjusted $R^2$
- $R^2$ :: coefficient of determination. Measures strength of relationship between dependent and independent variables.
  - $R^2$ never decreases as you add variables
  - $R^2$ = 1-(SSE/SST) = SSR/SST = *explained deviation/total deviation*
  - $R^2$ shows how much of variation in $Y$ (vs. mean) has been explained by the model.
- Adjusted $R^2$ :: accounts for the fact that adding variables increases predictiveness
  - Adjusted $R^2$ adds a penalty for the number of independent variables $p$
  - Adjusted $R^2$ = $1-\frac{SSE/(n-p-1)}{SST/(n-1)}$
*** Examples
- When $R^2$ = 1 :: X accounts for all variation in Y. SSE = 0
- When $R^2$ = 0 :: X accounts for none of variation in Y. SSE = 1
- When $R^2$ = 0.75 :: X accounts for most of the variation. SSE = 0.25
*** M1L4 lecture quiz
1. When $R^2$ = 0, X values account for none of the variation in Y.
2. $R^2$ can take values in $(0,1)$
** Simple regression with 1 variable in R
- Using ~lm~ in R to fit simple linear regression
- Residuals are shown first. Residuals are the unexplained deviation/error, i.e. $y_i-\widehat{y}_i$.
- Output of running regression models in R:
  1. Null and alternative hypotheses
  2. Coefficients:
     - ~Estimate~ shows the value
     - ~Std. Error~ shows the standard deviation
     - ~t value~ = coefficient / standard error
     - To reject null hypothesis, ~Pr(>|t|)~ i.e. p-value must be very small
  3. p-value: the probability of finding a t-value of this size if the null hypothesis is *true*.
  4. F-statistic: the probability that $b_1 = 0$.
*** Interpreting coefficients
~(Intercept) Estimate: 3.414e+04~
~lotsize Estimate: 6.599e+00~
- $b_0$ = 34,140 :: Intercept of regression line on y. When lotsize is 0. Less useful
- $b_1$ = 6.599 :: Increase of 1,000 sqft associated with increase of 6,599 of house keeping all else constant, /ceteris paribus/.
|           |  Df |     Sum Sq | *Error term* |
|-----------+-----+------------+--------------|
| lotsize   |   1 | 1.1156e+11 | SSR          |
| Residuals | 544 | 2.7704e+11 | SSE          |
- Regression output $R^2$ and Adjusted $R^2$. *Need to know how to calculate and interpret.*
- For simple regression, $\sqrt{R^2}$ is the correlation coefficient between price and lotsize.
*** F-test: shows whether model is significant
- i.e., reject $H_0$
- Result is the F statistic
- Can range from (0, $\infty$)
*** M1L5 no lecture quiz
** Multiple regression
1. Linear regression with multiple ($p$) explanatory variables
2. Terms:
   - Regression coefficients :: $b_0, ..., b_p$ estimate $\beta_0, ..., \beta_{p}$
   - Prediction for Y at $x_i$ :: $\widehat{y} = b_0 + b_1 x_{1i} + b_2 x_{2i} + ... + b_p x_{pi}$
   - Residual :: $e_i = y_i - \widehat{y}_i$
   - SSE :: $\sum_{i} (y_i - (b_0 + b_1 x_{1i} + b_2 x_{2i} + ... + b_p x_{pi}))^2$
3. Goal is to choose $b$ terms to minimize SSE
4. Add a variable, bedrooms. Question: does number of bedrooms explain the price?
5. Do EDA with plots/histogram
*** Regression output
b terms are estimates of true parameters $\beta$ terms
$H_0$ the parameters are $0$, $H_1$ they are not $0$.
|             |  Estimate |
|-------------+-----------|
| (Intercept) | 5.613e+03 |
| lotsize     | 6.053e+00 |
| Bedrooms    | 1.057e+04 |
all /ceteris paribus/:
- $b_0$ = 5,613 :: intercept on y axis
- $b_1$ = 6.053 :: increase per square feet is $6.053
- $b_2$ = 10,570 :: increase per bedroom is $10,570
*** M1L6 no lecture quiz
** $R^2$ and adjusted $R^2$ in multiple regression
Anova table:
|           |  Df |      Sum Sq | *Error term* |
|-----------+-----+-------------+--------------|
| lotsize   |   1 |  1.1156e+11 | $SSR_1$      |
| bedrooms  |   1 | 3.23239e+10 | $SSR_2$      |
| Residuals | 543 |  2.4472e+11 | SSE          |
- $\text{SSR}_1 + \text{SSR}_2$ = SSR
- SSR + SSE = SST
- Rejecting null hypothesis by analysing the F statistic means the model is significant.
** Simple vs multiple regression
- $R^2$ will not decrease when adding variables
- Higher adjusted $R^2$ in multiple regression
- Comparing 2 models with different number of variables:
  $$
  F = \frac{(R^2_2 - R^2_1)/(p_2-p_1)}{(1-R^2_2)/(n-p_2-1)}
  $$
** M1L7 lecture quiz
1. In general adding more variable *does not decrease* overall $R^2$ value of multiple regression.
2. *Small* p-value means that *there is evidence* that coefficient is *not* 0.
** M1L8 Common problems and fixes in fixing regression (I)
*** Assumptions of linear regression
1. Linearity i.e. $E(y) = b_0 + b_1 x$, i.e. the expected value of Y at each X is approximately a straight line
2. Assumptions about errors:
   1. The error terms are independent, and identically distributed (*iid*)
   2. Each error term has mean 0 and constant variance \sigma^2, i.e. *homoscedastic*
3. Assumptions about predictors:
   1. In multiple recession, the predictor variables are linearly independent of each other
*** Common problems in fitting Linear Regression
1. Response/predictor relationship is non-linear
2. Error terms are correlated
3. Error terms have non-constant variance
4. Outliers
5. High leverage points
6. Collinearity
*** (1) Non linear relationship
**** Checks
1. Check scatter plots of Y vs X, before fitting model
2. Check residuals vs fitted values (no pattern = good)
**** Workarounds
1. Model non-linear relationship with higher order terms e.g. x^2
2. Use variance reducing transformations, e.g. log, to get better linear fit
3. Remove outliers or some parts of the observations driving non-linearity
4. Check for omitted variables
5. Check for systematic bias when collecting daya
6. *Check residuals!*
*** (2) Correlation of error terms
- There is autocorrelation if error terms are correlated
- Knowing error term e_i should not have influence on size of e_{i+1}
- This assumption is used to estimate standard errors of the model parameters
- If autocorrelation exists:
  - Estimated standard errors < true standard errors
  - Confidence intervals appear narrower than actual
  - P-values appear lower than actual
  - False sense of confidence
**** Checks
With the Durbin-Watson test
*** (3) Heteroskedasticity (non-constant error variance)
- Model assumes: spread of responses around the straight line is the same at all $x$
- Violated if there is non-constant error present, e.g. if errors increase with fitted values
- If heteroskedastic:
  - Hypothesis test and confidence intervals misleading
**** Checks
1. Residuals vs fitted values plot
   [[./img/01-residuals-hskd.png]]
**** Workarounds
1. Transform the Y variable, e.g. $ln(Y)$, $\frac{1}{Y}$, etc.

***
***
