#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: MGT 6203: Data Analytics in Business
* Module 01: Linear Regression
** Steps in Regression Analysis
1. State problem
2. Use regression for:
   - Diagnostic, or
   - Predictive, or
   - Prescriptive analytics?
3. Select relevant response and explanatory variables
4. Collect data
   - Internal
   - External
   - Purchased
   - Experiment
5. Choose fitting method
   - Ordinary least squares (OLS)
   - Generalized least squares
   - Maximum likelihood
6. Fit model
7. Validate model with diagnostics
8. Refine and iterate from step 3
9. Use model
*** Business examples (Dependent vs independent variables)
- Used car price :: odometer reading, age of car, condition
- Sales :: ad spend
- Time taken to repair product :: experience of technician
- Product added to shopping card :: ratings/price
- Starting salary of new employee :: work and education experience
- Sale price of house :: square feet, bedrooms, location
- Customer default? :: Credit score, income, age
- Customer churn? :: Length of contract, age of customer
*** M1L1 lecture quiz
1. Variable eg price *can* be either dependent or independent variable, depends on purpose of model
2. Binary value variable *can* be dependent variable, eg logistic regression
** Real estate pricing example
1. Example: sell house, predict listing price.
2. Can ask realtors, but is there a more analytical approach?
   - They normally use "comparables" to suggest a listing price
3. If actual data with price, size, bedroom and bathroom number, etc, are available, regression can be used to help get a better price estimate
*** Housing dataset ~ecdat~
- Can be used for this problem
- Data from Canadian city Windsor from 1987
- Or, collect or scrape prices from the web
- Do EDA, e.g.:
  - histogram
  - correlation matrix
  - scatter plot with linear regression line
*** M1L2 lecture quiz
1. For right-skewed distribution, mean is *larger* than median
2. Correlation coefficient only captures *linear* relationships
** Notation in regression analysis
- Explanatory :: independent variables
- Response :: dependent variables
- $i = 1, 2, ..., n$ :: $i$ th observation or record in dataset, typically a sample
- $\{x_{11}, ..., x_{p1}\}, \{...\}, \{x_{1n}, ..., x_{pn}\}$ :: $n$ observations of $p$ explanatory variables
- $y_1, ..., y_n$ :: $n$ observations of the dependent variable
- $\bar{y}$ :: mean value of dependent variable, $y$
- $\bar{x}_k$ :: mean value of $x_k$ th explanatory variable
- $\beta_0, ..., \beta_p$ :: parameters of regression line for population
- $b_0, ..., b_p$ :: estimates of $\beta$ parameters obtained by fitting the *sample* data
- $\epsilon_i$ :: error term for $i$ th observation in population
- $e_i$ :: error term for $i$ th observation int he sample
- $\widehat{y}_i$ :: estimated value of $y$ for the $i$ th observation in sample. Obtain by evaluating regression function at $x_i$
*** Steps with example
1. Observe data in ~ecdat~, which is a sample
2. Build model for population (valid relation)
   $$
   Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
   $$
3. $\epsilon_i$ are independently and identically distributed (iid) random variables, normally distributed with mean 0 and s.d. $\sigma$
4. We don't know $\beta_0$, $\beta_1$, $\sigma$ so need to estimate with *sample* data
5. Using sample, we build model:
   $$
   Y_i = b_0 + b_1 X_i + e_i
   $$
6. Population model is *homoskedastic* i.e., has constant $\sigma$, and has mean 0.
7. Split data into say 100 samples and model these. Of 100, 95 will have the *population* slope.
**** Use OLS to fit the line
Interpretation:
1. Find mean $\bar{y}$
2. Regression line: $\widehat{y} = b_0 + b_1 x$
3. Intercept at $x=0$, $y=b_0$
4. For any point $x_i$:
   - Predicted value is $\widehat{y}_i$.
   - Actual value is $y_i$.
   - Need to minimize total deviation as defined by sum of squared error.
   - Total deviation is against $\bar{y}$:
     - Explained deviation is $\widehat{y} - \bar{y}$
     - Total deviation is $y_i - \bar{y}$
     - *Residual (error term) is unexplained deviation*, i.e. $y_i - \widehat{y}_i$
*** M1L3 lecture quiz
1. The total deviation at $(x_i, y_i)$ is $y_i - \bar{y}$: true
2. In OLS, the intercepts of slope and intercept do depend on the sample being used.
** $R^2$ and Adjusted $R^2$
*** Deviations
- Sum of deviations :: SST = SSE + SSR
- SST :: Total sum of squares = $\sum_{i}(y_i-\bar{y})^2$
- SSE :: Sum of squared errors = $\sum_{i}(y_i-\widehat{y}_i)^2$
- SSR :: Sum of squares regression = $\sum_{i}(\bar{y} - \widehat{y}_i)^2$
*** Regression output $R^2$ and Adjusted $R^2$
- $R^2$ :: coefficient of determination. Measures strength of relationship between dependent and independent variables.
  - $R^2$ never decreases as you add variables
  - $R^2$ = 1-(SSE/SST) = SSR/SST = *explained deviation/total deviation*
  - $R^2$ shows how much of variation in $Y$ (vs. mean) has been explained by the model.
- Adjusted $R^2$ :: accounts for the fact that adding variables increases predictiveness
  - Adjusted $R^2$ adds a penalty for the number of independent variables $p$
  - Adjusted $R^2$ = $1-\frac{SSE/(n-p-1)}{SST/(n-1)}$
*** Examples
- When $R^2$ = 1 :: X accounts for all variation in Y. SSE = 0
- When $R^2$ = 0 :: X accounts for none of variation in Y. SSE = 1
- When $R^2$ = 0.75 :: X accounts for most of the variation. SSE = 0.25
*** M1L4 lecture quiz
1. When $R^2$ = 0, X values account for none of the variation in Y.
2. $R^2$ can take values in $(0,1)$
** Simple regression with 1 variable in R
- Using ~lm~ in R to fit simple linear regression
- Residuals are shown first. Residuals are the unexplained deviation/error, i.e. $y_i-\widehat{y}_i$.
- Output of running regression models in R:
  1. Null and alternative hypotheses
  2. Coefficients:
     - ~Estimate~ shows the value
     - ~Std. Error~ shows the standard deviation
     - ~t value~ = coefficient / standard error
     - To reject null hypothesis, ~Pr(>|t|)~ i.e. p-value must be very small
  3. p-value: the probability of finding a t-value of this size if the null hypothesis is *true*.
  4. F-statistic: the probability that $b_1 = 0$.
*** Interpreting coefficients
~(Intercept) Estimate: 3.414e+04~
~lotsize Estimate: 6.599e+00~
- $b_0$ = 34,140 :: Intercept of regression line on y. When lotsize is 0. Less useful
- $b_1$ = 6.599 :: Increase of 1,000 sqft associated with increase of 6,599 of house keeping all else constant, /ceteris paribus/.
|           |  Df |     Sum Sq | *Error term* |
|-----------+-----+------------+--------------|
| lotsize   |   1 | 1.1156e+11 | SSR          |
| Residuals | 544 | 2.7704e+11 | SSE          |
- Regression output $R^2$ and Adjusted $R^2$. *Need to know how to calculate and interpret.*
- For simple regression, $\sqrt{R^2}$ is the correlation coefficient between price and lotsize.
*** F-test: shows whether model is significant
- i.e., reject $H_0$
- Result is the F statistic
- Can range from (0, $\infty$)
*** M1L5 no lecture quiz
** Multiple regression
1. Linear regression with multiple ($p$) explanatory variables
2. Terms:
   - Regression coefficients :: $b_0, ..., b_p$ estimate $\beta_0, ..., \beta_{p}$
   - Prediction for Y at $x_i$ :: $\widehat{y} = b_0 + b_1 x_{1i} + b_2 x_{2i} + ... + b_p x_{pi}$
   - Residual :: $e_i = y_i - \widehat{y}_i$
   - SSE :: $\sum_{i} (y_i - (b_0 + b_1 x_{1i} + b_2 x_{2i} + ... + b_p x_{pi}))^2$
3. Goal is to choose $b$ terms to minimize SSE
4. Add a variable, bedrooms. Question: does number of bedrooms explain the price?
5. Do EDA with plots/histogram
*** Regression output
b terms are estimates of true parameters $\beta$ terms
$H_0$ the parameters are $0$, $H_1$ they are not $0$.
|             |  Estimate |
|-------------+-----------|
| (Intercept) | 5.613e+03 |
| lotsize     | 6.053e+00 |
| Bedrooms    | 1.057e+04 |
all /ceteris paribus/:
- $b_0$ = 5,613 :: intercept on y axis
- $b_1$ = 6.053 :: increase per square feet is $6.053
- $b_2$ = 10,570 :: increase per bedroom is $10,570
*** M1L6 no lecture quiz
** $R^2$ and adjusted $R^2$ in multiple regression
Anova table:
|           |  Df |      Sum Sq | *Error term* |
|-----------+-----+-------------+--------------|
| lotsize   |   1 |  1.1156e+11 | $SSR_1$      |
| bedrooms  |   1 | 3.23239e+10 | $SSR_2$      |
| Residuals | 543 |  2.4472e+11 | SSE          |
- $\text{SSR}_1 + \text{SSR}_2$ = SSR
- SSR + SSE = SST
- Rejecting null hypothesis by analysing the F statistic means the model is significant.
** Simple vs multiple regression
- $R^2$ will not decrease when adding variables
- Higher adjusted $R^2$ in multiple regression
- Comparing 2 models with different number of variables:
  $$
  F = \frac{(R^2_2 - R^2_1)/(p_2-p_1)}{(1-R^2_2)/(n-p_2-1)}
  $$
** M1L7 lecture quiz
1. In general adding more variable *does not decrease* overall $R^2$ value of multiple regression.
2. *Small* p-value means that *there is evidence* that coefficient is *not* 0.
** M1L8 Common problems and fixes in fixing regression (I)
*** Assumptions of linear regression
- Linearity :: i.e. $E(y) = b_0 + b_1 x$, i.e. the expected value of Y at each X is approximately a straight line
- Assumptions about error terms :: (a) The error terms are independent, and identically distributed (*iid*). (b) Each error term has mean 0 and constant variance \sigma^2, i.e. *homoscedastic*
- Assumptions about predictors :: In multiple recession, the predictor variables are linearly independent of each other
*** Common problems in fitting Linear Regression
1. Response/predictor relationship is non-linear
2. Error terms are correlated
3. Error terms have non-constant variance
4. Outliers
5. High leverage points
6. Collinearity
*** (1) Non linear relationship
**** Checks
1. Check scatter plots of Y vs X, before fitting model
2. Check residuals vs fitted values (no pattern = good)
**** Workarounds
1. Model non-linear relationship with higher order terms e.g. x^2
2. Use variance reducing transformations, e.g. log, to get better linear fit
3. Remove outliers or some parts of the observations driving non-linearity
4. Check for omitted variables
5. Check for systematic bias when collecting data
6. *Check residuals!*
*** (2) Correlation of error terms
- There is autocorrelation if error terms are correlated
- Knowing error term e_i should not have influence on size of e_{i+1}
- This assumption is used to estimate standard errors of the model parameters
- If autocorrelation exists:
  - Estimated standard errors < true standard errors
  - Confidence intervals appear narrower than actual
  - P-values appear lower than actual
  - False sense of confidence
**** Checks
With the Durbin-Watson test
*** (3) Heteroskedasticity (non-constant error variance)
- Model assumes: spread of responses around the straight line is the same at all $x$
- Violated if there is non-constant error present, e.g. if errors increase with fitted values
- If heteroskedastic:
  - Hypothesis test and confidence intervals misleading
**** Checks
1. Residuals vs fitted values plot
   [[./img/01-residuals-hskd.png]]
**** Workarounds
1. Transform the Y variable, e.g. $ln(Y)$, $\frac{1}{Y}$, etc.
*** M1L8 lecture quiz
1. Scatter plot of Y vs X shows non-linear pattern -> SHOULD change linear regression model
2. Autocorrelation is the correlation between each of the e_i variables -> True
3. Heteroskedasticity = constant error variance -> FALSE
** M1L9 Common problems and fixes in fixing regression (II)
*** (4) Outliers
1. y_i value far from predicted $\widehat{y}_i$
2. Visualise by plotting residuals, or standardized residuals, vs predicted $y$
3. Points with std residuals > 2-3 sd away from mean (0) are outliers
4. Causes:
   1. Incorrect data recording
   2. Phenomenon is not linear (model is wrong, missing predictor, etc)
5. Should ensure fit is *not* overly determined by one/ a few observations i.e. outliers
6. Outlier identified as *influential point* if it unduly influences the regression analysis
   [[./img/0109-plot-x-y-influential.png]]
**** Types
1. Y, response outlier
2. X, predictor outlier, i.e. leverage point
**** Causes
1. Case by case
2. Analyse by creating models with/without outlier to see effect on fitted line
   1. For both predictor and response outliers
*** (5) High leverage points
[[./img/0108-high-leverage.png]]
1. These are observations with predictor $x$ value outside normal range of observations for $x$
   1. Does not have large std residual
   2. Can affect regression results
2. Point has high leverage if its deletion (by itself or with 2-3 other points) causes noticeable changes in the model
3. With many predictors: it's possible to have an observation within range of each predictor's value but still be unusual
4. Measure with *Cook's distance C_i*, i.e. the difference between coeff's obtained from:
   1. The full data
   2. The full data deleting observation $i$
5. C_i > 1  = highly influential
   1. *Also* an influential point if C_i > 1
*** Plots in R
plot(lm) model provides plots:
- Residual vs fitted :: check if residuals have non-linear patterns
- Normal Q-Q :: check if residuals are normally distributed
- Scale-location ::  check if standardized residuals are spread equally along fitted values
- Residuals vs leverage :: finds influential points, with C_i > 1
*** (6) Multicollinearity
- e.g. mpg ~ cyl; mpg ~ cyl + disp + wt.
- The 2nd module exhibits multicollinearity.
- Two or more predictors are linearly related
**** Check
1. With variance inflation factors, VIF
   1. Regress X_j against all other X. Name resulting R^2 as $R^2_j$
   2. Define VIF = $\frac{1}{1-R^2_j}$, $j=1,2,...,p$
   3. If X_j has strong linear relationship to other X predictors, then R_j^2 is close to 1 and VIF_j is large
   4. VIF > 5 -> Multicollinearity
   5. In R, use ~vif(lm(model))~ to check the VIF values. ~library(car)~ is required.
2. Use correlation matrix
**** Consequences
1. OLS estimated parameters may have large variances and covariances, thus precise estimates difficult
2. Confidence intervals of estimated parameters are larger, hence can't reject H_0 (that b_i = 0)
3. Regression coeffs might have wrong sign
4. Regression coeffs might be not significantly different from 0 even for large R^2
5. Adding explanatory variable changes other varS coeff
**** Solution
1. Pick one variable if multiple ones measure the same "thing"
2. Use PCA or factor analysis to create even more useful variable(s)
* Module 02: Indicator variables and Interaction Terms
** M2L1: Intro (Customer Analytics Dataset)
- Direct Marketing dataset
- Modeling customer characteristics to predict AmountSpent
- Understand why some individuals spent more than others
  - In particular, the effect of Salary
- Scatter plot of Salary vs AmountSpent
  - Can include the regression line
*** No lecture quiz
** M2L2: Creating and Using Indicator Variables
- a.k.a. dummy variables
- Categorical variables, e.g. age, are also known as factor variables
  - They have distinct values, e.g. Age = Old, Age = Young, Age = Middle
- How to include age variable in regression model that requires numeric values?
- e.g. if we want to investigate the effect of age on AmountSpent
  - Need to quantify this variable
- With 3 possible values for $\text{Age}$, we need 2 indicator variables
  - Base case is both dummies = 0, i.e. Age = Young.
  - This is the reference group to compare for the other values of the dummy variable.
  - Any value of the categorical can be used as the base case
  - In this instance, the dummies created are:
    1. ~AgeMid~ = 1 if Age = Middle, 0 otherwise
    2. ~AgeOld~ = 1 if Age = Old, 0 otherwise
- Regression equation: $\text{AmountSpent} = b_0 + b_1*\text{AgeMid} + b_2*\text{AgeOld}$
*** M2L2 lecture quiz
- Can record have AgeMid = 0, AgeOld = 0? :: Yes, when Age = Yound
- Can record have AgeMid = 1, AgeOld = 1? :: No
** M2L3: Interpreting the coefficients of indicator variables
*** DR1: regression with dummy variable
- Coeffs:
  |           | Estimate | t-value  |
  |-----------+----------+----------|
  | Intercept |   55.862 | 10.93*** |
  | AgeMid    |   94.307 | 14.75*** |
  | AgeOld    |   87.350 | 11.03*** |
- Equation: $\text{AmountSpent} = b_0 + b_1*\text{AgeMid} + b_2*\text{AgeOld}$
- Age = Young spends average $55.862
  - This is the base case
- Middle age average AmountSpent = $55.862+$94.307
  - The AgeMid coefficient is the increase in AmountSpent for middle aged customers compared to young customers
- Graphically, the indicator variable shifts up the Mid and Old AmountSpent. All are parallel to x-asis.
- In R, a factor variable can be used directly without constructing the indicator variable, e.g.
  ~lm(AmountSpent ~ Age, data=dirmkt)~
  - R provides the new variables AgeMiddle and AgeYoung
  - Use contrasts function to find the base case, e.g. ~contrasts(dirmkt$Age)~ which shows:
    |        | Middle | Young |
    |--------+--------+-------|
    | Old    |      0 |     0 |
    | Middle |      1 |     0 |
    | Young  |      0 |     1 |
*** DR2: regression with salary and dummy variables
- Coeffs
  |           | Estimate | t-value |
  |-----------+----------+---------|
  | Intercept |    -6.12 |   -1.30 |
  | Salary    |    0.002 |      25 |
  | AgeMid    |    -4.81 |   -0.75 |
  | AgeOld    |    23.28 |    3.46 |

- Equation: $\text{AmountSpent} = b_0 + b_1*\text{Salary} + b_2*\text{AgeMid} + b_3*\text{AgeOld}$
- Graphically:
  1. b_1 and b_2 are shifting up/down the Amount
  2. Each age group has lines with slope b_0
*** M2L3 lecture quiz
- For the above coeffs, old customers spend more than young customers at the same salary level


** M2L4: Interaction term and interpreting its coefficient
- Example: regression with dummy variables.
- ~Location~ is categorical variable with values:
  - "Close" = customer lives close to store that sells similar merchandise
  - "Far" = customer does not live close...
- Create a new variable ~Far~:
  - $1$ if ~Location~ = Far
  - $0$ otherwise
- Coeffs:
  |           | Estimate | t-value |
  |-----------+----------+---------|
  | Intercept |   -20.48 |   -4.64 |
  | Salary    |    0.002 |   34.05 |
  | Far       |    59.06 |   13.38 |
- Equation: $\text{Amount Spent} = b_0 + b_1 \text{Salary} + b_2 \text{Far}$
- Estimated amt spent for customer who lives $Far$:
  - 38.58 + 0.002*Salary
- Assumptions:
  - Customers who live far away will spend *at the same rate* that customers who live nearby. Is this realistic?
*** Interaction term
- Same example but construct new variable
- $\text{SalaryFar} = \text{Salary} \times \text{Far}$
- $\text{Amount Spent} = b_0 + b_1 \text{Salary} + b_2 \text{Far} + b_3 \text{SalaryFar}$
- New coeffs:
|           | Estimate | t-value |
|-----------+----------+---------|
| Intercept |    1.448 |     0.3 |
| Salary    |    0.002 |   24.72 |
| Far       |   -13.46 |   -1.55 |
| SalaryFar |    0.001 |    9.57 |
- Interpretation of coefficient for $\text{SalaryFar}$:
  b_3 is the amount to add to b_1 to get the slope for people who live far away
  [[./img/0204-interaction_term.png]]
*** M2L4 lecture quiz
1. If the salary for customer who lives close increases by $10,000, what is the projected increase in AmountSpent for that customer?
   - $10000*0.002=$20
2. What is the equivalent increase for a customer who lives Far?
   - $10000*0.002+$10000*0.001 = $30
*** Categorical variable with $\text{M}$ values
- Indicator variable has $M$ possible values, then need to construct $M-1$ dummy variables.
- Base case always applied to the group with indicator variables set to $0$.
- All other cases interpreted with reference to base case.
** M2L5: Another example of using indicator variables
*** Airbnb Los Angeles dataset
- e.g. owner aiming to understand key factors that influence price
- Questions:
  1. Is there a relationship between capacity and price?
  2. Does the type of rental (shared, private or full home) change the relationship?
*** 2DR1: How does room price vary by capacity?
|              |  Value |
|--------------+--------|
| Intercept    | 15.039 |
| Capacity     | 38.272 |
| R^2          |  0.367 |
| Adjusted R^2 |  0.367 |
Create two dummy variables as follows. Base case (both 0) is "Shared" ~room type~.
1. ~Private_ind~:
   1. 1 if "Private room" ~room type~
   2. 0 otherwise
2. ~House_ind~
   1. 1 if "Entire home/apt" ~room type~
   2. 0 otherwise
**** 2DR1: Equation
$$
\text{Price} = b_0 + b_1*\text{Private_ind} + b_2*\text{House_ind}
$$
|             | Estimate | p-value |
|-------------+----------+---------|
| Intercept   |   37.149 |   12.58 |
| Private_ind |   35.666 |   11.42 |
| House_ind   |  133.442 |   43.64 |
**** 2DR1: Interpretation:
- Average price of room = 37.149 :: Shared room
- Average price of private room :: 37.149+35.666
- Average price of entire house :: 37.149+133.442
*** 2DR2: 2nd regression with capacity and dummies
|             | Estimate | t-value |
|-------------+----------+---------|
| Intercept   |  -19.017 |  -7.101 |
| Capacity    |   29.292 |  82.605 |
| Private_ind |   30.339 |  11.076 |
| House_ind   |   75.776 |  27.346 |
- Equation:
  $$
  \text{Price} = b_0 + b_1*\text{Capacity} + b_2*\text{Private_ind} + b_3*\text{House_ind}
  $$
- Question: What's the average increase in price for each extra person (capacity)?
- Image: [[./img/0205-2dr2-slope.png]]
*** Interaction terms
1. Create 2 new variables:
   1. ~P_Cap~: ~Private_ind~ * ~Capacity~
   2. ~H_Cap~: ~House_ind~ * ~Capacity~
2. New equation
 $$
  \text{Price} = b_0 + b_1*\text{Capacity} + b_2*\text{Private_ind} + b_3*\text{House_ind} + b_4*\text{P_Cap} + b_5*\text{H_Cap}
  $$
3. Regression coeff's:
   |             | Estimate |   t-value |
   |-------------+----------+-----------|
   | Intercept   |   35.885 |  8.728*** |
   | Capacity    |    0.659 |     0.391 |
   | Private_ind |   20.684 |  4.427*** |
   | House_ind   |    2.293 |     0.518 |
   | P_Cap       |    7.080 |  3.636*** |
   | H_Cap       |   33.414 | 19.323*** |
   |             |          |       <r> |

**** Interpretation
1. b_4 is the amount to add to b_1 to get slope for Private room
2. b_5 is the amount to add to b_1 to get slope for a House
3. Statistically, capacity and house_ind are not very different from 0.
   [[./img/0205-interaction-terms-2dr2.png]]
*** No lecture quiz
* Module 03: Non-linear transformation models
** M3L1: Intro (and why needed)
Examples:
1. Population and rank of cities
2. Residuals and predicted price (Housing dataset)
   1. Non-constant variance (heteroskedascity)
   2. Model A: $\text{price} = b_0 +b_1 *\text{lotsize}$
      - Q-Q plot shows non-linearity
      - Residuals vs fitted shows non-constant variance
- *FOCUS* on natural log transformations in 6203 as these are easier to interpret
*** Summary of models
|        | Y                                               | log(Y)                                              |
|--------+-------------------------------------------------+-----------------------------------------------------|
| X      | Model A: level-level model     $Y=b_0 + b_1X$   | Model C: log-linear model $\log(Y)=b_0+b_1X$        |
| log(x) | Model B: linear-log model $Y=b_0 + b_1\log(X)$  | Model D log-log model $\log(Y) = b_0 + b_1\log(X)$  |
Notes: if variable $x$ has values = 0, then use \log(x+1) transformation.


** M3L2: Linear-Log model
- Coeff table:
  |               | Estimate | t-value |
  |---------------+----------+---------|
  | Intercept     | -250.728 |  -12.42 |
  | \log(lotsize) |    37660 |   15.81 |

  - R^2 = 0.315
  - Adjusted R^2 = 0.3137
  - Equation:
    $\text{price} = b_0 + b_1\log(\text{lotsize})$
    - Creates a new variable \log(lotsize), which is the natural log of $\text{lotsize}$
*** Interpretation of coefficients
b_1 = 37660 implies that:
- for every 1% increase in $\text{lotsize}$:
- the price increases by $376.60 approximately (i.e. 1/100 coefficient)
- This works because increasing X by 1% is ~ increasing \log(X) by 0.01
- Hence the increase changes Y variable by 0.01*b_1
*** No lecture quiz
** M3L3: Log-Linear model
Model C: $\log(\text{price}) = b_0 + b_1\text{lotsize}$
- Coeff table
  |           |   Estimate | t-value |
  |-----------+------------+---------|
  | Intercept |      10.58 |  306.51 |
  | lotsize   | 0.00009315 |   15.08 |
  - R^2 = 0.2947
  - Adjusted R^2 = 0.2935
- Creates a new variable \log(price)
*** Interpretation of coefficients
- Coefficient b_1 indicates:
  - When lotsize increases by 1 sqft,
  - Price increases by 0.009315% on average, _i.e._
- Dependent variable changes by $100\times\text{coefficient}$ percent
  - for a 1 unit increase in independent variable
  - Keeping all other variables constant
*** Derivation
- Increase x by 1 unit increases \log(y) by b_1 units
- $\log(price) = b_0 + b_1x$ is the same as:
  $y = e^{(b_0+b_1x)}$
- Hence: $\frac{dy}{dx} = b_1y$, and $\frac{dy}{y} = b_1dx$
- Multiplying by 100:
  - $100\times\frac{dy}{y} = 100\times b_1\times dx$
- $100\times\frac{dy}{y}$ is the percentage change in $Y$
- If dx=1, then this 1 unit change in x => 100b_1 % change in $Y$
- Note that this approximation works when $b_0+b_1x$ is very small
- Accurate calc: $\text{percentage change in Y} = (e^{b_1}-1)\times100$ for 1 unit change in $X$.
*** No lecture quiz
** M3L4: Log-Log model
Both LHS and RHS are log transformed.
$$
\log(\text{price}) = b_0 + b_1 \times \log(\text{lotsize})
$$
|           | Estimate | t-value |
|-----------+----------+---------|
| Intercept |    6.468 |   23.37 |
| lotsize   |  0.54218 |   16.61 |
- R^2 = 0.3364
- Adjusted R^2 = 0.3352
*** Interpretation
- Increasing independent variable by 1% increases dependent variable by b_1 %
- i.e., Increase 1% in lotsize increases price by 0.54218%
- This works because increasing log(X) by 0.01 is almost equal to increase X by 1%, which implies changing Y by b_1 %
- *Elasticity* = % change in Y for 1% increase in X
- For log-log model, b_1 = *elasticity*
*** Reasons for log-transforming data
1. Achieve more linear r/s
2. Make distribution more normal
3. Make variance more constant
4. Get better fit in model i.e. increase R^2
*** Summary of models and interpretation

|        | Y                                               | log(Y)                                              |
|--------+-------------------------------------------------+-----------------------------------------------------|
| X      | Model A: level-level model     $Y=b_0 + b_1X$   | Model C: log-linear model $\log(Y)=b_0+b_1X$        |
| log(x) | Model B: linear-log model $Y=b_0 + b_1\log(X)$  | Model D log-log model $\log(Y) = b_0 + b_1\log(X)$  |

|        | Y                                                     | log(Y)                                             |
|--------+-------------------------------------------------------+----------------------------------------------------|
| X      | Model A: level-level (X + 1 unit, Y \Delta b_1 units) | Model C: log-linear (X + 1 unit, Y + (b1\times100)% ) |
| log(x) | Model B: linear-log (X+1%, Y+$\frac{b_1}{100}$ units)  | Model D log-log (X+1%, Y \Delta b_1%) |
*** M3L4 no lecture quiz
** M3L5: Polynomial model
Example:
$$
\text{Price} = b_0 + b_1 \times \text{lotsize} + b_2 \times \text{lotsize}^2
$$
- Creates a new variable lot_square = lotsize^2 and fit on the formula above
- *Cannot* interpret different powers by 'holding constant'
- i.e., a quadratic and higher power model does not allow for an isolated interpretation of coefficients
  - since $\frac{\text{d(price)}}{\text{d(lotsize)}} = b_1 + 2b_2\times\text{lotsize}$
- This means the slope is not constant and changes at every point of the quadratic curve
*** M3L5 no lecture quiz
* Module 04: Logistic Regression
** M4L1: Odds
- Odds express the *likelihood* of an event
- Written as X to Y or X:Y
- Gambling odds are "odds against" (i.e., the probability that event will *not* happen is greater than that it will)
  - 10 to 1 means that betting $1 wins $10, /on top of/ getting back the $1
- In this module, we deal with *odds for* or *odds on*
  - The probability that an event is more likely to happen than not
  - 2 to 1 means event is 2x as likely to happen as not
    - Gambler in this case stakes $2 and wins $1 (+$2) if event happens
*** /Odds For/ in statistics
- Odds :: ratio of probabilities
- Odds for :: ratio of $\frac{\text{probability of event happening}}{\text{probability of event NOT happening}}$
Take $p$ as probability of event happening, then: Odds for = $\frac{p}{1-p}$
- If Odds For is 2:1, then:
  $$
  \text{Odds for} = \frac{2}{1}
  = \frac{p}{1-p} \\
  p = 2-2p \\
  2=3p \\
  p=\frac{2}{3}
  $$
- Knowing Odds we can get $p$:
  $$
  p = \frac{\text{odds for}}{1+\text{odds for}}
  $$
*** M4L1 lecture quiz
1. A betting site shows odds of NE Patriots winning next Super Bowl is 5 to 1 (odds against). What is the probability of them winning the next super bowl?
   - Answer: 1/6. Odds for = 1/5, apply above formula.
2. Team Germany has a 12.5% (1/8) probability of winning the next World Cup. What is the odds for Team Germany winning the next World Cup?
   - Answer: 1/7. Apply above formula.
** M4L2: Binary Dependent Variable
- Recap: relationship between /Odds For/ and $p$
- Example of binary dependent variables
  1. whether student will get A
  2. whether firm goes bankrupt
  3. whether customer makes a purchase
  4. whether debtor defaults
  5. whether loan is approved
- Example: `GradesR.csv`
  - Variables
    - Grade = 1, student got A, else 0
    - Hours = amount of time spent
  - Using linear regression to model *binary* outcomes leads to weirdness, as regression line is continuous
    1. Predicted values \neq 0 or 1
    2. Predicted values < 0
    3. Predicted values > 1
*** M4L2 no lecture quiz
** M4L3: Logistic Regression
*** Comparison of logisitic and linear regression
Logistic regression is similar to linear regression with 2 main differences:
1. $Y$, i.e. outcome or response, is a *categorical* variable, _e.g._, Yes/No, Approve/Reject, Pass/Fail
2. Result expressed as *probability* of being in a group. Implies predicted value is [0,1].
*** Implementing logistic regression
- We use the logistic function, which gives probability of being in a group
- $p(x) = \text{Prob}(y=1|x)$, i.e. the probability of $y$ == 1 given an $x$ value
- Logistic function:
  $$
  p(x) = \frac{e^{b_0+b_1x}}{1+e^{b_0+b_1x}}, \text{i.e.,} \\
  p(x) = \frac{\exp(b_0+b_1x)}{1+\exp(b_0+b_1x)}
  $$
- p(x), simplified to $p$, will always be between 0 and 1 for all values of x
- hence:
  $$
  \frac{p}{1-p} = \exp(b_0+b_1x) \\
  \log(\frac{p}{1-p}) = b_0+b_1x
  $$
- $\log(\frac{p}{1-p})$ is the log of odds for, or *logit*
- <<Logit model>> is:
  $$
  \text{logit}(p) = \log(\frac{p}{1-p}) \\
  = b_0+b_1x
  $$
- Other components of the regression model are identical to linear regression
*** Why transform from probability to log odds?
- Mapping probability with range (0,1) to log odds with range (-\infty,\infty) will:
  1. Difficult to model variable with restricted range such as probability
  2. Get around restricted range problem of probability
  3. Among possible transformation methods, log odds is among easiest to understand and interpret
*** Interpreting logistic regression model
- *logit(p)* = $\log(\frac{p}{1-p}) = b_0+b_1x$ means:
  1. As $x$ increases by 1 unit, $\log(\text{odds})$ increases by b_1
  2. Equal to odds increasing by a factor of $\exp(b_1)$, which is approximately 100*b_1 %
  3. Exact change is (e^{b_1}-1)\times100%
*** M4L3 lecture quiz
1. The logistic function, p(x), returns values
   - [0,1]
2. $\log{\frac{p}{1-p}}=b_0+b_1x$ means that when $x$ increases by 1 unit,
   - $\log(\text{odds})$ increase by b_1
   - odds increase by factor of $\exp(b_1)$
   - odds increase by roughly $100\times b_1\%$
** M4L4: Logistic Regression using the /Default/ dataset
- Dataset from ISLR library.
- Do EDA:
  - income vs balance, color=Default
  - box plot for balance vs Default
- Logistic regression models:
  1. M1: no predictors $\text{logit}(p) = b_0$
  2. M2: single binary variable: $\text{logit}(p) = b_0+b_1\times \text{stdt}$
  3. M3: single continuous predictor var:  $\text{logit}(p) = b_0+b_1\times \text{balance}$
  4. M4: multiple predictors: $\text{logit}(p) = b_0 + b_1\times \text{balance} + b_2 \times \text{income} + b_3 \times \text{stdt}$
- Defining $p=\frac{\text{odds}}{1+\text{odds}}$
  - prob that default = "Yes"
  - $\text{logit}(p) = \log(\frac{p}{1-p})$
*** M1: no predictors $\text{logit}(p) = b_0$
- Intercept = -3.368 = log-odds of being in default for entire population
- hence $\exp(-3.368)$ = odds = p/(1-p)
- p = 0.0333
*** M2: single binary predictor  $\text{logit}(p) = b_0+b_1\times \text{stdt}$
- Coefficients:
  - Intercept = -3.50
  - stdt = 0.40
- Intercept is for non-students (stdt = 0)
  - This is the reference / base case
- Odds for non-student = $\exp(-3.50)$
  - p=0.0292
- Odds for student = $\exp(-3.50+0.40)$
  - p=0.0431 (higher than non-students)
*** M3: single continuous predictor  $\text{logit}(p) = b_0+b_1\times \text{balance}$
- Coefficients:
  - Intercept = -1.065e+01
  - balance = 5.499e-03 (i.e. 0.0055)
- $b_1$ = 0.0055
- Increase in balance is associated with increase in log-odds (and hence the odds for) of default
- Adding $1 to balance increases log-odds by 0.0055
- Examples, with $p(x)=\frac{e^{b_0+b_1x}}{1+{e^{b_0+b_1x}}}$
  |    x |       p |
  |------+---------|
  | 1000 | 0.00576 |
  | 1500 | 0.08317 |
  | 2000 | 0.5866  |
- Increasing x by 500 has non-linear effect on p(x)
*** M4: multiple predictors $\text{logit}(p) = b_0 + b_1\times \text{balance} + b_2 \times \text{income} + b_3 \times \text{stdt}$
- Coefficients
   - Intercept: -1.087e+01 (10.87)
  - balance: 5.737e-03 (0.0057)
  - income: 3.033e-06 (0.000003)
  - stdt: -6.468e-01 (-0.65)
- Interpretation:
  - Increase of balance associated with increasing log odds and hence probability of default
  - Adding $1 to balance increases log odds by 0.0057
  - Students now less likely to default due to -0.65 log odds, *different result from M2*
    - This can be explained by students carrying more balances, hence they default at higher rate
    - Log curve is shifted to the right
    - *Confounding* -> there is correlation between student and balance
*** M4L4 lecture quiz
1. For logistic model where Y=1 is the default case:
   - $b_0$ means the log odds of being in default for the entire population
** M4L5: Predictions and Confusion Matrix
- Making predictions on fitted data, e.g. M4: $\text{logit}(p) = b_0 + b_1\times \text{balance} + b_2 \times \text{income} + b_3 \times \text{stdt}$
- Make predictions by using predict with Model 4, and use 0.5 as the cutoff
*** True Negative and False Positive
- Consider all Y=0 observed (no defaults)
- For each observation, use the logic model to make prediction with the X values
- If predicted value == 0, that's a true negative
- Else predicted value == 1, false positive
- As the cutoff is increased, true negatives increase and false positives decrease
*** True Positive and False Negative
- Consider all Y=1 observed (defaults)
- For each observation, use the logic model to make prediction with the X values
- If predicted value == 1, that's a true positive
- Else predicted value == 0, false negative
- As the cutoff is increased, false negatives increase and true positives decrease
*** Confusion matrix
- Shows the intersection of actual and predicted values of model
- Matrix records the performance of classifier, i.e. lets you gauge how models perform
- Record predicted value $\hat{y}$ after fitting logit model on dataset.
** M4L6: Sensitivity, Specificity, and the ROC curve
*** Definitions
- Sensitivity :: true positive rate: $\frac{\text{true pos}}{\text{true pos + false neg}}$
- Specificity :: true negative rate $\frac{\text{true neg}}{\text{true neg + false pos}}$
- 1-Specificity :: False positive rate $\frac{\text{false pos}}{\text{true neg + false pos}}$
- Precision :: $P(Y=1|\hat{y}=1) = \frac{\text{true pos}}{\text{true pos+false pos}}$
- Accuracy :: $\frac{\text{true pos+true neg}}{\text{true pos + false pos + true neg + false neg}}$
*** Type I and II errors
- False positive error :: type I error. Falsely reject true (null) hypothesis. true Y=0, predicted Y=1.
- False negative error :: type II error. Incorrectly retain - fail to reject - false (null) hypothesis. True Y=1, predicted Y=0
**** Cutoff
1. Increasing cutoff decreases Type I error
2. Increasing cutoff increases Type II error
**** In business applications
Cost of making error depends on situation, e.g.:
- When modeling marketing spend, false positive means spending marketing costs on non-purchasers. Moderate loss. (Type I) error
- When modeling customer as non-defaulter who actually defaults. (Type II) error
*** Calculating sensitivity and specificity for cutoff $p = 0.5$
|        |   | Predicted     | Values       |       |
|--------+---+---------------+--------------+-------|
|        |   | 0             | 1            | Total |
| True   | 0 | 9627 true neg | 40 false pos |  9667 |
| Values | 1 | 228 false neg | 105 true pos |   333 |
|        |   | 9855          | 145          | 10000 |
- Sensitivity =  $\frac{\text{true pos}}{\text{true pos + false neg}}$ = 105/(105+228) = 0.32
- Specificity = $\frac{\text{true neg}}{\text{true neg + false pos}}$ = 9627/(9627+40) = 0.996
*** Increasing cutoff to $p=0.9$
- Sensitivity decreases to 0.03
- Specificity increases to 0.9998
*** Increasing cutoff value of $p$
- True neg increase, false pos decrease
- False neg increase, true pos decrease
*** ROC Curve
- "Receiver Operating Characteristic" curve
- Shows diagnostic ability of binary classifier, when the cutoff value is varied
- Look at area under curve. We want > 0.5
*** M4L6 lecture quiz
1. Formula of sensitivity  $\frac{\text{true pos}}{\text{true pos + false neg}}$
2. Formula of specificity $\frac{\text{true neg}}{\text{true neg + false pos}}$
