#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: MGT 6203: Data Analytics in Business
* Module 01: Linear Regression
** Steps in Regression Analysis
1. State problem
2. Use regression for:
   - Diagnostic, or
   - Predictive, or
   - Prescriptive analytics?
3. Select relevant response and explanatory variables
4. Collect data
   - Internal
   - External
   - Purchased
   - Experiment
5. Choose fitting method
   - Ordinary least squares (OLS)
   - Generalized least squares
   - Maximum likelihood
6. Fit model
7. Validate model with diagnostics
8. Refine and iterate from step 3
9. Use model
*** Business examples (Dependent vs independent variables)
- Used car price :: odometer reading, age of car, condition
- Sales :: ad spend
- Time taken to repair product :: experience of technician
- Product added to shopping card :: ratings/price
- Starting salary of new employee :: work and education experience
- Sale price of house :: square feet, bedrooms, location
- Customer default? :: Credit score, income, age
- Customer churn? :: Length of contract, age of customer
*** M1L1 lecture quiz
1. Variable eg price *can* be either dependent or independent variable, depends on purpose of model
2. Binary value variable *can* be dependent variable, eg logistic regression
** Real estate pricing example
1. Example: sell house, predict listing price.
2. Can ask realtors, but is there a more analytical approach?
   - They normally use "comparables" to suggest a listing price
3. If actual data with price, size, bedroom and bathroom number, etc, are available, regression can be used to help get a better price estimate
*** Housing dataset ~ecdat~
- Can be used for this problem
- Data from Canadian city Windsor from 1987
- Or, collect or scrape prices from the web
- Do EDA, e.g.:
  - histogram
  - correlation matrix
  - scatter plot with linear regression line
*** M1L2 lecture quiz
1. For right-skewed distribution, mean is *larger* than median
2. Correlation coefficient only captures *linear* relationships
** Notation in regression analysis
- Explanatory :: independent variables
- Response :: dependent variables
- $i = 1, 2, ..., n$ :: $i$ th observation or record in dataset, typically a sample
- $\{x_{11}, ..., x_{p1}\}, \{...\}, \{x_{1n}, ..., x_{pn}\}$ :: $n$ observations of $p$ explanatory variables
- $y_1, ..., y_n$ :: $n$ observations of the dependent variable
- $\bar{y}$ :: mean value of dependent variable, $y$
- $\bar{x}_k$ :: mean value of $x_k$ th explanatory variable
- $\beta_0, ..., \beta_p$ :: parameters of regression line for population
- $b_0, ..., b_p$ :: estimates of $\beta$ parameters obtained by fitting the *sample* data
- $\epsilon_i$ :: error term for $i$ th observation in population
- $e_i$ :: error term for $i$ th observation int he sample
- $\widehat{y}_i$ :: estimated value of $y$ for the $i$ th observation in sample. Obtain by evaluating regression function at $x_i$
*** Steps with example
1. Observe data in ~ecdat~, which is a sample
2. Build model for population (valid relation)
   $$
   Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
   $$
3. $\epsilon_i$ are independently and identically distributed (iid) random variables, normally distributed with mean 0 and s.d. $\sigma$
4. We don't know $\beta_0$, $\beta_1$, $\sigma$ so need to estimate with *sample* data
5. Using sample, we build model:
   $$
   Y_i = b_0 + b_1 X_i + e_i
   $$
6. Population model is *homoskedastic* i.e., has constant $\sigma$, and has mean 0.
7. Split data into say 100 samples and model these. Of 100, 95 will have the *population* slope.
**** Use OLS to fit the line
Interpretation:
1. Find mean $\bar{y}$
2. Regression line: $\widehat{y} = b_0 + b_1 x$
3. Intercept at $x=0$, $y=b_0$
4. For any point $x_i$:
   - Predicted value is $\widehat{y}_i$.
   - Actual value is $y_i$.
   - Need to minimize total deviation as defined by sum of squared error.
   - Total deviation is against $\bar{y}$:
     - Explained deviation is $\widehat{y} - \bar{y}$
     - Total deviation is $y_i - \bar{y}$
     - *Residual (error term) is unexplained deviation*, i.e. $y_i - \widehat{y}_i$
*** M1L3 lecture quiz
1. The total deviation at $(x_i, y_i)$ is $y_i - \bar{y}$: true
2. In OLS, the intercepts of slope and intercept do depend on the sample being used.
** $R^2$ and Adjusted $R^2$
*** Deviations
- Sum of deviations :: SST = SSE + SSR
- SST :: Total sum of squares = $\sum_{i}(y_i-\bar{y})^2$
- SSE :: Sum of squared errors = $\sum_{i}(y_i-\widehat{y}_i)^2$
- SSR :: Sum of squares regression = $\sum_{i}(\bar{y} - \widehat{y}_i)^2$
*** Regression output $R^2$ and Adjusted $R^2$
- $R^2$ :: coefficient of determination. Measures strength of relationship between dependent and independent variables.
  - $R^2$ never decreases as you add variables
  - $R^2$ = 1-(SSE/SST) = SSR/SST = *explained deviation/total deviation*
  - $R^2$ shows how much of variation in $Y$ (vs. mean) has been explained by the model.
- Adjusted $R^2$ :: accounts for the fact that adding variables increases predictiveness
  - Adjusted $R^2$ adds a penalty for the number of independent variables $p$
  - Adjusted $R^2$ = $1-\frac{SSE/(n-p-1)}{SST/(n-1)}$
*** Examples
- When $R^2$ = 1 :: X accounts for all variation in Y. SSE = 0
- When $R^2$ = 0 :: X accounts for none of variation in Y. SSE = 1
- When $R^2$ = 0.75 :: X accounts for most of the variation. SSE = 0.25
*** M1L4 lecture quiz
1. When $R^2$ = 0, X values account for none of the variation in Y.
2. $R^2$ can take values in $(0,1)$
** Simple regression with 1 variable in R
- Using ~lm~ in R to fit simple linear regression
- Residuals are shown first. Residuals are the unexplained deviation/error, i.e. $y_i-\widehat{y}_i$.
- Output of running regression models in R:
  1. Null and alternative hypotheses
  2. Coefficients:
     - ~Estimate~ shows the value
     - ~Std. Error~ shows the standard deviation
     - ~t value~ = coefficient / standard error
     - To reject null hypothesis, ~Pr(>|t|)~ i.e. p-value must be very small
  3. p-value: the probability of finding a t-value of this size if the null hypothesis is *true*.
  4. F-statistic: the probability that $b_1 = 0$.
*** Interpreting coefficients
~(Intercept) Estimate: 3.414e+04~
~lotsize Estimate: 6.599e+00~
- $b_0$ = 34,140 :: Intercept of regression line on y. When lotsize is 0. Less useful
- $b_1$ = 6.599 :: Increase of 1,000 sqft associated with increase of 6,599 of house keeping all else constant, /ceteris paribus/.
|           |  Df |     Sum Sq | *Error term* |
|-----------+-----+------------+--------------|
| lotsize   |   1 | 1.1156e+11 | SSR          |
| Residuals | 544 | 2.7704e+11 | SSE          |
- Regression output $R^2$ and Adjusted $R^2$. *Need to know how to calculate and interpret.*
- For simple regression, $\sqrt{R^2}$ is the correlation coefficient between price and lotsize.
*** F-test: shows whether model is significant
- i.e., reject $H_0$
- Result is the F statistic
- Can range from (0, $\infty$)
*** M1L5 no lecture quiz
** Multiple regression
1. Linear regression with multiple ($p$) explanatory variables
2. Terms:
   - Regression coefficients :: $b_0, ..., b_p$ estimate $\beta_0, ..., \beta_{p}$
   - Prediction for Y at $x_i$ :: $\widehat{y} = b_0 + b_1 x_{1i} + b_2 x_{2i} + ... + b_p x_{pi}$
   - Residual :: $e_i = y_i - \widehat{y}_i$
   - SSE :: $\sum_{i} (y_i - (b_0 + b_1 x_{1i} + b_2 x_{2i} + ... + b_p x_{pi}))^2$
3. Goal is to choose $b$ terms to minimize SSE
4. Add a variable, bedrooms. Question: does number of bedrooms explain the price?
5. Do EDA with plots/histogram
*** Regression output
b terms are estimates of true parameters $\beta$ terms
$H_0$ the parameters are $0$, $H_1$ they are not $0$.
|             |  Estimate |
|-------------+-----------|
| (Intercept) | 5.613e+03 |
| lotsize     | 6.053e+00 |
| Bedrooms    | 1.057e+04 |
all /ceteris paribus/:
- $b_0$ = 5,613 :: intercept on y axis
- $b_1$ = 6.053 :: increase per square feet is $6.053
- $b_2$ = 10,570 :: increase per bedroom is $10,570
*** M1L6 no lecture quiz
** $R^2$ and adjusted $R^2$ in multiple regression
Anova table:
|           |  Df |      Sum Sq | *Error term* |
|-----------+-----+-------------+--------------|
| lotsize   |   1 |  1.1156e+11 | $SSR_1$      |
| bedrooms  |   1 | 3.23239e+10 | $SSR_2$      |
| Residuals | 543 |  2.4472e+11 | SSE          |
- $\text{SSR}_1 + \text{SSR}_2$ = SSR
- SSR + SSE = SST
- Rejecting null hypothesis by analysing the F statistic means the model is significant.
** Simple vs multiple regression
- $R^2$ will not decrease when adding variables
- Higher adjusted $R^2$ in multiple regression
- Comparing 2 models with different number of variables:
  $$
  F = \frac{(R^2_2 - R^2_1)/(p_2-p_1)}{(1-R^2_2)/(n-p_2-1)}
  $$
** M1L7 lecture quiz
1. In general adding more variable *does not decrease* overall $R^2$ value of multiple regression.
2. *Small* p-value means that *there is evidence* that coefficient is *not* 0.
** M1L8 Common problems and fixes in fixing regression (I)
*** Assumptions of linear regression
- Linearity :: i.e. $E(y) = b_0 + b_1 x$, i.e. the expected value of Y at each X is approximately a straight line
- Assumptions about error terms :: (a) The error terms are independent, and identically distributed (*iid*). (b) Each error term has mean 0 and constant variance \sigma^2, i.e. *homoscedastic*
- Assumptions about predictors :: In multiple recession, the predictor variables are linearly independent of each other
*** Common problems in fitting Linear Regression
1. Response/predictor relationship is non-linear
2. Error terms are correlated
3. Error terms have non-constant variance
4. Outliers
5. High leverage points
6. Collinearity
*** (1) Non linear relationship
**** Checks
1. Check scatter plots of Y vs X, before fitting model
2. Check residuals vs fitted values (no pattern = good)
**** Workarounds
1. Model non-linear relationship with higher order terms e.g. x^2
2. Use variance reducing transformations, e.g. log, to get better linear fit
3. Remove outliers or some parts of the observations driving non-linearity
4. Check for omitted variables
5. Check for systematic bias when collecting data
6. *Check residuals!*
*** (2) Correlation of error terms
- There is autocorrelation if error terms are correlated
- Knowing error term e_i should not have influence on size of e_{i+1}
- This assumption is used to estimate standard errors of the model parameters
- If autocorrelation exists:
  - Estimated standard errors < true standard errors
  - Confidence intervals appear narrower than actual
  - P-values appear lower than actual
  - False sense of confidence
**** Checks
With the Durbin-Watson test
*** (3) Heteroskedasticity (non-constant error variance)
- Model assumes: spread of responses around the straight line is the same at all $x$
- Violated if there is non-constant error present, e.g. if errors increase with fitted values
- If heteroskedastic:
  - Hypothesis test and confidence intervals misleading
**** Checks
1. Residuals vs fitted values plot
   [[./img/01-residuals-hskd.png]]
**** Workarounds
1. Transform the Y variable, e.g. $ln(Y)$, $\frac{1}{Y}$, etc.
*** M1L8 lecture quiz
1. Scatter plot of Y vs X shows non-linear pattern -> SHOULD change linear regression model
2. Autocorrelation is the correlation between each of the e_i variables -> True
3. Heteroskedasticity = constant error variance -> FALSE
** M1L9 Common problems and fixes in fixing regression (II)
*** (4) Outliers
1. y_i value far from predicted $\widehat{y}_i$
2. Visualise by plotting residuals, or standardized residuals, vs predicted $y$
3. Points with std residuals > 2-3 sd away from mean (0) are outliers
4. Causes:
   1. Incorrect data recording
   2. Phenomenon is not linear (model is wrong, missing predictor, etc)
5. Should ensure fit is *not* overly determined by one/ a few observations i.e. outliers
6. Outlier identified as *influential point* if it unduly influences the regression analysis
   [[./img/0109-plot-x-y-influential.png]]
**** Types
1. Y, response outlier
2. X, predictor outlier, i.e. leverage point
**** Causes
1. Case by case
2. Analyse by creating models with/without outlier to see effect on fitted line
   1. For both predictor and response outliers
*** (5) High leverage points
[[./img/0108-high-leverage.png]]
1. These are observations with predictor $x$ value outside normal range of observations for $x$
   1. Does not have large std residual
   2. Can affect regression results
2. Point has high leverage if its deletion (by itself or with 2-3 other points) causes noticeable changes in the model
3. With many predictors: it's possible to have an observation within range of each predictor's value but still be unusual
4. Measure with *Cook's distance C_i*, i.e. the difference between coeff's obtained from:
   1. The full data
   2. The full data deleting observation $i$
5. C_i > 1  = highly influential
   1. *Also* an influential point if C_i > 1
*** Plots in R
plot(lm) model provides plots:
- Residual vs fitted :: check if residuals have non-linear patterns
- Normal Q-Q :: check if residuals are normally distributed
- Scale-location ::  check if standardized residuals are spread equally along fitted values
- Residuals vs leverage :: finds influential points, with C_i > 1
*** (6) Multicollinearity
- e.g. mpg ~ cyl; mpg ~ cyl + disp + wt.
- The 2nd module exhibits multicollinearity.
- Two or more predictors are linearly related
**** Check
1. With variance inflation factors, VIF
   1. Regress X_j against all other X. Name resulting R^2 as $R^2_j$
   2. Define VIF = $\frac{1}{1-R^2_j}$, $j=1,2,...,p$
   3. If X_j has strong linear relationship to other X predictors, then R_j^2 is close to 1 and VIF_j is large
   4. VIF > 5 -> Multicollinearity
   5. In R, use ~vif(lm(model))~ to check the VIF values. ~library(car)~ is required.
2. Use correlation matrix
**** Consequences
1. OLS estimated parameters may have large variances and covariances, thus precise estimates difficult
2. Confidence intervals of estimated parameters are larger, hence can't reject H_0 (that b_i = 0)
3. Regression coeffs might have wrong sign
4. Regression coeffs might be not significantly different from 0 even for large R^2
5. Adding explanatory variable changes other varS coeff
**** Solution
1. Pick one variable if multiple ones measure the same "thing"
2. Use PCA or factor analysis to create even more useful variable(s)
* Module 02: Indicator variables and Interaction Terms
** M2L1: Intro (Customer Analytics Dataset)
- Direct Marketing dataset
- Modeling customer characteristics to predict AmountSpent
- Understand why some individuals spent more than others
  - In particular, the effect of Salary
- Scatter plot of Salary vs AmountSpent
  - Can include the regression line
*** No lecture quiz
** M2L2: Creating and Using Indicator Variables
- a.k.a. dummy variables
- Categorical variables, e.g. age, are also known as factor variables
  - They have distinct values, e.g. Age = Old, Age = Young, Age = Middle
- How to include age variable in regression model that requires numeric values?
- e.g. if we want to investigate the effect of age on AmountSpent
  - Need to quantify this variable
- With 3 possible values for $\text{Age}$, we need 2 indicator variables
  - Base case is both dummies = 0, i.e. Age = Young.
  - This is the reference group to compare for the other values of the dummy variable.
  - Any value of the categorical can be used as the base case
  - In this instance, the dummies created are:
    1. ~AgeMid~ = 1 if Age = Middle, 0 otherwise
    2. ~AgeOld~ = 1 if Age = Old, 0 otherwise
- Regression equation: $\text{AmountSpent} = b_0 + b_1*\text{AgeMid} + b_2*\text{AgeOld}$
*** M2L2 lecture quiz
- Can record have AgeMid = 0, AgeOld = 0? :: Yes, when Age = Yound
- Can record have AgeMid = 1, AgeOld = 1? :: No
** M2L3: Interpreting the coefficients of indicator variables
*** DR1: regression with dummy variable
- Coeffs:
  |           | Estimate | t-value  |
  |-----------+----------+----------|
  | Intercept |   55.862 | 10.93*** |
  | AgeMid    |   94.307 | 14.75*** |
  | AgeOld    |   87.350 | 11.03*** |
- Equation: $\text{AmountSpent} = b_0 + b_1*\text{AgeMid} + b_2*\text{AgeOld}$
- Age = Young spends average $55.862
  - This is the base case
- Middle age average AmountSpent = $55.862+$94.307
  - The AgeMid coefficient is the increase in AmountSpent for middle aged customers compared to young customers
- Graphically, the indicator variable shifts up the Mid and Old AmountSpent. All are parallel to x-asis.
- In R, a factor variable can be used directly without constructing the indicator variable, e.g.
  ~lm(AmountSpent ~ Age, data=dirmkt)~
  - R provides the new variables AgeMiddle and AgeYoung
  - Use contrasts function to find the base case, e.g. ~contrasts(dirmkt$Age)~ which shows:
    |        | Middle | Young |
    |--------+--------+-------|
    | Old    |      0 |     0 |
    | Middle |      1 |     0 |
    | Young  |      0 |     1 |
*** DR2: regression with salary and dummy variables
- Coeffs
  |           | Estimate | t-value |
  |-----------+----------+---------|
  | Intercept |    -6.12 |   -1.30 |
  | Salary    |    0.002 |      25 |
  | AgeMid    |    -4.81 |   -0.75 |
  | AgeOld    |    23.28 |    3.46 |

- Equation: $\text{AmountSpent} = b_0 + b_1*\text{Salary} + b_2*\text{AgeMid} + b_3*\text{AgeOld}$
- Graphically:
  1. b_1 and b_2 are shifting up/down the Amount
  2. Each age group has lines with slope b_0
*** M2L3 lecture quiz
- For the above coeffs, old customers spend more than young customers at the same salary level


** M2L4: Interaction term and interpreting its coefficient
- Example: regression with dummy variables.
- ~Location~ is categorical variable with values:
  - "Close" = customer lives close to store that sells similar merchandise
  - "Far" = customer does not live close...
- Create a new variable ~Far~:
  - $1$ if ~Location~ = Far
  - $0$ otherwise
- Coeffs:
  |           | Estimate | t-value |
  |-----------+----------+---------|
  | Intercept |   -20.48 |   -4.64 |
  | Salary    |    0.002 |   34.05 |
  | Far       |    59.06 |   13.38 |
- Equation: $\text{Amount Spent} = b_0 + b_1 \text{Salary} + b_2 \text{Far}$
- Estimated amt spent for customer who lives $Far$:
  - 38.58 + 0.002*Salary
- Assumptions:
  - Customers who live far away will spend *at the same rate* that customers who live nearby. Is this realistic?
*** Interaction term
- Same example but construct new variable
- $\text{SalaryFar} = \text{Salary} \times \text{Far}$
- $\text{Amount Spent} = b_0 + b_1 \text{Salary} + b_2 \text{Far} + b_3 \text{SalaryFar}$
- New coeffs:
|           | Estimate | t-value |
|-----------+----------+---------|
| Intercept |    1.448 |     0.3 |
| Salary    |    0.002 |   24.72 |
| Far       |   -13.46 |   -1.55 |
| SalaryFar |    0.001 |    9.57 |
- Interpretation of coefficient for $\text{SalaryFar}$:
  b_3 is the amount to add to b_1 to get the slope for people who live far away
  [[./img/0204-interaction_term.png]]
*** M2L4 lecture quiz
1. If the salary for customer who lives close increases by $10,000, what is the projected increase in AmountSpent for that customer?
   - $10000*0.002=$20
2. What is the equivalent increase for a customer who lives Far?
   - $10000*0.002+$10000*0.001 = $30
*** Categorical variable with $\text{M}$ values
- Indicator variable has $M$ possible values, then need to construct $M-1$ dummy variables.
- Base case always applied to the group with indicator variables set to $0$.
- All other cases interpreted with reference to base case.
** M2L5: Another example of using indicator variables
*** Airbnb Los Angeles dataset
- e.g. owner aiming to understand key factors that influence price
- Questions:
  1. Is there a relationship between capacity and price?
  2. Does the type of rental (shared, private or full home) change the relationship?
*** 2DR1: How does room price vary by capacity?
|              |  Value |
|--------------+--------|
| Intercept    | 15.039 |
| Capacity     | 38.272 |
| R^2          |  0.367 |
| Adjusted R^2 |  0.367 |
Create two dummy variables as follows. Base case (both 0) is "Shared" ~room type~.
1. ~Private_ind~:
   1. 1 if "Private room" ~room type~
   2. 0 otherwise
2. ~House_ind~
   1. 1 if "Entire home/apt" ~room type~
   2. 0 otherwise
**** 2DR1: Equation
$$
\text{Price} = b_0 + b_1*\text{Private_ind} + b_2*\text{House_ind}
$$
|             | Estimate | p-value |
|-------------+----------+---------|
| Intercept   |   37.149 |   12.58 |
| Private_ind |   35.666 |   11.42 |
| House_ind   |  133.442 |   43.64 |
**** 2DR1: Interpretation:
- Average price of room = 37.149 :: Shared room
- Average price of private room :: 37.149+35.666
- Average price of entire house :: 37.149+133.442
*** 2DR2: 2nd regression with capacity and dummies
|             | Estimate | t-value |
|-------------+----------+---------|
| Intercept   |  -19.017 |  -7.101 |
| Capacity    |   29.292 |  82.605 |
| Private_ind |   30.339 |  11.076 |
| House_ind   |   75.776 |  27.346 |
- Equation:
  $$
  \text{Price} = b_0 + b_1*\text{Capacity} + b_2*\text{Private_ind} + b_3*\text{House_ind}
  $$
- Question: What's the average increase in price for each extra person (capacity)?
- Image: [[./img/0205-2dr2-slope.png]]
*** Interaction terms
1. Create 2 new variables:
   1. ~P_Cap~: ~Private_ind~ * ~Capacity~
   2. ~H_Cap~: ~House_ind~ * ~Capacity~
2. New equation
 $$
  \text{Price} = b_0 + b_1*\text{Capacity} + b_2*\text{Private_ind} + b_3*\text{House_ind} + b_4*\text{P_Cap} + b_5*\text{H_Cap}
  $$
3. Regression coeff's:
   |             | Estimate |   t-value |
   |-------------+----------+-----------|
   | Intercept   |   35.885 |  8.728*** |
   | Capacity    |    0.659 |     0.391 |
   | Private_ind |   20.684 |  4.427*** |
   | House_ind   |    2.293 |     0.518 |
   | P_Cap       |    7.080 |  3.636*** |
   | H_Cap       |   33.414 | 19.323*** |
   |             |          |       <r> |

**** Interpretation
1. b_4 is the amount to add to b_1 to get slope for Private room
2. b_5 is the amount to add to b_1 to get slope for a House
3. Statistically, capacity and house_ind are not very different from 0.
   [[./img/0205-interaction-terms-2dr2.png]]
*** No lecture quiz
* Module 03: Non-linear transformation models
** M3L1: Intro (and why needed)
Examples:
1. Population and rank of cities
2. Residuals and predicted price (Housing dataset)
   1. Non-constant variance (heteroskedascity)
   2. Model A: $\text{price} = b_0 +b_1 *\text{lotsize}$
      - Q-Q plot shows non-linearity
      - Residuals vs fitted shows non-constant variance
- *FOCUS* on natural log transformations in 6203 as these are easier to interpret
*** Summary of models
|        | Y                                               | log(Y)                                              |
|--------+-------------------------------------------------+-----------------------------------------------------|
| X      | Model A: level-level model     $Y=b_0 + b_1X$   | Model C: log-linear model $\log(Y)=b_0+b_1X$        |
| log(x) | Model B: linear-log model $Y=b_0 + b_1\log(X)$  | Model D log-log model $\log(Y) = b_0 + b_1\log(X)$  |
Notes: if variable $x$ has values = 0, then use \log(x+1) transformation.


** M3L2: Linear-Log model
- Coeff table:
  |               | Estimate | t-value |
  |---------------+----------+---------|
  | Intercept     | -250.728 |  -12.42 |
  | \log(lotsize) |    37660 |   15.81 |

  - R^2 = 0.315
  - Adjusted R^2 = 0.3137
  - Equation:
    $\text{price} = b_0 + b_1\log(\text{lotsize})$
    - Creates a new variable \log(lotsize), which is the natural log of $\text{lotsize}$
*** Interpretation of coefficients
b_1 = 37660 implies that:
- for every 1% increase in $\text{lotsize}$:
- the price increases by $376.60 approximately (i.e. 1/100 coefficient)
- This works because increasing X by 1% is ~ increasing \log(X) by 0.01
- Hence the increase changes Y variable by 0.01*b_1
*** No lecture quiz
** M3L3: Log-Linear model
Model C: $\log(\text{price}) = b_0 + b_1\text{lotsize}$
- Coeff table
  |           |   Estimate | t-value |
  |-----------+------------+---------|
  | Intercept |      10.58 |  306.51 |
  | lotsize   | 0.00009315 |   15.08 |
  - R^2 = 0.2947
  - Adjusted R^2 = 0.2935
- Creates a new variable \log(price)
*** Interpretation of coefficients
- Coefficient b_1 indicates:
  - When lotsize increases by 1 sqft,
  - Price increases by 0.009315% on average, _i.e._
- Dependent variable changes by $100\times\text{coefficient}$ percent
  - for a 1 unit increase in independent variable
  - Keeping all other variables constant
*** Derivation
- Increase x by 1 unit increases \log(y) by b_1 units
- $\log(price) = b_0 + b_1x$ is the same as:
  $y = e^{(b_0+b_1x)}$
- Hence: $\frac{dy}{dx} = b_1y$, and $\frac{dy}{y} = b_1dx$
- Multiplying by 100:
  - $100\times\frac{dy}{y} = 100\times b_1\times dx$
- $100\times\frac{dy}{y}$ is the percentage change in $Y$
- If dx=1, then this 1 unit change in x => 100b_1 % change in $Y$
- Note that this approximation works when $b_0+b_1x$ is very small
- Accurate calc: $\text{percentage change in Y} = (e^{b_1}-1)\times100$ for 1 unit change in $X$.
*** No lecture quiz
** M3L4: Log-Log model
Both LHS and RHS are log transformed.
$$
\log(\text{price}) = b_0 + b_1 \times \log(\text{lotsize})
$$
|           | Estimate | t-value |
|-----------+----------+---------|
| Intercept |    6.468 |   23.37 |
| lotsize   |  0.54218 |   16.61 |
- R^2 = 0.3364
- Adjusted R^2 = 0.3352
*** Interpretation
- Increasing independent variable by 1% increases dependent variable by b_1 %
- i.e., Increase 1% in lotsize increases price by 0.54218%
- This works because increasing log(X) by 0.01 is almost equal to increase X by 1%, which implies changing Y by b_1 %
- *Elasticity* = % change in Y for 1% increase in X
- For log-log model, b_1 = *elasticity*
*** Reasons for log-transforming data
1. Achieve more linear r/s
2. Make distribution more normal
3. Make variance more constant
4. Get better fit in model i.e. increase R^2
*** Summary of models and interpretation

|        | Y                                               | log(Y)                                              |
|--------+-------------------------------------------------+-----------------------------------------------------|
| X      | Model A: level-level model     $Y=b_0 + b_1X$   | Model C: log-linear model $\log(Y)=b_0+b_1X$        |
| log(x) | Model B: linear-log model $Y=b_0 + b_1\log(X)$  | Model D log-log model $\log(Y) = b_0 + b_1\log(X)$  |

|        | Y                                                     | log(Y)                                             |
|--------+-------------------------------------------------------+----------------------------------------------------|
| X      | Model A: level-level (X + 1 unit, Y \Delta b_1 units) | Model C: log-linear (X + 1 unit, Y + (b1\times100)% ) |
| log(x) | Model B: linear-log (X+1%, Y+$\frac{b_1}{100}$ units)  | Model D log-log (X+1%, Y \Delta b_1%) |
*** M3L4 no lecture quiz
** M3L5: Polynomial model
Example:
$$
\text{Price} = b_0 + b_1 \times \text{lotsize} + b_2 \times \text{lotsize}^2
$$
- Creates a new variable lot_square = lotsize^2 and fit on the formula above
- *Cannot* interpret different powers by 'holding constant'
- i.e., a quadratic and higher power model does not allow for an isolated interpretation of coefficients
  - since $\frac{\text{d(price)}}{\text{d(lotsize)}} = b_1 + 2b_2\times\text{lotsize}$
- This means the slope is not constant and changes at every point of the quadratic curve
*** M3L5 no lecture quiz
* Module 04: Logistic Regression
** M4L1: Odds
- Odds express the *likelihood* of an event
- Written as X to Y or X:Y
- Gambling odds are "odds against" (i.e., the probability that event will *not* happen is greater than that it will)
  - 10 to 1 means that betting $1 wins $10, /on top of/ getting back the $1
- In this module, we deal with *odds for* or *odds on*
  - The probability that an event is more likely to happen than not
  - 2 to 1 means event is 2x as likely to happen as not
    - Gambler in this case stakes $2 and wins $1 (+$2) if event happens
*** /Odds For/ in statistics
- Odds :: ratio of probabilities
- Odds for :: ratio of $\frac{\text{probability of event happening}}{\text{probability of event NOT happening}}$
Take $p$ as probability of event happening, then: Odds for = $\frac{p}{1-p}$
- If Odds For is 2:1, then:
  $$
  \text{Odds for} = \frac{2}{1}
  = \frac{p}{1-p} \\
  p = 2-2p \\
  2=3p \\
  p=\frac{2}{3}
  $$
- Knowing Odds we can get $p$:
  $$
  p = \frac{\text{odds for}}{1+\text{odds for}}
  $$
*** M4L1 lecture quiz
1. A betting site shows odds of NE Patriots winning next Super Bowl is 5 to 1 (odds against). What is the probability of them winning the next super bowl?
   - Answer: 1/6. Odds for = 1/5, apply above formula.
2. Team Germany has a 12.5% (1/8) probability of winning the next World Cup. What is the odds for Team Germany winning the next World Cup?
   - Answer: 1/7. Apply above formula.
** M4L2: Binary Dependent Variable
- Recap: relationship between /Odds For/ and $p$
- Example of binary dependent variables
  1. whether student will get A
  2. whether firm goes bankrupt
  3. whether customer makes a purchase
  4. whether debtor defaults
  5. whether loan is approved
- Example: `GradesR.csv`
  - Variables
    - Grade = 1, student got A, else 0
    - Hours = amount of time spent
  - Using linear regression to model *binary* outcomes leads to weirdness, as regression line is continuous
    1. Predicted values \neq 0 or 1
    2. Predicted values < 0
    3. Predicted values > 1
*** M4L2 no lecture quiz
** M4L3: Logistic Regression
*** Comparison of logisitic and linear regression
Logistic regression is similar to linear regression with 2 main differences:
1. $Y$, i.e. outcome or response, is a *categorical* variable, _e.g._, Yes/No, Approve/Reject, Pass/Fail
2. Result expressed as *probability* of being in a group. Implies predicted value is [0,1].
*** Implementing logistic regression
- We use the logistic function, which gives probability of being in a group
- $p(x) = \text{Prob}(y=1|x)$, i.e. the probability of $y$ == 1 given an $x$ value
- Logistic function:
  $$
  p(x) = \frac{e^{b_0+b_1x}}{1+e^{b_0+b_1x}}, \text{i.e.,} \\
  p(x) = \frac{\exp(b_0+b_1x)}{1+\exp(b_0+b_1x)}
  $$
- p(x), simplified to $p$, will always be between 0 and 1 for all values of x
- hence:
  $$
  \frac{p}{1-p} = \exp(b_0+b_1x) \\
  \log(\frac{p}{1-p}) = b_0+b_1x
  $$
- $\log(\frac{p}{1-p})$ is the log of odds for, or *logit*
- <<Logit model>> is:
  $$
  \text{logit}(p) = \log(\frac{p}{1-p}) \\
  = b_0+b_1x
  $$
- Other components of the regression model are identical to linear regression
*** Why transform from probability to log odds?
- Mapping probability with range (0,1) to log odds with range (-\infty,\infty) will:
  1. Difficult to model variable with restricted range such as probability
  2. Get around restricted range problem of probability
  3. Among possible transformation methods, log odds is among easiest to understand and interpret
*** Interpreting logistic regression model
- *logit(p)* = $\log(\frac{p}{1-p}) = b_0+b_1x$ means:
  1. As $x$ increases by 1 unit, $\log(\text{odds})$ increases by b_1
  2. Equal to odds increasing by a factor of $\exp(b_1)$, which is approximately 100*b_1 %
  3. Exact change is (e^{b_1}-1)\times100%
*** M4L3 lecture quiz
1. The logistic function, p(x), returns values
   - [0,1]
2. $\log{\frac{p}{1-p}}=b_0+b_1x$ means that when $x$ increases by 1 unit,
   - $\log(\text{odds})$ increase by b_1
   - odds increase by factor of $\exp(b_1)$
   - odds increase by roughly $100\times b_1\%$
** M4L4: Logistic Regression using the /Default/ dataset
- Dataset from ISLR library.
- Do EDA:
  - income vs balance, color=Default
  - box plot for balance vs Default
- Logistic regression models:
  1. M1: no predictors $\text{logit}(p) = b_0$
  2. M2: single binary variable: $\text{logit}(p) = b_0+b_1\times \text{stdt}$
  3. M3: single continuous predictor var:  $\text{logit}(p) = b_0+b_1\times \text{balance}$
  4. M4: multiple predictors: $\text{logit}(p) = b_0 + b_1\times \text{balance} + b_2 \times \text{income} + b_3 \times \text{stdt}$
- Defining $p=\frac{\text{odds}}{1+\text{odds}}$
  - prob that default = "Yes"
  - $\text{logit}(p) = \log(\frac{p}{1-p})$
*** M1: no predictors $\text{logit}(p) = b_0$
- Intercept = -3.368 = log-odds of being in default for entire population
- hence $\exp(-3.368)$ = odds = p/(1-p)
- p = 0.0333
*** M2: single binary predictor  $\text{logit}(p) = b_0+b_1\times \text{stdt}$
- Coefficients:
  - Intercept = -3.50
  - stdt = 0.40
- Intercept is for non-students (stdt = 0)
  - This is the reference / base case
- Odds for non-student = $\exp(-3.50)$
  - p=0.0292
- Odds for student = $\exp(-3.50+0.40)$
  - p=0.0431 (higher than non-students)
*** M3: single continuous predictor  $\text{logit}(p) = b_0+b_1\times \text{balance}$
- Coefficients:
  - Intercept = -1.065e+01
  - balance = 5.499e-03 (i.e. 0.0055)
- $b_1$ = 0.0055
- Increase in balance is associated with increase in log-odds (and hence the odds for) of default
- Adding $1 to balance increases log-odds by 0.0055
- Examples, with $p(x)=\frac{e^{b_0+b_1x}}{1+{e^{b_0+b_1x}}}$
  |    x |       p |
  |------+---------|
  | 1000 | 0.00576 |
  | 1500 | 0.08317 |
  | 2000 | 0.5866  |
- Increasing x by 500 has non-linear effect on p(x)
*** M4: multiple predictors $\text{logit}(p) = b_0 + b_1\times \text{balance} + b_2 \times \text{income} + b_3 \times \text{stdt}$
- Coefficients
   - Intercept: -1.087e+01 (10.87)
  - balance: 5.737e-03 (0.0057)
  - income: 3.033e-06 (0.000003)
  - stdt: -6.468e-01 (-0.65)
- Interpretation:
  - Increase of balance associated with increasing log odds and hence probability of default
  - Adding $1 to balance increases log odds by 0.0057
  - Students now less likely to default due to -0.65 log odds, *different result from M2*
    - This can be explained by students carrying more balances, hence they default at higher rate
    - Log curve is shifted to the right
    - *Confounding* -> there is correlation between student and balance
*** M4L4 lecture quiz
1. For logistic model where Y=1 is the default case:
   - $b_0$ means the log odds of being in default for the entire population
** M4L5: Predictions and Confusion Matrix
- Making predictions on fitted data, e.g. M4: $\text{logit}(p) = b_0 + b_1\times \text{balance} + b_2 \times \text{income} + b_3 \times \text{stdt}$
- Make predictions by using predict with Model 4, and use 0.5 as the cutoff
*** True Negative and False Positive
- Consider all Y=0 observed (no defaults)
- For each observation, use the logic model to make prediction with the X values
- If predicted value == 0, that's a true negative
- Else predicted value == 1, false positive
- As the cutoff is increased, true negatives increase and false positives decrease:
  -  *Specificity* increases
*** True Positive and False Negative
- Consider all Y=1 observed (defaults)
- For each observation, use the logic model to make prediction with the X values
- If predicted value == 1, that's a true positive
- Else predicted value == 0, false negative
- As the cutoff is increased, false negatives increase and true positives decrease:
  -  *Sensitivity* decreases
*** Confusion matrix
- Shows the intersection of actual and predicted values of model
- Matrix records the performance of classifier, i.e. lets you gauge how models perform
- Record predicted value $\hat{y}$ after fitting logit model on dataset.
** M4L6: Sensitivity, Specificity, and the ROC curve
*** Definitions
- Sensitivity :: true positive rate: $\frac{\text{true pos}}{\text{true pos + false neg}}$
- Specificity :: true negative rate $\frac{\text{true neg}}{\text{true neg + false pos}}$
- 1-Specificity :: False positive rate $\frac{\text{false pos}}{\text{true neg + false pos}}$
- Precision :: $P(Y=1|\hat{y}=1) = \frac{\text{true pos}}{\text{true pos+false pos}}$
- Accuracy :: $\frac{\text{true pos+true neg}}{\text{true pos + false pos + true neg + false neg}}$
*** Type I and II errors
- False positive error :: type I error. Falsely reject true (null) hypothesis. true Y=0, predicted Y=1.
- False negative error :: type II error. Incorrectly retain - fail to reject - false (null) hypothesis. True Y=1, predicted Y=0
**** Cutoff
1. Increasing cutoff decreases Type I error
2. Increasing cutoff increases Type II error
**** In business applications
Cost of making error depends on situation, e.g.:
- When modeling marketing spend, false positive means spending marketing costs on non-purchasers. Moderate loss. (Type I) error
- When modeling customer as non-defaulter who actually defaults. (Type II) error
*** Calculating sensitivity and specificity for cutoff $p = 0.5$
|        |   | Predicted     | Values       |       |
|--------+---+---------------+--------------+-------|
|        |   | 0             | 1            | Total |
| True   | 0 | 9627 true neg | 40 false pos |  9667 |
| Values | 1 | 228 false neg | 105 true pos |   333 |
|        |   | 9855          | 145          | 10000 |
- Sensitivity =  $\frac{\text{true pos}}{\text{true pos + false neg}}$ = 105/(105+228) = 0.32
- Specificity = $\frac{\text{true neg}}{\text{true neg + false pos}}$ = 9627/(9627+40) = 0.996
*** Increasing cutoff to $p=0.9$
- Sensitivity decreases to 0.03
- Specificity increases to 0.9998
*** Increasing cutoff value of $p$
- True neg increase, false pos decrease.
- False neg increase, true pos decrease.
*** ROC Curve
- "Receiver Operating Characteristic" curve
- Shows diagnostic ability of binary classifier, when the cutoff value is varied
- Look at area under curve. We want > 0.5
*** M4L6 lecture quiz
1. Formula of sensitivity  $\frac{\text{true pos}}{\text{true pos + false neg}}$
2. Formula of specificity $\frac{\text{true neg}}{\text{true neg + false pos}}$
* Module 05: Treatment Effects
** M5L1: Correlations vs. Causality
*** Correlation
$$
Corr(X,Y) = \frac{\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}
{\sqrt{\sum^n_{i=1}(x_i-\bar{x})^2} \sqrt{\sum^n_{i=1}(y_i-\bar{y})^2}}
$$
- Correlation measures the *linear* relationship between X and Y
  - i.e., if Y=X^2, Corr(X,Y) is 0 even though they're perfectly related
- Correlation(X,Y) has range (-1,1)
*** Strong Corr(A,B)
Possibilities:
1. A causes B
2. B causes A
   - Note reverse causality! e.g., the faster windmill rotates -> faster windspeed observed
3. A and B are consequence of another common cause C
   - e.g. ice cream sales increase, more drownings.
   - common cause is summer
4. A -> C -> B
5. Correlation is by chance
*** /Post hoc ergo propter hoc/
#+BEGIN_QUOTE
After this, therefore because of this
#+END_QUOTE
- Logical fallacy that if A happened, then B happened, A must have caused B to happen
- E.g. rooster crows just before sunrise, hence rooster causes sun to rise
- Please consider other factors that could be responsible for the result
  - that might rule out the sequence connection
*** Causation
To establish causation:
1. Time-wise, hypothesized cause *must* be before anticipated effect
2. Change in cause *must* lead to change in effect
3. *Must* discount all plausible explanations that could explain the relationship, other than the proposed cause
*** Relevance
- Needed in fields e.g. medicine
- Causal models used to build theories
  - Managers need to know "how things work" i.e. theories
- For managers to make changes to price, mix, promotions, hence causal impact must be attributable
- If X does not cause Y, then don't bother spending effort in X
- If X causes Y, then can use a theory to explain why
*** M5L1 no lecture quiz
** M5L2: Selection Bias
- Occurs when individuals selected for treatment without proper randomization
- Can be caused by:
  1. Self-selection, especially in poorly-designed experiments. Participants could be motivated to participate, or not.
  2. Voluntary response bias, participants are already interested in topic, thus over-represents vs population.
  3. Non-response bias, problem in surveys with low response rate
*** Assumptions in OLS estimation when estimating slope coefficient, $b_1$
- OLS estimator is used to estimate b_1 in regression:
  $$
  Y = b_0 + b_1 X + \epsilon \\
  b_{OLS} = \frac{Cov(X,Y)}{Cov(X,X)} \\
  = \frac{Cov(b_0+b_1X+e,X)}{Cov(X,X)} \\
  = \frac{b_1 Cov(X,X)+Cov(e,X)}{Cov(X,X)}
  = b_1 + \frac{Cov(e,X)}{Cov(X,X)}
  $$
- This assumes *orthogonality*: $Cov(e,X)=0$
- When X, e are *uncorrelated*, b_{OLS} is a good estimator of b_1
- When X, e are *correlated*, b_{OLS} is a bad estimator of b_1
- When X is a dummy variable,
  $$
  b_{OLS} = b_1 + \frac{Cov(e,X)}{Cov(X,X)} \\
  = b_1 + (\bar{e_1}-\bar{e_0})
  $$
  - b_1 :: treatment effect
  - $(\bar{e_1}-\bar{e_0})$ :: selection bias
- $(\bar{e_1}-\bar{e_0})$, the selection bias
  - if = 0, b_{OLS} is good estimate of b_1
  - if \neq 0, b_{OLS} is bad estimate of b_1
*** Controlling selection bias
- Randomized controlled experiment
  - Assign test subjects into treatment and control groups
    - Used in science, e.g. medicine, agriculture
    - Difficult in economics
    - Getting easier in business
- Natural experiment
- Add control variables
  - However this is a weaker approach
*** M52 no lecture quiz
** M5L3: Randomized Controlled Experiment and the Difference Estimator
Setting up RCT
1. Create initial sample
   Random assignment by generating random numbers in [0,1]
2. If rn < 0.5 -> go to control group, no treatment (or placebo). $d=0$ (dummy variable)
3. If rn >= 0.5 -> go to test group, treatment. $d=1$
*** Regression model
- To analyze results, define $d$ as:
  $$
  d_i = 1 \text{ if individual i is in treatment group, else 0}
  $$
- Regression model with $N$ individuals being studied
  $$
  y_i = b_0 + b_1 d_i+e_i, \\
  i = 1, 2, ..., N
  $$
- Regression functions:
  $$
  \text{In treatment group: } (d_i=1) \\
  E(y_i) = b_0 + b_1 \\
  \text{In control group: } (d_i=0)\\
  E(y_i) = b_0
  $$
*** Difference estimator
- OLS estimator for b_1 is:
  $$
  b_{OLS} = \frac{Cov(X,Y)}{Cov(X,X)} \\
  = \frac{\sum^N_{i=1}(d_i-\bar{d})(y_i-\bar{y})}
  {\sum^N_{i=1}(d_i-\bar{d})^2} \\
  = \bar{y_1} - \bar{y_0} \\
  \bar{y_1} = \sum^N_{i=1}\frac{y_i}{N_1} \\
  \bar{y_0} = \sum^{N_0}_{i=1}\frac{y_i}{N_0}
  $$
  - N_1 :: number of obs. in treatment group
  - N_0 ::  number of obs. in control group
  - b_{OLS} :: difference estimate, as it's the difference between sample means of treatment and control groups
- Difference estimator can be rewritten as:
  $$
  b_{OLS} = \frac{\sum^N_{i=1}(d_i-\bar{d})(e_i-\bar{e})}
  {\sum^N_{i=1}(d_i-\bar{d})^2} \\
  = b_1 + (\bar{e}_1 - \bar{e}_0)
  $$
- If individuals can self-select into groups, then the selection bias in estimating treatment effect is:
  $E(\bar{e}_1 - \bar{e}_0)$
- Random assignment of individuals:
  - No systematic differences
  - The only difference is the treatment
- Random assignment: $E(\bar{e}_1 - \bar{e}_0) = 0$
  - OLS estimator is thus unbiased.
  - $E$ is commutative.

** M5L4: Star Experiment: Effect of Small Class Size
*** Summary
- Tennessee students randomly assigned in schools to:
  1. small class
  2. regular class
  3. regular class with paid aide
- Teachers randomly assigned to same groups
- data in ~ecdat::star~
- ~totalscore = tmathssk + treadssk~ (math + reading scores)
- Indicator variables:
  1. ~small~ = 1 if it's a small class
  2. ~boy~ = 1 if sex = boy
  3. ~whiteother~ = 1 if race = 'white or other'
  4. ~freelunch~ = 1 if lunch was free
*** Check for random assignment
- Regress $\text{small ~ .}$ to check for significant coefficients
  - If random assignment, no significant coefficients
  - Use linear probability as small is 1/0
  - All coefficients are *not* statistically significantly different from 0
    - Hence there is random assignment
  - Cannot reject b_0 = 0.5
*** Summary stats
- Difference between small = 0 and small = 1 classes of ~totalscore~ in summary stats:
  - mean of small = 0 (regular): 917.94
  - mean of small = 1 (small): 932.05
*** First regression
Regression of $\text{totalscore} = b_0 + b_1\text{small}+e$
- small class sizes have significantly different scores than regular
- b_1 estimate, 14.109, is statistically different from 0
- This is also the difference in the means in summary stats above
- Interpretation: 14.109 on above is added to students' total score if moved from regular-sized class to small-sized class
*** Second regression
Add teacher experience, i.e. $\text{totalscore} = b_0 + b_1\text{small}+b_2\text{totexpk}+e$
- b_2 estimate is 1.1580, and it is statistically significant
- i.e. each additional year of teacher experience adds 1.16 points to total score on average
- Difference estimator, b_1, = 14.21
- The effect of small class size is approximately the same as having teacher with $\frac{14.21}{1.16} = 12$ additional years of experience.
*** M5L4 no lecture quiz
** M5L5: Natural Experiments and Difference-in-Difference Estimator
*** Definition
- Observational study from real-world conditions
- **Approximates** what would happen in randomized controlled trial
- Subjects **cannot** choose which group they're in
  - Treatment
  - Control
- This choice is made by external factor, e.g. weather, policy, etc
- There must be subjects in both groups
- Compare the average change, over time, of the response Y variable in treatment group vs control group.
  - **a.k.a.** difference in difference
- Use panel data to measure these differences
*** Examples
- Treatment that just happened, not intentionally designed, e.g.
  1. Change in law for some people only
  2. IT system for BOPIS in some stores not others
  3. Hurricane hits some stores, not others
  4. Mobile carrier tweak price plan for some customers, not others
  5. Minimum wage changed in some states, not others
*** Counterfactual
These are required to estimate the causal impact of treatment.
1. Need to compare the outcome with intervention vs. what would have been **without** the intervention
   1. This is the counterfactual
2. Control group must be relatively similar to the treatment group
3. If not possible to establish counterfactuals, it is impossible to estimate treatment effects properly
*** Example of natural experiment
- NYC lower sales tax rates
  - Other neighbouring states do not
- Estimate the difference in purchase behaviour between NYC and neighbouring states
- Is the effect:
  1. Lower internet sales
  2. Stronger tendency to purchase locally?
*** Difference-in-difference (D-in-D)
- Consider times
  - t_1: occurs before treatment
  - t_2: occurs after treatment
- Measure the average value of dependent variable $Y$
  1. $A$ = $\bar{Y}$ for control group at t_1
  2. $B$ = $\bar{Y}$ for treatment group at t_1
  3. $C$ = $\bar{Y}$ for control group at t_2
  4. $D$ = $\bar{Y}$ for treatment group at t_2
  |         | Before | After | Difference |
  |---------+--------+-------+------------|
  | Control | A      | C     | C-A        |
  | Treated | B      | D     | D-B        |

  Hence difference-in-difference is (D-B) - (C-A)
  [[./img/0505-dind.png]]
  - This is D-E in the graph
*** Estimating D-in-D with regression
- NYC dataset: create two dummies:
  1. $\text{NYC}$ = 1 if store in NYC, else 0
  2. $\text{After}$ = 1 if observation is in after, else 0
- Define an interaction variable, $\text{NYCAfter} = \text{NYC}*\text{After}*$
- Observe sales in stores in
  - regions: NYC and other
  - time: before and after change in sales tax
- Model:
  $$
  \text{sales} = b_0 + b_1\text{NYC} + b_2\text{After} + b_3\text{NYCAfter}
  $$
- Graphically:
  [[./img/0505-dind1.png]]
*** Model
|         | Before   | After                 | Diff. (Before-After) |
|---------+----------+-----------------------+----------------------|
| Control | b_0      | b_0 + b_2             | b_2                  |
| Treated | b_0 + b1 | b_0 + b_1 + b_2 + b_3 | b_2 + b_3            |
Hence the diff-in-diff estimator is the difference between the two differences, i.e.
b_2 + b_3 - b_2 = b_3
*** Steps in natural experiment
1. Understand the treatment that just happened
2. Determine if the treatment appears to be randomly assigned:
   1. assignment orthogonal to unobserved factors, X orthogonal to \epsilon
3. Determine if there's control and treatment groups
4. Determine if there's evidence to show the two groups are roughly the same before experiment
5. Analyze treatment effect with diff-in-diff estimator
*** M5L5 no lecture quiz
* Module 06: Measuring Returns
** M6L1: Introduction
Parts of discussion:
1. Measuring risk and return
2. Measuring risk-adjusted performance and market efficiency
3. Factors that drive returns
** M6L2: Simple and compound returns
*** Objectives
1. Understand simple and compound returns
2. Using R to calculate values for a given asset
*** Simple return
- Simple return :: The percentage change in the stock price from one period to the next
E.g.:
- April 2019: $15
- May 2019: $17
- Stock return = (17-15)/15 = 13.33%
**** Caveat on calculating returns
- Need to adjust for stock splits and dividends.
- General expression: $r_t = \frac{p_tf_t+d_t}{p_{t-1}}-1$
  - p :: price
  - f :: adjustment factor for stock splits
  - d :: dividend
- Stock splits are only cosmetic events
**** Example of simple returns
| Date   |  Price | Dividend | Split   | Return                        |
|--------+--------+----------+---------+-------------------------------|
| Sep 91 | 31.125 |          |         |                               |
| Oct 91 | 37.750 |     0.06 |         | (37.75+0.06)/31.25-1 = 20.9%/ |
| Nov 91 |  32.75 |          |         | (32.75/37.75)-1 = -13.25%     |
| Dec 91 | 30.375 |          | 3 for 2 | (30.37*1.5)/32.75-1 = 39.1%   |
*** Compounded returns
- Assets are normally held for multiple periods
- How to calculate total return over this period?
- Definitions
  - Compound return :: cumulative effect a series of gains or losses has on original investment over a period of time
  - Formula :: (r_1 + 1) x (r_2 + 1) + ... + (r_n+1)-1
- E.g. for above example, compounded return = (1+0.299)*(1-0.1325)*(1+0.3912)-1=46%
*** Calculating compound returns in R
- Use ~PerformanceAnalytics~, ~xts~, ~lubridate~
- Formula for cumulative return: ~Return.cumulative(df$ContraRet, geometric=T)~
- Compounded return as chart:
  ~chart.CumReturns(df$ContraRet, wealth.index=F, geometric=T)~
*** M6L2 Summary (No lecture quiz)
- Returns allow us to understand how investments are growing
- Simple and compound returns are both informative
** M6L3: Measuring risk
*** Both risk *and* return matter when investing
- Risk also matters
- Investors care about:
  - Volatility: how much prices fluctuate, i.e.
  - How much money they stand to gain or lose
*** Standard deviation of returns
- Measures variation by looking at how far the observations' values are from the mean value
- Formula: $s=\sqrt{\frac{\sum(x_i-\bar x)^2}{n-1}}$
- Higher sd = higher level of risk
*** Standard deviation ~ Total risk
- Total risk comprises both:
  1. Firm specific risk: good or bad news about the firm, e.g. product recall, lawsuit
  2. Market-wide: news about the overall economy, e.g. interest rate movements, recession likelihoods
- As more stocks are held in portfolio, the firm-specific risk decreases
  [[./img/m6l2-firmrisk.png]]
*** Decomposing risk
- Into firm-specific and market-wide components
- Can be estimated with this regression model:
  $r_i=\alpha + \beta r_m + \epsilon$
  - r_m :: return on broad stock index portfolio
  - \beta :: measure of stock sensitivity to overall market movement
  - R^2 :: measures the % of fund performance as a result of the market
- Higher R^2 = more correlated with the market
*** Interpreting \beta
- Higher \beta represent higher market risk
  - Risk free asset \beta = 0
  - Overall stock market \beta = 1
*** High-water mark and drawdown
- Formulas
  - High-water mark (HWM) :: highest price a fund has achieved in the past
  - Drawdown (DD) :: cumulative loss since losses started. Formula is $DD_t = \frac{HWM_t-P_t}{HWM_t}$
- Drawdown measures the peak-to-trough decline in investment
*** Estimating mean return and standard deviation
- Use the same file ~contrafund.csv~ and the same packages as above
- To calculate standard deviation, use ~table.Stats(df$ContrRet)~
*** Estimating \beta and R^2
- \beta and R^2 can be estimated with linear regression:
  ~model = lm(ContraRet~Market.Return, df)~
- Summary regression output example:
  - \beta is the coefficient on Market.Return, e.g. 0.9004
  - Adjusted R^2 = 0.8313 indicating the fund is correlated with overall market
*** Drawdown
- Plotting: ~chart.Drawdown(df$ContraRev)~~
- Show largest 5 drawdowns: ~table.Drawdowns(df$ContraRev, top=5, digits=4)~
*** Summary (no lecture quiz)
- Not sufficient to know just the returns
- Standard deviation measures total return, while \beta measures sensitivity to market moveemnts
** M6L4: Historical returns
*** Asset classes
1. Small cap stocks: smallest 30% traded on US exchanges
2. Large cap stocks: largest 30% traded on US exchanges
3. Treasury bills: short-term US treasury debt
4. Treasury bonds: long-term US treasury debt
5. Comparison with inflation rate
*** Value of $1 invested in different asset classes:
Largest to smallest:
- small cap
- large cap
- bonds
- bills
- inflation
Conclusion: equities > government debt
*** Risk
- Risk between equities and government debt is very different. Equities much riskier
- Riskier investments have higher return and higher standard deviation
  |           | Standard deviation | Mean return |
  |-----------+--------------------+-------------|
  | Small cap |              33.3% |       16.5% |
  | Large cap |              19.3% |       11.2% |
  | Inflation |               3.8% |        3.1% |
  | Bills     |               3.2% |        3.4% |
  | Bonds     |               2.8% |        5.0% |
*** Summary (no lecture quiz)
- Risk and return are linked
- Over time, more risky assets generate higher returns.
- However, they also have higher year-to-year variations
- This is the risk-return tradeoff
* Module 07: Measuring Risk-Adjusted Performance
** M7L1: Risk Adjusted Performance
*** A Way to Keep Score
- Need some way to measure whether investments are outperforming expectations
- To measure abnormal return i.e.
  Abnormal return = Actual return - Expected return
*** Benchmark comparisons
- Compare to a benchmark, e.g.
  ~Return.cumulative(All.data, geometric=T)~
  ~chart.CumReturns(All.dat, wealth.index=F, geometric=T)~
*** Sharpe ratio
- Measures investment reward per unit of risk
- $\frac{R-R^f}{\sigma(R-R^f)}$
- Definitions
  - R-R^f :: excess of portfolio returns above risk free rate
  - \sigma(R-R^f) :: standard deviation of the excess return
- Higher ratios are better
- Higher ratio indicates higher return per unit of risk
**** Calculating Sharpe ratio in R
- ~SharpeRatio(All.dat$ContraRet, All.dat$Risk.Free)~
  - Output: 0.19
- Market's Sharpe ratio: 0.153
- Hence the Contra fund is out-performing market
*** Treynor Ratio
- Similar to Sharpe, except the denominator is \beta
- $\frac{(R-R^f)}{\beta}$
- Similarly, higher Treynor Ratio means higher reward per unit of risk
**** In R
- Function is ~TreynorRatio(All.dat$ContrRev, All.dat$Market.Return, All.dat$Risk.Free)~
  - Contra fund: 0.10
  - Market index: 0.07
- Contra fund has outperformed market
*** Jensen's alpha
- Measures the abnormal return that portfolio earns after adjusting for \beta
- Estimate this regression:
- $r_{i,t} - r_f = \alpha_i + \beta_i(R_{m,t}-r_f) + \epsilon_{i,t}$
- Definitions:
  - $r_{i,t}$ :: return on asset $i$ at time $t$
  - $r_f$ :: risk-free rate
  - $R_{m,t}$ :: return on market index at time $t$
  - \alpha_i :: regression coefficient to be estimated
  - \beta_i :: regression coefficient to be estimated
**** Steps to calculate Jensen's alpha
1. Calculate return in excess of risk-free rate, $r_{i,t}-r_f$
2. Calculate the return on market index in excess of risk-free rate, $R_{m,t}-r_f$
3. Run regression of $(r_{i,t}-rf)$ on $(R_{m,t}-r_f)$
4. Focus on regression intercepts:
   1. If positive and statistically significant, the fund has outperformed benchmark
   2. Else, the fund has underperformed
** M7L2: Transaction Costs
*** Impact of transaction costs on performance
- Without costs:
  - acquire $1000 of shares, get 10% return, get $1100, net return 10%
- with transaction costs, 150bps, only $985 worth of shares bought
  - when sold, another 150bps paid, after 10% increase, only get $1067.25
- Transaction costs *always* lower returns
*** Over investing lifetime, transaction costs can have big impact
- Low fee fund 0.1% per year
- High fee fund 1% per year
- Initial investment $100,000
- Amount after 30 years:
  - Low fee fund: $1.74M
  - High fee fund: $1.33M
*** Types / components of transaction costs
- Commission :: fixed charge for executing trade
- Bid-ask spread :: price diff between immediate sale (offer/ask) and immediate buy (bid)
- Delay :: loss in investment decision between investment decision made and time trade executed
*** Bid-ask spread illustration
- Buying 1000 shares of XYZ
  - Best offer price: $25.42 (lowest price to buy)
  - Best bid price: $25.38 (highest price to sell)
  - Quote spread: $0.04, 16bps
    - difference between best bid and offer
  - Larger orders pay higher transaction costs
*** Delay cost example
- Fund manager wants to buy current stock at $50, for total $250K. By the time the order can be executed, price rose to $50.25
- Delay cost: $0.25, 50bps
*** Size of trading cost
- bps :: basis point
- 1 bps :: 0.01%
*** Summary (no lecture quiz)
- Transaction costs lower investment returns
- Components:
  1. commission
  2. Bid-ask spread
  3. Delay
- As investors, we care about returns net of transaction costs
** M7L3: Market Efficiency
*** Stock price reflects information
- If some stock pattern *guarantees* profit, what should you do?
  - Definitely exploit it
  - By borrowing as much money as possible to invest
- Process of exploiting it makes the opportunity vanish as:
  - stock price bids up when you buy it
  - stock price bids down when it's cold
- Pattern is eliminated by observing it
*** The Army of Investors
- Other than individual investors, there is also an 'army' of intelligent and well-informed security analysts, traders, etc hunting for mispriced or following patters
  - Have computers, databases, analytical techniques, etc
  - Can assess and act on information very quickly
  - May police market so efficiently that they drive asset prices to fully reflect all available information quickly
*** Implications
- Competition is fierce for finding mispriced securities
- Competition always kills the sure-profit pattern
  - If there is one, it would be exploited by the first person
  - First person can make profit
  - First person not likely to be you
- Implies:
  - Stock prices should have reflected all available information
  - Stock returns can be unpredictable
*** Efficient markets
- Efficient capital market :: one in which stock prices fully reflect available information
- Weak form :: security prices reflect all information found in past price and volume
- Semi-strong :: security prices reflect all publicly-available information
- Strong :: Security prices reflect all information, public and private
*** Market efficiency: 'theory of sharks'
- Instead of presenting market efficiency as one of a rational man,
- It's a theory of intense competition:
  - In liquid markets, profit opportunities bring about infinite discrepancies between supply and demand
  - Well financed, knowledgeable arbitrageurs spot these opportunities, etc, and by their actions close the aberrant price diff
*** Evidence of market efficiency
- If market is semi-strongly efficient, then no matter what publicly available information is used to pick stocks, they should have same average returns as those of average investor in the market
- Can test efficiency by comparing performance of professionally managed mutual funds with performance of market index
*** Not easy to beat market
- Very few active US equity funds outperform passive benchmarks
- At long time horizons, most funds fail to outperform
- Most evidence shows you can't predict the funds that will outperform
*** Summary (no lecture quiz)
- Efficient markets result from intense competition
- If markets are efficient, it should not be possible to consistently generate alpha or to outperform a passive benchmark
- Performance of mutual funds suggest the markets is efficient
** M7L4: Behavioural Finance
*** Markets cannot be perfectly efficient
- If they were, there would be no incentive to collect information
- A lot of evidence on behavioural biases and failure of market efficiency
- E.g. CUBA fund
  - 69% are US stocks, others Mexican
  - No assets in Cuba
  - Upon lifting of assets on Cuba, the CUBA fund price increased by 70%
*** Behavioural finance
- Key ingredients
  1. Investors are not fully rational. Deviation from rationality are correlated across investors
     - i.e., many investors make the same mistake
  2. Arbitrage forces are limited, by:
     - Limited capital
     - Investment constraints, e.g. unable to short
     - Noise trader risk. Arbitrageurs may have limited time horizons so mispricing can persist over long periods of time
- If true:
  - Irrational behavior can push prices away from fundamental value
  - Arbitrage is limited and cannot completely eliminated mispricing
*** Investors have many behavioural biases
- Overconfidence :: tendency to overestimate one's ability
- Loss aversion :: tendency of individuals to seek pride and avoid regret in their decisions
- Recency :: overemphasis of recent information when making investment decisions
- Anchoring :: Individuals tend to decide based on single fact or figure that should have little bearing on their decision, while ignoring more important information
*** Summary (no lecture quiz)
- Investors suffer from various biases
- Coupled with market frictions, this may push prices away from fundamental value
* Module 08: Factor-based Investing (Investing Analytics)
** M8L1: Identifying factors
Which factors drive returns?
*** Size effect
- Smaller firms have higher returns
  - By around 3% / year since 1927
- Size measured by market capitalization
*** Value effect
- Inexpensive stocks have higher returns
  - Fama and French (1993)
- Measured by book value-market value ratio (B/M)
  - High B/M means stock is inexpensive
  - By around 4.67% / year
*** Momentum effect
- Stocks that have performed well tend to continue performing well
  - Past winners outperform past losers
  - By around 9.23% / year
*** Profitability effect (i.e., Quality)
- Profitable stocks outperform unprofitable stocks
  - Fama and French (2015)
  - By around 3.2% a year
*** Risk effect
- Low beta assets outperform high beta assets
  - Betting against beta
  - Frazzini and Pedersen (2014)
  - By around 12.2% / year
** M8L2: Interpreting factor regressions
- Factors driving stock returns (recap):
  1. Beta: r^m - r^t
  2. Size: SMB
  3. Value: HML
  4. Momentum: MOM
  5. Quality: BAB
  6. Risk: QMJ
*** Factor regressions
- Can estimate model factors using linear regression
- Dependent (response) variable is fund's excess return above risk-free rate
- Factors are independent variables
- Typically:
  $$
  r_t^{fund}-r^f_t = \alpha + \beta_1(r_t^m-r_t^f) + \beta_2 SMB_t + \beta_3 HML_t + \beta_4 MOM_t + \beta_5 BAB_t + \beta_6 QMJ_t + \epsilon_t
  $$
*** Interpreting factor regressions
- Fund's coefficients tell us about exposure to different factors. A *positive* coefficient for this factor indicates
  - SMB :: tilt towards small cap stocks
  - HML :: tilt towards value stocks
  - MOM :: tilt towards high momentum stocks
  - QMJ :: tilt towards profitable stocks
  - BAB :: tilt towards safe stocks
- Intercept represents \alpha
  - Indicates skill of fund manager
  - Positive and significant: fund manager has outperformed
  - True \alpha indicated after adjusting for all factors e.g. beta, size, value...
*** Takeaways from factor regression
- Allows us to uncover reasons for fund's out-performance
- It's becoming popular
- But:
  - Performance may dissipate due to popularity
  - Each factor has experienced prolonged periods of underperformance as well
    - Patience is required for some of these factors
*** Summary
- Factor regressions help us understand the drivers of a fund's return
- Intercept from regression indicates skill of fund manager
** M8L3: Concluding
- 3 core concepts covered
  1. Quantifying prices in financial markets
  2. Identifying superior performance in financial markets
  3. Describing driving forces of returns in stock markets
*** Topic 1 recap
- Ways to think about stock and fund prices:
  - Simple and compound returns
  - Standard deviation
  - Beta
  - R^2
  - Drawdown
*** Topic 2 recap
- Measuring performance via:
  - Comparing to benchmark
  - Sharpe ratio
  - Treynor ratio
  - Jensen's alpha
*** Topic 3 recap
- Drivers of returns:
  - Size
  - Value
  - Momentum
  - Profitability
  - Volatility

* Module 09: Marketing and Advertising
#+BEGIN_QUOTE
Midterms ended at Module 9
Finals has 1/3 of content (Mod 1-8) and 2/3 of content (Mod 9-15).
Finals has 1 paper of cheatsheet with 2 sides total. Calculator permitted.
#+END_QUOTE
** M09L01: Marketing and Advertising
*** Traditional advertising (5 types)
1. Outdoors display and promotions
2. Print: newspapers and magazines
3. Mailings door to door
4. Radio
5. TV
*** Methods of sale for traditional
- CPM = Cost per Mille
- Thousands of people watching
*** TV advertising: rating methods
1. HUT = Households using TV/Total TV households = 6/10 or 60
2. Rating = Channel 2 households/Total TV households = 3/10 or 30
3. Share = Channel 2 households/Households Using TV = 3/6 or 50
4. Rating = Share x HUT
5. GRPs = sum of ratings
6. Reach = Channel 2 households/Total TV households e.g. 7/10 or 70
7. Frequency = GRPs/Reach = 150/70 = 2.1
*** Digital advertising (5 types)
1. Display ads
2. Search engine marketing
3. Social media marketing
4. Mobile marketing
5. Email marketing
** M09L02: Evolution of advertising companies and methods
*** Key examples of digital advertising companies
1. Netscape
2. Google
3. Facebook
*** Ad serving platforms / programmatic ads
Advertiser -> Ad placement system -> ads
*** Media buying agency
Optimize ad placement according to content and audience relevance
*** What digital marketers do
1. Build campaigns
2. Buy media (ad placements)
3. Optimize campaigns
** M09L03: Overview of digital ad market
*** Overview
- High growth 1996-2018 overall
- Strong growth in mobile advertising
- Search > banner > video
- Social media advertising also has strong growth
*** Pricing models
1. CPM is decreasing
2. Performance is increasing
3. Hybrid is around the same
*** Next
- Privacy and trust is compromised
* Module 10: Implementing Integrated Digital Marketing
** M10L01-L03: Five methods of digital advertising
1. Display advertising
   Many sizes but low CTR
2. Search engine marketing
   1. Pay per click
      1. Paid vs organic
   2. Search engine results page
   3. Keyword
   4. Click through rate
   5. Landing page
   6. Vickrey auction model
   7. Relevance is key for Google
      1. Quality score factors
         1. Keywords
         2. Ad copy
         3. Landing page
         4. Historical CTR
      2. CPC x Quality score = Ad rank = Ad position
   8. Profitability of ad camapaign
      1. CPC
      2. Conversion rate
      3. Sale value
      4. profit margin
      5. lifetime value
         1. Repeat sale is cheaper than new sale
      6. Low CPC breakeven:
         1. Use long-tail keywords
         2. High CTR, high conversion rate
         3. Profit margin per sale -> BEP CPC per average sale (based on conversion rate)
         4. BEP CPC for LTV -> factor up LTV/Avg Sale value
3. Social media marketing
   1. Facebook > Insta > LinkedIn
   2. Goals -> Presence -> Content Size/Type
** M10L04: Advertising Deals and Tools
*** Payment methods
1. Fixed
2. CPM
3. CPV (view)
4. CPC (click)
5. CPA (acquisition)
6. CPS (sale/commission)
*** Risk principle
Advertisers: highest risk CPM
Publishers: highest risk CPS
*** Tools
1. E.g. Adwords, Audience Planner, Analytics, Facebook Ads
2. Allow insight into digital marketing
3. Use to interpret campaign performance
4. Analyse and measure performance
*** What Digital Marketers do?
1. Plan and build campaigns
2. Buy media or ad placements
3. Optimize campaigns
* Module 11: Implementing Predictive Marketing Across Channels
** M11L01: Conversion rate optimization, A/B testing and Funnel Analysis
*** Conversion rate optimization
Goals:
1. Increase traffic
2. Increase engagement
3. Drive leads
4. Grow sales
5. Improve conversion
   1. To leads
   2. To sales
*** Site analytics considerations
1. Source of traffic
2. Audience characteristics
3. Audience behaviours
4. Mobile metrics
*** Data viz
1. Graphs
2. Charts
3. heat maps
4. word counts
*** Sales funnel step - example
- Step 1 :: Search for all available hotel dates
- Step 2 :: Check prices and amenities
- Step 3 :: Select hotel and check out
- Step 4 :: Enter details and pay
*** Search ads - {{{hl(variables)}}}
1. Headline
2. Body text
3. Link
4. Ad extensions
*** Landing page - {{{hl(variables)}}}
1. Heading
2. Copy
3. CTA
4. Color
5. Images
6. Offer
*** E-Commerce - variables
1. Images
2. CTA
3. Shipping details
4. Credibility
*** Data analysis process
1. Gather data
2. Mine data
3. Come up with hypothesis
4. Validate hypotheses
5. Make decision and act
6. Monitor
7. Universal analytics
** M11L02: Google Analytics
*** What's Google Analytics
- Allow tracking of data on site performance, traffic and user behaviour
- Automated analytics
- Free for small biz
*** Why use Google Analytics?
- Automates data collection
- Easily integrates with other G tools
- Customized reports
- Can measure internal site search
  - Helps understand why internal traffic not leading to conversions
- To understand audience characteristics
- Free
*** Google Analytics Hierarchy
- Account
  - Property
    - Views
*** Types of reports
- Acquisition
- Behaviour
- Conversion
- Real-time
- Audience
*** Dimensions
- Browser
- Landing Page
- Campaign
- Exit page
*** Metrics
- Users
- Bounce rate
- Sessions
- Average duration
- % of new sessions
- Pages per session (engagement)
- Pageviews
** M11L03: Social Media Insights and Ads
*** Facebook pages types
- Personal
- Group
- Public for brands, fans, etc.
*** Content posts displayed in
- News feed
- Pages
- Sponsored posts
- Sidebar ads
** M11L04: Advertisers in Social Media
*** Types of ads in Facebook
- Sponsored side bar ads
- Sponsored posts in news feed
*** Settings
Can be customized in your personal page
** M11L05: Ad campaigns in Social Media
*** Steps for advertising
Ad = Audience + Creative
1. Decide on objective
2. Select target audience
3. Upload the creative
*** Types of ad placement
Where?
1. Facebook
2. Instagram
3. Audience Network

Who optimize?
1. Facebook
2. DIY
*** Facebook Blueprint
i.e., Facebook's social media academy
* Module 12: Operations Management
** M12L01: What is Operations Management
*** Intro
- OM :: how to make stuff
- Stuff :: includes *services* and products
- Goals :: make it well, on time, low cost, enough to meet demand
- Formally :: the management (design, operation and improvement) of the processes that transform material, labour, energy and information into goods and services
*** It's in the middle
- OM sits between Inputs and Outputs.
- OM is the Transformation Process.
*** Decisions in OM
- Strategic :: long term impact
  - Product, process selection
  - Location
  - Capacity (manufacturing)
  - Projects (product development)
  - Training of staff (services)

- Tactical :: medium term impact
  - Number of employees, hours of work
  - Inventory levels / should we make more? (manufacturing)
  - Order quantity and frequency
  - Share design info with manufacturing? (product dev)
  - Types of queues (Services)

- Operational :: short term impact
  - Job scheduling
  - Priorities
  - Which part to make first? (Manufacturing)
  - Should queue be FCFS? (Services)
  - Critical path of project? (Product dev)
*** Goal of OM
Needs a *tradeoff* between:
- Efficiency :: lowest cost
- Effectiveness :: do the right things
- Value :: Quality / Price

** M12L02: Importance of OM
*** Value of OM
1. Supply chain disruption is bad
   1. Lower sales, profits, returns, stock price, etc
2. Reasons for disruptions
   1. Internal (equipment breakdown, quality, poor records, labour shortages, etc)
   2. Supplier failure
   3. Customer issue
   4. Part shortages
*** Which fields need OM?
All fields.
** M12L03: Future of OM
*** New technologies
1. 3D printing
2. Artificial intelligence
3. Robotics
4. Autonomous vehicles
*** Commonalities
- All require data
- All will fundamentally change how companies do business
** M12L04: Queuing Theory Basics
*** Suggestions
1. Determine an acceptable waiting time for your customers
2. Try to divert the customers attention when waiting
3. Inform the customer of what to expect
4. Keep employees not serving the customers out of sight
5. Segment the customers
6. Train your employees to be friendly
7. Encourage customers to come during off-peak times (happy hours)
8. Take a long-term perspective towards minimizing queues
*** Concepts
- State :: System is at steady state
- Customer population :: finite vs infinite
- Arrival rate \lambda :: Constant vs variable
  - Does inter-arrival time follow a distribution?
    - exponential distribution.
      $$
      f(t) = \lambda e^{-\lambda t}
      $$
  - How many arrivals per time period $T$?
    - Poisson
      $$
      P_T(n) = \frac{(\lambda T)^n e^{-\lambda T}}{n!}
      $$
- Line length :: finite vs infinite
- Number of lines :: self explanatory
- Queue discipline :: FCFS, reservations or most profitable first?
- Serivce rate \mu :: Constant vs variable
  - Exponentially distributed service time
    $$
    f(t) = \mu e^{-\mu t}
    $$
- Number of channels :: workers
- Number of phases :: how many queues does customer need to join?!?!
- Probability of re-service :: self-explanatory
** M12L05: Queuing Theory MM1 model
*** Assumptions
- Customer population :: \infty
- Arrivals :: Random, poisson
- Queue length :: unlimited
- Lines :: 1
- Queue discipline :: FCFS
- Channels :: 1, exponential
- Phase :: 1
- Examples :: 1-lane toll bridge, McDonalds Drive Thru
*** Formulas
- Utilization
  $$
  \rho = \frac{\lambda}{\mu}
  $$
- Average number of customers in system (not queue)
  $$
  L_s = \frac{\lambda}{\mu-\lambda}
  $$
- Average number of customers in queue
  $$
  L_q = \frac{\lambda ^2}{\mu (\mu - \lambda)} = L_s \times \rho
  $$
- Average time of customer in system
  $$
  W_s = \frac{1}{\mu - \lambda} = \frac{L_s}{\lambda}
  $$
- Average time of customer in queue
  $$
  W_q = \frac{\lambda}{\mu(\mu - \lambda)} = \frac{L_q}{\lambda}
  $$
- Probability of $n$ units in the system
  $$
  P_n = (1 - \frac{\lambda}{\mu})(\frac{\lambda}{\mu})^n = (1-\rho)(\rho)^n
  $$
*** Operationalising
- Find likelihood of 3 or fewer cars (i.e., find 0, 1, 2, 3)
- Find what value of \rho gives 95% service level
  $$
  P_0 + ... + P_3 = 0.95 \\
  1-\rho^4 = 0.95 \\
  \rho = \lambda / \mu
  $$

** M12L06: Operations Management Recap
- Operations management is the direction and control of the processes that transform material, labor, energy, and information into finished goods and services.
- Operations management is at the core of every company.
- Several technologies maturing that promise to radically change how companies transform inputs. Analytics will be key here.
- Queuing theory can be used to analyze waiting lines.

* Module 13: Quality
** M13L01: Quality intro
*** What is quality?
- Quality :: meeting or exceeding customer's expectations
*** Garvin's 8 dimensions of product quality
Refers to {{{hl(product)}}}.
1. Performance
2. Functionality
3. Durability
4. Reliability
5. Conformance to specifications
6. Serviceability
7. Aesthetics
8. Perceived quality
*** Dimensions of service quality
1. Consistency
2. Courtesy
3. Convenience and availability
4. Communication
5. Accuracy and reliability
6. Timeliness and responsiveness
7. Credibility and trustworthiness
** M13L02: Costs of Quality
*** Costs of good quality
**** Appraisal costs
1. Inspection and testing
2. Lab
3. Calibration
4. Product/process audits
**** Prevention costs
1. Training
2. Quality improvement
3. Quality audits
4. Quality planning
*** Costs of bad quality
**** Internal failure costs
1. Scrap
2. Rework
3. Re-testing
**** External failure costs
1. Warranty
2. Reputation
3. Returns
4. Litigation
** M13L03: Variation
*** Definition
A measure of the change in data, a variable or a function.
*** Causes of variation
- Random causes
  - Inherent in process
  - Unavoidable
- Assignable or special causes
  - Can be identified
  - Can be corrected/fixed
*** Distribution of weights with only 'normal/random' causes of variation
- Normal distribution
- Mostly within 3 s.d.
** M13L04: Control chart basics
*** What's a control chart
- Has central line (mean)
- Has upper control limit (UCL)
- Has lower control limit (LCL)
- X-dimension is time or # of observations
*** Relationship to types of variation
- UCL and LCL are set based on common and random causes of variation for the process
  - These will be on a normal distribution
- Plot data and monitor to watch for assignable and special causes of variation
  - These we can do something about
*** Investigate when
- Control points outside UCL or LCL
- Two points near control (UCL or LCL)
- Run on 5 points above or below central line
- Trends of 5 points in the same direction
- Erratic behaviour bouncing between extremes
- Sudden change in level
** M13L05: Control chart for variables
*** Central limit theorem
Sample of 5 boxes of cereal, repeated 20-30 points on the average (5 box) of weight should be a normal distribution.
*** Thinking more
- Take periodic samples and use information to represent population
  - Good for expensive measurements
- 99.73% of all values should fall within +/- 3 SD of mean
  - Also can be stated as: 99.73% probability that at assignable cause of variation has occurred if it falls outside 3 SD of mean
- Use mean and SD to monitor the distribution
  - $\bar{x}$ chart and $r$ chart
*** $\bar{x}$ chart
- monitors the mean
- $\bar{R}$ is the *average* range
- assuming 3\sigma limits:
  - UCLx = $\bar{\bar{x}} + A_2 \times \bar{R}$
    - A_2 depends on sample size
  - LCLx = $\bar{\bar{x}} - A_2 \times \bar{R}$
*** R chart
- monitors the spread
- assuming 3\sigma limits:
  - UCLr = $D_4 \times \bar{R}$
  - LCLr = $D_3 \times \bar{R}$
- D_3 and D_4 depends on sample size
*** Steps to set up control charts
1. Collect data
2. Calculate $\bar{R}$
   1. Calculate UCLr and LCLr
   2. Plot the R chart
3. Calculate $\bar{x}$
   1. Calculate UCLx and LCLx
   2. Plot the $\bar{X}$ chart
*** Summary
1. Control charts look to identify *assignable* causes of variation
2. Can be used to reduce defects
** M13L06: Sample problem
** M13L07: Process capability
- SPC tells us if a process is showing signs of an assignable cause of variation
  - But there is another important aspect of a given process:
  - Is a process actually *capable* of meeting a desired specification?
*** Process capability
- Parts are often given design tolerances
  - e.g. 15 inches \pm 0.5
- Common measurements are:
  1. Process capability ratio
  2. Process capability index
*** Process capability ratio, $Cp$
$$
Cp = \frac{\text{Upper specification} - \text{Lower specification}}{6\sigma}
$$
- 6\sigma equates to Cp \geq 2.0
- Cp looks only at spread
- Cp does *not* look at how well a process is centered on target.
*** Process capability ratio, $Cpk$
$$
Cpk = \text{Min of} (\frac{\text{upper spec} - \bar{\bar{x}}}{ 3\sigma}, \frac{\bar{\bar{x}}-\text{lower spec}} {3\sigma})
$$
- Gives the proportion of variation between the center of process and the nearest specification limit
*** Interpretation of Cpk values
Bigger Cpk is better.
- Cpk = 1 :: process meets specification
- Cpk < 1 :: process *does not* meet specification
  - can't meet requirement
  - if = 0, the process mean = upper spec, and cannot meet spec as half will fall outside
- Cpk > 1 :: process is better than what the spec requires
  - tighter normal distribution
** M13L08: SPC recap
- Cpk is more often used
- Assignable vs common causes of variation
- Spc monitors for presence of assignable variation
  - Still requires investigation
  - Some variation might be good (i.e. variance unexpectedly lower)
- Other uses:
  - P and C charts for attributes in services (good/bad, pass/fall)
* Module 14: Forecasting (Demand)
** M14L01: Forecasting
*** What's forecasting
Prediction of future events for planning.

Uses:
- Strategic planning :: long-term capacity
- Finance and accounting :: budgeting, cost control
- Marketing :: Future sales trends, NPI
- Production and operations :: staffing, supplier relations
*** Forecast characteristics
- Almost always wrong
- More accurate at aggregate level (errors offset)
- More accurate at short time horizon
- Should include {{{hl(error estimate)}}}/ranges
- Does not sub for actual demand
*** Patterns of demand
1. Trend
2. Seasonality
3. Cyclic elements
4. Autocorrelation
5. Random variation
*** Multiple patterns
Are possible. e.g. seasonal variation can exist within a wider linear trend
*** Questions
- Why forecast?
- What systems will consume the forecast?
- How important is the past in predicting future?

Answers will determine time horizons, techniques, LOD.
*** Forecasting methods
- Qualitative :: rely on expert opinion
- Quantitative :: rely on data and analytical techniques
*** Quantitative methods
- Time series :: models predict future demand based on past history trends
- Causal relationships :: models use stats to establish relationships between various drivers and demand, e.g.: Linear Regression
- Simulation :: models that can incorporate randomness and non-linear effects
*** Lecture summary
- Forecasts seek to predict future events for planning
- In OM, forecasting demand is key
- Demand exhibits multiple patterns
- Focus: time series
** M14L02: Exponential smoothing
*** ES is one time series technique
- Prediction of future mostly depends on:
  - mostly most recent observation and
  - the error on the latest forecast
*** Smoothing constant, \alpha
- Denotes the importance of past error
*** Benefits of ES
- Uses less data
- Extremely accurate
- Easy to understand
- Low complexity in calculations
*** ES formula
$$
F_t = F_{t-1} + \alpha(A_{t-1}-F_{t-1}) \\
0 \leq \alpha \leq 1
$$
Where:
- $F_{t-1}$ :: Forecast for last period
- $A_{t-1}$ :: Actual demand for last period
- \alpha :: smoothing constant, denotes how much forecast reacts to diff between A and F

For \alpha:
- Low :: little reaction to difference, smooth graph
- High :: big reaction to difference, responsive graph
*** Getting started
- Must have initial forecast
- Can set to equal actual demand, or use other methods
- Initial forecast not calc from ES.
  - Do not use initial forecast to evaluate accuracy.
*** Summary
- ES is a common method used to forecast random behaviours in demand
- Uses prior forecast and error to predict the next period's demand
- Smoothing constant \alpha determines how much error alters the next prediction
** M14L03: Exponential smoothing with trend
*** Trends without adjustments
ES will lag behind trends if not adjusted for.
*** Forecast Including Trend (FIT)
$$
FIT_t = F_t + T_t \\
F_t = FIT_{t-1} + \alpha (A_{t-1} - FIT_{t-1}) \\
T_t = T_{t-1} + \delta (F_t - FIT_{t-1})
$$
where:
- \delta :: trend smoothing constant
*** Results
- Will result in ES prediction being closer to actuals than without adjusting for seasonality
*** More on ES with/without trend
- ES and FIT require initial estimates for F and T.
  - Prior period actuals can be used as baseline
- Values for \alpha or \delta are found by:
  - trial and error
  - optimization
- If initial guesses are incorrect, model takes longer to stabilize
- ES and FIT use only prior period's data to estimate current period demand, hence making calc and storage easier
*** Lecture summary
- To account for trends, FIT are more accurate than just ES
- Initial trend and forecast values may cause model to take some time to stabilize if very 'off'
** M14L04: Exponential smoothing with seasonality
*** Seasonality calculation
- Measures the seasonal variation in demand
- Relates average demand in particular period to the average demand across all periods
- Seasonal index: (period average demand) / (average demand for all periods)
*** Calculation, e.g.
- June sales = 400
- Average monthly sales = 200
- Seasonal index for June = 400/200 = 2.0
*** Calculate ES with Seasonal Forecast
1. Inputs:
   - realized seasonal demand in previous period (~$A_t$)
   - seasonal forecast for previous period (~$F_t$) and
   - smoothing constant, \alpha
2. De-seasonalize demand and forecasts to obtain $A_t$ and $F_t$
   $$
   A_t = \sim\frac{A_t}{SI_t} \\
   F_t = \sim\frac{F_t}{SI_t}
   $$
3. Use ES formula to obtain de-seasonalized forecast $F_{t+1}$
   $$
   F_{t+1} = \alpha A_t + (1-\alpha)F_t
   $$
4. Re-seasonalize the forecast to get ~ $F_{t+1}$
   $$
   \sim F_{t+1} = F_{t+1} \times SI_{t+1}
   $$
*** Lecture summary
- Method can capture more than 1 seasonal pattern
- Method can be combined with FIT to model demand that has random, trend, and seasonal behaviour
** M14L05: Error methods
*** Comparing different models
- Needs a metric that provides estimates of accuracy
- Forecast error = difference between actual and forecast value,
  - AKA 'residual'
- Errors can be:
  - Biased (consistent)
  - Random
*** Measures of forecast accuracy
- Error :: Actual - Forecast
  $$
  e_t = A_t - F_t
  $$
  - e_t can be positive or negative
    - +ve: forecast was too low
    - -ve: forecast too high
- RSFE :: Running sum of forecast error
  $$
  \sum (A_i-F_i) = \sum e_i
  $$
- MFE :: Mean forecast error (bias)
  $$
  MFE = \frac{\sum(A_i - F_i)}{n} = frac{RSFE}{n}
  $$
  - Means {{{hl(average erorr in the observation)}}}
  - More +ve or -ve means worse performance
    - i.e., the forecast on average is biased vs. actual demand
- MAD :: mean absolute deviation
  $$
  MAD = \frac{\sum |A_i - F_i|}{n}
  $$
  - {{{hl(Average absolute error in the observation)}}}
  - Higher = worse
- Tracking signal :: fraction of
  $$
  TS = \frac{RSFE}{MAD}
  $$
*** Dartboard analogy for MFE and MAD
[[./img/1405-dartboard.png]]
- MFE measures overall average accuracy, and lower is better
- MAD measures overall variability of errors terms, and lower is better
*** Tracking accuracy with Tracking Signal
- Measures how often estimates are above or below actuals
- Used to decide {{{hl(when to)}}} re-evaluate model

Interpretations:
- Positive :: Most of the time, actuals are above forecast
- Negative :: actuals below forecast
- <-4, or > 4 :: Investigate!
*** Lecture summary
- Error measurements are used to evaluate different forecast models
- Tracking signal is used to alert to changes in pattern of demand
** M14L06: Forecasting recap
*** Which forecast method to use?
- Gather data
- Divide into train and validation
- Use train data to fit models
- Use validation data to evaluate models
- Compare:
  - MAD, MFE
  - Tracking signal
- Use seasonality and trending if it seems that the data exhibits those
*** Recap
- Forecast assumes past can predict future
- Components of demand: average, trend, seasonality, cyclical elements, random variation, auto-correlation
- Qualitative methods used when hard data is not available
- Exponential smoothing uses the most recent data and forecast error
- FIT extends ES to applications with clear trend
- Seasonality can be further incorporated into forecasting models
- Forecast error: difference between actual and forecast values
- MFE is Mean Forecast Error, bias. Measure of the overall average of forecast to actual demand
- MAD is Mean Absolute Deviation. Measure of the overall variation in error between forecast and actuals.
- Tracking Signal: whether a forecast is above/below actual, and by how much. Investigate |TS| > 4.
