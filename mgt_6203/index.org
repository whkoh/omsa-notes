#+AUTHOR: W
#+SETUPFILE: /Users/whkoh/git-repos/org-html-themes/org/theme-readtheorg-local-parent.setup
#+TITLE: MGT 6203: Data Analytics in Business
* Module 01: Linear Regression
** Steps in Regression Analysis
1. State problem
2. Use regression for:
   - Diagnostic, or
   - Predictive, or
   - Prescriptive analytics?
3. Select relevant response and explanatory variables
4. Collect data
   - Internal
   - External
   - Purchased
   - Experiment
5. Choose fitting method
   - Ordinary least squares (OLS)
   - Generalized least squares
   - Maximum likelihood
6. Fit model
7. Validate model with diagnostics
8. Refine and iterate from step 3
9. Use model
*** Business examples (Dependent vs independent variables)
- Used car price :: odometer reading, age of car, condition
- Sales :: ad spend
- Time taken to repair product :: experience of technician
- Product added to shopping card :: ratings/price
- Starting salary of new employee :: work and education experience
- Sale price of house :: square feet, bedrooms, location
- Customer default? :: Credit score, income, age
- Customer churn? :: Length of contract, age of customer
*** M1L1 lecture quiz
1. Variable eg price *can* be either dependent or independent variable, depends on purpose of model
2. Binary value variable *can* be dependent variable, eg logistic regression
** Real estate pricing example
1. Example: sell house, predict listing price.
2. Can ask realtors, but is there a more analytical approach?
   - They normally use "comparables" to suggest a listing price
3. If actual data with price, size, bedroom and bathroom number, etc, are available, regression can be used to help get a better price estimate
*** Housing dataset ~ecdat~
- Can be used for this problem
- Data from Canadian city Windsor from 1987
- Or, collect or scrape prices from the web
- Do EDA, e.g.:
  - histogram
  - correlation matrix
  - scatter plot with linear regression line
*** M1L2 lecture quiz
1. For right-skewed distribution, mean is *larger* than median
2. Correlation coefficient only captures *linear* relationships
** Notation in regression analysis
- Explanatory :: independent variables
- Response :: dependent variables
- $i = 1, 2, ..., n$ :: $i$ th observation or record in dataset, typically a sample
- $\{x_{11}, ..., x_{p1}\}, \{...\}, \{x_{1n}, ..., x_{pn}\}$ :: $n$ observations of $p$ explanatory variables
- $y_1, ..., y_n$ :: $n$ observations of the dependent variable
- $\bar{y}$ :: mean value of dependent variable, $y$
- $\bar{x}_k$ :: mean value of $x_k$ th explanatory variable
- $\beta_0, ..., \beta_p$ :: parameters of regression line for population
- $b_0, ..., b_p$ :: estimates of $\beta$ parameters obtained by fitting the *sample* data
- $\epsilon_i$ :: error term for $i$ th observation in population
- $e_i$ :: error term for $i$ th observation int he sample
- $\widehat{y}_i$ :: estimated value of $y$ for the $i$ th observation in sample. Obtain by evaluating regression function at $x_i$
*** Steps with example
1. Observe data in ~ecdat~, which is a sample
2. Build model for population (valid relation)
   $$
   Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
   $$
3. $\epsilon_i$ are independently and identically distributed (iid) random variables, normally distributed with mean 0 and s.d. $\sigma$
4. We don't know $\beta_0$, $\beta_1$, $\sigma$ so need to estimate with *sample* data
5. Using sample, we build model:
   $$
   Y_i = b_0 + b_1 X_i + e_i
   $$
6. Population model is *homoskedastic* i.e., has constant $\sigma$, and has mean 0.
7. Split data into say 100 samples and model these. Of 100, 95 will have the *population* slope.
**** Use OLS to fit the line
Interpretation:
1. Find mean $\bar{y}$
2. Regression line: $\widehat{y} = b_0 + b_1 x$
3. Intercept at $x=0$, $y=b_0$
4. For any point $x_i$:
   - Predicted value is $\widehat{y}_i$.
   - Actual value is $y_i$.
   - Need to minimize total deviation as defined by sum of squared error.
   - Total deviation is against $\bar{y}$:
     - Explained deviation is $\widehat{y} - \bar{y}$
     - Total deviation is $y_i - \bar{y}$
     - *Residual (error term) is unexplained deviation*, i.e. $y_i - \widehat{y}_i$
*** M1L3 lecture quiz
1. The total deviation at $(x_i, y_i)$ is $y_i - \bar{y}$: true
2. In OLS, the intercepts of slope and intercept do depend on the sample being used.
** $R^2$ and Adjusted $R^2$
*** Deviations
- Sum of deviations :: SST = SSE + SSR
- SST :: Total sum of squares = $\sum_{i}(y_i-\bar{y})^2$
- SSE :: Sum of squared errors = $\sum_{i}(y_i-\widehat{y}_i)^2$
- SSR :: Sum of squares regression = $\sum_{i}(\bar{y} - \widehat{y}_i)^2$
*** Regression output $R^2$ and Adjusted $R^2$
- $R^2$ :: coefficient of determination. Measures strength of relationship between dependent and independent variables.
  - $R^2$ never decreases as you add variables
  - $R^2$ = 1-(SSE/SST) = SSR/SST = *explained deviation/total deviation*
  - $R^2$ shows how much of variation in $Y$ (vs. mean) has been explained by the model.
- Adjusted $R^2$ :: accounts for the fact that adding variables increases predictiveness
  - Adjusted $R^2$ adds a penalty for the number of independent variables $p$
  - Adjusted $R^2$ = $1-\frac{SSE/(n-p-1)}{SST/(n-1)}$
*** Examples
- When $R^2$ = 1 :: X accounts for all variation in Y. SSE = 0
- When $R^2$ = 0 :: X accounts for none of variation in Y. SSE = 1
- When $R^2$ = 0.75 :: X accounts for most of the variation. SSE = 0.25
*** M1L4 lecture quiz
1. When $R^2$ = 0, X values account for none of the variation in Y.
2. $R^2$ can take values in $(0,1)$
** Simple regression with 1 variable in R
- Using ~lm~ in R to fit simple linear regression
- Residuals are shown first. Residuals are the unexplained deviation/error, i.e. $y_i-\widehat{y}_i$.
- Output of running regression models in R:
  1. Null and alternative hypotheses
  2. Coefficients:
     - ~Estimate~ shows the value
     - ~Std. Error~ shows the standard deviation
     - ~t value~ = coefficient / standard error
     - To reject null hypothesis, ~Pr(>|t|)~ i.e. p-value must be very small
  3. p-value: the probability of finding a t-value of this size if the null hypothesis is *true*.
  4. F-statistic: the probability that $b_1 = 0$.
*** Interpreting coefficients
~(Intercept) Estimate: 3.414e+04~
~lotsize Estimate: 6.599e+00~
- $b_0$ = 34,140 :: Intercept of regression line on y. When lotsize is 0. Less useful
- $b_1$ = 6.599 :: Increase of 1,000 sqft associated with increase of 6,599 of house keeping all else constant, /ceteris paribus/.
|           |  Df |     Sum Sq | *Error term* |
|-----------+-----+------------+--------------|
| lotsize   |   1 | 1.1156e+11 | SSR          |
| Residuals | 544 | 2.7704e+11 | SSE          |
- Regression output $R^2$ and Adjusted $R^2$. *Need to know how to calculate and interpret.*
- For simple regression, $\sqrt{R^2}$ is the correlation coefficient between price and lotsize.
*** F-test: shows whether model is significant
- i.e., reject $H_0$
- Result is the F statistic
- Can range from (0, $\infty$)
*** M1L5 no lecture quiz
** Multiple regression
1. Linear regression with multiple ($p$) explanatory variables
2. Terms:
   - Regression coefficients :: $b_0, ..., b_p$ estimate $\beta_0, ..., \beta_{p}$
   - Prediction for Y at $x_i$ :: $\widehat{y} = b_0 + b_1 x_{1i} + b_2 x_{2i} + ... + b_p x_{pi}$
   - Residual :: $e_i = y_i - \widehat{y}_i$
   - SSE :: $\sum_{i} (y_i - (b_0 + b_1 x_{1i} + b_2 x_{2i} + ... + b_p x_{pi}))^2$
3. Goal is to choose $b$ terms to minimize SSE
4. Add a variable, bedrooms. Question: does number of bedrooms explain the price?
5. Do EDA with plots/histogram
*** Regression output
b terms are estimates of true parameters $\beta$ terms
$H_0$ the parameters are $0$, $H_1$ they are not $0$.
|             |  Estimate |
|-------------+-----------|
| (Intercept) | 5.613e+03 |
| lotsize     | 6.053e+00 |
| Bedrooms    | 1.057e+04 |
all /ceteris paribus/:
- $b_0$ = 5,613 :: intercept on y axis
- $b_1$ = 6.053 :: increase per square feet is $6.053
- $b_2$ = 10,570 :: increase per bedroom is $10,570
*** M1L6 no lecture quiz
** $R^2$ and adjusted $R^2$ in multiple regression
Anova table:
|           |  Df |      Sum Sq | *Error term* |
|-----------+-----+-------------+--------------|
| lotsize   |   1 |  1.1156e+11 | $SSR_1$      |
| bedrooms  |   1 | 3.23239e+10 | $SSR_2$      |
| Residuals | 543 |  2.4472e+11 | SSE          |
- $\text{SSR}_1 + \text{SSR}_2$ = SSR
- SSR + SSE = SST
- Rejecting null hypothesis by analysing the F statistic means the model is significant.
** Simple vs multiple regression
- $R^2$ will not decrease when adding variables
- Higher adjusted $R^2$ in multiple regression
- Comparing 2 models with different number of variables:
  $$
  F = \frac{(R^2_2 - R^2_1)/(p_2-p_1)}{(1-R^2_2)/(n-p_2-1)}
  $$
** M1L7 lecture quiz
1. In general adding more variable *does not decrease* overall $R^2$ value of multiple regression.
2. *Small* p-value means that *there is evidence* that coefficient is *not* 0.
** M1L8 Common problems and fixes in fixing regression (I)
*** Assumptions of linear regression
- Linearity :: i.e. $E(y) = b_0 + b_1 x$, i.e. the expected value of Y at each X is approximately a straight line
- Assumptions about error terms :: (a) The error terms are independent, and identically distributed (*iid*). (b) Each error term has mean 0 and constant variance \sigma^2, i.e. *homoscedastic*
- Assumptions about predictors :: In multiple recession, the predictor variables are linearly independent of each other
*** Common problems in fitting Linear Regression
1. Response/predictor relationship is non-linear
2. Error terms are correlated
3. Error terms have non-constant variance
4. Outliers
5. High leverage points
6. Collinearity
*** (1) Non linear relationship
**** Checks
1. Check scatter plots of Y vs X, before fitting model
2. Check residuals vs fitted values (no pattern = good)
**** Workarounds
1. Model non-linear relationship with higher order terms e.g. x^2
2. Use variance reducing transformations, e.g. log, to get better linear fit
3. Remove outliers or some parts of the observations driving non-linearity
4. Check for omitted variables
5. Check for systematic bias when collecting data
6. *Check residuals!*
*** (2) Correlation of error terms
- There is autocorrelation if error terms are correlated
- Knowing error term e_i should not have influence on size of e_{i+1}
- This assumption is used to estimate standard errors of the model parameters
- If autocorrelation exists:
  - Estimated standard errors < true standard errors
  - Confidence intervals appear narrower than actual
  - P-values appear lower than actual
  - False sense of confidence
**** Checks
With the Durbin-Watson test
*** (3) Heteroskedasticity (non-constant error variance)
- Model assumes: spread of responses around the straight line is the same at all $x$
- Violated if there is non-constant error present, e.g. if errors increase with fitted values
- If heteroskedastic:
  - Hypothesis test and confidence intervals misleading
**** Checks
1. Residuals vs fitted values plot
   [[./img/01-residuals-hskd.png]]
**** Workarounds
1. Transform the Y variable, e.g. $ln(Y)$, $\frac{1}{Y}$, etc.
*** M1L8 lecture quiz
1. Scatter plot of Y vs X shows non-linear pattern -> SHOULD change linear regression model
2. Autocorrelation is the correlation between each of the e_i variables -> True
3. Heteroskedasticity = constant error variance -> FALSE
** M1L9 Common problems and fixes in fixing regression (II)
*** (4) Outliers
1. y_i value far from predicted $\widehat{y}_i$
2. Visualise by plotting residuals, or standardized residuals, vs predicted $y$
3. Points with std residuals > 2-3 sd away from mean (0) are outliers
4. Causes:
   1. Incorrect data recording
   2. Phenomenon is not linear (model is wrong, missing predictor, etc)
5. Should ensure fit is *not* overly determined by one/ a few observations i.e. outliers
6. Outlier identified as *influential point* if it unduly influences the regression analysis
   [[./img/0109-plot-x-y-influential.png]]
**** Types
1. Y, response outlier
2. X, predictor outlier, i.e. leverage point
**** Causes
1. Case by case
2. Analyse by creating models with/without outlier to see effect on fitted line
   1. For both predictor and response outliers
*** (5) High leverage points
[[./img/0108-high-leverage.png]]
1. These are observations with predictor $x$ value outside normal range of observations for $x$
   1. Does not have large std residual
   2. Can affect regression results
2. Point has high leverage if its deletion (by itself or with 2-3 other points) causes noticeable changes in the model
3. With many predictors: it's possible to have an observation within range of each predictor's value but still be unusual
4. Measure with *Cook's distance C_i*, i.e. the difference between coeff's obtained from:
   1. The full data
   2. The full data deleting observation $i$
5. C_i > 1  = highly influential
   1. *Also* an influential point if C_i > 1
*** Plots in R
plot(lm) model provides plots:
- Residual vs fitted :: check if residuals have non-linear patterns
- Normal Q-Q :: check if residuals are normally distributed
- Scale-location ::  check if standardized residuals are spread equally along fitted values
- Residuals vs leverage :: finds influential points, with C_i > 1
*** (6) Multicollinearity
- e.g. mpg ~ cyl; mpg ~ cyl + disp + wt.
- The 2nd module exhibits multicollinearity.
- Two or more predictors are linearly related
**** Check
1. With variance inflation factors, VIF
   1. Regress X_j against all other X. Name resulting R^2 as $R^2_j$
   2. Define VIF = $\frac{1}{1-R^2_j}$, $j=1,2,...,p$
   3. If X_j has strong linear relationship to other X predictors, then R_j^2 is close to 1 and VIF_j is large
   4. VIF > 5 -> Multicollinearity
   5. In R, use ~vif(lm(model))~ to check the VIF values
2. Use correlation matrix
**** Consequences
1. OLS estimated parameters may have large variances and covariances, thus precise estimates difficult
2. Confidence intervals of estimated parameters are larger, hence can't reject H_0 (that b_i = 0)
3. Regression coeffs might have wrong sign
4. Regression coeffs might be not significantly different from 0 even for large R^2
5. Adding explanatory variable changes other varS coeff
**** Solution
1. Pick one variable if multiple ones measure the same "thing"
2. Use PCA or factor analysis to create even more useful variable(s)
* Module 02: Indicator variables and Interaction Terms
** M2L1: Intro (Customer Analytics Dataset)
** M2L2: Creating and Using Indicator Variables
a.k.a. dummy variables
** M2L3: Interpreting the coefficients of indicator variables
** M2L4: Interaction term and interpreting its coefficient
** M2L5: Another example of using indicator variables
*** Airbnb Los Angeles dataset
- e.g. owner aiming to understand key factors that influence price
- Questions:
  1. Is there a relationship between capacity and price?
  2. Does the type of rental (shared, private or full home) change the relationship?
*** 2DR1: How does room price vary by capacity?
|              |  Value |
|--------------+--------|
| Intercept    | 15.039 |
| Capacity     | 38.272 |
| R^2          |  0.367 |
| Adjusted R^2 |  0.367 |
Create two dummy variables as follows. Base case (both 0) is "Shared" ~room type~.
1. ~Private_ind~:
   1. 1 if "Private room" ~room type~
   2. 0 otherwise
2. ~House_ind~
   1. 1 if "Entire home/apt" ~room type~
   2. 0 otherwise
**** 2DR1: Equation
$$
\text{Price} = b_0 + b_1*\text{Private_ind} + b_2*\text{House_ind}
$$
|             | Estimate | p-value |
|-------------+----------+---------|
| Intercept   |   37.149 |   12.58 |
| Private_ind |   35.666 |   11.42 |
| House_ind   |  133.442 |   43.64 |
**** 2DR1: Interpretation:
- Average price of room = 37.149 :: Shared room
- Average price of private room :: 37.149+35.666
- Average price of entire house :: 37.149+133.442
*** 2DR2: 2nd regression with capacity and dummies
|             | Estimate | t-value |
|-------------+----------+---------|
| Intercept   |  -19.017 |  -7.101 |
| Capacity    |   29.292 |  82.605 |
| Private_ind |   30.339 |  11.076 |
| House_ind   |   75.776 |  27.346 |
- Equation:
  $$
  \text{Price} = b_0 + b_1*\text{Capacity} + b_2*\text{Private_ind} + b_3*\text{House_ind}
  $$
- Question: What's the average increase in price for each extra person (capacity)?
- Image: [[./img/0205-2dr2-slope.png]]
*** Interaction terms
1. Create 2 new variables:
   1. ~P_Cap~: ~Private_ind~ * ~Capacity~
   2. ~H_Cap~: ~House_ind~ * ~Capacity~
2. New equation
 $$
  \text{Price} = b_0 + b_1*\text{Capacity} + b_2*\text{Private_ind} + b_3*\text{House_ind} + b_4*\text{P_Cap} + b_5*\text{H_Cap}
  $$
3. Regression coeff's:
   |             | Estimate |   t-value |
   |-------------+----------+-----------|
   | Intercept   |   35.885 |  8.728*** |
   | Capacity    |    0.659 |     0.391 |
   | Private_ind |   20.684 |  4.427*** |
   | House_ind   |    2.293 |     0.518 |
   | P_Cap       |    7.080 |  3.636*** |
   | H_Cap       |   33.414 | 19.323*** |
   |             |          |       <r> |

**** Interpretation
1. b_4 is the amount to add to b_1 to get slope for Private room
2. b_5 is the amount to add to b_1 to get slope for a House
3. Statistically, capacity and house_ind are not very different from 0.
   [[./img/0205-interaction-terms-2dr2.png]]
